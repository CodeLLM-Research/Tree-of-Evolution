[
    {
        "content": "def _extract_n_bits(value: int, num_bits: int) -> int:\n    \"\"\"Extracts n bits from a number\"\"\"\n    limit_bit = 1 << num_bits\n    result = value & (limit_bit - 1)\n    if value >= limit_bit:\n        result |= limit_bit\n    return result",
        "sha1": "7757d2530fd04e55f61c1983c48603ba85a1f338",
        "id": 613993
    },
    {
        "content": "def bwgd_to_epoch(bwgd: int) -> float:\n    \"\"\"Convert bwgd time to Unix epoch time in sec\n\n    First convert bwgd time to gps time (in ms) by adjusting the 25.6 ms\n    bwgd windows into ms, then calibrate gps time with unix epoch time.\n    \"\"\"\n    real_gps_time = bwgd * 256 / 10\n    gps_time = real_gps_time - 18000\n    unix_time_ms = gps_time + 315964800000\n    return unix_time_ms / 1000",
        "sha1": "30c0ada22e20fc903b9b21ea0fb635d3452a1dca",
        "id": 564871
    },
    {
        "content": "import re\n\n\ndef safe_name(name):\n    \"\"\"\n    Converts IAM user names to UNIX user names\n\n    1) Illegal IAM username characters are removed\n    2) +/=/,/@ are converted to plus/equals/comma/at\n    \"\"\"\n    # IAM users only allow [\\w+=,.@-]\n    name = re.sub(r'[^\\w+=,.@-]+', '', name)\n    name = re.sub(r'[+]', 'plus', name)\n    name = re.sub(r'[=]', 'equals', name)\n    name = re.sub(r'[,]', 'comma', name)\n    name = re.sub(r'[@]', 'at', name)\n    return name",
        "sha1": "077cb576f3c8cb6b4da905ba9c6f6410fbf691fe",
        "id": 59456
    },
    {
        "content": "from typing import Optional\n\n\ndef _default_progress_str(progress: int, total: Optional[int], final: bool):\n    \"\"\"\n    Default progress string\n\n    Args:\n        progress: The progress so far as an integer.\n        total: The total progress.\n        final: Whether this is the final call.\n\n    Returns:\n        A formatted string representing the progress so far.\n    \"\"\"\n    prefix = \"completed \" if final else \"\"\n    if total is not None:\n        percent = (100.0 * progress) / total\n        return \"%s%d / %d (%3d%%)\" % (prefix, progress, total, int(percent))\n    else:\n        return \"%s%d / %d\" % (prefix, progress, progress)",
        "sha1": "0aa2daf0e1750fb534142125af253e036bb65148",
        "id": 54096
    },
    {
        "content": "import torch\n\n\ndef get_pmf(prelim_assignment):\n    \"\"\"\n    Calculates the probabilitiy mass function (pmf) which here is simplified the number of weights assigned to a\n    specific centroid devided by th number of all weights. The preliminary assignment considers only the minimal\n    distance from  centroids to weights as cost.\n    With \"spars_bound\" we ensure that at least 50% of all weights would be assigned to the zero-centroid w_0 such that\n    the entropy score (information content) for w_0 is always the lowest.\n\n    Parameters:\n    -----------\n        prelim_assignment:\n            Minimal arguments for all 3 centroid distances to all layer weights\n\n    Returns:\n    --------\n        pmf_prelim:\n            Percentage frequencies of -only distance dependent- centroid assignments (w_n, w_0, w_p)\n            with pmf[w_n] + pmf[w_0] + pmf[w_p] = 1 and  pmf[w_0] always > 0.5\n    \"\"\"\n    C_counts = torch.ones(3)\n    C_val, C_cts = torch.unique(prelim_assignment, return_counts=True)\n\n    # For the usual case that layer weights are assigned to three centroids (ternary)\n    if C_cts.shape[0] == 3:\n        C_counts = C_cts\n\n    # The following two cases, especially the last one, should not occur as precautions were taken such that the\n    # w_0 centroid can't absorb all assignments\n    # If layer weights are assigned to two only centroids (binary)\n    elif C_cts.shape[0] == 2:\n        if 0 not in C_val:\n            C_counts[1] = C_cts[0]\n            C_counts[2] = C_cts[1]\n        if 1 not in C_val:\n            C_counts[0] = C_cts[0]\n            C_counts[2] = C_cts[1]\n        if 2 not in C_val:\n            C_counts[0] = C_cts[0]\n            C_counts[1] = C_cts[1]\n    # If layer weights are assigned to only one centroid\n    elif C_cts.shape[0] == 1:\n        if (0 not in C_val and 1 not in C_val):\n            C_counts[2] = C_cts[0]\n        if (0 not in C_val and 2 not in C_val):\n            C_counts[1] = C_cts[0]\n        if (1 not in C_val and 2 not in C_val):\n            C_counts[0] = C_cts[0]\n\n    pmf_prelim = torch.div(C_counts.type(torch.float32), torch.numel(prelim_assignment))\n\n    # Ensuring that at least 50% of all weights are assigned to w_0 and probabilities still sum up to 1\n    spars_bound = 0.5\n    if pmf_prelim[1] < spars_bound:\n        pmf_prelim[0] -= (pmf_prelim[0]/(pmf_prelim[0] +\n                                        pmf_prelim[2]))*(spars_bound - pmf_prelim[1])\n        pmf_prelim[2] -= (pmf_prelim[2] / (pmf_prelim[0] +\n                                           pmf_prelim[2])) * (spars_bound - pmf_prelim[1])\n        pmf_prelim[1] = spars_bound\n\n    return pmf_prelim",
        "sha1": "e2155349362445f40abcce586df70771ed88a453",
        "id": 116515
    },
    {
        "content": "from typing import Union\nfrom typing import List\nfrom typing import Tuple\n\n\ndef first(data: Union[List, Tuple]):\n    \"\"\"\n    Returns first element from list or tuple.\n    \"\"\"\n    if isinstance(data, (list, tuple)) and data:\n        return data[0]\n    return None",
        "sha1": "736664c0a9142c6507ee7de45c8e4986036ca194",
        "id": 614736
    },
    {
        "content": "def get_FTPdetect_coordinates(FTPdetect_file_content, ff_bin, meteor_no = 1):\n    \"\"\" Returns a list of FF*.bin coordinates of a specific bin file and a meteor on that image as a list of tuples e.g. [(15, 20), (16, 21), (17, 22)] and the rotation angle of the meteor.\n    \"\"\"\n\n    if int(FTPdetect_file_content[0].split('=')[1]) == 0: #Solving issue when no meteors are in the file\n        return [], 0, 0\n\n    skip = 0\n    skip_to_end = False\n    coord_list = []\n    HT_rho = 0\n    HT_phi = 0\n\n    found_bin = False\n    found_meteor = False\n    read_angle = False\n\n    for line in FTPdetect_file_content[12:]:\n\n        if skip_to_end:\n            if (\"-------------------------------------------------------\" in line):\n                skip_to_end = False\n                continue\n            continue\n\n        if skip>0:\n            skip -= 1\n            continue\n\n        if ff_bin in line:\n            found_bin = True\n            skip = 1\n            continue\n\n        if found_bin and not found_meteor:\n            line = line.split()\n            if int(float(line[1])) == meteor_no:\n                found_meteor = True\n            else:\n                skip_to_end = True\n\n        if found_bin and found_meteor:\n            if read_angle == False:\n                HT_phi = float(line[-1])\n                HT_rho = float(line[-2])\n                read_angle = True\n                continue\n\n            if (\"-------------------------------------------------------\" in line):\n                break\n\n            line = line.split()\n            coord_list.append((float(line[0]), int(round(float(line[1]), 0)), int(round(float(line[2]), 0))))\n\n    return (coord_list, HT_rho, HT_phi)",
        "sha1": "3a5adf21d545848ac8807d3bd03e0e2ef11eda77",
        "id": 657419
    },
    {
        "content": "def make_location(location, protocol):\n    \"\"\"\n    Creates location object given a location and a protocol.\n    :param str location: file path\n    :param str protocol: protocol, for now only accepting `uri`\n    :return: the location subconfiguration\n    :rtype: obj\n    :raises ValueError: if a protocol other than `uri` is used.\n\n    \"\"\"\n    # elif protocol == \"localPath\":\n    # return { \"uri\": location, \"locationType\": \"LocalPathLocation\"}\n    if protocol == \"uri\":\n        return {\"uri\": location, \"locationType\": \"UriLocation\"}\n    else:\n        raise TypeError(f\"invalid protocol {protocol}\")",
        "sha1": "1b8350f5c71017fcdfed90a7965d3c4dbf219785",
        "id": 551460
    },
    {
        "content": "def format_bed_key(key: str) -> str:\n    \"\"\"Clean a bedtools parameter key.\"\"\"\n    return '-' + key.replace('_', '')",
        "sha1": "fe0d4ddb7e795724c589a7545464ca85e8968f92",
        "id": 193599
    },
    {
        "content": "import hashlib\n\n\ndef sha256(fp_in, block_size=65536):\n  \"\"\"Generates SHA256 hash for a file\n  :param fp_in: (str) filepath\n  :param block_size: (int) byte size of block\n  :returns: (str) hash\n  \"\"\"\n  sha = hashlib.sha256()\n  with open(fp_in, 'rb') as fp:\n    for block in iter(lambda: fp.read(block_size), b''):\n      sha.update(block)\n  return sha.hexdigest()",
        "sha1": "6fcd85de1bc746aba7623e67f6b11dd9cb047582",
        "id": 655869
    },
    {
        "content": "import csv\n\n\ndef input_performances(name):\n    \"\"\"\n    Generates from a .csv file the list of actions and the performance dictionary.\n\n    :param name: Name of the .csv file which must contain on the first line the names\n        of the actions, on the second line the names of the criteria and on the\n        following lines the performance of each action against each criterion.\n\n    :return A: List of strings corresponding to the names of the actions in the order\n        in which they are given in the csv file.\n\n    :return P: Dictionary in which the keys are the names of the actions and the values\n        are sub-dictionary where the keys are the criteria and the values are the\n        performances.\n    \"\"\"\n    A = []\n    C = []\n    P = {}\n    with open(name, 'r', newline='') as P_csv:\n        reader = csv.reader(P_csv, delimiter=',')\n        line_count = 0\n        for row in reader:\n            if line_count == 0:\n                [A.append(item) for item in row]\n            elif line_count == 1:\n                [C.append(item) for item in row]\n            else:\n                perf_A = {}\n                for j in range(len(row)):\n                    perf_A[C[j]] = float(row[j])\n                P[A[line_count-2]] = perf_A\n            line_count += 1\n    return A, P",
        "sha1": "366afa5736d700f717706db86037e2f400b70442",
        "id": 71018
    },
    {
        "content": "def meta_command(name, bases, attrs):\n    \"\"\"\n    Look for attrs with a truthy attribute __command__ and add them to an\n    attribute __commands__ on the type that maps names to decorated methods.\n    The decorated methods' doc strings also get mapped in __docs__.\n\n    Also adds a method run(command_name, *args, **kwargs) that will\n    execute the method mapped to the name in __commands__.\n    \"\"\"\n    commands = {}\n    docs = {}\n    for attr, value in attrs.items():\n        if getattr(value, '__command__', False):\n            commands[attr] = value\n            # methods have always have a __doc__ attribute, sometimes empty\n            docs[attr] = (getattr(value, '__doc__', None) or\n                          'perform the %s command' % attr).strip()\n    attrs['__commands__'] = commands\n    attrs['__docs__'] = docs\n\n    def run(self, command, *args, **kwargs):\n        return self.__commands__[command](self, *args, **kwargs)\n    attrs.setdefault('run', run)\n    return type(name, bases, attrs)",
        "sha1": "39ca4b4b7ffd9a09ba1e0715976d5a8c4c4b2b58",
        "id": 398560
    },
    {
        "content": "def crop_to_multiple(img, size_multiple):\n  \"\"\" Crops the image so that its dimensions are multiples of size_multiple.\"\"\"\n  new_width = (img.shape[1] // size_multiple) * size_multiple\n  new_height = (img.shape[0] // size_multiple) * size_multiple\n  offset_x = (img.shape[1] - new_width) // 2\n  offset_y = (img.shape[0] - new_height) // 2\n  return img[offset_y:offset_y + new_height, offset_x:offset_x + new_width, :]",
        "sha1": "554d44488c63e844edc797c18e85e36f7a9b9ce1",
        "id": 582236
    },
    {
        "content": "def drop_dark_data(df):\n    \"\"\"Return dataframe in which the index does not contain the string 'dark' \"\"\"\n    s = df.index.str.contains(\"dark\")\n    return df[~s].copy()",
        "sha1": "02f21d7122537fe92041d6ca55313b6a18207d5c",
        "id": 233712
    },
    {
        "content": "import socket\nimport pickle\n\n\ndef send_request(port, message):\n    \"\"\"Send a request to an IPCServer.\n\n    Args:\n        port (int): port to connect to\n        message (any type)\n\n    Returns:\n        response (any type)\n    \"\"\"\n    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    s.connect((\"localhost\", port))\n    pickle.dump(message, s.makefile(\"wb\"))\n    return pickle.load(s.makefile(\"rb\"))",
        "sha1": "87085ee36674fbbeec1ff8f50a577861831a774f",
        "id": 206142
    },
    {
        "content": "def product(numbers):\n    \"\"\"Calculate product of multiplying numbers.\"\"\"\n    result = 1\n    for number in numbers:\n        result *= number\n    return result",
        "sha1": "d9d0e189fc95265b7f8241c1449eb2c3fa6a6cf1",
        "id": 435142
    },
    {
        "content": "def rotated_array_search(input_list, number):\n    \"\"\"\n    Find the index by searching in a rotated sorted array\n    \"\"\"\n    high = len(input_list) - 1\n    low = 0\n    while low <= high:\n        mid = (low + high) // 2\n        if input_list[mid] == number:\n            return mid\n        elif input_list[mid] < number <= input_list[high]:\n            low = mid + 1\n        else:\n            if input_list[low] <= number:\n                high = mid - 1\n            else:\n                low = mid + 1\n    return -1",
        "sha1": "c3deb50e608c58e5e11665d949a602cc661305ae",
        "id": 703152
    },
    {
        "content": "def path_distance(path_1, path_2):\n    \"\"\"Compute the prefix distance between the two paths.\n\n    >>> p1 = [1, 0, 3, 4, 5, 6]\n    >>> p2 = [1, 0, 2, 2, 2, 2, 2, 2]\n    >>> print path_distance(p1, p2)\n    6\n    \"\"\"\n    d = max(len(path_1), len(path_2))\n    for a, b in zip(path_1, path_2):\n        if a != b:\n            break\n        d -= 1\n    return d",
        "sha1": "0f5f4cf38ecb81a03ce6b4c1a6f834cced9afad1",
        "id": 411914
    },
    {
        "content": "def wrap_env(env, wrappers):\n    \"\"\"\n    Wrap `env` in all gym wrappers specified by `wrappers`\n    \"\"\"\n    for wrapper, args in wrappers:\n        env = wrapper(env, **args)\n    return env",
        "sha1": "27962d235ad67a82e0f920323bd8e34ef3b9821e",
        "id": 446061
    },
    {
        "content": "import logging\n\n\ndef get_labels(events, labels=None):\n    \"\"\"\n    Determine the labels applied to an issue.\n\n    Args:\n        events: a list of (event_type str, event_body dict, timestamp).\n    Returns:\n        labels: the currently applied labels as {label_name: label_color}\n    \"\"\"\n    labels = labels or {}\n    for event, body, _timestamp in events:\n        if 'issue' in body:\n            # issues come with labels, so we can update here\n            labels = {l['name']: l['color'] for l in body['issue']['labels']}\n        # pull_requests don't include their full labels :(\n        action = body.get('action')\n        if event == 'pull_request':\n            # Pull request label events don't come with a full label set.\n            # Track them explicitly here.\n            try:\n                if action in ('labeled', 'unlabeled') and 'label' not in body:\n                    logging.warning('label event with no labels (multiple changes?)')\n                elif action == 'labeled':\n                    label = body['label']\n                    if label['name'] not in labels:\n                        labels[label['name']] = label['color']\n                elif action == 'unlabeled':\n                    labels.pop(body['label']['name'], None)\n            except:\n                logging.exception('??? %r', body)\n                raise\n    return labels",
        "sha1": "b715e606175d178c1f86e3d1d1cc0ca8b3412245",
        "id": 408083
    },
    {
        "content": "def _get_subdict(master_dict, subkeys):\n    \"\"\"Helper method to get a set of keys from a larger dictionary\"\"\"\n    return {k: master_dict[k] for k in subkeys if k in master_dict and master_dict[k] is not None}",
        "sha1": "b0a4a1cfffae235307d1e6ae8ef979c2458d99f2",
        "id": 169158
    },
    {
        "content": "def _has_dimensions(quant, dim):\n    \"\"\"Checks the argument has the right dimensionality.\n\n    Parameters\n    ----------\n    quant : :py:class:`unyt.array.unyt_quantity`\n        Quantity whose dimensionality we want to check.\n    dim : :py:class:`sympy.core.symbol.Symbol`\n        SI base unit (or combination of units), eg. length/time\n\n    Returns\n    -------\n    bool\n        True if check successful.\n\n    Examples\n    --------\n    >>> import unyt as u\n    >>> from unyt.dimensions import length, time\n    >>> _has_dimensions(3 * u.m/u.s, length/time)\n    True\n    >>> _has_dimensions(3, length)\n    False\n    \"\"\"\n    try:\n        arg_dim = quant.units.dimensions\n    except AttributeError:\n        arg_dim = None\n    return arg_dim == dim",
        "sha1": "805d06cf993b46fc6770ee7dda1d694a88cbf291",
        "id": 549419
    },
    {
        "content": "def index_where(collection, predicate):\n    \"\"\":yaql:indexWhere\n\n    Returns the index in the collection of the first item which value\n    satisfies the predicate. -1 is a return value if there is no such item\n\n    :signature: collection.indexWhere(predicate)\n    :receiverArg collection: input collection\n    :argType collection: iterable\n    :arg predicate: function of one argument to apply on every value\n    :argType predicate: lambda\n    :returnType: integer\n\n    .. code::\n\n        yaql> [1, 2, 3, 2].indexWhere($ > 2)\n        2\n        yaql> [1, 2, 3, 2].indexWhere($ > 3)\n        -1\n    \"\"\"\n    for i, t in enumerate(collection):\n        if predicate(t):\n            return i\n    return -1",
        "sha1": "90f1ca55ecc61a9b5ac9338d71d36c927122e99e",
        "id": 397331
    },
    {
        "content": "def storeUpdateResult(cache):\n    \"\"\" Store results\n    Update T0\n    Store temperaure results into a dataframe and \n    save it in the cache.\n    \"\"\"\n    \n    timeStep = cache['ts']\n    TProfile = cache['TProfile']\n    T = cache['T']\n    cache['T0'] = T.copy()\n    TProfile[:,timeStep] = T.reshape(1,-1)\n    return cache",
        "sha1": "7272855fd553b5571a8451f779ddc0c60c8d8d67",
        "id": 639129
    },
    {
        "content": "def format_column_names(df):\n    \"\"\"\n    standardize  pandas df columns names.\n    \"\"\"\n    def reformat(txt):\n        txt = txt.lower()\n        txt = txt.replace(' ', '_')\n        txt = txt.replace('-', '_')\n        txt = txt.replace('.', '_')\n        return txt\n    columns = {col: reformat(col) for col in df.columns}\n    df.rename(columns, axis=1, inplace=True)",
        "sha1": "463398ff48699318ee4bf05e6fb823e05feb9ae0",
        "id": 71362
    },
    {
        "content": "def rev(iterable):\n    \"\"\"Reverse an iterable.\n\n    If a sequence, the return value is ``reversed(iterable)``.\n\n    Otherwise the return value is ``reversed(tuple(iterable))``.\n\n    Hence generators will be fully evaluated until they stop; the input\n    ``iterable`` must be finite for ``rev`` to make any sense.\n    \"\"\"\n    # Unlike further below, here we \"return\" instead of \"yield from\",\n    # because \"rev\" is such a thin layer of abstraction that it has become\n    # effectively transparent (PG, \"On Lisp\"). The call site expects\n    # reversed output, and the \"reversed\" generator is the standard\n    # pythonic representation for that.\n    try:  # maybe a sequence?\n        return reversed(iterable)\n    except TypeError:\n        return reversed(tuple(iterable))",
        "sha1": "1b0ad512c7b06066545d3b0d17435c0684520836",
        "id": 369627
    },
    {
        "content": "def format_string(string, *args):\n    \"\"\"\n    Format & return a string\n    \"\"\"\n    return string if not args else string.format(*args)",
        "sha1": "b0927a7781da47fa3b0563a44f42ebb44b048538",
        "id": 96589
    },
    {
        "content": "def build_wordembed_dict(embeddings, vocab):\n    \"\"\"Builds a wordembedding dictionary (word->np vector) from tensor\"\"\"\n    ret = {}\n    for i, embedding in enumerate(embeddings):\n        word = vocab.i2f[i]\n        ret[word] = embedding.numpy()\n\n    return ret",
        "sha1": "441e18f9e33ab310790522b394901b74601e922d",
        "id": 174984
    },
    {
        "content": "def worst(category):\n    \"\"\"\n    Category status is the worst lowest status of all modules of the same\n    type as the category (only plots or rated modules, depending) with\n    positive status value (no error, data acquisition succeeded).\n    'unrated' modules are always per definition excluded.\n    \n    If there is no correct module with positive status, the\n    category status is set to -1 (no information).\n    \"\"\"\n    status = 1.0\n    for mod in category.module_list:\n        if mod.dataset is None:\n            continue\n        if status > mod.dataset['status'] >= 0 and mod.type == category.type:\n            status = mod.dataset['status']\n    return status",
        "sha1": "876a758707be691305446fc0a106ca4c279ef13b",
        "id": 214552
    },
    {
        "content": "from typing import Counter\n\n\ndef count_elements(data_lst):\n    \"\"\"Count how often each element occurs in a list.\n\n    Parameters\n    ----------\n    data_lst : list\n        List of items to count.\n\n    Returns\n    -------\n    counts : collections.Counter\n        Counts for how often each item occurs in the input list.\n    \"\"\"\n\n    counts = Counter(data_lst)\n\n    try:\n        counts.pop(None)\n    except KeyError:\n        pass\n\n    return counts",
        "sha1": "a12f0a35a228e8a8627a8fcfc703d3231984e3f4",
        "id": 23467
    },
    {
        "content": "def contains_switch(turn_info):\n    \"\"\"\n    Determine if switching info contains Switch information.\n\n    Args:\n        turn_info (list): List of event that happened that turn.\n\n    Returns:\n        Boolean whether or not a switch happened that turn.\n\n    \"\"\"\n    for info in turn_info:\n        if info[\"type\"] == \"SWITCH\":\n            return True\n\n    return False",
        "sha1": "0dabe65a4211a3101df7a9d523c58aa9978e9b27",
        "id": 442733
    },
    {
        "content": "def get_median_watch_time(event):\n    \"\"\"Computes the median watch time based on LA1-provided mapping from\n    watch times to number of unique viewers.\n    NOTE: This data is only available at 5 minute granularities.\"\"\"\n    times = []\n    for m, v in event['geodata']['watchTimes'].items():\n        times += [int(m)] * v\n    return times[len(times) // 2]",
        "sha1": "3ebd69548ce4803518bffca0a2851a9235c4b371",
        "id": 269054
    },
    {
        "content": "def unpack_meta(*inputs, **kwinputs):\n    \"\"\"Update ``kwinputs`` with keys and values from its ``meta`` dictionary.\"\"\"\n    if 'meta' in kwinputs:\n        new_kwinputs = kwinputs['meta'].copy()\n        new_kwinputs.update(kwinputs)\n        kwinputs = new_kwinputs\n\n    return inputs, kwinputs",
        "sha1": "4a6cc28b668459f54890da0778a9ed5f8a232ff6",
        "id": 212334
    },
    {
        "content": "import string\nimport random\n\n\ndef random_password(size):\n    \"\"\"Generate a random password of length 'size'\n    \n    The resulting password may contain any of:\n        upper or lowercase letters A-Z\n        the digits 0-9\n        valid punctuation marks defined in the 'punctuation' variable below\n    \"\"\"\n    punctuation = '#$%&!'\n    chars = string.ascii_letters + string.digits + punctuation\n    return ''.join(random.choice(chars) for _ in range(size))",
        "sha1": "39c9c3379686329f6d31779953c432fb36364622",
        "id": 388733
    },
    {
        "content": "def load_lines(cluster_file):\n\t\"\"\"load a tab delimited file into memory as a list of lines\"\"\"\n\tlist_of_lines = []\n\twith open(cluster_file) as file:\n\t\tfor line in file:\n\t\t\tstrip_line = line.rstrip()\n\t\t\tsplit_line = strip_line.split('\\t')\n\t\t\tlist_of_lines.append(split_line)\n\treturn list_of_lines",
        "sha1": "1ad21b76d91a2fd06a3fab1df8cc02e780385f2e",
        "id": 342547
    },
    {
        "content": "def sql_spans_to_string(sql_spans, sep=' '):\n  \"\"\"Converts list of SqlSpan tuples to string.\"\"\"\n  strings = []\n  for span in sql_spans:\n    if span.sql_token:\n      strings.append(span.sql_token)\n    elif span.column:\n      strings.append('%s.%s' %\n                     (span.column.table_name, span.column.column_name))\n    elif span.table_name:\n      strings.append(span.table_name)\n    elif span.value_literal:\n      strings.append(span.value_literal)\n    elif span.nested_statement:\n      strings.append(sql_spans_to_string(span.nested_statement))\n  return sep.join(strings)",
        "sha1": "26bfafeb055eaa0ee3e19c8b7f158e99dfd7c4ce",
        "id": 541460
    },
    {
        "content": "def get_operation(event):\n    \"\"\"\n    Get the operation from the event data.\n\n    If operation exists in the event data, it may be either a string or list.\n    If it is a list, only the first element will be returned.\n\n    Args:\n        event (dict): Event data as passed to the lambda.\n\n    Returns:\n        (str): 'unknown' if key not found or if given an empty list.\n    \"\"\"\n\n    if 'operation' not in event:\n        return 'unknown'\n\n    op = event['operation']\n    if isinstance(op, str):\n        return op\n    elif isinstance(op, list) and len(op) > 0:\n        return op[0]\n\n    return 'unknown'",
        "sha1": "780959f86b9dc4d58f802bb6b3b241d60fa62217",
        "id": 145033
    },
    {
        "content": "from typing import List\n\n\ndef ls_elements_replace_strings(ls_elements: List[str], s_old: str, s_new: str) -> List[str]:\n    \"\"\"\n    f\u00fchre ein <str>.replace(s_old, s_new) in allen Elementen der Liste durch, welche vom Typ str sind\n\n    >>> ls_elements_replace_strings(['a', 'b', 'c', 1], 'a', 'z')\n    ['z', 'b', 'c', 1]\n    >>> ls_elements_replace_strings([], 'a', 'z')\n    []\n\n    \"\"\"\n\n    if not ls_elements:\n        return ls_elements\n\n    ls_results = []\n    for s_element in ls_elements:\n        if type(s_element) == str:\n            s_element = s_element.replace(s_old, s_new)\n        ls_results.append(s_element)\n    return ls_results",
        "sha1": "ef612eced6f3717d8002612d461ab790ec65a339",
        "id": 557118
    },
    {
        "content": "def factorials(number: int, iteratively=True) -> int:\n    \"\"\"\n    Calculates factorials iteratively as well as recursively. Default iteratively. Takes linear time.\n        Args:\n            - ``number`` (int): Number for which you want to get a factorial.\n            - ``iteratively`` (bool): Set this to False you want to perform a recursion factorial calculation. By default calculates iteratively\n    \"\"\"\n    if iteratively:\n        if not (isinstance(number, int) and number >= 0):\n            # Raise non negative number error\n            raise ValueError(\"'number' must be a non-negative integer.\")\n\n        result = 1\n        if number == 0:\n            return 1\n        for i in range(2, number+1):\n            result *= i\n\n        return result\n    \n    else:\n        # If user want's to perform a recursive factorial calculation\n        if not (isinstance(number, int) and number >= 0):\n            # Raise non negative number error\n            raise ValueError(\"'number' must be a non-negative integer.\")\n\n        if number == 0:\n            return 1\n        result = number * factorials(number - 1)\n\n        return result",
        "sha1": "d4af8b05d91b9ac9fba9f852564e8043cf231e40",
        "id": 668845
    },
    {
        "content": "def replace_lines(regexer, handler, lines):\n    \"\"\"Uses replacement handler to perform replacements on lines of text\n\n    First we strip off all whitespace\n    We run the replacement on a clean 'content' string\n    Finally we replace the original content with the replaced version\n    This ensures that we retain the correct whitespace from the original line\n    \"\"\"\n    result = []\n    for line in lines:\n        content = line.strip()\n        replaced = regexer.sub(handler, content)\n        result.append(line.replace(content, replaced, 1))\n    return result",
        "sha1": "cb5c1ceb135da6677b3cca43e173def34c7cf412",
        "id": 583993
    },
    {
        "content": "def _detect_artifact(artifact_id):\n    \"\"\"Detects type of artifact based on artifact URL.\n\n    Parameters\n    ----------\n    artifact_id : str\n        String in form of resolvable URL\n\n    Returns\n    -------\n    artifact_type : str\n        String representing artifact type, it can be ['field',element', 'template' or 'instance template']\n    artifact_url_str : str\n        Unique string from the artifact id representing artifact_type\n    \"\"\"\n\n    artifact_type = {\n        \"field\": \"/template-fields/\",\n        \"element\": \"/template-elements/\",\n        \"template\": \"/templates/\",\n        \"instance\": \"/template-instances/\",\n    }\n    if artifact_id.find(artifact_type[\"field\"]) != -1:\n        return \"field\", artifact_type[\"field\"].replace(\"/\", \"\")\n    if artifact_id.find(artifact_type[\"element\"]) != -1:\n        return \"element\", artifact_type[\"element\"].replace(\"/\", \"\")\n    if artifact_id.find(artifact_type[\"template\"]) != -1:\n        return \"template\", artifact_type[\"template\"].replace(\"/\", \"\")\n    if artifact_id.find(artifact_type[\"instance\"]) != -1:\n        return \"instance\", artifact_type[\"instance\"].replace(\"/\", \"\")\n    else:\n        raise ValueError(\n            \"artifact_id does not contain information about CEDAR artifact.\"\n        )",
        "sha1": "ae5f432b496da5c9fdeb40529a6aff34cc864aa0",
        "id": 478731
    },
    {
        "content": "def ueGas(ub, umf, emf, delta, fw):\n    \"\"\"\n    This function calculates the velocity of the gas in the emulsion phase with\n    K/L eqs. 6.39-6.40.\n\n    Parameters\n    ----------\n    ub : float\n        Bubble rise velocity [m/s]\n    umf : float\n        Minimum fluidization velocity [m/s]\n    emf : float\n        Void fraction at minimum fluidization [-]\n    delta : float\n        Fraction of bed volume in bubbles [-]\n    fw : float\n        Ratio of wake volume to bubble volume [-]\n\n    Returns\n    -------\n    ue : float\n        Emulsion gas velocity [m/s]\n    \"\"\"\n\n    ue = umf / emf - fw * delta * ub / (1 - delta - fw * delta)\n    return ue",
        "sha1": "c3756c353a7f317cf249db74f8e8f32503b00ea6",
        "id": 26113
    },
    {
        "content": "def _is_dim(obj):\n    \"\"\"Return True if the object is a shape dimension.\"\"\"\n    return (isinstance(obj, tuple) and len(obj) == 2\n            and isinstance(obj[0], int) and isinstance(obj[1], int))",
        "sha1": "eb12f4e034c70bcdc85d093026238e0aecb2063a",
        "id": 285981
    },
    {
        "content": "def Garen2005(th, cloud_factor):\n    \"\"\"\n    Cloud correction is based on the relationship in Garen and Marks (2005)\n    :cite:`Garen&Marks:2005` between the cloud factor and measured long\n    wave radiation using measurement stations in the Boise River Basin.\n\n    .. math::\n        L_{cloud} = L_{clear} * (1.485 - 0.488 * cloud\\\\_factor)\n\n    Args:\n        th: clear sky thermal radiation [W/m2]\n        cloud_factor: fraction of sky that are not clouds, 1 equals no clouds,\n            0 equals all clouds\n\n    Returns:\n        cloud corrected clear sky thermal radiation\n\n    20170515 Scott Havens\n    \"\"\"\n\n    return th * (1.485 - 0.488 * cloud_factor)",
        "sha1": "4e2b1990a8b94cfa6364ee562fa3a2ad44e12865",
        "id": 577834
    },
    {
        "content": "def get_gaze_thres(gaze_df, frame_count):\n    \"\"\"Get the lowest threshold of confidence user can set so that there is at\n    least one gaze point in each frame\n    \n    :param gaze_df: dataframe containing gaze info\n    :param frame_count: number of video's frames\n\n    :return threshold: lowest threshold\n    :rtype: float\n    \"\"\"\n\n    threshold_list = gaze_df['confidence'].drop_duplicates().sort_values()\n    for threshold in threshold_list:\n        good_gaze_df = gaze_df.loc[gaze_df['confidence'] > threshold] \n        if good_gaze_df['world_index'].nunique() < frame_count:\n            return threshold",
        "sha1": "1d772e3c4c0218ebaab55f7129832f7d8aa6648f",
        "id": 351512
    },
    {
        "content": "def calc_drawdown(df, price_col, window_size):\n    \"\"\"Calculates drawdown\n\n    Reference: https://www.investopedia.com/terms/d/drawdown.asp\n\n    Args:\n        df (data.frame): Price history dataframe\n        price_col (str): Column name for price data\n        window_size (int): How many units to look back. Units change based on aggregate provided\n\n    Returns:\n        data.frame: Original dataframe with drawdown appended\n    \"\"\"\n\n    # Calculate rolling prior maximum to compare against current price\n    prior_max = df[price_col].rolling(window=window_size, min_periods=1).max()\n\n    # Calculate percentage change, aka drawdown\n    df[\"market_drawdown\"] = (df[price_col] / prior_max - 1.0) * 100\n    df[\"market_drawdown\"].replace(-100, 0, inplace=True)\n\n    return df",
        "sha1": "3bd8d197b367083e9bce8cff3859eca6d76d76ac",
        "id": 449037
    },
    {
        "content": "from typing import Dict\nimport yaml\n\n\ndef read_yaml(path: str) -> Dict:\n    \"\"\" Read YAML file. \"\"\"\n    with open(path, \"r\") as file:\n        return yaml.load(file, Loader=yaml.FullLoader)",
        "sha1": "12e4cc483b0003a6be0b70ce1b500d3d9b97b730",
        "id": 184907
    },
    {
        "content": "import random\nimport string\n\n\ndef random_string(length=1, unicode=False):\n    \"\"\"\n    Returns random ascii or unicode string.\n    \"\"\"\n    if unicode:\n        def random_fun():\n            return chr(random.choice((0x300, 0x2000)) + random.randint(0, 0xff))\n    else:\n        def random_fun():\n            return random.choice(string.ascii_lowercase + string.ascii_uppercase + string.digits)\n\n    return ''.join(random_fun() for _ in range(length))",
        "sha1": "45760b3e3c42e484d51abf76f9eb2ae0c8132fd1",
        "id": 13928
    },
    {
        "content": "def get_ORM_instance(ORM_class, session, instance):\n    \"\"\"\n    Given an ORM class and *either an instance of this class, or the name attribute of an instance\n    of this class*, return the instance\n    \"\"\"\n    if isinstance(instance, str):\n        return session.query(ORM_class).filter(ORM_class.name == instance).one()\n    else:\n        return instance",
        "sha1": "5b3427147b6934704b300ec5026254e6bce1e2a6",
        "id": 527236
    },
    {
        "content": "from datetime import datetime\n\n\ndef time_left(expire_date):\n    \"\"\"Return remaining days before feature expiration or 0 if expired.\"\"\"\n    today_dt = datetime.today()\n    expire_dt = datetime.strptime(expire_date, \"%d-%b-%Y\")\n    \n    # Calculate remaining days before expiration\n    days_left_td = expire_dt - today_dt\n    days_left = days_left_td.days\n    if days_left <= 0:\n        days_left = 0\n        \n    return days_left",
        "sha1": "652acd27b0d4fa9b21321df4ff8ce6ce15b97ed6",
        "id": 5460
    },
    {
        "content": "def schema_exists(cursor, schema_name):\n    \"\"\"\n    Returns whether or not a schema exists in the DB given the schema name.\n    \"\"\"\n    cursor.execute(\"\"\"\n        SELECT EXISTS (SELECT 1 FROM information_schema.schemata WHERE schema_name = %s)\n    \"\"\", (schema_name,))\n    return cursor.fetchone()[0]",
        "sha1": "c6a3d4462f7042ab486952f37609e5ccc3777724",
        "id": 548392
    },
    {
        "content": "def get_padding(size, kernel_size, strides):\n  \"\"\" Calculate the padding array for same padding in the Tensorflow fashion.\\\\\n  See https://www.tensorflow.org/api_guides/python/nn#Convolution for more.\n  \"\"\"\n  if size[0] % strides[0] == 0:\n    pad_h = max(kernel_size[0] - strides[0], 0)\n  else:\n    pad_h = max(kernel_size[0] - (size[0] % strides[0]), 0)\n  if size[1] % strides[1] == 0:\n    pad_w = max(kernel_size[1] - strides[1], 0)\n  else:\n    pad_w = max(kernel_size[1] - (size[1] % strides[1]), 0)\n  return [pad_h//2, pad_w//2, pad_h-pad_h//2, pad_w-pad_w//2]",
        "sha1": "6ba716444bbadfcdea7205911a3c97418ae8d4c2",
        "id": 601624
    },
    {
        "content": "import _datetime\n\n\ndef combine(date_: _datetime.date, time_: _datetime.time, tz: str='UTC') -> _datetime.datetime:\n    \"\"\"Similar to datetime.datetime.combine, but tz-aware.  The optional\n    tz argument won't override a tz included in the time component.\"\"\"\n    return _datetime.datetime.combine(date_, time_), tz",
        "sha1": "cb0cc9c31bc6787928872341538314d4b1b094c2",
        "id": 181793
    },
    {
        "content": "import string\nimport random\n\n\ndef fake_text_id(size=6, chars=string.ascii_lowercase + string.digits):\n    \"\"\"Create a random text id.\n    \"\"\"\n    return ''.join(random.choice(chars) for x in range(size))",
        "sha1": "e8273e652f83a8d73d4462b8a0af00ec6681aa11",
        "id": 325877
    },
    {
        "content": "import unicodedata\n\n\ndef normalize_unicode(text):\n    \"\"\"\n    Normalize unicode underlying representation\n    \"\"\"\n    text = unicodedata.normalize(\"NFC\", text)\n\n    return text",
        "sha1": "8f45b2d85a4f8840d562bcd1122863725d1dd4ef",
        "id": 233637
    },
    {
        "content": "def SumSquare(inputList):\n  \"\"\"\n  Return the sum of the squares of entries in the supplied list\n  \"\"\" \n  \n  sum = 0.0\n  for val in inputList:\n    sum += val ** 2\n    \n  return sum",
        "sha1": "07fb79e9f73f3de4637e9a36adf9cf659fcaa541",
        "id": 499370
    },
    {
        "content": "def uint_to_little_endian_bytearray(number, size):\n    \"\"\"Converts an unsigned interger to a little endian bytearray.\n\n    :param in number: The number to convert.\n    :param in size: The length of the target bytearray.\n    :rtype: [int]\n\n    >>> uint_to_little_endian_bytearray(0x42, 1)\n    [66]\n    >>> uint_to_little_endian_bytearray(0x42, 2)\n    [66, 0]\n    >>> uint_to_little_endian_bytearray(0xFF42, 2)\n    [66, 255]\n    >>> uint_to_little_endian_bytearray(0xFF42, 4)\n    [66, 255, 0, 0]\n    >>> uint_to_little_endian_bytearray(0xFFFFFF, 2)\n    Traceback (most recent call last):\n        ...\n    ValueError: integer overflow\n    \"\"\"\n    if number > (2 ** (8 * size) - 1):\n        raise ValueError(\"integer overflow\")\n    nle = [0] * size\n    for i in range(size):\n        nle[i] = number >> i * 8 & 0xFF\n    return nle",
        "sha1": "f7c122c72a2b9ca23fa98d72282d2477efd342d3",
        "id": 549253
    },
    {
        "content": "def enabled_bool(enabled_statuses):\n    \"\"\"Switch from enabled_status to bool\"\"\"\n    return [True if s == 'ENABLED_STATUS_ENABLED' else False for s in enabled_statuses]",
        "sha1": "6aab4e176b01a45779a21c4633beb0a80da0522a",
        "id": 394256
    },
    {
        "content": "def parse_callback(callback, reduce=True):\n    \"\"\"\n        Parses callbacks for CellRenderers: it splits the \n        callback from its arguments if present. Additionally this method will \n        not create singleton argument lists, but pass them as a single argument.\n        \n        Returns the callback and its argument(s) (or an empty tuple)\n    \"\"\"\n    args = tuple()\n    try:\n        callback, args = callback\n    except TypeError as ValueError:\n        pass\n    #deconvolve things:\n    if reduce and len(args) == 1: args = args[0]\n    return callback, args",
        "sha1": "b925a0e82ee8046e48594fb6f8a99a5aaa017ded",
        "id": 649029
    },
    {
        "content": "def read_lines(filename=\"\", nb_lines=0):\n    \"\"\"read n lines of file\"\"\"\n\n    i = 0\n    with open(filename, \"r\") as my_file:\n        for line in my_file:\n            i += 1\n            print(line, end=\"\")\n            if i == nb_lines:\n                break\n    my_file.close()\n    return (i)",
        "sha1": "88daa680ff27f4053788ab9073c6be30942ade1f",
        "id": 333943
    },
    {
        "content": "import itertools\n\n\ndef flatten(sequence_list, cls=list):\n    \"\"\"\n    Flatten one level of nesting\n    :param sequence_list: list of sequence\n    :param cls: create instance of cls by flatten_gen\n    :return: cls instance or generator\n    \"\"\"\n    flatten_gen = itertools.chain.from_iterable(sequence_list)\n    return cls(flatten_gen) if cls else flatten_gen",
        "sha1": "2e28b9c44b26f6749dfa8f02337aacaf2e74adc5",
        "id": 689578
    },
    {
        "content": "def map_chars(inline, cmap):\n    \"\"\"\n    Replaces all characters of the inline string with the values form the\n    cmap dict. If the char is not in the map it will remain unchanged.\n\n    Args:\n        inline (str): Input string.\n        cmap(dict): A dictionary to use for translation.\n    \"\"\"\n    return \"\".join([cmap.get(c, c) for c in inline])",
        "sha1": "03e0a25c2aa218561d7e44b1da494842a584414b",
        "id": 167932
    },
    {
        "content": "from typing import OrderedDict\n\n\ndef load_mhd_header(filename):\n    \"\"\"Return an OrderedDict containing metadata loaded from an mhd file.\"\"\"\n\n    metadata = OrderedDict()\n\n    with open(filename) as header_file:\n        for line in header_file:\n            (key, val) = [x.strip() for x in line.split(\"=\")]\n            if key in ['ElementSpacing', 'Offset', 'CenterOfRotation',\n                       'TransformMatrix', 'ElementSize']:\n                new_val = [float(s) for s in val.split()]\n            elif key in ['NDims', 'ElementNumberOfChannels']:\n                new_val = int(val)\n            elif key in ['DimSize']:\n                new_val = [int(s) for s in val.split()]\n            elif key in ['BinaryData', 'BinaryDataByteOrderMSB',\n                         'CompressedData']:\n                # pylint: disable=simplifiable-if-statement\n                if val.lower() == \"true\":\n                    new_val = True\n                else:\n                    new_val = False\n            else:\n                new_val = val\n\n            metadata[key] = new_val\n\n    return metadata",
        "sha1": "f69031727735f0279c895aea2d70701efbfe0a38",
        "id": 608613
    },
    {
        "content": "def winsorize_at_explicit_input(x, lower_bound, upper_bound):\n    \"\"\"\n    Winsorizes the array x at the lower_bound and upper_bound.\n\n    :param x: a numpy array-like object\n    :param lower_bound: a scalar for the lower bound\n    :param upper_bound: a scalar for the upper bound\n    :return: the winsorized array\n    \"\"\"\n    ret = x\n    ret[x < lower_bound] = lower_bound\n    ret[x > upper_bound] = upper_bound\n    return ret",
        "sha1": "4aabf8a3fcc4f5b1ac301f69d622edc88d5923ed",
        "id": 677026
    },
    {
        "content": "def _to_ndimage_mode(mode):\n    \"\"\"Convert from `numpy.pad` mode name to the corresponding ndimage mode.\"\"\"\n    mode_translation_dict = dict(edge='nearest', symmetric='reflect',\n                                 reflect='mirror')\n    if mode in mode_translation_dict:\n        mode = mode_translation_dict[mode]\n    return mode",
        "sha1": "51d9ead5404d7622db1dc9e401b4e57666e7953d",
        "id": 246349
    },
    {
        "content": "def pandasdf2pdb(df):\n    \"\"\"Return a string in PDB format from a pandas dataframe.\n\n    Parameters\n    ----------\n    df : pandas dataframe with columns \"atnum\", \"atname\", \"resname\", \"resnum\",\n         \"x\", \"y\", \"z\"\n\n    Returns\n    -------\n    str\n        A string representing the PDB.\n    \"\"\"\n    s = \"\"\n    chain = \"\"\n    for _, row_atom in df.iterrows():\n        atnum, atname, resname, resnum, x, y, z = row_atom\n        atnum = int(atnum)\n        resnum = int(resnum)\n        # See for pdb format:\n        # https://www.cgl.ucsf.edu/chimera/docs/UsersGuide/tutorials/pdbintro.html.\n        # \"alt\" means alternate location indicator\n        # \"code\" means code for insertions of residues\n    \t# \"seg\" means segment identifier\n        # \"elt\" means element symbol\n        if len(atname) == 4:\n            s += (\"{record_type:6s}{atnum:5d} {atname:<4s}{alt:1s}{resname:>4s}\"\n                  \"{chain:1s}{resnum:>4d}{code:1s}   {x:>8.3f}{y:>8.3f}{z:>8.3f}\"\n                  \"{occupancy:>6.2f}{temp_fact:>6.2f}          {seg:<2s}{elt:>2s}\\n\"\n                  .format(record_type=\"ATOM\", atnum=atnum, atname=atname, alt=\"\",\n                          resname=resname, chain=chain, resnum=resnum, code=\"\",\n                          x=x, y=y, z=z, occupancy=1.0, temp_fact=0.0, seg=\"\",\n                          elt=atname[0]))\n        else:\n            s += (\"{record_type:6s}{atnum:5d}  {atname:<3s}{alt:1s}{resname:>4s}\"\n                  \"{chain:1s}{resnum:>4d}{code:1s}   {x:>8.3f}{y:>8.3f}{z:>8.3f}\"\n                  \"{occupancy:>6.2f}{temp_fact:>6.2f}          {seg:<2s}{elt:>2s}\\n\"\n                  .format(record_type=\"ATOM\", atnum=atnum, atname=atname, alt=\"\",\n                          resname=resname, chain=chain, resnum=resnum, code=\"\",\n                          x=x, y=y, z=z, occupancy=1.0, temp_fact=0.0, seg=\"\",\n                          elt=atname[0]))\n    return s",
        "sha1": "d03c80e0a3e183ad068c246630cc2c93fb500a08",
        "id": 463995
    },
    {
        "content": "def sort_batch_of_lists(uids, batch_of_lists, lens):\n    \"\"\"Sort batch of lists according to len(list). Descending\"\"\"\n    sorted_idx = [i[0] for i in sorted(enumerate(lens), key=lambda x: x[1], reverse=True)]\n    uids = [uids[i] for i in sorted_idx]\n    lens = [lens[i] for i in sorted_idx]\n    batch_of_lists = [batch_of_lists[i] for i in sorted_idx]\n    return uids, batch_of_lists, lens",
        "sha1": "a7c1a0315fc5d0effc730f7a4adcb19f484416d8",
        "id": 117019
    },
    {
        "content": "import requests\n\n\ndef get_page(url: str, store_num: str= '095'):\n    \"\"\"\n    Given a Microcenter URL, return request object\n    :param url: str, microcenter url\n    :param store_num: str, store number as string, defaults to MO - Brentwood\n    :return: requests.model.Response, html from url\n    \"\"\"\n    headers = {\n        'Cookie': 'storeSelected=' + store_num,\n        'DNT': '1',\n        'Host': 'www.microcenter.com',\n    }\n    return requests.get(url, headers=headers)",
        "sha1": "02eb540cd26f9552b22c9ad305553080054df3d5",
        "id": 639028
    },
    {
        "content": "def scale_features(X, approach='standard'):\n    \"\"\"Scale feature matrix.\n\n    Parameters\n    ----------\n    X : torch.Tensor\n        Tensor of shape (n_samples, n_channels, lookback, n_assets). Unscaled\n\n    approach : str, {'standard', 'percent'}\n        How to scale features.\n\n    Returns\n    -------\n    X_scaled : torch.tensor\n        Tensor of shape (n_samples, n_channels, lookback, n_assets). Scaled.\n    \"\"\"\n    n_samples, n_channels, lookback, n_assets = X.shape\n\n    if approach == 'standard':\n        means = X.mean(dim=[2, 3])  # for each sample and each channel a mean is computed (over lookback and assets)\n        stds = X.std(dim=[2, 3]) + 1e-6  # for each sample and each channel a std is computed (over lookback and assets)\n\n        means_rep = means.view(n_samples, n_channels, 1, 1).repeat(1, 1, lookback, n_assets)\n        stds_rep = stds.view(n_samples, n_channels, 1, 1).repeat(1, 1, lookback, n_assets)\n\n        X_scaled = (X - means_rep) / stds_rep\n\n    elif approach == 'percent':\n        X_scaled = X * 100\n\n    else:\n        raise ValueError('Invalid scaling approach {}'.format(approach))\n\n    return X_scaled",
        "sha1": "c7e73e5789f9418f5215298da0cee9301e8a0447",
        "id": 126726
    },
    {
        "content": "def fix_line_breaks(s):\n    \"\"\"\n    Convert \\r\\n and \\r to \\n chars. Strip any leading or trailing whitespace\n    on each line. Remove blank lines.\n    \"\"\"\n    l = s.splitlines()\n    x = [i.strip() for i in l]\n    x = [i for i in x if i]  # remove blank lines\n    return \"\\n\".join(x)",
        "sha1": "8ac0a9cd1bb14e0817746e5f9c70623ee3667a97",
        "id": 649271
    },
    {
        "content": "from typing import Mapping\n\n\ndef check_dict_nested_attrs(item: Mapping, dict_data: Mapping) -> bool:\n    \"\"\" Checks the values from `dict_data` are contained in `item`\n\n    >>> d = {'a': 1, 'b': {'c': 2}}\n    >>> check_dict_nested_attrs(d, {'a': 1})\n    True\n    >>> check_dict_nested_attrs(d, {'b': {'c': 2}})\n    True\n    >>> check_dict_nested_attrs(d, {'d': []})\n    False\n    \"\"\"\n    for key, value in dict_data.items():\n        if key not in item:\n            return False\n\n        item_value = item[key]\n\n        if isinstance(item_value, Mapping):\n            if not check_dict_nested_attrs(item_value, value):\n                return False\n        elif item_value != value:\n            return False\n\n    return True",
        "sha1": "08ed8dbc405e236b95e33e10e9c342e15b6363c9",
        "id": 27132
    },
    {
        "content": "def generate_device_class_methods(use_async: bool) -> str:\n    \"\"\"Generates the default methods for a device class.\n\n    Args:\n        use_async (bool): True if asynchronous code should be generated, false otherwise.\n\n    Returns:\n        str: The Python code for the default methods of a device class.\n\n    \"\"\"\n    if use_async:\n        return \"\"\"\n    def __enter__(self):\n        return self\n\n    def __exit__(self):\n        raise RuntimeError()\n\n    async def __aenter__(self):\n        await self.open()\n        return self\n\n    async def __aexit__(self, *args):\n        await self.close()\n\n    def __await__(self):\n        return self.__aenter__().__await__()\n\n    async def open(self) -> None:\n        await self.__client.open()\n\n    async def close(self) -> None:\n        await self.__client.close()\n\n\"\"\"\n    else:\n        return \"\"\"\n    def __enter__(self):\n        self.open()\n        return self\n\n    def __exit__(self, *args):\n        self.close()\n\n    def open(self) -> None:\n        self.__client.open()\n\n    def close(self) -> None:\n        self.__client.close()\n\n    def run(self, timeout: int = None) -> None:\n        self.__client.run(timeout)\n\n    def stop(self) -> None:\n        self.__client.stop()\n\n    def poll(self) -> None:\n        self.__client.poll()\n\n\"\"\"",
        "sha1": "d5fce0e678184447ae989e855937a6745d0238cd",
        "id": 367023
    },
    {
        "content": "def sha256_key(sha256):\n    \"\"\" Create key from sha256\n    :param sha256: hashval\n    :return: path to return\n    :rtype: str\n    \"\"\"\n    try:\n        path = \"%s%s/%s%s/%s%s/%s%s/%s\" % (\n            sha256[0], sha256[1], sha256[2], sha256[3], sha256[4],\n            sha256[5], sha256[6], sha256[7], sha256)\n    except IndexError:\n        return sha256\n    return path",
        "sha1": "cbe04b21a8a89a98f1cc6f4ab15086a8f2f61468",
        "id": 70527
    },
    {
        "content": "def get_index(nodes, name):\n    \"\"\"\n    Gets the index of the character based on name\n    :param nodes: List of all character nodes\n    :param name: Character name to get the index of\n    :return: (int) index of the character name in nodes\n    \"\"\"\n    index = 0\n    for node in nodes:\n        if node['name'] == name:\n            return index\n        index += 1\n    return index",
        "sha1": "c336317f6ae7055fb4989a29ba930ee8f0d331f2",
        "id": 225208
    },
    {
        "content": "def jaccard_index_calc(TP, TOP, P):\n    \"\"\"\n    Calculate Jaccard index for each class.\n\n    :param TP: true positive\n    :type TP : int\n    :param TOP: test outcome positive\n    :type TOP : int\n    :param P:  condition positive\n    :type P : int\n    :return: Jaccard index as float\n    \"\"\"\n    try:\n        return TP / (TOP + P - TP)\n    except Exception:\n        return \"None\"",
        "sha1": "13f5a53ca114db2b3af928db0ab9fac885ad2923",
        "id": 52565
    },
    {
        "content": "import torch\nimport gc\n\n\ndef torch_pca(  X, \n                device='cpu', \n                mean_sub=True, \n                zscore=False, \n                rank=None, \n                return_cpu=True, \n                return_numpy=False):\n    \"\"\"\n    Principal Components Analysis for PyTorch.\n    If using GPU, then call torch.cuda.empty_cache() after.\n    RH 2021\n\n    Args:\n        X (torch.Tensor or np.ndarray):\n            Data to be decomposed.\n            2-D array. Columns are features, rows are samples.\n            PCA will be performed column-wise.\n        device (str):\n            Device to use. ie 'cuda' or 'cpu'. Use a function \n             torch_helpers.set_device() to get.\n        mean_sub (bool):\n            Whether or not to mean subtract ('center') the \n             columns.\n        zscore (bool):\n            Whether or not to z-score the columns. This is \n             equivalent to doing PCA on the correlation-matrix.\n        rank (int):\n            Maximum estimated rank of decomposition. If None,\n             then rank is X.shape[1]\n        return_cpu (bool):  \n            Whether or not to force returns/outputs to be on \n             the 'cpu' device. If False, and device!='cpu',\n             then returns will be on device.\n        return_numpy (bool):\n            Whether or not to force returns/outputs to be\n             numpy.ndarray type.\n\n    Returns:\n        components (torch.Tensor or np.ndarray):\n            The components of the decomposition. \n            2-D array.\n            Each column is a component vector. Each row is a \n             feature weight.\n        scores (torch.Tensor or np.ndarray):\n            The scores of the decomposition.\n            2-D array.\n            Each column is a score vector. Each row is a \n             sample weight.\n        singVals (torch.Tensor or np.ndarray):\n            The singular values of the decomposition.\n            1-D array.\n            Each element is a singular value.\n        EVR (torch.Tensor or np.ndarray):\n            The explained variance ratio of each component.\n            1-D array.\n            Each element is the explained variance ratio of\n             the corresponding component.\n    \"\"\"\n    \n    if isinstance(X, torch.Tensor) == False:\n        X = torch.from_numpy(X).to(device)\n    elif X.device != device:\n            X = X.to(device)\n            \n    if mean_sub and not zscore:\n        X = X - torch.mean(X, dim=0)\n    if zscore:\n        X = X - torch.mean(X, dim=0)\n        stds = torch.std(X, dim=0)\n        X = X / stds[None,:]        \n        \n    if rank is None:\n        rank = X.shape[1]\n    \n    (U,S,V) = torch.pca_lowrank(X, q=rank, center=False, niter=2)\n    components = V\n    scores = torch.matmul(X, V[:, :rank])\n\n    singVals = (S**2)/(len(S)-1)\n    EVR = (singVals) / torch.sum(singVals)\n    \n    if return_cpu:\n        components = components.cpu()\n        scores = scores.cpu()\n        singVals = singVals.cpu()\n        EVR = EVR.cpu()\n    if return_numpy:\n        components = components.cpu().numpy()\n        scores = scores.cpu().numpy()\n        singVals = singVals.cpu().numpy()\n        EVR = EVR.cpu().numpy()\n        \n    gc.collect()\n    torch.cuda.empty_cache()\n    gc.collect()\n    torch.cuda.empty_cache()\n    gc.collect()\n    return components, scores, singVals, EVR",
        "sha1": "8fbfa9c5cd55d9311913f496ece8cbc79f4d0cea",
        "id": 664263
    },
    {
        "content": "def osm_zoom_level_to_pixels_per_meter(\n    zoom_level: float, equator_length: float\n) -> float:\n    \"\"\"\n    Convert OSM zoom level to pixels per meter on Equator. See\n    https://wiki.openstreetmap.org/wiki/Zoom_levels\n\n    :param zoom_level: integer number usually not bigger than 20, but this\n        function allows any non-negative float value\n    :param equator_length: celestial body equator length in meters\n    \"\"\"\n    return 2.0**zoom_level / equator_length * 256.0",
        "sha1": "7351c289dde4bc42a46e343efcd3e750841b2c8c",
        "id": 39157
    },
    {
        "content": "from typing import List\nimport uuid\n\n\ndef uuid_as_list() -> List[int]:\n    \"\"\"Returns a list of integers each representing a symbol of random UUID.\n    Dashes are replaced with zeros.\n    \"\"\"\n    return [int(i, 16) for i in str(uuid.uuid4()).replace(\"-\", \"0\")]",
        "sha1": "3f30c7801df8608d9a6af958dd7ce69cfda99ad0",
        "id": 541272
    },
    {
        "content": "from typing import Union\n\n\ndef _tristate_bool_option(val: str) -> Union[None, bool]:\n    \"\"\"\n    A parsing function for a tri-state boolean option.\n\n    :raise ValueError:\n        if not one of: `None`, true, 1, yes, on, false, 0, no, off\n    \"\"\"\n    val = val and val.strip().lower()\n    if not val:\n        return None\n    if val in \"true 1 yes on\".split():\n        return True\n    if val in \"false 0 no off\".split():\n        return False\n    raise ValueError(f\"invalid boolean {val!r} supplied\")",
        "sha1": "ec7220fc1bd2158a0b3e31a188e8feda6cf1f4f2",
        "id": 438762
    },
    {
        "content": "def identity(test_item):\n    \"\"\"Identity decorator\n\n    \"\"\"\n    return test_item",
        "sha1": "799d2325e04066c0dfa405d24d5718b67eb64a00",
        "id": 37709
    },
    {
        "content": "def _int32_to_bytes(i):\n    # NOTE: This course is done on a Mac which is little-endian\n    \"\"\"Convert an integer to four bytes in little-endian format.\"\"\"\n\n    # &:    Bitwise 'and'\n    # >>:   Right shift\n\n    return bytes((i & 0xff,\n                    i >> 8 & 0xff,\n                    i >> 16 & 0xff,\n                    i >> 24 & 0xff))",
        "sha1": "61606a87d7f074117637b3a322dcded43a514cd0",
        "id": 694632
    },
    {
        "content": "def squash_to_unit_interval(x: float, constant: float) -> float:\n  \"\"\"Scales non-negative x to be in range [0, 1], with a squash.\"\"\"\n  if constant <= 0:\n    raise ValueError('Squash constant must be greater than zero.')\n  if x < 0:\n    raise ValueError('Squash can only be performed on a positive value.')\n  return x / (x + constant)",
        "sha1": "0a99803cfad0b82f15ebefe1a59ef24cfd8bbd33",
        "id": 662815
    },
    {
        "content": "def gen_record_set(use_resource_record=False, **kwargs):\n    \"\"\"Generate a record set.\"\"\"\n    data = {\n        'Name': 'placeholder_name',\n        'Type': 'CNAME',\n        'Value': 'placeholder_value'\n    }\n    if use_resource_record:\n        data['ResourceRecords'] = kwargs.pop('ResourceRecords', [\n            {'Value': kwargs.pop('Value', data['Value'])}\n        ])\n        del data['Value']\n\n    data.update(kwargs)\n    return data",
        "sha1": "e190c94ec9280d892361fc78177db5e2d84991a8",
        "id": 603286
    },
    {
        "content": "import re\n\n\ndef max_repetitions(s, threshold=8):\n  \"\"\"Find the largest contiguous repeating substring in s, repeating itself at\n     least `threshold` times. Example:\n     >>> max_repetitions(\"blablasbla\")  # returns ['bla', 2].\"\"\"\n  repetitions_re = re.compile(r'(.+?)\\1{%d,}' % threshold)\n  max_repeated = None\n  for match in repetitions_re.finditer(s):\n    new_repeated = [match.group(1), len(match.group(0))/len(match.group(1))]\n    # pylint: disable=unsubscriptable-object\n    if max_repeated is None or max_repeated[1] < new_repeated[1]:\n      max_repeated = new_repeated\n  return max_repeated",
        "sha1": "04ea540a9cec1e937ba78f880f889b787f65b450",
        "id": 575032
    },
    {
        "content": "import re\n\n\ndef is_uuid(text):\n    \"\"\"\n    >>> is_uuid('13f79535-47bb-0310-9956-ffa450edef68')\n    True\n    >>> is_uuid('d670460b4b4aece5915caf5c68d12f560a9fe3e4')\n    False\n    >>> is_uuid('13z79535-47bb-0310-9956-ffa450edef68')\n    False\n    \"\"\"\n\n    return bool(re.match(r'''\n        # https://tools.ietf.org/html/rfc4122#section-3\n        [a-f0-9]{8}  -   # time-low\n        [a-f0-9]{4}  -   # time-mid\n        [a-f0-9]{4}  -   # time-high-and-version\n        [a-f0-9]{4}  -   # clock-seq-and-reserved clock-seq-low\n        [a-f0-9]{12}     # node\n        $\n    ''', text, re.VERBOSE))",
        "sha1": "6718908a551b23eaa86974aac6edf74367fd98f7",
        "id": 533007
    },
    {
        "content": "def elem_to_dict(elem, ns, key_map, value_map={}):\n    \"\"\"Convert an XML etree.Element to a desired dictionary\n    as specified by the key map and value map.\n    Args:\n        elem (etree.Element): An ancestor element\n            of the tags specified in the key map.\n        ns (string): The namespace to use\n            when searching for XML tags.\n        key_map (dict): A mapping from desired\n            dictionary keys to XML tag names.\n        value_map (dict): A mapping from XML tag names to\n            dictionaries of mappings from XML text\n            values to desired dictionary values.\n    Returns:\n        The desired dictionary.\n    \"\"\"\n    to_dict = {}\n    for k, v in key_map.items():\n        field = elem.find('.//{0}{1}'.format(ns, v))\n        if field is not None:\n            text = field.text\n            to_dict[k] = value_map.get(v, {}).get(text, text)\n\n    return to_dict",
        "sha1": "fc5c894bba1d374679db23e6cfa176a598c18fdc",
        "id": 635631
    },
    {
        "content": "def find_stop_codon(sequence):\n    \"\"\"\n    finds the nucleotide position of the first stop codon\n    Note: the search is done in the 1st fram of the sequence\n    Args:\n        sequence (str): dna sequence 4 letter code\n    Returns:\n        int: nucleotide position of stop codon\n        None: sequence does not contain stop codon\n    \"\"\"\n    stop_codons = ['TAG', 'TAA', 'TGA']\n    sequence = sequence.upper()\n    # iterate over the codons\n    for i in range(0, len(sequence) - 2, 3):\n        if sequence[i: i + 3] in stop_codons:\n            return i\n    return None",
        "sha1": "321ec93b86591d7b69059ce4ab167339715e5c09",
        "id": 131445
    },
    {
        "content": "import torch\n\n\ndef create_mask_from_length(length_tensor, mask_size, zeros_at_end=True):\n    \"\"\"\n    Creates a binary mask based on length.\n\n    :param length_tensor: ND Tensor containing the lengths.\n    :param mask_size: Int specifying the mask size. Usually the largest length.\n    :param zeros_at_end: Whether to put the zeros of the mask at the end.\n    :return: (N+1)D Int Tensor (..., mask_size).\n    \"\"\"\n\n    if zeros_at_end:\n        mask = torch.arange(0, mask_size, dtype=torch.int, device=length_tensor.device)\n    else:\n        mask = torch.arange(mask_size - 1, -1, step=-1, dtype=torch.int, device=length_tensor.device)\n\n    mask = mask.int().view([1] * (len(length_tensor.shape)) + [-1])\n\n    return mask < length_tensor.int().unsqueeze(-1)",
        "sha1": "ecc06c1841e5d32b80e24705adb4cc12dba3b425",
        "id": 475883
    },
    {
        "content": "from typing import Set\nimport inspect\n\n\ndef function_call_stack() -> Set[str]:\n    \"\"\"Returns the function call stack context.\n    source: https://stackoverflow.com/a/2654130\n    \"\"\"\n    curframe = inspect.currentframe()\n    calframe = inspect.getouterframes(curframe, 2)\n    funcs = set(map(lambda x: x.function, calframe))\n    return funcs",
        "sha1": "d410db8bea1d3d675ccce66d94a7ad44153347c2",
        "id": 541081
    },
    {
        "content": "def find_longest(arr):\n    \"\"\" Find the number with the most digits.\n        If two numbers in the argument array have the same number of digits,\n        return the first one in the array.\n    \"\"\"\n    return max(arr, key=lambda x: len(str(x)))",
        "sha1": "340bfd92ab2e88bbec071ef0e9ea584cc4c18cf0",
        "id": 595250
    },
    {
        "content": "from pathlib import Path\n\n\ndef GetTestsPath() -> Path:\n    \"\"\"path to the \"tests\" folder\"\"\"\n    return Path(__file__).parent.absolute()",
        "sha1": "2223ae9f0af57f7beeac3bc1f1edc54fc652f202",
        "id": 605571
    },
    {
        "content": "def load_spans(file):\n    \"\"\"\n    Loads the predicted spans\n    \"\"\"\n    article_id, span_interval = ([], [])\n    with open(file, 'r', encoding='utf-8') as f:\n        for line in f.readlines():\n            art_id, span_begin, span_end = [int(x) for x in line.rstrip().split('\\t')]\n            span_interval.append((span_begin, span_end))\n            article_id.append(art_id)\n\n    return article_id, span_interval",
        "sha1": "8f8de31e1d1df7f0d2a44d8f8db7f846750bd89f",
        "id": 5643
    },
    {
        "content": "import pytz\n\n\ndef convert_to_UTC(dt):\n    \"\"\"\n    This function converts a datetime object with timezone information to a datetime object in UTC.\n    \"\"\"\n    tz_utc = pytz.utc\n    dt_utc = dt.astimezone(tz_utc)\n    return dt_utc",
        "sha1": "02747a5b31cd6e5ac7443314aad9b640b01b5395",
        "id": 121585
    },
    {
        "content": "def remove_items(collection, subset):\n    \"\"\"From a collection of defaults, remove a subset and return the rest.\"\"\"\n    the_rest = collection.copy()\n    for name, param in subset.items():\n        assert (name, the_rest[name].default) == (name, param.default)\n        del the_rest[name]\n    return the_rest",
        "sha1": "8dba0783631dba0235897a6b0d44bbce959cf53e",
        "id": 214421
    },
    {
        "content": "def get_list_duplicates(in_list):\n    \"\"\"Identify duplicates in a list.\"\"\"\n    seen = set()\n    duplicates = set()\n    for item in in_list:\n        if item in seen and item != 'N/A':\n            duplicates.add(item)\n        seen.add(item)\n    return list(duplicates)",
        "sha1": "a9b7fa3f996f0109f266d31f80fd809cef88333a",
        "id": 54079
    },
    {
        "content": "def extract_subID(list_):\n    \"\"\"\n    Function to extract subject IDs\n    \"\"\"\n    res = []\n    for item in list_:\n        sub = item.split('_')[1]\n        res.append(sub)\n    return res",
        "sha1": "0594c3307988fd7cd1837f1ef04393d0b23a85b4",
        "id": 278414
    },
    {
        "content": "def rotate_axes(position, axes):\n    \"\"\"Rotate axes in position\n\n    Examples\n    --------\n    >>> rotate_axes([1, 2, 3], \"xy\")\n    (1, 2, 3)\n    >>> rotate_axes([1, 2, 3], \"yz\")\n    (2, 3, 1)\n    \"\"\"\n    missing_axes = set('xyz') - set(axes)\n    for a in missing_axes:\n        axes += a\n    assert len(axes) == 3\n\n    mapping = dict(x=0, y=1, z=2)\n    return tuple(position[mapping[a]] for a in axes)",
        "sha1": "df6af95a758e3042653e0be638b29e574789dec3",
        "id": 384559
    },
    {
        "content": "def get_heading_indices(row: list) -> dict:\n    \"\"\"generates a dictionary mapping desired headings to row indices to allow for changing order of columns in source data\n\n    Args:\n        row (list): row of data from CSV file\n\n    Returns:\n        dict: dictionary of heading matched with row index\n    \"\"\"\n    headings = [\n        \"Date Dispensed\",\n        \"Patient Name\",\n        \"Street\",\n        \"Town or City\",\n        \"Birth Date\",\n        \"PPSN No\",\n        \"Gender\",\n        \"Qty\",\n        \"Script Dispensed As\",\n        \"Directions Expanded\",\n        \"Contract GP Name\",\n        \"Contract GP Address\",\n    ]\n\n    heading_indices = dict()\n    for heading in headings:\n        heading_indices[heading] = row.index(heading)\n    return heading_indices",
        "sha1": "01875dc1e38bfaccb1fd62606e967459cce1b75d",
        "id": 353839
    },
    {
        "content": "def lix_calc(sentences):\n    \"\"\"\n    Calculate LIX, assuming that all tokens are actual words, not punctuation or delimiters.\n\n    >>> print(\"%.2f\" % lix_calc(4*[\"a bc def ghij klmno pqrstu vxyz\u00e5\u00e4\u00f6\".split()]))\n    21.29\n    \"\"\"\n    sentence_counter = 0.0\n    word_counter = 0.0\n    length_counter = 0.0\n    for words in sentences:\n        sentence_counter += 1\n        for word in words:\n            word_counter += 1\n            length_counter += int(len(word) > 6)\n    if word_counter == 0 and sentence_counter == 0:\n        return float('NaN')\n    elif word_counter == 0 or sentence_counter == 0:\n        return float('inf')\n    else:\n        return word_counter / sentence_counter + 100 * length_counter / word_counter",
        "sha1": "fb2db0e3a6932a1d041b5f20aa45247e75e58259",
        "id": 453445
    },
    {
        "content": "def strike_volume(grain_mass, ratio):\n    \"\"\"\n    Calculate amount of liquor to strike the grain with\n    :param grain_mass: [float] in pounds\n    :param ratio: [float] desired ratio in quarts / pound\n    :return: [float] quarts of H2O\n    \"\"\"\n    return grain_mass * ratio",
        "sha1": "748dc1fbaf5833b6969c2f9a4793e1a1750c8d33",
        "id": 270992
    },
    {
        "content": "def get_unique_covered_percentage(fuzzer_row_covered_regions,\n                                  fuzzer_col_covered_regions):\n    \"\"\"Returns the number of regions covered by the fuzzer of the column\n    but not by the fuzzer of the row.\"\"\"\n\n    unique_region_count = 0\n    for region in fuzzer_col_covered_regions:\n        if region not in fuzzer_row_covered_regions:\n            unique_region_count += 1\n    return unique_region_count",
        "sha1": "83b4b69ced91c3a1d279fe6b2fe1b43ce3131b61",
        "id": 678545
    },
    {
        "content": "def is_level(coord):\n    \"\"\"\n    Determines if a coordinate is level.\n\n    :param coord: coordinate of xarray dataset e.g. coord = ds.coords[coord_id]\n    :return: (bool) True if the coordinate is level.\n    \"\"\"\n    if \"vertical\" in coord.cf and coord.cf[\"vertical\"].name == coord.name:\n        return True\n\n    if hasattr(coord, \"positive\"):\n        if coord.attrs.get(\"positive\", None) == \"up\" or \"down\":\n            return True\n\n    if hasattr(coord, \"axis\"):\n        if coord.attrs.get(\"axis\", None) == \"Z\":\n            return True\n\n    return False",
        "sha1": "3ed2c5b8865a37d9df9208081a092b791562e348",
        "id": 547558
    },
    {
        "content": "import yaml\n\n\ndef read_yml_config(path):\n    \"\"\"Read YAML formatted configuration file.\n\n    Args:\n        path (str): path to configuration file to read\n\n    Returns:\n        dict: machines with attributes\n\n    \"\"\"\n    with open(path, 'rt') as f:\n        data = yaml.load(f.read(), Loader=yaml.Loader)\n    # Machines are entries with a 'mac' attribute\n    data = {machine: desc for machine, desc in data.items() if 'mac' in desc}\n    return data",
        "sha1": "9e0cc7223f6eb196a21c950c8b4e68e496d88150",
        "id": 416629
    },
    {
        "content": "def image_person_object_factory(image_id, person_id):\n    \"\"\"Cook up a fake imageperson json object from given ids.\"\"\"\n    personimage = {\n        'image_id': image_id,\n        'person_id': person_id\n    }\n    return personimage",
        "sha1": "4aa59382f39b9acb5869b1839ab5cd5240404972",
        "id": 597615
    },
    {
        "content": "def dwid_exists(dwid, cursor):\n    \"\"\"See if a dwid exists in the database or not. If yes, return 1,\n       if not return 0.\"\"\"\n    array = (dwid,)\n    cursor.execute('SELECT * from dwids where dwid=?', array)\n    if cursor.fetchone():\n        return 1\n    else:\n        return 0",
        "sha1": "08586088a14cdc11f8308da3d3038667c672fc13",
        "id": 644637
    },
    {
        "content": "def na_padding(x, max_len, pad_offset = 0, na_value = \"NA\"):\n    \"\"\"\n    Pads a list with NA's to achive a target length.\n    \"\"\"\n\n    if len(x) < max_len:\n        x = x[: -pad_offset] + ([\"NA\" for i in range(max_len - len(x))]) + x[-pad_offset:]\n        return x\n    else:\n        return x",
        "sha1": "9cbcb4b7e1b6468d89450688212c177e13976dfa",
        "id": 460191
    },
    {
        "content": "def compatible_versions(actual_version: str, required_version: str) -> bool:\n    \"\"\"Determine whether two versions are equal.\n\n    Only the dot separated elements in common are taken into account, so\n    actual \"3.7.4\" compared with \"3.7\" will return True.\n\n    Args:\n        actual_version: A dot separated version.\n        required_version: A dot separated version.\n\n    Returns:\n        True if the actual_version is compatible with the required_version,\n        otherwise False.\n    \"\"\"\n    return all(\n        actual == expected\n        for actual, expected in zip(actual_version.split(\".\"), required_version.split(\".\"))\n    )",
        "sha1": "1876c7ce1ca1b992640ba7bb4f96cc9420de7965",
        "id": 44422
    },
    {
        "content": "def datetime2iso(d):\n\t\"\"\"Convert a datetime object in a datetime string in ISO format.\n\n\tParameters\n\t----------\n\td : datetime\n\t\tdatetime object\n\n\tReturns\n\t-------\n\tstr\n\t\tdatetime in ISO format\n\n\tNotes\n\t-----\n\tResult is rounded to the nearest integer second.\n\tThe ouput uses 'Z' for UTC datetimes.\n\n\t\"\"\"\n\treturn d.replace(microsecond=0).isoformat().replace('+00:00','Z')",
        "sha1": "1f16ba7ddb84774a95b0dbd39a57796a53ede5bb",
        "id": 502221
    },
    {
        "content": "from typing import Counter\n\n\ndef aa_composition(seq):\n    \"\"\"\n    Number of amino acids in a given sequence.\n\n    Args:\n        seq (str): A string representing a sequence\n\n    Returns:\n        Counter with amino acid composition\n\n    \"\"\"\n    return Counter(seq)",
        "sha1": "7f161591d6426083bf032b341a2142f270cb9ef5",
        "id": 312553
    },
    {
        "content": "def build_question_scanerio_map(utterances):\n    \"\"\"Builds a map from question to scenarios\"\"\"\n    question_scenario_map = dict()\n    for utterance in utterances:\n        question = utterance['question']\n        scenario = utterance['scenario']\n        if scenario == '':\n            continue\n        if question not in question_scenario_map.keys():\n            question_scenario_map[question] = set([scenario])\n        else:\n            question_scenario_map[question].add(scenario)\n    return question_scenario_map",
        "sha1": "b1c853de8e8043ffbdd0c827534907706475975a",
        "id": 175534
    },
    {
        "content": "import string\nimport random\n\n\ndef id_generator(size=6, chars=string.ascii_lowercase + string.digits):\n    \"\"\"Generate a random ID of specified length\n\n    Args:\n        size: the length of the id to generate\n        chars: the characters to use\n\n    Returns:\n        string of random id generated\n    \"\"\"\n    return ''.join(random.choice(chars) for _ in range(size))",
        "sha1": "f8e53b7ed2967ce32bfafd20ecf49d6c67c55907",
        "id": 345823
    },
    {
        "content": "def volmesh_vertex_lift(volmesh, vkey, target_xyz, hfkeys):\n    \"\"\"Duplicates and lifts a vertex, then creates cells using the vertex halffaces and the duplicated vertex.\n\n    Parameters\n    ----------\n    volmesh : VolMesh\n        A volmesh object representing a polyhedral force diagram.\n    vkey : int\n        The key of the vertex to lift.\n    hfkeys : list\n        List of halfface keys to create new cells from.\n    xyz : tuple\n        Target xyz coordinates of the lifted vertex.\n\n    Notes\n    -----\n    The lifting vertex must be on the boundary of the volmesh.\n\n    See Also\n    --------\n    * :func:`compas_3gs.operations.face_pinch`\n\n    \"\"\"\n\n    # check if vertex is interior ----------------------------------------------\n    if not volmesh.is_vertex_on_boundary(vkey):\n        raise Exception('This vertex is interior.')\n\n    # add new, lifted vertex ---------------------------------------------------\n    x, y, z = target_xyz\n    w = volmesh.add_vertex(x=x, y=y, z=z)\n\n    # add cells ----------------------------------------------------------------\n    for hfkey in hfkeys:\n        halffaces = [volmesh.halfface_vertices(hfkey)[::-1]]\n        for u, v in volmesh.halfface_halfedges(hfkey):\n            halffaces.append([u, v, w])\n        volmesh.add_cell(halffaces)\n\n    # --------------------------------------------------------------------------\n    return volmesh",
        "sha1": "a835e19167c15f914ff7e41d0bc811b797044c7a",
        "id": 592920
    },
    {
        "content": "def byte_to_str(byte):\n    \"\"\"Convert a byte to a string.\"\"\"\n    return str(byte, 'utf-8')",
        "sha1": "021a3a620ccc68945bc79ccf09d2197628900df0",
        "id": 528075
    },
    {
        "content": "import collections\n\n\ndef _group_tasks_by_jobid(tasks):\n  \"\"\"A defaultdict with, for each job, a list of its tasks.\"\"\"\n  ret = collections.defaultdict(list)\n  for t in tasks:\n    ret[t.get_field('job-id')].append(t)\n  return ret",
        "sha1": "440a8b666ca93db5d1253afe8dfd0383bda85af2",
        "id": 375674
    },
    {
        "content": "def compute_form_no(stroke_length, orifice_d=25.6, piston_d=160., num_orifices=1):\n    \"\"\"\n    Compute formation number (L/D) of a vortex ring\n    Parameters\n    ----------\n    stroke_length: float\n        stroke length of piston in mm\n    orifice_d: float\n        diameter of orfice in mm\n    piston_d: float\n        diameter of piston in mm\n    num_orifices: int or float\n        Number of orfices in a box\n\n    Returns\n    -------\n    ld: float\n        formation number of vortex ring\n    \"\"\"\n    ld = (piston_d / orifice_d)**2 * stroke_length / orifice_d / float(num_orifices)\n    return ld",
        "sha1": "39ff8bb7540eb756f5ffb5a95d144e740ee513c1",
        "id": 142384
    },
    {
        "content": "import json\nimport codecs\n\n\ndef encode_blob(data):\n    \"\"\" Encode a dictionary to a base64-encoded compressed binary blob.\n\n    :param data:    data to encode into a blob\n    :type data:     dict\n    :returns:       The data as a compressed base64-encoded binary blob\n    \"\"\"\n    blob_data = json.dumps(data).encode('utf8')\n    for codec in ('zlib', 'base64'):\n        blob_data = codecs.encode(blob_data, codec)\n    return blob_data.replace(b'\\n', b'')",
        "sha1": "266f0b6440adfae165a9d8d6877a111736e22488",
        "id": 678060
    },
    {
        "content": "import torch\n\n\ndef load_ckp(model, optimizer, lr_scheduler, path):\n    \"\"\"load training checkpoint\"\"\"\n\n    checkpoint = torch.load(path)\n\n    model.load_state_dict(checkpoint['model_state_dict'])\n    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    lr_scheduler.load_state_dict(checkpoint['lr_scheduler_state_dict'])\n\n    epoch = checkpoint['epoch']\n    train_loss = checkpoint['train_loss']\n    val_loss = checkpoint['val_loss']\n    cider = checkpoint['cider']\n\n    return epoch, model, optimizer, lr_scheduler, train_loss, val_loss, cider",
        "sha1": "f758d3fb2ad6d7a4c0263a3958c50d9339b009b0",
        "id": 376786
    },
    {
        "content": "import json\nfrom io import StringIO\n\n\ndef generate_validation_error_report(e, json_object, lines_before=7, lines_after=7):\n    \"\"\"\n    Generate a detailed report of a schema validation error.\n\n    'e' is a jsonschema.ValidationError exception that errored on\n    'json_object'.\n\n    Steps to discover the location of the validation error:\n    1. Traverse the json object using the 'path' in the validation exception\n       and replace the offending value with a special marker.\n    2. Pretty-print the json object indendented json text.\n    3. Search for the special marker in the json text to find the actual\n       line number of the error.\n    4. Make a report by showing the error line with a context of\n      'lines_before' and 'lines_after' number of lines on each side.\n    \"\"\"\n\n    if json_object is None:\n        return \"'json_object' cannot be None.\"\n\n    if not e.path:\n        if e.schema_path and e.validator_value:\n            return \"Toplevel:\\n\\t{}\".format(e.message)\n        else:\n            return str(e)\n\n    marker = \"3fb539deef7c4e2991f265c0a982f5ea\"\n\n    # Find the object that is erroring, and replace it with the marker.\n    ob_tmp = json_object\n    for entry in list(e.path)[:-1]:\n        ob_tmp = ob_tmp[entry]\n\n    orig, ob_tmp[e.path[-1]] = ob_tmp[e.path[-1]], marker\n\n    # Pretty print the object and search for the marker.\n    json_error = json.dumps(json_object, indent=4)\n    io = StringIO(json_error)\n    errline = None\n\n    for lineno, text in enumerate(io):\n        if marker in text:\n            errline = lineno\n            break\n\n    if errline is not None:\n        # Re-create report.\n        report = []\n        ob_tmp[e.path[-1]] = orig\n        json_error = json.dumps(json_object, indent=4)\n        io = StringIO(json_error)\n\n        for lineno, text in enumerate(io):\n            if lineno == errline:\n                line_text = \"{:4}: >>>\".format(lineno + 1)\n            else:\n                line_text = \"{:4}:    \".format(lineno + 1)\n            report.append(line_text + text.rstrip(\"\\n\"))\n\n        report = report[max(0, errline - lines_before):errline + 1 + lines_after]\n\n        s = \"Error in line {}:\\n\".format(errline + 1)\n        s += \"\\n\".join(report)\n        s += \"\\n\\n\" + str(e).replace(\"u'\", \"'\")\n    else:\n        s = str(e)\n    return s",
        "sha1": "ccba131c6e524ee0a990bdd6dc440248d3684ac2",
        "id": 691253
    },
    {
        "content": "def _find_device(cs, device):\n    \"\"\"Get a device by ID.\"\"\"\n\n    return cs.devices.get(device)",
        "sha1": "9ad875aebd82f6179fa697cd0dd99a83da873f3f",
        "id": 287089
    },
    {
        "content": "def r_squared(y, estimated):\n    \"\"\"\n    Calculate the R-squared error term.\n\n    Args:\n        y: 1-d pylab array with length N, representing the y-coordinates of the\n            N sample points\n        estimated: an 1-d pylab array of values estimated by the regression\n            model\n\n    Returns:\n        a float for the R-squared error term\n    \"\"\"\n    estimated_error = ((y - estimated)**2).sum()\n    mean_samples = (y.sum()) / len(y)\n    measured_variability = ((y - mean_samples)**2).sum()\n\n    return 1 - estimated_error / measured_variability",
        "sha1": "0abace1d25bee54ef6d1badad270404bc76ad496",
        "id": 471813
    },
    {
        "content": "def add_TBranT_config_args(parser):\n    \"\"\"TBranT arguments\"\"\"\n\n    group = parser.add_argument_group('model_TBranT', 'TBranT model configuration')\n    group.add_argument('--head_num', type=int, default=1, help='Number of heads of Transformer Encoder in T-BranT')\n    group.add_argument('--layer_num', type=int, default=1, help='Number of Transformer Encoder layers in T-BranT')\n    group.add_argument('--tree_gate', default=False, action='store_true', \n        help='Concatenate the tree features and candidate variable features.')\n    group.add_argument('--graph', default=False, action='store_true', \n        help='Use the history search tree to obtain global tree features.')\n    \n    return parser",
        "sha1": "b56675ce4af4602790388d7ebc14a6d1afe54f0a",
        "id": 337100
    },
    {
        "content": "def generate_proba_coeffs(noise_list, mult=1.):\n  \"\"\"Generates probability coefficients s.t `coeff*(beta_i-beta_j)=const`\"\"\"\n  n_replicas = len(noise_list)\n  betas = sorted([1/n for n in noise_list])\n  diffs = [betas[i] - betas[i+1] for i in range(n_replicas - 1)]\n  coeffs = [1.]\n  for i in range(1, n_replicas - 1):\n    coeffs.append(coeffs[i-1]*(diffs[i-1]/diffs[i]))\n\n  return [c*mult for c in coeffs]",
        "sha1": "f1e766cbdf8c8a3f20c2e278f21a5c5068de742a",
        "id": 465449
    },
    {
        "content": "from typing import Optional\nfrom typing import Match\nfrom typing import Any\nimport re\n\n\ndef _valid_return_path(return_to: str) -> Optional[Match[Any]]:\n    \"\"\"Make sure this is a valid relative path to prevent redirect attacks.\"\"\"\n    return re.match(\n        '^/[^/]',\n        return_to,\n    )",
        "sha1": "e6a95226ac2f78dd818604af93e8324eec7995f8",
        "id": 402805
    },
    {
        "content": "def write_dot(g):\n    \"\"\"Replacement for pygraph.readwrite.dot.write, which is dog slow.\n\n    Note:\n        This isn't a general replacement. It will work for the graphs that\n        Rez generates, but there are no guarantees beyond that.\n\n    Args:\n        g (`pygraph.digraph`): Input graph.\n\n    Returns:\n        str: Graph in dot format.\n    \"\"\"\n    lines = [\"digraph g {\"]\n\n    def attrs_txt(items):\n        if items:\n            txt = \", \".join(('%s=\"%s\"' % (k, str(v).strip('\"')))\n                            for k, v in items)\n            return '[' + txt + ']'\n        else:\n            return ''\n\n    for node in g.nodes():\n        atxt = attrs_txt(g.node_attributes(node))\n        txt = \"%s %s;\" % (node, atxt)\n        lines.append(txt)\n\n    for e in g.edges():\n        edge_from, edge_to = e\n        attrs = g.edge_attributes(e)\n\n        label = str(g.edge_label(e))\n        if label:\n            attrs.append((\"label\", label))\n\n        atxt = attrs_txt(attrs)\n        txt = \"%s -> %s %s;\" % (edge_from, edge_to, atxt)\n        lines.append(txt)\n\n    lines.append(\"}\")\n    return '\\n'.join(lines)",
        "sha1": "01b34ff8644ece4d3204e59b934040e66eecdd08",
        "id": 659734
    },
    {
        "content": "import torch\n\n\ndef batch_transform(batch, transform):\n    \"\"\"Applies a transform to a batch of samples.\n\n    Keyword arguments:\n    - batch (): a batch os samples\n    - transform (callable): A function/transform to apply to ``batch``\n\n    \"\"\"\n\n    # Convert the single channel label to RGB in tensor form\n    # 1. torch.unbind removes the 0-dimension of \"labels\" and returns a tuple of\n    # all slices along that dimension\n    # 2. the transform is applied to each slice\n    transf_slices = [transform(tensor) for tensor in torch.unbind(batch)]\n\n    return torch.stack(transf_slices)",
        "sha1": "9f225450ac90d203aafdaaeab2c846c5715b4d05",
        "id": 478697
    },
    {
        "content": "def unroll_rids(rids):\n    \"\"\"Recursively unroll rid id ranges into individual rids.\"\"\"\n    if not rids:\n        return []\n\n    m = rids[0]\n    if m[1]:\n        return ['R%d' % r for r in range(int(m[0]),\n                                         int(m[1])+1)] + unroll_rids(rids[1:])\n    return ['R%s' % m[0]] + unroll_rids(rids[1:])\n\n    return []",
        "sha1": "f8dea8f8ffd69315a48f7fdb22eac49390214ea7",
        "id": 129209
    },
    {
        "content": "def adjust_brightness(im, brightness, sat=255):\n    \"\"\"\n    Adjusts brightness of image by scaling all pixels by the\n    `brightness` parameter.\n\n    Parameters\n    ----------\n    im : (M, N, P) numpy array\n        Image whose brightness is scaled. P >= 3,\n        so RGB, BGR, RGBA acceptable.\n    brightness : float\n        Scaling factor for pixel values. If < 0, returns junk.\n    sat : int\n        Saturation value for pixels (usually 255 for uint8)\n\n    Returns\n    -------\n    im : (M, N, P) numpy array\n        Original image with pixel values scaled\n\n    \"\"\"\n    # if image is 4-channel (e.g., RGBA) extracts first 3\n    is_rgba = (len(im.shape) == 3) and (im.shape[2] == 4)\n    if is_rgba:\n        im_to_scale = im[:,:,:3]\n    else:\n        im_to_scale = im\n    # scales pixel values in image\n    im_to_scale = im_to_scale.astype(float)\n    im_to_scale *= brightness\n    # sets oversaturated values to saturation value\n    im_to_scale[im_to_scale >= sat] = sat\n    # loads result into original image\n    if is_rgba:\n        im[:,:,:3] = im_to_scale.astype('uint8')\n    else:\n        im = im_to_scale.astype('uint8')\n\n    return im",
        "sha1": "cc3cfd0fb2aba08b6c4be3924b22600173252078",
        "id": 229018
    },
    {
        "content": "import re\n\n\ndef hdir(obj, magics=False, internals=False):\n    \"\"\"Print object methods and attributes, by default excluding magic methods.\n\n    Parameters\n    -----------\n    obj: any type\n        The object to print methods and attributes for.\n    magics: bool\n        Specifies whether to include magic methods (e.g. __name__, __hash__).\n        Default False.\n    internals: bool\n        Specifies whether to include internal methods (e.g. _dfs, _name).\n        Default False.\n\n    Returns\n    --------\n    dict\n        Keys are method/attribute names, values are strings specifying whether\n        the corresponding key is a 'method' or an 'attr'.\n    \"\"\"\n    output = dict()\n    for attr in dir(obj):\n        # Exclude magics or internals if specified.\n        if (not magics and attr.startswith('__')) or \\\n           (not internals and re.match('_[^_]', attr)):\n            continue\n\n        # Handle rare case where attr can't be invoked (e.g. df.sparse on a\n        # non-sparse Pandas dataframe).\n        try:\n            is_method = callable(getattr(obj, attr))\n        except Exception:\n            continue\n\n        # Update output to specify whether attr is callable.\n        if is_method:\n            output[attr] = 'method'\n        else:\n            output[attr] = 'attribute'\n    return output",
        "sha1": "3cd9686de9659ddf15c0317736835462329e160c",
        "id": 61564
    },
    {
        "content": "def is_palindrome(n):\n    \"\"\"Checks if n is a palindrome\"\"\"\n    n = str(n)\n    return n == n[::-1]",
        "sha1": "4f1fab3178b1e0b35a08611298e8baaa00562bb7",
        "id": 673610
    },
    {
        "content": "from typing import Dict\nfrom typing import List\nimport requests\nimport csv\n\n\ndef get_edam_ontology(version: str) -> Dict[str, List[str]]:\n    \"\"\"\n    Parameters\n    ----------\n    version : str\n        version of edam ontology to get.\n\n    Returns\n    -------\n    Dict[str, List[str]]\n        mapping between edam label to its parents, For example:\n        {\n            \"Manual segmentation\": [\n                \"Image segmentation\",\n                \"Dense image annotation\"\n            ],\n            \"Semi-automatic segmentation\": [\n                \"Image segmentation\"\n            ],\n            \"Sparse image annotation\": [\n                \"Image annotation\"\n            ]\n        }\n    \"\"\"\n    version = f\"_{version}\" if version else version\n    url = f'https://edamontology.org/EDAM-bioimaging{version}.csv'\n    response = requests.get(url)\n    if response.status_code != requests.codes.ok:\n        response.raise_for_status()\n\n    ontology = {\n        category['Class ID']: {\n            \"label\": category['Preferred Label'],\n            \"parents\": category['Parents'].split(\"|\")\n        }\n        for category in csv.DictReader(response.content.decode('utf-8').splitlines())\n    }\n\n    return {\n        values['label']: [ontology[parent]['label'] for parent in values['parents'] if parent in ontology]\n        for class_id, values in ontology.items()\n    }",
        "sha1": "b3304868f653529aa5eb0746ba3e63069d7a2c93",
        "id": 555159
    },
    {
        "content": "def entity_emiss_o(x, n_lbs, tp, exp_term=2):\n    \"\"\"\n    The function that calculates the emission prior of entity labels to the non-entity label 'O'\n    according to the diagonal values of the emission prior\n\n    Parameters\n    ----------\n    x: diagonal values\n    n_lbs: number of entity labels (2e+1)\n    tp: turning point\n    exp_term: the exponential term that controls the slope of the function\n\n    Returns\n    -------\n    non-diagonal emission priors\n    \"\"\"\n    # separating piecewise function\n    low = x < tp\n    high = x >= tp\n\n    # parameters for the first piece\n    a = (2 - n_lbs) / ((exp_term - 1) * tp ** exp_term - exp_term * tp ** (exp_term - 1))\n    b = 1 - n_lbs\n    # parameter for the second piece\n    f_tp = a * tp ** exp_term + b * tp + 1\n    c = f_tp / (tp - 1)\n\n    # piecewise result\n    y = low * (a * x ** exp_term + b * x + 1) + high * (c * x - c)\n    return y",
        "sha1": "34b9473e5799d7beddd20bea81a6fe926b8fde3f",
        "id": 37759
    },
    {
        "content": "def get_integer(prompt):\n    \"\"\"\n    Get an integer from Standard Input (stdin).\n\n    The function will continue looping, and prompting\n    the user, until a valid `int` is entered.\n\n    :param prompt: The String that the user will see, when\n        they're prompted to enter the value\n\n    :return: The integer that the user enters.\n    \"\"\"\n    while True:\n        temp = input(prompt)\n        if temp.isnumeric():\n            return int(temp)\n        # else:\n        print(\"{0} is not a valid number\".format(temp))",
        "sha1": "42f246e54a1fa6ef48c28d796d84010c0d5fd9e3",
        "id": 645664
    },
    {
        "content": "def must_replace_suffix(str, suffix, replacement):\n    \"\"\"\n    Replaces the given suffix in the string. If the string does not have the suffix, a runtime\n    error will be raised.\n    \"\"\"\n    splits = str.rsplit(suffix, maxsplit=1)\n    if len(splits) != 2 or splits[1]:\n        raise RuntimeError(str + \" does not contain \" + suffix)\n    return splits[0] + replacement",
        "sha1": "d4577bfe05c95866553a776fd65493dec858d524",
        "id": 640835
    },
    {
        "content": "def readFernetKey(keyPath: str) -> bytes:\n    \"\"\"Read Fernet symmetric key from file and return key.\"\"\"\n    with open(keyPath, \"rb\") as keyFile:\n        key: bytes = keyFile.read()\n    return key",
        "sha1": "56a373449a5631f2c1ecd6af69f568961c7925bc",
        "id": 155875
    },
    {
        "content": "def get_domain_source_concept_id(domain_table):\n    \"\"\"\n    A helper function to create the domain_source_concept_id field\n    :param domain_table: the cdm domain table\n    :return: the domain_source_concept_id\n    \"\"\"\n    return domain_table.split('_')[0] + '_source_concept_id'",
        "sha1": "9bc04f373b419319a4f5a47e8e2a53f5eb44b81a",
        "id": 193105
    },
    {
        "content": "def deltas(errors, epsilon, mean, std):\n    \"\"\"Compute mean and std deltas.\n\n    delta_mean = mean(errors) - mean(all errors below epsilon)\n    delta_std = std(errors) - std(all errors below epsilon)\n    \"\"\"\n    below = errors[errors <= epsilon]\n    if not len(below):\n        return 0, 0\n\n    return mean - below.mean(), std - below.std()",
        "sha1": "b91d95b09eb3a138f2d6323ac818a90be5e0d534",
        "id": 51139
    },
    {
        "content": "def is_function(f):\n    \"\"\"\n    Is it a function?\n    :param f: function\n    :return: boolean\n    \"\"\"\n    return hasattr(f, '__call__')",
        "sha1": "c330b81c3b09a0ba8e475e322df5f90d89e39e21",
        "id": 697210
    },
    {
        "content": "def get_file_contents(file_path):\n    \"\"\"Return a string containing the file contents of the file located at the\n    specified file path \"\"\"\n    with open(file_path, encoding=\"utf-8\") as f:\n        file_contents = f.read()\n\n    return file_contents",
        "sha1": "4e00422c05de3b90e3457b8d468514957c28b5f3",
        "id": 625560
    },
    {
        "content": "def references(name):\n    \"\"\"Provides references for the DMSP instruments and experiments\n\n    Parameters\n    ----------\n    name : str\n        Instrument name\n\n    Returns\n    -------\n    refs : str\n        String providing reference guidenance for the DMSP data\n\n    \"\"\"\n\n    refs = {'ivm': ' '.join(('F. J. Rich, Users Guide for the Topside',\n                             'Ionospheric Plasma Monitor (SSIES, SSIES-2 and',\n                             'SSIES-3) on Spacecraft of the Defense',\n                             'Meteorological Satellite Program (Air Force',\n                             'Phillips Laboratory, Hanscom AFB, MA, 1994),',\n                             'Vol. 1, p. 25.'))}\n\n    return refs[name]",
        "sha1": "0a21695582f826fb67fd9007944bf75fed8b35d9",
        "id": 494257
    },
    {
        "content": "def travel_object(obj, key_functions=[], val_functions=[]):\n    \"\"\"Recursively apply functions to the keys and values of a dictionary\n\n    Parameters\n    ----------\n    obj : dict/list\n        List or dict to recurse through.\n    key_functions : list\n        Functions to apply to the keys in 'obj'.\n    val_functions : list\n        Functions to apply to the values in 'obj'\n\n    Returns\n    -------\n    list/dict\n        A list or dict in which all nested keys and values have been\n        altered by the key_functions and val_functions respectively.\n    \"\"\"\n\n    def operate_on_dict(the_dict):\n        new_dict = {}\n        for key, val in the_dict.items():\n            new_key = key\n            for key_func in key_functions:\n                new_key = key_func(new_key)\n            if isinstance(val, dict) or isinstance(val, list):\n                new_val = travel_object(val, key_functions=key_functions, val_functions=val_functions)\n            else:\n                new_val = val\n                for val_func in val_functions:\n                    new_val = val_func(val)\n            new_dict[new_key] = new_val\n        return new_dict\n\n    if isinstance(obj, list):\n        new_list = []\n        for item in obj:\n            new_item = operate_on_dict(item) if isinstance(item, dict) else item\n            new_list.append(new_item)\n        return new_list\n    elif isinstance(obj, dict):\n        altered_dict = operate_on_dict(obj)\n        return altered_dict\n    else:\n        err_msg = 'Invalid type: the passed \"obj\" argument was not of type \"dict\" or \"list\".'\n        raise TypeError(err_msg)",
        "sha1": "e02aba54a4e209204de6c044a86be9758cff0fef",
        "id": 510696
    },
    {
        "content": "def is_same_module_or_submodule(orig, incoming):\n    \"\"\"\n    Returns true if the incoming module is the same module as the original, \n    or is a submodule of the original module\n    \"\"\"\n    if incoming is None:\n        return False\n    if orig == incoming:\n        return True\n    if incoming.__name__.startswith(orig.__name__):\n        return True\n    return False",
        "sha1": "a3ecf3f1d9d1546a9f8a30f9ef6609ae9e94cc6e",
        "id": 627884
    },
    {
        "content": "import requests\n\n\ndef send_request_no_check( url, authKey ):\n    \"\"\"Send a request that could fail\n    \n        Send a request using the provided url and authentication key.\n        Return the request directly, possibly including an error code.\n    \"\"\"\n    headers = {'Accept' : 'application/json', 'X-Auth-AccessKey' :authKey }\n    r = requests.get(url, headers=headers)\n    return r",
        "sha1": "0d09d85f6e432bdcd6ce75402e1ebca2bf1659d8",
        "id": 454600
    },
    {
        "content": "from pathlib import Path\n\n\ndef create_module_scratch(module_name):\n    \"\"\"Create a scratch that is easily found (by developers) and removed, and doesn't\n    clutter up the working directories.\n    \"\"\"\n    path = Path(\"~\").expanduser()\n    for path_part in \".idaes\", \"_scratch\", module_name:\n        path = path / path_part\n        if not path.exists():\n            path.mkdir()\n    return path",
        "sha1": "32ba13f647805e26488cb8191afa40d5d1a02eb1",
        "id": 652049
    },
    {
        "content": "def _parse_hyphenated_string(s):\n    \"\"\"Parses a hyphenated range into a list of integers.\"\"\"\n\n    # In: \"2004-2007\"\n    # Out: [2004, 2005, 2006, 2007]\n    list_of_lists = [list(range(*[int(second) + int(first) \n        for second, first in enumerate(substring.split('-'))])) \n        if '-' in substring else [int(substring)]\n        for substring in s.split()]\n    \n    return [item for sublist in list_of_lists for item in sublist]",
        "sha1": "b1673ffb4b62d70f9d43ba3acbd296a3bd810d3b",
        "id": 565979
    },
    {
        "content": "def dataCorrection(data, offset):\n    \"\"\" Add a position offset to relevant column of data.\n\n    data is a DataFrame object from the pandas module.\n    offset is list of offset such as [x_offset, y_offset, yaw_offset].\n    \"\"\"\n\n    data['droneX'] = data['droneX'].map(lambda x: x-offset[0])\n    data['droneY'] = data['droneY'].map(lambda x: x-offset[1])\n    data['yaw'] = data['yaw'].map(lambda x: x-offset[2])\n    data['lidarX'] = data['lidarX'].map(lambda x: x-offset[0])\n    data['lidarY'] = data['lidarY'].map(lambda x: x-offset[1])\n\n    return data",
        "sha1": "6fa71cc54eadeb77238d0a4856a300d43be8e886",
        "id": 317801
    },
    {
        "content": "def is_missing(obj):\n    \"\"\"Check if an object is missing\"\"\"\n    return getattr(obj, \"moya_missing\", False)",
        "sha1": "f80ac81f546848852e01b5d8ab5cea57dfafe3a7",
        "id": 435019
    },
    {
        "content": "def _resolve_layout(N: int, gate_wires: list):\n    \"\"\"\n    Resolve the layout per layer to make sure the space usage is optimal\n\n    Args:\n        *N (int)*:\n            The number of qubits.\n\n        *gate_wires (list)*:\n            List of lists containing the gate wires.\n\n\n    Returns (dict, int):\n        First return value is dictionary containing the shift with respect to zero for each gate in the layer.\n        The second return value is an integer specifying the maximum shift necessary.\n\n    \"\"\"\n    shift = dict(zip(range(N), [0 for _ in range(N)]))\n    for gw in gate_wires:\n        if len(gw) == 2:\n            max_shift_gw = max([shift[gw[0]], shift[gw[1]]])\n            shift[gw[0]], shift[gw[1]] = (max_shift_gw, max_shift_gw)\n            mingw, maxgw = (min(gw), max(gw))\n            if maxgw - mingw > 1:\n                for w in range(mingw + 1, maxgw):\n                    if shift[w] == max_shift_gw:\n                        shift[w] += 1\n        if len(gw) > 2:\n            raise NotImplementedError('Drawing circuits is not implemented for gates working on more than 2 qubits.')\n    return shift, max(shift.values())",
        "sha1": "da8f77002e5dc2d1ce30e219e7b09b15b00dea31",
        "id": 311615
    },
    {
        "content": "def compress_str(s):\n    \"\"\"\n    Compress a string by replacing consecutive duplicate characters with a count\n    of the number of characters followed by the character.\n    \"\"\"\n    if not s:\n        return s\n    compressed_str = []\n    count = 1\n    for i in range(1, len(s)):\n        if s[i] == s[i-1]:\n            count += 1\n        else:\n            compressed_str.append(str(count))\n            compressed_str.append(s[i-1])\n            count = 1\n    compressed_str.append(str(count))\n    compressed_str.append(s[-1])\n    return ''.join(compressed_str)",
        "sha1": "6e7d6da2244eef331b7e4f68cc31c91138dd4801",
        "id": 454067
    },
    {
        "content": "def is_integer(variable):\n    \"\"\"Checks if a variable is an integer value\"\"\"\n    return type(variable) == int",
        "sha1": "d8f13deab6aea7fbd3b2b546a43284ae5b787f15",
        "id": 416293
    },
    {
        "content": "def construct_path(u,v,discovered):\n    \"\"\"Use the discovered dictionary from \n    depth-first search to reconstruct \n    a path from node u to node v.\n    \"\"\"\n    path = []\n    # Deal with empty case\n    if v not in discovered:\n        return path\n    \n    # build list of edges \n    # that connect v to u,\n    # then reverse it.\n    path.append(v)\n    walk = v\n    while walk is not u:\n        e = discovered[walk]\n        parent = e.opposite(walk)\n        path.append(parent)\n        walk = parent\n    path.reverse()\n    return path",
        "sha1": "049fd09509765c64ff347f33fffda4719e8d34eb",
        "id": 665201
    },
    {
        "content": "from typing import Sequence\n\n\ndef nonStringSequence(obj):\n    \"\"\"\n    Returns: (bool) True if obj is non-string sequence, False otherwise\n    \"\"\"\n    return (not isinstance(obj, (str, bytes)) and isinstance(obj, Sequence) )",
        "sha1": "67a0e243755843442aada2520e301908f20d4779",
        "id": 336367
    },
    {
        "content": "import re\n\n\ndef _find_streams(text):\n    \"\"\"Finds data streams in text, returns a list of strings containing\n    the stream contents\"\"\"\n    re_stream = re.compile(r\"<< /Length \\d+ >>\\n(stream.*?endstream)\", re.DOTALL)\n    streams = []\n    for m in re_stream.finditer(text):\n        streams.append(text[m.start(1):m.end(1)])\n    return streams",
        "sha1": "37f011276d4ca2eeeb03927910b2d494519cd17e",
        "id": 690701
    },
    {
        "content": "def resize_image(image, percent):\n    \"\"\" Resize an image to a scale amount.\n\n    image: (PIL.Image) The image to scale.\n\n    percent: (float) The percentage of the original size to scale to.\n\n    Returns the scaled image.\n    \"\"\"\n    if percent == 100:\n        return image\n    prop = percent / 100.0\n    (width, height) = image.size\n    return image.resize( (int(width * prop), int(height * prop)) )",
        "sha1": "f222c998559b7ed56d5599402c2fa6f74efe85ae",
        "id": 390358
    },
    {
        "content": "from typing import List\nfrom typing import Any\n\n\ndef get_combinations(*args: List[Any]) -> List[List]:\n    \"\"\"Takes K lists as arguments and returns Cartesian product of them.\n\n    Cartesian product means all possible lists of K items where the first\n    element is from the first list, the second is from the second and so one.\n\n    Returns:\n        All possible combinations of items from function's arguments.\n\n    \"\"\"\n    result: List[List] = [[]]\n    for list_n in args:\n        result = [old_item + [new_item] for old_item in result for new_item in list_n]\n    return result",
        "sha1": "db3d2f48e650e647cec79d7bab0b6e555238cb3a",
        "id": 30169
    },
    {
        "content": "import hashlib\n\n\ndef md5_hash_file(fh):\n    \"\"\"Return the md5 hash of the given file-object\"\"\"\n    md5 = hashlib.md5()\n    while True:\n        data = fh.read(8192)\n        if not data:\n            break\n        md5.update(data)\n    return md5.hexdigest()",
        "sha1": "f572ec27add8024e5fa8b9a82b5d694905e4d0f8",
        "id": 708390
    },
    {
        "content": "def _calc_range_uncorrected_beta(beta, range_squared):\n    \"\"\"Calculates range uncorrected beta.\n\n    Args:\n        beta (ndarray): 2D attenuated backscatter.\n        range_squared (ndarray): 1D altitude vector (km), squared.\n\n    Returns:\n        ndarray: 2D range uncorrected beta.\n\n    \"\"\"\n    return beta / range_squared",
        "sha1": "36447beae3ba3cb1b88133926df6d0e679409d87",
        "id": 202906
    },
    {
        "content": "import operator\n\n\ndef output_SGML(conll_tokens, markstart_dict, markend_dict):\n\t\"\"\"\n\tOutputs analysis results as CWB SGML (with nesting), one token per line and markables in <referent> tags\n\n\t:param conll_tokens: List of all processed ParsedToken objects in the document\n\t:param markstart_dict: Dictionary from markable starting token ids to Markable objects\n\t:param markend_dict: Dictionary from markable ending token ids to Markable objects\n\t:return: serialized SGML\n\t\"\"\"\n\n\toutput_string = \"\"\n\tfor out_tok in conll_tokens:\n\t\tif int(out_tok.id) in markstart_dict:\n\t\t\tfor out_mark in sorted(markstart_dict[int(out_tok.id)], key=operator.attrgetter('end'), reverse=True):\n\t\t\t\toutput_string += '<referent id=\"' + str(out_mark.id) + '\" entity=\"' + out_mark.entity + '\" group=\"' + str(out_mark.group)\n\t\t\t\tif not out_mark.antecedent == \"none\":\n\t\t\t\t\toutput_string += '\" antecedent=\"' + str(out_mark.antecedent.id) + '\" type=\"' + out_mark.coref_type\n\t\t\t\toutput_string += '\">\\n'\n\t\tif int(out_tok.id) > 0:\n\t\t\toutput_string += out_tok.text + \"\\n\"\n\t\tif int(out_tok.id) in markend_dict:\n\t\t\tfor out_mark in markend_dict[int(out_tok.id)]:\n\t\t\t\toutput_string += \"</referent>\\n\"\n\n\treturn output_string",
        "sha1": "538e6658fde06b37fba5dcd8cbdb61afa48aad77",
        "id": 178756
    },
    {
        "content": "def mz_validator(number: str, nr_format: str = \"international\") -> list:\n    \"\"\" Verify if the given number is valid\n\n    Args:\n        number (str): The phone number to be validated\n        nr_format (str, optional): The number has international identification code. Defaults to international.\n\n    Returns:\n        list: return a list containing all errors identified. If the list is empty it means the number is valid.\n    \"\"\"\n    number = number.strip()\n    network_codes = [\"82\", \"83\", \"84\", \"85\", \"86\", \"87\"]\n    inter_format = True if nr_format == \"international\" else False\n    number_size = 12 if inter_format else 9\n    error_messages = []\n\n    error_type = {\n        \"size\": f\"The phone number must be :attr: characters long. currently with {len(number)}.\",\n        \"format\": \"The phone number must start with 258.\",\n        \"network_code\": f\"Invalid network code. Valid codes: {network_codes}.\"\n    }\n\n    if len(number) != number_size:\n        error_messages.append(\n            error_type[\"size\"].replace(\":attr:\", str(number_size)))\n\n    if inter_format:\n        number = number.replace(\"+\", \"\")\n\n        if \"258\" not in number[:3]:\n            error_messages.append(error_type[\"format\"])\n\n        if number[3:5] not in network_codes:\n            error_messages.append(error_type[\"network_code\"])\n    else:\n        if number[:2] not in network_codes:\n            error_messages.append(error_type[\"network_code\"])\n\n    return error_messages",
        "sha1": "4e89bcadb805b644d852e3a45190a0685f85f2ad",
        "id": 207785
    },
    {
        "content": "def identity(value):\n    \"\"\"\n    Node returning the input value.\n    \"\"\"\n    return value",
        "sha1": "e1af1346b11bc36a4426ad3c557dc01045657de8",
        "id": 703616
    },
    {
        "content": "def point_in_polygon(x, y, polygon):\n    \"\"\" Check if a point is inside a polygon\n        - x,y - Coordinates of the point\n        - polygon - List of the vertices of the polygon [(x1, x2), (x2, y2), ..., (xn, yn)]\"\"\"\n    i = 0\n    j = len(polygon) - 1\n    res = False\n    for i in range(len(polygon)):\n        if (polygon[i][1] < y and polygon[j][1] >= y) \\\n                or (polygon[j][1] < y and polygon[i][1] >= y):\n            if polygon[i][0] + (y - polygon[i][1]) / (polygon[j][1] - polygon[i][1]) * (\n                    polygon[j][0] - polygon[i][0]) < x:\n                res = not res\n        j = i\n    return res",
        "sha1": "5e52fead34f6c0cc768dea5006edba153a505655",
        "id": 219009
    },
    {
        "content": "def _add_dot_prefix(meta_value):\n    \"\"\"Add dot as the prefix if missing\"\"\"\n    if meta_value and not meta_value.startswith(\".\"):\n        return \".{0}\".format(meta_value)\n    return meta_value",
        "sha1": "5d22ca3a7e4fa9e0416f81f1dbeed7f5f92315c0",
        "id": 308599
    },
    {
        "content": "def is_input_op(node):\n    \"\"\"Return true when the node is the input of the graph.\"\"\"\n    return node.WhichOneof(\"op_type\") == \"input_conf\"",
        "sha1": "294f5f1b785bb4ac26be8f7d9cacde2a26e73ce6",
        "id": 373155
    },
    {
        "content": "def dodo_existing(tmpdir):\n    \"\"\"\n    Set up a project with a DODO file\n    \"\"\"\n    prjdir = tmpdir.join('projdir')\n    prjdir.ensure(dir=True)\n    prjdir.join('.project').ensure()\n    dofile = prjdir.join('DODO')\n    dofile.write(\"\\n - this is a task\\n\")\n    return (prjdir, dofile)",
        "sha1": "66bf3a3162091b957adfcac0d7f6d78690632477",
        "id": 647611
    },
    {
        "content": "def extract_cert_secret(key, device):\n    \"\"\"\n    Check if device is marked as secret.\n\n    :param key: Device entry.\n    :type key: dict\n\n    :param device: HWID, FCCID or name of device.\n    :param device: str\n    \"\"\"\n    not_secret = device in key['name'] and 'secret' not in key\n    return not_secret",
        "sha1": "734aed594a350ecfce85738a116d18450f43b691",
        "id": 404416
    },
    {
        "content": "def is_set_policy(policy):\n    \"\"\" Checks if the current policy action is a set action. \"\"\"\n\n    return 'setIamPolicy' in policy['action'] and not 'metadata' in policy",
        "sha1": "33bf358ec78a1ddf023b2fb890f4b067b6263273",
        "id": 57216
    },
    {
        "content": "def rgb012rgb(r, g, b):\n    \"\"\" Convert between RGB [0-1] and RGB [0-255] \"\"\"\n    return tuple([ int(round(float(i)*255.)) for i in [r,g,b] ])",
        "sha1": "64bfb6dfd61069516a1ee6e7070e8444d5afd0be",
        "id": 436327
    },
    {
        "content": "def group_definitions_by_pos(context: dict):\n    \"\"\"\n    Return a dict mapping from part of speech to a list of all the definitions with that part of speech.\n    \"\"\"\n    definitions = context[\"definitions\"]\n    parts_of_speech = set(definition[\"part_of_speech\"] for definition in definitions)\n\n    return {pos.lower(): [d for d in definitions if d[\"part_of_speech\"] == pos] for pos in parts_of_speech}",
        "sha1": "e06eb30ba8defae84bbd71d27bcb7f9ec29f9edd",
        "id": 582461
    },
    {
        "content": "def dependency_extracting(list_gate_qubits, count_program_qubit: int):\n    \"\"\"Extract dependency relations between the gates.\n    If two gates g_1 and g_2 both acts on a qubit *and there is no gate\n    between g_1 and g_2 that act on this qubit*, we then say that\n    g2 depends on g1, which means that (1,2) will be in dependency list.\n\n    Args:\n        list_gate_qubits: a list of gates in OLSQ IR\n        count_program_qubit: the number of logical/program qubit\n    \n    Returns:\n        list_dependency: a list of dependency between the gates\n    \"\"\"\n\n    list_dependency = []\n    list_last_gate = [-1 for i in range(count_program_qubit)]\n    # list_last_gate records the latest gate that acts on each qubit.\n    # When we sweep through all the gates, this list is updated and the\n    # dependencies induced by the update is noted.\n    for i, qubits in enumerate(list_gate_qubits):\n        \n        if list_last_gate[qubits[0]] >= 0:\n            list_dependency.append((list_last_gate[qubits[0]], i))\n        list_last_gate[qubits[0]] = i\n\n        if len(qubits) == 2:\n            if list_last_gate[qubits[1]] >= 0:\n                list_dependency.append((list_last_gate[qubits[1]], i))\n            list_last_gate[qubits[1]] = i\n\n    return tuple(list_dependency)",
        "sha1": "8e0cc44fec8c1b767868c9af24b7354f5842c981",
        "id": 609341
    },
    {
        "content": "def getSpan(W):\n    \"\"\"Return the span of W\n\n    sp(W) = max W(s) - min W(s)\n\n    \"\"\"\n    return W.max() - W.min()",
        "sha1": "27368e700e986f8352a7c2dc65cc5ca59a3105f3",
        "id": 622637
    },
    {
        "content": "import re\n\n\ndef quoted_split(text: str, delimiter: str) -> list[str]:\n    \"\"\"Split a string by 'delimiter', ignoring it if it is inside quotations.\n\n    Returns a list of strings with results.\n    Consecutive delimiters are returned as empty string.\n    \"\"\"\n\n    args = []\n    # gets first part\n    re_first = re.compile(\n        r\"^(?:\\s*([\\\"\\'])(?:(?!\\1).|\\\\\\1)*(?<!\\\\)\\1\\s*|[^\" + delimiter + r\"]?)+\"\n    )\n\n    while len(text):\n        match = re.search(re_first, text)\n\n        if not match:\n            break\n        text = text[match.end(0) + 1 :]\n\n        # remove spaces\n        arg = match[0].strip()\n\n        args.append(arg)\n\n    return args",
        "sha1": "8d0b82897d503b887ff42656f8227dc39eabb3f7",
        "id": 388912
    },
    {
        "content": "def convert_delay_number_to_delay_time(delay_num: int) -> float:\n    \"\"\" Converts delay number into delay time for sleep function. \"\"\"\n    return round((delay_num / 100) * (0.5 + delay_num/2), 2)",
        "sha1": "d250c625318bc5fbc0bff93d78687f95aa1bd4cb",
        "id": 528411
    },
    {
        "content": "def get_line_from_two_points(p1, p2):\n  \"\"\"\n  Returns a function which takes\n  an x-coordinate and returns the\n  corresponding y-coordinate on the\n  line defined by the points p1, p2\n\n  \"\"\"\n  slope = p2[1] - p1[1]\n  slope /= p2[0] - p1[0]\n  return lambda x: (slope * (x - p1[0])) + p1[1]",
        "sha1": "81acd90af283b3b09e57a3e8d10a28df44a88af3",
        "id": 86950
    },
    {
        "content": "import struct\n\n\ndef mac2str(mac):\n    \"\"\"Converts mac address to string .\n   \n        Args:\n            mac: 6 bytes mac address\n    \n        Returns:\n            readable string \n            \n    \"\"\"\n    return '%02x:%02x:%02x:%02x:%02x:%02x'%tuple(int(x) for x in struct.unpack('BBBBBB', mac))",
        "sha1": "01a51ef8a787d9a4e2ee0262698ef481bcadb6ff",
        "id": 66114
    },
    {
        "content": "import sqlite3\n\n\ndef get_directories_for_department(c: sqlite3.Cursor, department: int) -> list:\n    \"\"\"Gets the list of directories for a department.\n\n    :param c: An SQLite3 cursor.\n    :param department: The department number.\n    :return: The list of directories. Empty list if there is no directory for this department.\n    :rtype: list\n    \"\"\"\n\n    c.execute(\"\"\"\n    SELECT dpt.id as dpt_id, dpt.name as dpt_name, dpt.number as dpt_number,\n    dir.id as dir_id, dir.name as dir_name, dir.acronyme as dir_acr\n    FROM\n    (\n        SELECT id, name, number\n        FROM departments\n        WHERE number=?\n        UNION\n        SELECT id, name, number\n        FROM departments\n        WHERE parent_number=?\n    ) dpt\n    JOIN departments_directories dd ON dd.department_id = dpt.id\n    JOIN directories dir ON dd.directory_id = dir.id\n    \"\"\", (department, department))\n\n    return c.fetchall()",
        "sha1": "a33242394d85b7774cfe68f7fb18e97907edf8eb",
        "id": 396906
    },
    {
        "content": "def prob(A):\n    \"\"\"Computes the probability of a proposition, A.\n    \n    A: Boolean series\n    \n    returns: probability\n    \"\"\"\n    return A.mean()",
        "sha1": "6695c4fab3af436560282611c278ee7f08efa410",
        "id": 124991
    },
    {
        "content": "def filter_probes_by_nan_and_sd(data_df, probe_frac_cutoff, probe_sd_cutoff):\n    \"\"\" Filter out probes whose fraction of samples measured is less than probe_frac_cutoff.\n    Also remove probes with standard deviation higher than probe_sd_cutoff.\n\n    Args:\n        data_df (pandas df)\n        probe_frac_cutoff (float b/w 0 and 1)\n        probe_sd_cutoff (float)\n    Returns:\n        out_df (pandas df): potentially smaller than original df\n    \"\"\"\n    # Number of NaNs per probe\n    num_nans = data_df.isnull().sum(axis=1)\n\n    # Number of samples\n    num_samples = data_df.shape[1]\n\n    # Fraction non-NaN per probe\n    frac_non_nans_per_probe = 1 - num_nans/num_samples\n\n    # Probe standard deviations\n    probe_sds = data_df.std(axis=1)\n\n    # Only return probes with more non-NaN data than probe_frac_cutoff\n    # and lower sd than the cutoff\n    probes_to_keep = ((frac_non_nans_per_probe > probe_frac_cutoff) &\n                      (probe_sds < probe_sd_cutoff))\n    out_df = data_df.loc[probes_to_keep, :]\n    assert not out_df.empty, (\n        \"All probes were filtered out. Try reducing the NaN threshold and/or SD cutoff.\")\n    return out_df",
        "sha1": "1268244d4975be7bcf114b94d33e567fb7cff1b5",
        "id": 32741
    },
    {
        "content": "def paste_js(clipboard):\n    \"\"\"Paste the string ``clipboard`` into the selected text of the focused\n    element in the DOM using JavaScript/jQuery.\n    \"\"\"\n    return (\n        f\"var focused = document.activeElement;\\n\"\n        f\"var start = focused.selectionStart;\\n\"\n        f\"var end = focused.selectionEnd;\\n\"\n        f\"var val = focused.value;\\n\"\n        f\"var new_val = val.slice(0, start) + `{clipboard}` + val.slice(end, val.length);\\n\"\n        f\"focused.value = new_val;\\n\"\n        f\"var cursorPos = start + `{clipboard}`.length;\\n\"\n        f\"focused.setSelectionRange(cursorPos, cursorPos);\"\n    )",
        "sha1": "7bdf79308004698f1fd4d6a44af5d958ee78677c",
        "id": 117213
    },
    {
        "content": "def _GetEnvironmentVars(benchmark_spec):\n  \"\"\"Return a string containing HPCG-related environment variables.\n\n  Args:\n    benchmark_spec: benchmark spec\n\n  Returns:\n    string of environment variables\n  \"\"\"\n  return ' '.join([\n      'NUM_GPUS=%s' % benchmark_spec.total_gpus,\n      'OMP_NUM_THREADS=%s' % benchmark_spec.cpus_per_rank\n  ])",
        "sha1": "60d968b924376f738f0615638e50c71b0c66c09d",
        "id": 575679
    },
    {
        "content": "def tf_binary(dtm):\n    \"\"\"\n    Transform raw count document-term-matrix `dtm` to binary term frequency matrix. This matrix contains 1 whenever\n    a term occurred in a document, else 0.\n\n    :param dtm: (sparse) document-term-matrix of size NxM (N docs, M is vocab size) with raw term counts.\n    :return: (sparse) binary term frequency matrix of type integer of size NxM\n    \"\"\"\n    if dtm.ndim != 2:\n        raise ValueError('`dtm` must be a 2D array/matrix')\n\n    return (dtm > 0).astype(int)",
        "sha1": "7df0af552f70e8e38d86b01de6871d6db8b79b35",
        "id": 686027
    },
    {
        "content": "import struct\n\n\ndef get_real(bytearray_: bytearray, byte_index: int) -> float:\n    \"\"\"Get real value.\n\n    Notes:\n        Datatype `real` is represented in 4 bytes in the PLC.\n        The packed representation uses the `IEEE 754 binary32`.\n\n    Args:\n        bytearray_: buffer to read from.\n        byte_index: byte index to reading from.\n\n    Returns:\n        Real value.\n\n    Examples:\n        >>> data = bytearray(b'B\\\\xf6\\\\xa4Z')\n        >>> snap7.util.get_real(data, 0)\n            123.32099914550781\n    \"\"\"\n    x = bytearray_[byte_index:byte_index + 4]\n    real = struct.unpack('>f', struct.pack('4B', *x))[0]\n    return real",
        "sha1": "2937f54eba32bba8080eb22919bbc29858c7dac0",
        "id": 560018
    },
    {
        "content": "def get_bonds(mol):\n    \"\"\"\n    Identify the bonds in a molecule\n    \n    Input:      - mol       OBMol object of the molecule\n    \n    Outputs:    - atoms     List of atoms in the molecule\n                - bonds     List of bonded atoms for each atom in the molecule\n    \"\"\"\n    \n    # Get molfile of the molecule\n    pp = mol.write(\"mol\")\n    lines = pp.split(\"\\n\")\n    \n    # Get number of atoms\n    n_atoms = len(mol.atoms)\n    # Initialize arrays of neighbours\n    bonds = [[] for _ in mol.atoms]\n    # Initialize array of atoms\n    atoms = []\n    \n    # Parse the atom block\n    for l in lines[4:n_atoms + 4]:\n        atoms.append(l.split()[3])\n    \n    # Parse the bond block\n    for l in lines[n_atoms + 4:]:\n    \n        # Detect end of file\n        if \"END\" in l or len(l.split()) != 7:\n            break\n        \n        bond = [int(li)-1 for li in l.split()[:2]]\n        \n        # Update the list of bonds\n        bonds[bond[0]].append(bond[1])\n        bonds[bond[1]].append(bond[0])\n\n    return atoms, bonds",
        "sha1": "1a01dbc9c1e0a2cfd8dc12ae6dd757ec6644f1d7",
        "id": 500535
    },
    {
        "content": "def children_of(parents):\n    \"\"\"Return list of children per node given list of parents\n\n    Parameters\n    ----------\n        parents: 1D array of list, length (K,) of ints.\n            parent[j] is index of parent node of node j.\n\n    Returns\n    -------\n        children: list, length (K,).\n            children[k] contains variable length list of children node\n            indices. May be empty if node has no children.\n    \"\"\"\n\n    K = len(parents)\n\n    children = []\n    for k in range(K):\n        children.append([])\n        for j in range(k+1, K):\n            if parents[j] == k:\n                children[k].append(j)\n\n    return children",
        "sha1": "8701ec857a5c791720464342f1ad30909b73e953",
        "id": 369438
    },
    {
        "content": "from typing import Dict\n\n\ndef get_discord_webhook_url(configs: Dict) -> str:\n    \"\"\"Returns the discord portfolios webhook\n\n    Args:\n        configs (Dict): env file contents\n\n    Returns:\n        str: discord webhook url\n    \"\"\"\n    return configs[\"PORTFOLIOS_DISCORD_WEBHOOK_URL\"]",
        "sha1": "aeb7a5dde15e7c7e148ace6e4a72e6a3692f8896",
        "id": 404502
    },
    {
        "content": "import tempfile\n\n\ndef serialize(figure: tempfile._TemporaryFileWrapper) -> bytes:\n    \"\"\"\n    Serialize a figure that has been rendered\n\n    Parameters\n    ----------\n    figure\n        figure\n    \"\"\"\n    with open(figure.name, \"rb\") as f:\n        return f.read()",
        "sha1": "d8e10d2149bc25a53ebba7edf19213bd10997c8e",
        "id": 101071
    },
    {
        "content": "def get_rebalancing_flow(tnet):\n    \"\"\"\n    get rebalancing flow in a supergraph\n\n    Parameters\n    ----------\n\n    tnet: transportation network object\n\n    Returns\n    -------\n    float\n\n    \"\"\"\n    return sum([(tnet.G_supergraph[i][j]['flow']-tnet.G_supergraph[i][j]['flowNoRebalancing'])*tnet.G_supergraph[i][j]['length'] for i,j in tnet.G.edges()])",
        "sha1": "c66c2fd4a68d8ff948f5891b97c599c19f00099b",
        "id": 174585
    },
    {
        "content": "def absolute_max(array):\n    \"\"\"\n    Returns absolute max value of a array.\n    :param array: the array.\n    :return: absolute max value\n\n    >>> absolute_max([1, -2, 5, -8, 7])\n    -8\n    >>> absolute_max([1, -2, 3, -4, 5])\n    5\n    \"\"\"\n    return max(array, key=abs)",
        "sha1": "e01246883d83becadcb55f15917be02b8d8a9876",
        "id": 53534
    },
    {
        "content": "def euclid_exd(a, b):\n    \"\"\"\n    Return a tuple (u, v, d); they are the greatest common divisor d\n    of two integers a and b and u, v such that d = a * u + b * v.\n    \"\"\"\n    if not isinstance(a, int) or not isinstance(b, int):\n        raise TypeError\n    u = 1\n    d = a\n    if b == 0:\n        v = 0\n        return (u, v, d)\n    else:\n        v_1 = 0\n        v_3 = b\n\n        while 1:\n            if v_3 == 0:\n                v = (d - a*u) // b\n                return (u, v, d)\n            q = d // v_3\n            t_3 = d % v_3\n            t_1 = u - q*v_1\n            u = v_1\n            d = v_3\n            v_1 = t_1\n            v_3 = t_3",
        "sha1": "13980578084149f7df887c876f996a702c584bdd",
        "id": 563047
    },
    {
        "content": "import hashlib\n\n\ndef digest(binary_data: bytes) -> str:\n    \"\"\" Return a SHA256 message digest for the given bytes string\n\n    :param binary_data: The binary data to digest\n    :return: A length-64 hex digest of the binary data\n    \"\"\"\n    m = hashlib.sha256()\n    m.update(binary_data)\n    return m.hexdigest()",
        "sha1": "c37403e2e47cd49fbfddbc33c09444186e8b8a9e",
        "id": 385960
    },
    {
        "content": "import re\n\n\ndef format_companies(company):\n    \"\"\"\n    Format the selected company so that the API can find relevant articles.\n    \"\"\"\n    # Remove junk at the end of the names\n    if company[-6:] == \", Inc.\":\n        return re.sub(r' ', \"-\", company[:-6])\n\n    if company[-5:] == \", Inc\":\n        return re.sub(r' ', \"-\", company[:-5])\n\n    if company[-5:] == \" Inc.\":\n        return re.sub(r' ', \"-\", company[:-5])\n\n    return re.sub(r' ', \"-\", company)",
        "sha1": "76b1e7a108c982f9bc17dc9e3299a20e3887829d",
        "id": 631844
    },
    {
        "content": "def build_node_set(node, s=None):\n    \"\"\"Build a set of all the nodes in a streamz graph\n\n    Parameters\n    ----------\n    node : Stream\n        The node to use as a starting point for building the set\n    s : set or None\n        The set to put the nodes into. If None return a new set full of nodes\n\n    Returns\n    -------\n    s : set\n        The set of nodes in the graph\n\n    \"\"\"\n    if s is None:\n        s = set()\n    if node is None or (\n        node in s\n        and all(n in s for n in node.upstreams)\n        and all(n in s for n in node.downstreams)\n    ):\n        return\n    new_nodes = {n for n in node.downstreams}\n    new_nodes.update(node.upstreams)\n    new_nodes.add(node)\n    s.update(new_nodes)\n    [build_node_set(n, s) for n in list(new_nodes)]\n    return s",
        "sha1": "0d4912ec9874521d1d46b80bfefe01246e72c62b",
        "id": 186840
    },
    {
        "content": "def parse_entry(entry):\n    \"\"\"\n    Parse a single entry in JSON format, mostly flattening it.\n    \"\"\"\n\n    # Collect info for the root table\n    root = {\n        \"root_id\": entry[\"EGY_ROOT__ID\"],\n        \"root_form\": entry[\"EGY_ROOT__form\"],\n        \"root_meaning_en\": entry[\"EGY_ROOT__meaning_en\"],\n    }\n    if entry[\"EGY_ROOT__TLA_ROOT\"]:\n        tla_root = list(entry[\"EGY_ROOT__TLA_ROOT\"].values())[0]\n        root[\"tla_root_id\"] = tla_root[\"TLA_ROOT__ID\"]\n        root[\"tla_root_form\"] = tla_root[\"TLA_ROOT__form\"]\n        root[\"tla_root_meaning_en\"] = tla_root[\"TLA_ROOT__meaning_en\"]\n        root[\"tla_root_meaning_de\"] = tla_root[\"TLA_ROOT__meaning_de\"]\n    else:\n        root[\"tla_root_id\"] = \"\"\n        root[\"tla_root_form\"] = \"\"\n        root[\"tla_root_meaning_en\"] = \"\"\n        root[\"tla_root_meaning_de\"] = \"\"\n\n    # Collect info for the CCL table\n    ccl = []\n    for form in entry[\"CCL__forms\"].values():\n        ccl.append(form)\n\n    # Collect info for the TLA table\n    tla = []\n    for form in entry[\"TLA__forms\"].values():\n        tla.append(form)\n\n    # Collect matches\n    matches = []\n    for match in entry[\"MATCHES\"].values():\n        matches.append(match)\n\n    return root, ccl, tla, matches",
        "sha1": "2d27293960fc7ae260f9ba1f8293b8d9a120fcd3",
        "id": 283246
    },
    {
        "content": "def get_api_name(resp) -> str:\n    \"\"\"Get the server API name.\"\"\"\n    try:\n        api_name = resp.context['api_name']\n    except KeyError:\n        api_name = resp.context['api_name'] = \"api\"\n    return api_name",
        "sha1": "03658702264c3c03d9536929f8cb0b7c308bc448",
        "id": 469918
    },
    {
        "content": "def extractDidParts(did):\n    \"\"\"\n    Parses and returns a tuple containing the prefix method and key string contained in the supplied did string.\n    If the supplied string does not fit the pattern pre:method:keystr a ValueError is raised.\n\n    :param did: W3C DID string\n\n    :return: (pre, method, key string) a tuple containing the did parts.\n    \"\"\"\n    try:  # correct did format  pre:method:keystr\n        pre, meth, keystr = did.split(\":\")\n    except ValueError as ex:\n        raise ValueError(\"Malformed DID value\")\n\n    if not pre or not meth or not keystr:  # check for empty values\n        raise ValueError(\"Malformed DID value\")\n\n    return pre, meth, keystr",
        "sha1": "e106f197321e0686a5091ba03681f1e2a9752ba4",
        "id": 549985
    },
    {
        "content": "from typing import IO\nfrom typing import Dict\nfrom typing import List\nimport csv\n\n\ndef parse_package_to_repos_file(input_file: IO[str]) -> Dict[str, List[str]]:\n    \"\"\"Parse CSV file mapping package names to repositories.\n\n    :param IO[str] input_file: CSV file to parse.\n        The file needs to contain a column `package` and a column\n        `all_repos`. `all_repos` contains a comma separated string of\n        Github repositories that include an AndroidManifest.xml file for\n        package name in column `package`.\n    :returns Dict[str, List[str]]: A mapping from package name to\n        list of repository names.\n    \"\"\"\n    return {\n        row['package']: row['all_repos'].split(',')\n        for row in csv.DictReader(input_file)\n        }",
        "sha1": "25b36a3d965aa00fa6428fc9045ed5c99bb8e253",
        "id": 104316
    },
    {
        "content": "def partition(lst, size):\n    \"\"\"Partition list @lst into eveni-sized lists of size @size.\"\"\"\n    return [lst[i::size] for i in range(size)]",
        "sha1": "af7071a5aac36a51f449f153df145d9218808a4a",
        "id": 4808
    },
    {
        "content": "def f(N):\n    \"\"\"\n    1/(N^2 -1)\n    \"\"\"\n    # tmp = np.dtype(np.float32) # \u5355\u7cbe\u5ea6\n    tmp = 1/(pow(N,2) - 1)\n\n    return tmp",
        "sha1": "9cee33913b2044eca704eeee5b4edacdeb25102c",
        "id": 526665
    },
    {
        "content": "def _edge_func(G):\n    \"\"\"Returns the edges from G, handling keys for multigraphs as necessary.\n\n    \"\"\"\n    if G.is_multigraph():\n\n        def get_edges(nbunch=None):\n            return G.edges(nbunch, keys=True)\n\n    else:\n\n        def get_edges(nbunch=None):\n            return G.edges(nbunch)\n\n    return get_edges",
        "sha1": "ca488c5bde3b193ec308a99f66eb184bee6acd5c",
        "id": 111676
    },
    {
        "content": "def _get_ftrack_secure_key(hostname, key):\n    \"\"\"Secure item key for entered hostname.\"\"\"\n    return \"/\".join((\"ftrack\", hostname, key))",
        "sha1": "7513cd5807b1c1697e6c055de40f7873b2bf9d0a",
        "id": 39125
    },
    {
        "content": "def extract_helm_from_json(input_json):\n    \"\"\"Extracts the HELM strings out of a JSON array.\n\n    :param input_json: JSON array of Peptide objects\n    :return: output_helm: string (extracted HELM strings separated by a newline)\n    \"\"\"\n    output_helm = \"\"\n    for peptide in input_json:\n        if len(output_helm) > 0:\n            output_helm += \"\\n\"\n        output_helm += peptide[\"HELM\"]\n    return output_helm",
        "sha1": "8bad6bbc46f0c535f76cfb144d0d929a4d4b37c5",
        "id": 561806
    },
    {
        "content": "def trim_suffix(text, suffix):\n    \"\"\"Strip a suffix from text, if it appears (otherwise return text unchanged)\"\"\"\n    if not text.endswith(suffix):\n        return text\n    return text[: len(text) - len(suffix)]",
        "sha1": "8a6cb8f80a62be17897b16770e1a0fc68c16faef",
        "id": 600661
    },
    {
        "content": "def unique_values_from_query(query_result):\n    \"\"\"Simplify Athena query results into a set of values.\n\n    Useful for listing tables, partitions, databases, enable_metrics\n\n    Args:\n        query_result (dict): The result of run_athena_query\n\n    Returns:\n        set: Unique values from the query result\n    \"\"\"\n    return {\n        value\n        for row in query_result['ResultSet']['Rows'] for result in row['Data']\n        for value in result.values()\n    }",
        "sha1": "31dd32fdf548976694d50c8466c11c3a42f7b9cc",
        "id": 625817
    },
    {
        "content": "def green(msg: str) -> str:\n    \"\"\"Return green string in rich markdown\"\"\"\n    return f\"[green]{msg}[/green]\"",
        "sha1": "4f0d9271e2923a0c275bf46819b07f00cc4e8cb3",
        "id": 478659
    },
    {
        "content": "def mean(lyst):\n    \"\"\"Returns the mean of a list of numbers.\"\"\"\n    sum = 0\n    for number in lyst:\n        sum += number\n    if len(lyst) == 0:\n        return 0\n    else:\n        return sum / len(lyst)",
        "sha1": "376fb0bb0335228eb963d31a779ee08017f66b56",
        "id": 531138
    },
    {
        "content": "from typing import Any\n\n\ndef escape(val: Any) -> str:\n    \"\"\"Escapes a primitive value for inclusion within a Daikon file.\"\"\"\n    if isinstance(val, str):\n        val = val.replace('\\\\', '\\\\\\\\').replace('\"', '\\\\\"')\n        return f'\"{val}\"'\n    if isinstance(val, bool):\n        return 'true' if val else 'false'\n    return val",
        "sha1": "0da00d4e3096cd30d4d3d042ce364d914affbbbc",
        "id": 306350
    },
    {
        "content": "def color_str(color, raw_str):\n    \"\"\"Format a string with color.\n\n    :param color: a color name, can be r, g, b or y\n    :param raw_str: the string to be formatted\n    :returns: a colorful string\n    \"\"\"\n    if color == 'r':\n        fore = 31\n    elif color == 'g':\n        fore = 32\n    elif color == 'b':\n        fore = 36\n    elif color == 'y':\n        fore = 33\n    else:\n        fore = 37\n    color = \"\\x1B[%d;%dm\" % (1, fore)\n    return \"%s%s\\x1B[0m\" % (color, raw_str)",
        "sha1": "674f1c9e1bb0e874d4758d078d21a36b3d7ecca8",
        "id": 360361
    },
    {
        "content": "def sbs_annotation_converter(x: str) -> str:\n    \"\"\"\n    Eithers swaps from word -> arrow format for SBS or vice versa.\n        word: (REF)(ALT)(LEFT)(RIGHT)\n        arrow: (LEFT)[(REF)>(ALT)](RIGHT)\n    \"\"\"\n    if '>' in x:\n        return x[2]+x[4]+x[0]+x[6]\n    else:\n        return x[2]+'['+x[0]+'>'+x[1]+']'+x[3]",
        "sha1": "462eb604687354150fb6971ef4d1a633655bd8bb",
        "id": 658419
    },
    {
        "content": "import requests\n\n\ndef fetch_url(url):\n    \"\"\"Fetches the specified URL.\n\n    :param url: The URL to fetch\n    :type url: string\n    :returns: The response object\n    \"\"\"\n    return requests.get(url)",
        "sha1": "26198dbc4f7af306e7a09c86b59a7da1a4802241",
        "id": 794
    },
    {
        "content": "def _list_to_zeros(xs):\n    \"\"\"Takes a list-of-lists, with arbitrary nesting level;\n    returns a list-of-lists of the same shape but with every non-list\n    element replaced with zero.\"\"\"\n    if isinstance(xs, list):\n        return [_list_to_zeros(x) for x in xs]\n    return 0",
        "sha1": "ca42551cb10caf71e718f26732ccc1a0462fede7",
        "id": 397356
    },
    {
        "content": "def probability_sum(probability, token_frequency, n):\n    \"\"\"Assigns score to document based on summation of probabilities.\n    Args:\n        probability (float): Previously calculated probability.\n        token_frequency (float):  Number of appearances of token in text.\n        n (int): Length of text.\n    Returns:\n        probability_value (float): New caclculated probability.\n    \"\"\"\n    probability_value = probability+(token_frequency/n)\n    return probability_value",
        "sha1": "a86e8a66019e297805d4251789f71cd9140012ab",
        "id": 624967
    },
    {
        "content": "def best_rank(allele_scores):\n    \"\"\"\n    Determine the best rank score from the patient's alleles.\n    :param allele_scores: List of scores for patient's six alleles\n    :return: A best rank score\n    \"\"\"\n\n    # Return the minimum\n    return min(allele_scores)",
        "sha1": "10ef5c91ce5f2062ac8de819275765dcf22856d6",
        "id": 319792
    },
    {
        "content": "def _create_lat_lon_table_index(\n    lat_lon_table_info, seasons, viewer, root_dir, table_name\n):\n    \"\"\"\n    Create an index in the viewer that links the\n    individual htmls for the lat-lon table.\n    \"\"\"\n    viewer.add_page(f\"Table{table_name}\", seasons)\n    viewer.add_group(\"Summary Table\")\n    viewer.add_row(\"All variables\")\n\n    for s in seasons:\n        if s in lat_lon_table_info:\n            viewer.add_col(lat_lon_table_info[s][\"html_path\"], is_file=True, title=s)\n        else:\n            viewer.add_col(\"-----\", is_file=True, title=\"-----\")\n\n    url = viewer.generate_page()\n    return url",
        "sha1": "a9f9f016160f3064c27617c2c08533dce3668808",
        "id": 433900
    },
    {
        "content": "def Measurement_timeofday_method_diff(self,probe):\n    \"\"\"\n    Computes diff between time values and caches last time.\n    \"\"\"\n    delta = 0\n    \n    if hasattr(self,'_last'):\n        delta = probe.microsecondsSinceEpoch - self._last;\n        \n    self._last = probe.microsecondsSinceEpoch\n\n    return delta",
        "sha1": "2b4b4985249af7030de651d55d4a873b310417b1",
        "id": 75677
    },
    {
        "content": "def draw_dice(c, f, b, s, dot):\n    \"\"\"Return an ASCII art representation of a die roll.\n\n    c, f, b, & s are boolean arguments. This function returns a multi-line\n    string of the following form, where the letters in the diagram are either\n    filled if the corresponding argument is true, or empty if it is false.\n\n     -------\n    | b   f |\n    | s c s |\n    | f   b |\n     -------\n\n    The sides with 2 and 3 dots have 2 possible depictions due to rotation.\n    Either representation is acceptable.\n\n    This function uses Python syntax not yet covered in the course.\n\n    c, f, b, s -- booleans; whether to place dots in corresponding positions.\n    dot        -- A length-one string to use for a dot.\n    \"\"\"\n    assert len(dot) == 1, 'Dot must be a single symbol'\n    border = ' -------'\n    def draw(b):\n        return dot if b else ' '\n    c, f, b, s = map(draw, [c, f, b, s])\n    top = ' '.join(['|', b, ' ', f, '|'])\n    middle = ' '.join(['|', s, c, s, '|'])\n    bottom = ' '.join(['|', f, ' ', b, '|'])\n    return '\\n'.join([border, top, middle, bottom, border])",
        "sha1": "4576956f006173a5f24fcbb7bdd3c676bc5172be",
        "id": 425872
    },
    {
        "content": "from typing import List\n\n\ndef max_triangle_path(triangle: List[List[int]]) -> int:\n    \"\"\"Alters triangle and returns the maximum path sum from top to bottom.\n\n    A path is valid iff it consists of a number from each row of the triangle\n    such that each number is adjacent to those in the rows above and below it.\n    \"\"\"\n\n    num_rows = len(triangle)\n\n    # add maximum adjacent values from row above to each row\n    for i in range(1, num_rows):\n        for j in range(i + 1):\n            if j != 0 and j != i:\n                # two adjacent elements above; add maximal\n                triangle[i][j] += max(triangle[i-1][j-1], triangle[i-1][j])\n            elif j == 0:\n                # no adjacent element to left above; add right\n                triangle[i][j] += triangle[i - 1][j]\n            else:\n                # no adjacent element to right above; add left\n                triangle[i][j] += triangle[i - 1][j - 1]\n\n    # return maximal sum accumulated in last row of triangle\n    return max(triangle[-1])",
        "sha1": "e7e0e67331e3afd349937b6c703de883a6f66819",
        "id": 585175
    },
    {
        "content": "def create_influxdb_points_body(hostname, measurements, timestamp):\n    \"\"\"\n    :param str hostname: the hostname\n    :param dict measurements: a mapping of str measurement names to float values\n    :param timestamp: Unix timestamp in seconds\n    :type timestamp: :class:`datetime.datetime`\n    :return: a `list` of `dict`s\n    \"\"\"\n    return [\n        dict(\n            measurement=name,\n            tags=dict(\n                host=hostname,\n            ),\n            time=timestamp,\n            fields=dict(\n                value=value,\n            )\n        ) for name, value in measurements.items()\n    ]",
        "sha1": "0d58208ed86b3ee5bd307cf25e2ce9acad5af2d2",
        "id": 104042
    },
    {
        "content": "def _get_container_image_name(image_reference):\n    \"\"\"\n    Get the container image name from an image reference.\n\n    :param str image_reference: the image reference to analyze\n    :return: the container image name\n    \"\"\"\n    if '@' in image_reference:\n        return image_reference.split('@', 1)[0]\n    else:\n        return image_reference.rsplit(':', 1)[0]",
        "sha1": "42ee83c54879f543c3fb9ea3755f879f1d0a3ff0",
        "id": 510737
    },
    {
        "content": "def matmul(mat, rhs):\n    \"\"\"\n    Computes a matrix multiplication between a matrix (mat) and a right hand side (rhs).\n    If mat is a tensor, then this is the same as torch.matmul.\n    This function can work on lazy variables though\n\n    Args:\n        - mat (matrix nxn) - left hand size matrix\n        - rhs (matrix nxk) - rhs matrix or vector\n\n    Returns:\n        - matrix nxk\n    \"\"\"\n    return mat.matmul(rhs)",
        "sha1": "e2178df8d1cfd7da189b2549c7d6d7ec72f69e14",
        "id": 481831
    },
    {
        "content": "def justify_headings(elements, width=None):\n    \"\"\"Justifies the headings in a list of ``(heading, content)`` string tuples\n    by appending whitespace as necessary to each ``heading``.\n\n    Args:\n        elements: a list of ``(heading, content)`` tuples\n        width (None): an optional justification width. By default, the maximum\n            heading length is used\n\n    Returns:\n        a list of justified ``(heading, content)`` tuples\n    \"\"\"\n    if width is None:\n        width = max(len(e[0]) for e in elements)\n\n    fmt = \"%%-%ds\" % width\n    return [(fmt % e[0], e[1]) for e in elements]",
        "sha1": "cab2043d1a1feab8fd33d3b13629b5860f3494ed",
        "id": 380868
    },
    {
        "content": "def parse_range(s):\n    \"\"\"Parse a string \"a-b\" describing a range of integers a <= x <= b, returning the bounds a, b.\"\"\"\n    return tuple(map(int, s.split(\"-\")))",
        "sha1": "b17314dc729bec8130384a71ee10cf32e60da9c1",
        "id": 37828
    },
    {
        "content": "import math\n\n\ndef rotate(origin, points, angle):\n    \"\"\"\n    Rotate a list of list of 4 points, counterclockwise, by a given angle around\n    a given origin. The y axis is considered going downward.\n    \n    Inputs : 3\n        origin : tuple\n            coordinates of the origin around which the rotation will be done\n        points : list of list of list, as [[[px, py]]]\n            points that will be rotated\n        angle : int\n            angle in radians of the needed rotation\n    Output : 1\n        points : list of list\n            \n    \"\"\"\n    oy, ox = origin\n    # iterate throught all the elements of points\n    for column in range(len(points)):\n        for point in range(len(points[column])):\n            px, py = points[column][point]\n            # rotate the points according to the center and angle\n            qx = ox + math.cos(angle) * (px - ox) - math.sin(angle) * (py - oy)\n            qy = oy + math.sin(angle) * (px - ox) + math.cos(angle) * (py - oy)\n            # save the new values\n            points[column][point] = int(qx), int(qy)\n    return points",
        "sha1": "527a55b68f16e6d66325549ffa220bf6241a5663",
        "id": 360354
    },
    {
        "content": "def int_from_bit_str(s: str) -> int:\n    \"\"\"Convert binary representation to integer\n\n    ### Positional arguments\n\n    - s (str)\n        - A binary representation string\n\n    ### Returns\n\n    An integer of the binary pattern\n\n    ### Raises\n\n    - TypeError\n        - Raised when the parametres are of incorrect type\n\n    - ValueError\n        - Raised when the string contains character other than \"1\" and \"0\"\n    \"\"\"\n    # Type checking\n    if not isinstance(s, str):\n        raise TypeError(f\"Must be a string (given {type(s)})\")\n\n    # Remove extra 0 at the front\n    s = s.lstrip(\"0\")\n\n    # Consume bit-by-bit of the string\n    # Then add to result 1 << bit (2 ** bit)\n    length = len(s)\n    result = 0\n    for loc in range(length):\n        if s[loc] not in (\"0\", \"1\"):\n            raise ValueError(f\"String contains invalid character: '{s[loc]}'\")\n        result += int(s[loc]) << (length - loc - 1)\n\n    return result",
        "sha1": "10b1852316af78bae5ce5f40959572c5b0915f2c",
        "id": 271674
    },
    {
        "content": "def filter_dataframe(df, column, value):\n    \"\"\"Filter DataFrame for a column and a value or a list of value\n\n    Parameters\n    ----------\n    df: DataFrame\n        DataFrame to filter\n    column: str\n        column name\n    value: list or 1dim value\n        values\n\n    Returns\n    -------\n    DataFrame: filtered DataFrame\n    \"\"\"\n    if type(value) == list:\n        if len(value) > 1:\n            df_filtered = df.loc[df[column].isin(value)].copy()\n        else:\n            df_filtered = df.loc[df[column] == value[0]].copy()\n    else:\n        df_filtered = df.loc[df[column] == value].copy()\n\n    return df_filtered",
        "sha1": "e94d2d0be3d02bee999fdc9e5665785c862a19a8",
        "id": 680335
    },
    {
        "content": "def getBundleKey(bundlePath):\n    \"\"\"\n       Return all parts of a bundle's \"key\" as used in a timestamp file,\n       given its full filename.\n\n       >>> getBundleKey(\"/bundleinfo/tor-browser/win32/some-file-name.txt\")\n       '/bundleinfo/tor-browser/win32/'\n    \"\"\"\n    # No, we can't use \"os.path.directory\" or \"os.path.split\".  Those are\n    # OD-dependent, and all of our paths are in Unix format.\n    idx = bundlePath.rindex(\"/\")\n    return bundlePath[:idx+1]",
        "sha1": "4f50ba4d87a4c2f326cf7924c71fc5da7f01d346",
        "id": 114771
    },
    {
        "content": "from pathlib import Path\n\n\ndef get_project_tests() -> Path:\n    \"\"\"Returns project tests folder.\"\"\"\n    return Path(__file__).parent.parent.parent / 'tests'",
        "sha1": "ef3d510fac4af4ff97870e0f412baf5c197bf566",
        "id": 531295
    },
    {
        "content": "def kth_largest_element(nums, left, right, k):\n    \"\"\"\n    Find kth largest element in given array (like quick sort)\n\n    :param nums: given array\n    :type nums: list[int]\n    :param left: left position to begin\n    :type left: int\n    :param right: right position to end\n    :type right: int\n    :param k: kth\n    :type k: int\n    :return: kth-largest element\n    :rtype: int\n    \"\"\"\n    pivot = nums[right]\n    i = left - 1\n    for j in range(left, right):\n        if nums[j] <= pivot:\n            i += 1\n            nums[i], nums[j] = nums[j], nums[i]\n    nums[i + 1], nums[right] = nums[right], nums[i + 1]\n\n    if i + 1 == k:\n        return nums[i + 1]\n    elif i + 1 > k:\n        return kth_largest_element(nums, left, i, k)\n    else:\n        return kth_largest_element(nums, i + 2, right, k)",
        "sha1": "ecc852faf3fcf70f510baf5a193af2bf6aac8662",
        "id": 587495
    },
    {
        "content": "def resolve_attribute(name, bases, default=None):\n    \"\"\"Find the first definition of an attribute according to MRO order.\"\"\"\n    for base in bases:\n        if hasattr(base, name):\n            return getattr(base, name)\n    return default",
        "sha1": "ed358e230da7ca7bb2a413caee151f344c111b25",
        "id": 676034
    },
    {
        "content": "def byte_compare(stream_a, stream_b):\n    \"\"\"Byte compare two files (early out on first difference).\n\n    Returns:\n        (bool, int): offset of first mismatch or 0 if equal\n    \"\"\"\n    bufsize = 16 * 1024\n    equal = True\n    ofs = 0\n    while True:\n        b1 = stream_a.read(bufsize)\n        b2 = stream_b.read(bufsize)\n        if b1 != b2:\n            equal = False\n            if b1 and b2:\n                # we have two different buffers: find first mismatch\n                for a, b in zip(b1, b2):\n                    if a != b:\n                        break\n                    ofs += 1\n            break\n        ofs += len(b1)\n        if not b1:  # both buffers empty\n            break\n    return (equal, ofs)",
        "sha1": "59adfe50fefdb79edd082a35437018d4b954ec75",
        "id": 1218
    },
    {
        "content": "def note_to_f(note: int, tuning: int=440) -> float: \n    \"\"\"Convert a MIDI note to frequency.\n\n    Args:\n       note: A MIDI note\n       tuning: The tuning as defined by the frequency for A4.\n    \n    Returns:\n        The frequency in Hertz of the note.\n    \"\"\"\n    return (2**((note-69)/12)) * tuning",
        "sha1": "55201b54e525966ee7f8133c217111b22a26d827",
        "id": 24501
    },
    {
        "content": "import re\n\n\ndef titleize(name):\n    \"\"\" Titleize a course name or instructor, taking into account exceptions such as II. \"\"\"\n    name = re.sub(r'I(x|v|i+)', lambda m: 'I' + m.group(1).upper(), name.strip().title())\n    name = re.sub(r'(\\d)(St|Nd|Rd|Th)', lambda m: m.group(1) + m.group(2).lower(), name)\n    name = re.sub(r'Mc([a-z])', lambda m: 'Mc' + m.group(1).upper(), name)\n    name = name.replace(\"'S\", \"'s\")\n    return name",
        "sha1": "b4eb58ec092d89d23d1e878a8b0de077ec17c551",
        "id": 678624
    },
    {
        "content": "def calculate_ETI(pairwise_distance, eti_m, eti_c):\n    \"\"\"Calculate estimated time of infection (ETI)\"\"\"\n    eti = eti_m * pairwise_distance + eti_c\n    return eti",
        "sha1": "68560040ca2d1a60c667bf57ee03eda8ae8b8017",
        "id": 519952
    },
    {
        "content": "def _calc_instability(ca, ce):\n    \"\"\"Returns the instability ratio between ca and ce.\"\"\"\n    if ca or ce:\n        caf, cef = float(ca), float(ce)\n        instab = cef / (cef + caf)\n    else:\n        instab = 0\n    return instab",
        "sha1": "e658e05de2b422bc44fbb2fc7bb54339001fa4c7",
        "id": 409592
    },
    {
        "content": "def polygon_2_component(polygon):\n    \"\"\"\n    To convert a polygon into a component\n\n    Parameters\n    ----------\n    polygon: shapely.geometry.Polygon\n        The polygon to convert to a componen\n\n    Returns\n    -------\n    tuple(list, list)\n        the first list contains the coordinates of the exterior ring\n        the second list contains the interior rings, each defined by a list of coordinates\n    \"\"\"\n    exterior = list(polygon.exterior.coords)\n    interiors = []\n    for interior in polygon.interiors:\n        interiors.append(list(interior.coords))\n    return exterior, interiors",
        "sha1": "0480af6fea00baccd9ddb606fab830eb20035144",
        "id": 476047
    },
    {
        "content": "import binascii\n\n\ndef bitarray_to_hex(b):\n    \"\"\"Convert a bitarray to a hexidecimal string.\"\"\"\n    bytes_ = b.tobytes()\n    s = binascii.hexlify(bytes_).decode('ascii')\n    if b.length() % 8 and not (b.length() % 4):\n        return s[:-1]\n    return s",
        "sha1": "ef0a1ec2daabb78b00de045c627c6973307f9ce4",
        "id": 584239
    },
    {
        "content": "def calc_max_quant_value(bits):\n    \"\"\"Calculate the maximum symmetric quantized value according to number of bits\"\"\"\n    return 2**(bits) - 1",
        "sha1": "0cd845c06aef8742cf62132b6569ed725fae4147",
        "id": 57206
    },
    {
        "content": "import time\n\n\ndef wait_for_condition(condition_predictor,\n                       timeout_ms=1000,\n                       retry_interval_ms=100):\n    \"\"\"A helper function that waits until a condition is met.\n\n    Args:\n        condition_predictor: A function that predicts the condition.\n        timeout_ms: Maximum timeout in milliseconds.\n        retry_interval_ms: Retry interval in milliseconds.\n\n    Return:\n        Whether the condition is met within the timeout.\n    \"\"\"\n    time_elapsed = 0\n    while time_elapsed <= timeout_ms:\n        if condition_predictor():\n            return True\n        time_elapsed += retry_interval_ms\n        time.sleep(retry_interval_ms / 1000.0)\n    return False",
        "sha1": "4102429e31ee52fa19f78c44d04209cb6530251d",
        "id": 615762
    },
    {
        "content": "def zscore_normalization(data, normalize='samples'):\n    \"\"\"\n    This function normalizes each sample by using its mean and standard deviation (mean=0, std=1).\n\n    :param data:\n    :param str normalize: whether the normalization should be done by 'features' (columns) or 'samples' (rows)\n    :return: Pandas dataframe.\n\n    Example::\n        data = pd.DataFrame({'a': [2,5,4,3,3], 'b':[4,4,6,5,3], 'c':[4,14,8,8,9]})\n        result = zscore_normalization(data, normalize='samples')\n        result\n                  a         b         c\n                0 -1.154701  0.577350  0.577350\n                1 -0.484182 -0.665750  1.149932\n                2 -1.000000  0.000000  1.000000\n                3 -0.927173 -0.132453  1.059626\n                4 -0.577350 -0.577350  1.154701\n    \"\"\"\n    if normalize is None or normalize == 'samples':\n        normData = data.sub(data.mean(axis=1), axis=0).div(data.std(axis=1), axis=0)\n\n    else:\n        normData = data.sub(data.mean(axis=0), axis=1).div(data.std(axis=0), axis=1)\n\n    return normData",
        "sha1": "e02f50e51aaef9a00a0d103ed45ce302365370ed",
        "id": 478511
    },
    {
        "content": "import re\n\n\ndef wordcount(text, unique=False):\n  \"\"\"Counts words in a string, optionally limiting to unique words.\"\"\"\n  words = re.findall(r'\\w+', text.lower())\n  if unique:\n    words = set(words)\n  return len(words)",
        "sha1": "26daff6d1b5c77d5ff53cebe37d054c80bef1276",
        "id": 300732
    },
    {
        "content": "import queue\n\n\ndef bfs(G, start):\n    \"\"\" A simple breadth-first search algorithm implemented using native queues. \"\"\"\n    seen = set()\n    q = queue.Queue()\n    # we don't care about threading so don't ask Queue to block execution\n    q.put_nowait(start)\n\n    while not q.empty():\n        # get the waiting node, again without blocking execution\n        u = q.get_nowait()\n\n        if u not in seen:\n            seen.add(u)\n            # get all of u's neighbors and enqueue them\n            for n in G[u]: q.put_nowait(n)\n\n    return seen",
        "sha1": "a15f6ef42f8873b108c2bd48c462175c77fdeee8",
        "id": 698136
    },
    {
        "content": "def create_import_error_msg(\n    extra_module: str,\n    forte_module: str,\n    component_name: str,\n    pip_installable: bool = True,\n):\n    \"\"\"\n    Create an error message for importing package extra required by a forte\n    module.\n\n\n    Args:\n        extra_module: module name should be installed by pip.\n        forte_module: forte module User should install by\n            ``pip install forte[`forte_module`]`` to install all\n            extra packages for using the forte module.\n        component_name: the forte component that needs the module.\n\n    \"\"\"\n    install_msg = (\n        f\" `{extra_module}` is not installed correctly.\"\n        + f\" Consider install {extra_module}\"\n    )\n    pip_msg = f\" via `pip install {extra_module}` \"\n    refer_msg = (\n        f\" or refer to extra requirement for {component_name}\"\n        + \" at https://github.com/asyml/forte#installation\"\n        + f\" for more information about installing {forte_module}. \"\n    )\n    if pip_installable:\n        error_msg = install_msg + pip_msg + refer_msg\n    else:\n        error_msg = install_msg + refer_msg\n    return error_msg",
        "sha1": "1e13415aceda878b09fefdef84abc83f6224bff3",
        "id": 240923
    },
    {
        "content": "def to_binary(val, expected_length=8):\n    \"\"\"Converts decimal value to binary\n\n    :param val: decimal\n    :param expected_length: length of data\n    :return: binary value\n    \"\"\"\n\n    val = bin(val)[2:]  # to binary\n\n    while len(val) < expected_length:\n        val = \"0\" + val\n\n    return val",
        "sha1": "ab84ace5a149d4dcb709e9865457c0d0fe461cfd",
        "id": 194896
    },
    {
        "content": "def sort(d):\n    \"\"\"sort dictionary by keys\"\"\"\n    if isinstance(d, dict):\n        return {k: d[k] for k in sorted(d)}\n    return d",
        "sha1": "613317358ab76e149de64a4106614c32374af15e",
        "id": 678777
    },
    {
        "content": "def _matches2str(matches):\n    \"\"\"\n    Get matches into a multi-line string\n    :param matches: matches to convert\n    :return: multi-line string containing matches\n    \"\"\"\n    output_text = ''\n    for match in matches:\n        output_text += f\"{match[0]} - {match[1]}\\n\"\n\n    return output_text",
        "sha1": "61eef45b59a9143cc6fbdf0701c38cf4135df558",
        "id": 637046
    },
    {
        "content": "import base64\n\n\ndef encode_image(filename):\n    \"\"\"\n    \u7f16\u7801\u56fe\u7247\n    :param filename: str \u672c\u5730\u56fe\u7247\u6587\u4ef6\u540d\n    :return: str \u7f16\u7801\u540e\u7684\u5b57\u7b26\u4e32\n        eg:\n        src=\"data:image/gif;base64,R0lGODlhMwAxAIAAAAAAAP///\n            yH5BAAAAAAALAAAAAAzADEAAAK8jI+pBr0PowytzotTtbm/DTqQ6C3hGX\n            ElcraA9jIr66ozVpM3nseUvYP1UEHF0FUUHkNJxhLZfEJNvol06tzwrgd\n            LbXsFZYmSMPnHLB+zNJFbq15+SOf50+6rG7lKOjwV1ibGdhHYRVYVJ9Wn\n            k2HWtLdIWMSH9lfyODZoZTb4xdnpxQSEF9oyOWIqp6gaI9pI1Qo7BijbF\n            ZkoaAtEeiiLeKn72xM7vMZofJy8zJys2UxsCT3kO229LH1tXAAAOw==\"\n\n    \"\"\"\n    # 1\u3001\u6587\u4ef6\u8bfb\u53d6\n    ext = filename.split(\".\")[-1]\n\n    with open(filename, \"rb\") as f:\n        img = f.read()\n\n    # 2\u3001base64\u7f16\u7801\n    data = base64.b64encode(img).decode()\n\n    # 3\u3001\u56fe\u7247\u7f16\u7801\u5b57\u7b26\u4e32\u62fc\u63a5\n    src = \"data:image/{ext};base64,{data}\".format(ext=ext, data=data)\n    return src",
        "sha1": "d30c5ad11fe894157b5a26f850eb062f547de3a2",
        "id": 321752
    },
    {
        "content": "import re\n\n\ndef clean_token(token):\n    \"\"\"\n    Removes symbols from a token\n    \"\"\"\n\n    clean = re.compile(r'[\u00a7%]')\n    return re.sub(clean, '', token)",
        "sha1": "701932e390047b177911e3200df74543a7b6e15a",
        "id": 313184
    },
    {
        "content": "def get_info(information):\n    \"\"\"\n    Retrieves title of the source\n    For example, from the information below,\n    HYPERLINK(\"https://drive.google.com/open?id=0B9q-Bz2y-5byRWZwRHptZmk0eU0\",\"Baby And Her Health\")\n    Returns `Baby And Her Health`\n    \"\"\"\n    information = information.lower()\n    if information.startswith(\"=hyperlink\"):\n        splits = information.split(\",\")\n        return splits[1].split(\"\\\"\")[1].strip()\n    return information.strip()",
        "sha1": "ff3576efd3cd32cdfbe8c844971021381d5cea93",
        "id": 608394
    },
    {
        "content": "def construct_console_connect_url(instance_id: str, region: str = \"us-east-1\") -> str:\n    \"\"\"Assemble the AWS console instance connect url with the current instance id and region.\"\"\"\n    instance_connect_url = f\"https://console.aws.amazon.com/ec2/v2/home?region={region}#ConnectToInstance:instanceId={instance_id}\"  # noqa: E501\n    return instance_connect_url",
        "sha1": "dbbb284a1b5bbc7eb75f0646d8516d2cf83f70b1",
        "id": 127720
    },
    {
        "content": "import torch\n\n\ndef covariance_output_to_cholesky(pred_bbox_cov):\n    \"\"\"\n    Transforms output to covariance cholesky decomposition.\n    Args:\n        pred_bbox_cov (kx4 or kx10): Output covariance matrix elements.\n\n    Returns:\n        predicted_cov_cholesky (kx4x4): cholesky factor matrix\n    \"\"\"\n    # Embed diagonal variance\n    diag_vars = torch.sqrt(torch.exp(pred_bbox_cov[:, 0:4]))\n    predicted_cov_cholesky = torch.diag_embed(diag_vars)\n\n    if pred_bbox_cov.shape[1] > 4:\n        tril_indices = torch.tril_indices(row=4, col=4, offset=-1)\n        predicted_cov_cholesky[:, tril_indices[0],\n                               tril_indices[1]] = pred_bbox_cov[:, 4:]\n\n    return predicted_cov_cholesky",
        "sha1": "8af55e6804958af9b755b74b0b49eefd486f2606",
        "id": 499267
    },
    {
        "content": "def process_xml(xml_data, path):\n    \"\"\"Process XML Data\n\n    Args:\n        xml_data (Element): XML data from firewall\n        path (str): A string containing the path to the expected result\n\n    Returns:\n        item_list (list): List of item names\n    \"\"\"\n    xml_list = xml_data.findall(path)\n    item_list = []\n    for item in xml_list:\n        item_list.append(item.attrib.get('name'))\n    return item_list",
        "sha1": "e1ebc20e587e102f2ec83e6afee4347f2f026b9d",
        "id": 273463
    },
    {
        "content": "def ensure_one(found):\n    \"\"\"\n    \u786e\u4fdd\u5217\u8868\u4e2d\u4ec5\u6709\u4e00\u4e2a\u9879\uff0c\u5e76\u8fd4\u56de\u8fd9\u4e2a\u9879\uff0c\u5426\u5219\u629b\u51fa `ValueError` \u5f02\u5e38\n\n    :param found: \u5217\u8868\n    :return: \u552f\u4e00\u9879\n    \"\"\"\n    if not isinstance(found, list):\n        raise TypeError('expected list, {} found'.format(type(found)))\n    elif not found:\n        raise ValueError('not found')\n    elif len(found) > 1:\n        raise ValueError('more than one found')\n    else:\n        return found[0]",
        "sha1": "15825dcec49d18a2dd69b858d50cab195f539388",
        "id": 463428
    },
    {
        "content": "import asyncio\n\n\nasync def async_unawaited(request):\n    \"\"\"Return an unawaited coroutine (common error for async views).\"\"\"\n    return asyncio.sleep(0)",
        "sha1": "cf350768c995dac000826187312d48822424adf9",
        "id": 345744
    },
    {
        "content": "import torch\n\n\ndef assemble_recon_data(heldin, heldin_forward, heldout, heldout_forward):\n    \"\"\"Combines heldin/heldout and observed/forward data\n    into a single tensor.\n\n    Parameters\n    ----------\n    heldin : torch.Tensor\n        A BxTxN tensor.\n    heldin_forward : torch.Tensor\n        A BxT_FWDxN tensor.\n    heldout : torch.Tensor\n        A BxTxN_OUT tensor.\n    heldout_forward : torch.Tensor\n        A BxT_FWDxN_OUT tensor.\n\n    Returns\n    -------\n    torch.tensor\n        A Bx(T+T_FWD)x(N+N_OUT) tensor.\n    \"\"\"\n    heldin_full = torch.cat([heldin, heldin_forward], dim=1)\n    heldout_full = torch.cat([heldout, heldout_forward], dim=1)\n    recon_data = torch.cat([heldin_full, heldout_full], dim=2)\n    return recon_data",
        "sha1": "98708f51f624c4c8c7f11363aa20afd63836f86e",
        "id": 635231
    },
    {
        "content": "def ifstr(columnname, repls):\n    \"\"\"Returns a mipmap function string with encapsulated if statements \n    for replacing given values of a column with predefined ones. \n    This is used in a categorical/nominal column type\n\n    Arguments:\n    :param columnname: the column name(str)\n    :param repls: list with Replacement namedtuples\n                  Replacement('source', 'target')   \n    \"\"\"\n    local_repls = repls.copy()\n    if len(repls) == 1:\n        return 'if({} == \\\"{}\\\", \\\"{}\\\", null())'.format(columnname, repls[0].source, repls[0].target)\n    elif len(repls) > 1:\n        current = local_repls.pop(0)\n        return 'if({} == \\\"{}\\\", \\\"{}\\\", {})'.format(columnname,\n                                                     current.source,\n                                                     current.target,\n                                                     ifstr(columnname, local_repls))",
        "sha1": "430103a764efe8bc4bb4a0d5c89b5dad9fdebe6e",
        "id": 213112
    },
    {
        "content": "def _ComputeEditDistance(hs, rs):\n  \"\"\"Compute edit distance between two list of strings.\n\n  Args:\n    hs: the list of words in the hypothesis sentence\n    rs: the list of words in the reference sentence\n\n  Returns:\n    edit distance as an integer\n  \"\"\"\n  dr, dh = len(rs) + 1, len(hs) + 1\n  dists = [[]] * dr\n\n  # initialization for dynamic programming\n  for i in range(dr):\n    dists[i] = [0] * dh\n    for j in range(dh):\n      if i == 0:\n        dists[0][j] = j\n      elif j == 0:\n        dists[i][0] = i\n\n  # do dynamic programming\n  for i in range(1, dr):\n    for j in range(1, dh):\n      if rs[i - 1] == hs[j - 1]:\n        dists[i][j] = dists[i - 1][j - 1]\n      else:\n        tmp0 = dists[i - 1][j - 1] + 1\n        tmp1 = dists[i][j - 1] + 1\n        tmp2 = dists[i - 1][j] + 1\n        dists[i][j] = min(tmp0, tmp1, tmp2)\n\n  return dists[-1][-1]",
        "sha1": "2280dd7825c5296bdb814563449cbbbac0a95427",
        "id": 349007
    },
    {
        "content": "import time\n\n\ndef get_current_timestamp() -> float:\n    \"\"\"\n    \u83b7\u53d6\u5f53\u524d\u65f6\u95f4\u6233\n    :return:\n    \"\"\"\n\n    return time.time()",
        "sha1": "d6dbf5e26376a47397344a97a25c72437c92bdeb",
        "id": 102843
    },
    {
        "content": "def get_account_courses(account):\n    \"\"\"Returns list of id's representing the Courses of a user.\n\n    :account: user to get courses of.\n    \"\"\"\n\n    if account.courses:\n        return [c.id for c in account.courses]\n\n    return []",
        "sha1": "399ebdcd9ed55139a68ad7e6c0777248fc9c4815",
        "id": 457169
    },
    {
        "content": "def iob_ent_dict(doc):\n    \"\"\"\n    Given a spaCy Doc object, this creates a list of dictionaries mapping each token to its text, IOB embedding, and entity\n    Parameters\n    ----------\n    doc : SpaCy Doc\n        SpaCy doc object outputted by spaCy model.\n    Returns\n    -------\n    d : list<dict>\n        List of dictionaries mapping each token in a Doc object to its text, IOB embedding, and entity.\n    \"\"\"\n    d = []\n    for i in doc:\n        d.append({\"Word\": i.text, \"IOB\": i.ent_iob_, \"Ent\": i.ent_type_})\n    return d",
        "sha1": "059af6996b90083841e8836b911b671c13953011",
        "id": 242866
    },
    {
        "content": "def load_statistics(log_dir, statistics_file_name):\n    \"\"\"\n    Loads the statistics in a dictionary.\n    :param log_dir: The directory in which the log is saved\n    :param statistics_file_name: The name of the statistics file\n    :return: A dict with the statistics\n    \"\"\"\n    data_dict = dict()\n    with open(\"{}/{}.csv\".format(log_dir, statistics_file_name), 'r') as f:\n        lines = f.readlines()\n        data_labels = lines[0].replace(\"\\n\", \"\").replace(\"\\r\", \"\").split(\",\")\n        del lines[0]\n\n        for label in data_labels:\n            data_dict[label] = []\n\n        for line in lines:\n            data = line.replace(\"\\n\", \"\").replace(\"\\r\", \"\").split(\",\")\n            for key, item in zip(data_labels, data):\n                if item not in data_labels:\n                    data_dict[key].append(item)\n    return data_dict",
        "sha1": "df1a51db9112d0aa65ff9a2b41c2d2420bb2b353",
        "id": 300291
    },
    {
        "content": "def getcount(item, choice):\n    \"\"\"returns the number of times choice has been selected for item\"\"\"\n    return item.userchoices.filter(choice=choice).count()",
        "sha1": "7946c1ad847c7c2c18cdcab249e923020991d2b3",
        "id": 291941
    },
    {
        "content": "def get_cell_entry(cell):\n    \"\"\"\n    function for reading cell entry of given xls spreadsheet cell\n\n    input: cell (xlrd.sheet.cell object), cell to be read from\n\n    output: entry (any), value stored in cell\n    \"\"\"\n\n    cell_str = str(cell)    # get cell description\n    if 'text' in cell_str:  # typical text cell: text:u'L'\n        entry = cell_str.split(\"'\")[1]\n    elif 'number' in cell_str:  # typical numerical cell: number:3.0\n        entry = cell_str.split(':')[1].split('.')[0]\n    else:\n        entry = ''\n\n    return entry",
        "sha1": "24471d68484941ec77bf617d19d6ee63ad154aab",
        "id": 42451
    },
    {
        "content": "def no_jitter(value):\n    \"\"\"\n    Don't provide any jitter.\n    \"\"\"\n    return value",
        "sha1": "1895db92ecb708eb87fb2e64833bf5540602ee37",
        "id": 224397
    },
    {
        "content": "def generate_module_cmd(module, input_json, output_json):\n    \"\"\"Generates a command string to use for subprocess calling\n\n    Parameters\n    ----------\n    module: str\n    The current module being run\n    input_json: str\n    The path of the input for the module\n    output_json: str\n    The path of the output for the module\n\n    Returns\n    -------\n    command_string: str\n    a string of the command string that will be used by the subprocess\n    \"\"\"\n    module_cmd = [\"python\", \"-W\", \"ignore\", \"-m\", module,\n                  \"--input_json\", input_json,\n                  \"--output_json\", output_json]\n    return module_cmd",
        "sha1": "9313fbea1bf7432c90932f96dbc6bd8292dc549f",
        "id": 572007
    },
    {
        "content": "def unwrap_text(text):\n    \"\"\"Turn wrapped text into flowing paragraphs, ready for rewrapping by\n    the console, browser, or textwrap.\n    \"\"\"\n    all_grafs = []\n    cur_graf = []\n    for line in text.splitlines():\n        line = line.strip()\n        if line:\n            cur_graf.append(line)\n        else:\n            all_grafs.append(' '.join(cur_graf))\n            cur_graf = []\n    if cur_graf:\n        all_grafs.append(' '.join(cur_graf))\n    return '\\n\\n'.join(all_grafs)",
        "sha1": "a0e4eb5d7ab34b3bc476d2a7e1254bd4960d1f84",
        "id": 108141
    },
    {
        "content": "def hex_rotate_60(x, y, z, n=1):\n    \"\"\"Rotates the given hex n * 60 degrees counter clockwise around the origin,\n    and returns the co-ordinates of the new hex.\"\"\"\n    n = n % 6\n    if n == 0:\n        return x, y, z\n    if n == 1:\n        return -y, -z, -x\n    if n == 2:\n        return z, x, y\n    if n == 3:\n        return -x, -y, -z\n    if n == 4:\n        return y, z, x\n    if n == 5:\n        return -z, -x, -y",
        "sha1": "247e3d47877003e4ad260e74f8cbc7451f5ac168",
        "id": 172765
    },
    {
        "content": "import configparser\n\n\ndef read_from_config(key):\n    \"\"\" Read the value of the given key from the configuration file. \"\"\"\n    config = configparser.ConfigParser()\n    config.read(\"appium.ini\")\n    return config[\"DEFAULT\"][key]",
        "sha1": "65d4357a92fd1fe994b42ed3771136776d9b53c2",
        "id": 160354
    },
    {
        "content": "def interpret(block: str):\n    \"\"\"\n    Interprets an element block, breaking it into element and number of that element.\n\n    :param block: string block describing an element\n    :return: composition dictionary\n    :rtype: dict\n    \"\"\"\n    if block[0].isdigit() is True:  # if isotope number is encountered\n        return {block: 1}\n    else:\n        ele = block[0]\n        i = 0\n        num = ''\n        while i < len(block) - 1:\n            i += 1\n            if block[i].isdigit() is True:  # add digits\n                num += block[i]\n            else:\n                ele += block[i]\n        if num == '':\n            num = 1\n        else:\n            num = int(num)\n        return {ele: num}",
        "sha1": "24a8ee49488099ce86be3518565510778429170a",
        "id": 474543
    },
    {
        "content": "def snake_case_to_camel_case(s):\n    \"\"\"Convert snake case names to camel case, for polyswarmd compatibility.\n\n    Unfortunate special case for ERC20Relay contract.\n\n    :param s: String to convert\n    :return: Converted string\n    \"\"\"\n    acronyms = {'ERC'}\n    ret = ''.join(c.title() for c in s.split('_'))\n    for a in acronyms:\n        ret = ret.replace(a.lower().title(), a)\n    return ret",
        "sha1": "2cba361f55ca233cd16df3bc37352cadb2410851",
        "id": 593739
    },
    {
        "content": "from typing import Iterable\n\n\ndef skip(n: int):\n    \"\"\"Skip the first n elements of a sequence. i.e, Return a generator that yields all elements after the n'th element.\n    >>> tuple(skip(3)([i for i in range(6)]))\n    (3, 4, 5)\n    \"\"\"\n\n    def skip(seq: Iterable):\n        for i, x in enumerate(seq):\n            if i < n:\n                continue\n            yield x\n\n    return skip",
        "sha1": "2df3b7a3f9cd682602113d665d672c776750b461",
        "id": 442147
    },
    {
        "content": "def fits_column_format(format):\n    \"\"\"Convert a FITS column format to a human-readable form.\n\n    Parameters\n    ----------\n    format : :class:`str`\n        A FITS-style format string.\n\n    Returns\n    -------\n    :class:`str`\n        A human-readable version of the format string.\n\n    Examples\n    --------\n    >>> fits_column_format('A')\n    'char[1]'\n    >>> fits_column_format('J')\n    'int32'\n    >>> fits_column_format('12E')\n    'float32[12]'\n    \"\"\"\n    if format.startswith('1P'):\n        cmap = {'B': '8-bit stream', 'I': '16-bit stream',\n                'J': '32-bit stream'}\n        return cmap[format[2]]\n    fitstype = format[-1]\n    if fitstype == 'A' and len(format) == 1:\n        return 'char[1]'\n    fmap = {'A': 'char', 'I': 'int16', 'J': 'int32', 'K': 'int64',\n            'E': 'float32', 'D': 'float64', 'B': 'binary', 'L': 'logical'}\n    if len(format) > 1:\n        return fmap[fitstype] + '[' + format[0:len(format)-1] + ']'\n    else:\n        return fmap[fitstype]",
        "sha1": "116b7cf55a0d5ec786a149e20217ce3bce6313d8",
        "id": 66879
    },
    {
        "content": "def accuracy(tp: float, fn: float, tn: float, fp: float) -> float:\n    \"\"\"\n    Calculate accuracy.\n\n    :param tp: True positive count.\n    :param fn: False negative count.\n    :param tn: True negative count.\n    :param fp: False positive count.\n    :return: Accuracy.\n    \"\"\"\n    return 0.0 if tp + tn + fp + fn == 0 else (tp + tn) / (tp + tn + fp + fn)",
        "sha1": "32b31e875e68090c6a5632cdc162febda974af45",
        "id": 453069
    },
    {
        "content": "def divide(string, length):\n    \"\"\"\n    Taken (with permission) from https://github.com/TheElementalOfCreation/creatorUtils\n\n    Divides a string into multiple substrings of equal length\n    :param string: string to be divided.\n    :param length: length of each division.\n    :returns: list containing the divided strings.\n\n    Example:\n    >>>> a = divide('Hello World!', 2)\n    >>>> print(a)\n    ['He', 'll', 'o ', 'Wo', 'rl', 'd!']\n    \"\"\"\n    return [string[length * x:length * (x + 1)] for x in range(int(len(string) / length))]",
        "sha1": "4ecc4fb03cd3c36435644e8e5a4846a7e4354fb3",
        "id": 111293
    },
    {
        "content": "def group_bools(df, column_in, column_out):\n    \"\"\"\n    group_bools indexes each grouping of anomalies (1) and valid points (0) as numbered sets.\n    Used for anomaly correction.\n    Arguments:\n        df: data frame with required columns:\n            'detected_event': boolean array of classified data points\n    Returns:\n        df: original data frame with additional column:\n            'group' containing an index of boolean groupings\n    \"\"\"\n    # initialize the 'group' column to zeros\n    df[column_out] = 0\n    # initialize placeholder for boolean state of previous group\n    last = df.iloc[0][column_in]\n    # initialize the group index to zero\n    gi = 0\n\n    # loop over every row in dataframe\n    for i in range(0, len(df[column_out])):\n\n        # if the anomaly bool has changed\n        if last != df.iloc[i][column_in]:\n            gi += 1  # increment the group index\n        # assign this row to the group index\n        df.iloc[i, df.columns.get_loc(column_out)] = gi\n\n        # update last boolean state\n        last = df.iloc[i][column_in]\n\n    return df",
        "sha1": "83b4d1c1eacc9de6d533caa9130b2ced211141b9",
        "id": 591129
    },
    {
        "content": "def gpr_to_abi(gpr):\n  \"\"\"Convert a general purpose register to its corresponding abi name\"\"\"\n  switcher = {\n    \"x0\"  : \"zero\",\n    \"x1\"  : \"ra\",\n    \"x2\"  : \"sp\",\n    \"x3\"  : \"gp\",\n    \"x4\"  : \"tp\",\n    \"x5\"  : \"t0\",\n    \"x6\"  : \"t1\",\n    \"x7\"  : \"t2\",\n    \"x8\"  : \"s0\",\n    \"x9\"  : \"s1\",\n    \"x10\" : \"a0\",\n    \"x11\" : \"a1\",\n    \"x12\" : \"a2\",\n    \"x13\" : \"a3\",\n    \"x14\" : \"a4\",\n    \"x15\" : \"a5\",\n    \"x16\" : \"a6\",\n    \"x17\" : \"a7\",\n    \"x18\" : \"s2\",\n    \"x19\" : \"s3\",\n    \"x20\" : \"s4\",\n    \"x21\" : \"s5\",\n    \"x22\" : \"s6\",\n    \"x23\" : \"s7\",\n    \"x24\" : \"s8\",\n    \"x25\" : \"s9\",\n    \"x26\" : \"s10\",\n    \"x27\" : \"s11\",\n    \"x28\" : \"t3\",\n    \"x29\" : \"t4\",\n    \"x30\" : \"t5\",\n    \"x31\" : \"t6\",\n    \"f0\"  : \"ft0\",\n    \"f1\"  : \"ft1\",\n    \"f2\"  : \"ft2\",\n    \"f3\"  : \"ft3\",\n    \"f4\"  : \"ft4\",\n    \"f5\"  : \"ft5\",\n    \"f6\"  : \"ft6\",\n    \"f7\"  : \"ft7\",\n    \"f8\"  : \"fs0\",\n    \"f9\"  : \"fs1\",\n    \"f10\" : \"fa0\",\n    \"f11\" : \"fa1\",\n    \"f12\" : \"fa2\",\n    \"f13\" : \"fa3\",\n    \"f14\" : \"fa4\",\n    \"f15\" : \"fa5\",\n    \"f16\" : \"fa6\",\n    \"f17\" : \"fa7\",\n    \"f18\" : \"fs2\",\n    \"f19\" : \"fs3\",\n    \"f20\" : \"fs4\",\n    \"f21\" : \"fs5\",\n    \"f22\" : \"fs6\",\n    \"f23\" : \"fs7\",\n    \"f24\" : \"fs8\",\n    \"f25\" : \"fs9\",\n    \"f26\" : \"fs10\",\n    \"f27\" : \"fs11\",\n    \"f28\" : \"ft8\",\n    \"f29\" : \"ft9\",\n    \"f30\" : \"ft10\",\n    \"f31\" : \"ft11\",\n  }\n  return switcher.get(gpr, \"na\")",
        "sha1": "2dac47f4cb3cbae9fa92b6696831352eb80ebed4",
        "id": 351313
    },
    {
        "content": "def get_year_list(yearstring):\n    \"\"\"\n    Given a [yearstring] of forms\n    year1\n    year1-year2\n    year1,year2,year3\n    year1-year2,year3-year4\n    Expands into a list of year integers, and returns\n    \"\"\"\n    years = []\n    for subset in yearstring.split(','):\n        if subset == 'latest':\n            years.append('latest')\n            continue\n        sublist = subset.split('-')\n        start = int(sublist[0])\n        end = int(sublist[1])+1 if len(sublist) > 1 else start+1\n        years.extend(range(start,end))\n    return years",
        "sha1": "bc5ffd49bcdaa2e04f04a0ad1ac488981f84f630",
        "id": 112882
    },
    {
        "content": "from bs4 import BeautifulSoup\n\n\ndef remove_tags(html):\n    \"\"\"Remove all html tags.\n\n    Args:\n        html: html string.\n\n    Returns:\n        cleaned text.\n\n    Example:\n        >>> html = 'hoge<a href=\"fuga\">bar</a>buzz'\n        >>> remove_tags(html)\n        hogebarbuzz\n    \"\"\"\n    soup = BeautifulSoup(html, 'html.parser')\n    text = soup.get_text()\n\n    return text",
        "sha1": "796d642c425e3bee1ab078e0905f36b4b1acb35a",
        "id": 245193
    },
    {
        "content": "def _box(bl, ur):\n    \"\"\"(x, y) coordinates for the 4 lines making up a rectangular box.\n\n    Parameters\n    ==========\n    bl : float\n        The bottom left corner of the box\n    ur : float\n        The upper right corner of the box\n\n    Returns\n    =======\n    coords : 2-tuple\n        The first and second elements of the tuple are the x and y coordinates\n        of the box.\n    \"\"\"\n    xl, xr = bl[0], ur[0]\n    yb, yt = bl[1], ur[1]\n    box_x = [xl, xr,\n             xr, xr,\n             xr, xl,\n             xl, xl]\n    box_y = [yb, yb,\n             yb, yt,\n             yt, yt,\n             yt, yb]\n    return (box_x, box_y)",
        "sha1": "172e7167c173e6700839f6b60c342d8333083a42",
        "id": 174495
    },
    {
        "content": "import re\n\n\ndef get_last_ids(file_path: str) -> tuple:\n    \"\"\"\n    Get last IDs of entities, relations, attributes and annaotions for a given brat document\n    Args:\n        file_path (str): brat annotation filepath\n\n    Returns:\n        (int, int, int, int): last entity ID, last attribute ID, last relation ID, last annotation ID\n    \"\"\"\n\n    regex_entity = re.compile(r'^T(\\d+)\\t([^\\s]+)\\s(.*)\\t(.*)')\n    regex_relation = re.compile(r'^R(\\d+)\\t([^\\s]+)\\sArg1:T(\\d+)\\sArg2:T(\\d+)')\n    regex_attribute = re.compile(r'^A(\\d+)\\t([^\\s]+)\\sT(\\d+)\\s(.*)')\n    regex_annotation = re.compile(r'#(\\d+)\\tAnnotatorNotes\\s(T|R)(\\d+)\\t(.*)')\n\n    last_entity_id = 0\n    last_att_id = 0\n    last_relation_id = 0\n    last_ann_id = 0\n\n    with open(file_path, \"r\", encoding=\"UTF-8\") as input_file:\n\n        for line in input_file:\n\n            entity_match = regex_entity.match(line)\n            if entity_match:\n                if int(entity_match.group(1)) > last_entity_id:\n                    last_entity_id = int(entity_match.group(1))\n\n            relation_match = regex_relation.match(line)\n            if relation_match:\n                if int(relation_match.group(1)) > last_relation_id:\n                    last_relation_id = int(relation_match.group(1))\n\n            attribute_match = regex_attribute.match(line)\n            if attribute_match:\n                if int(attribute_match.group(1)) > last_att_id:\n                    last_att_id = int(attribute_match.group(1))\n\n            annotation_match = regex_annotation.match(line)\n            if annotation_match:\n                if int(annotation_match.group(1)) > last_ann_id:\n                    last_ann_id = int(annotation_match.group(1))\n\n    return last_entity_id, last_att_id, last_relation_id, last_ann_id",
        "sha1": "5a5ea4124659047786b0c184eedc01f40ee83192",
        "id": 151506
    },
    {
        "content": "def beta_match_moments(\u03bc, \u03a3):\n    \"\"\"\n    Match a Beta distribution given mean \u03bc and variance \u03a3.\n    \"\"\"\n    factor = \u03bc * (1-\u03bc) / \u03a3 - 1\n    \u03b1 = factor * \u03bc\n    \u03b2 = factor * (1-\u03bc)\n    return \u03b1, \u03b2",
        "sha1": "8e671b7b05524d9bfc6e3fef52259aca84e81823",
        "id": 288093
    },
    {
        "content": "from pathlib import Path\n\n\ndef _parse_segmentation_atlases(anat_modality, template_dir):\n    \"\"\"\n    Parse segmentation templates directory for anatomical and segmentation files.\n\n    This is currently hardcoded to match DCAN lab templates.\n    Will need to rethink standardization for more general cases.\n    \"\"\"\n\n    anats, segs = [], []\n\n    for f in Path(template_dir).glob(\"**/*.nii*\"):\n        if \"Segmentation\" in f.name:\n            segs.append(str(f.absolute()))\n        elif anat_modality in f.name:\n            anats.append(str(f.absolute()))\n\n    assert anats\n    assert segs\n    # there should matching files per template\n    assert len(anats) == len(segs)\n\n    return sorted(anats), sorted(segs)",
        "sha1": "ecd5cf3329fe7886dcee25131420b69fd1c3d4c7",
        "id": 464546
    },
    {
        "content": "def all_caps(value):\n    \"\"\"This template filter converts a string into all caps.\"\"\"\n    return value.upper()",
        "sha1": "ff8a94bb712d7a33784ec0f6a96a8e6b2ab910c8",
        "id": 467842
    },
    {
        "content": "def comp_Ncspc(self, Zs):\n    \"\"\"Compute the number of coils in series per parallel circuit\n\n    Parameters\n    ----------\n    self : Winding\n        A Winding object\n    Zs : int\n        number of slot\n\n    Returns\n    -------\n    Ncspc: float\n        Number of coils in series per parallel circuit\n\n    \"\"\"\n\n    (Nrad, Ntan) = self.get_dim_wind()\n    Ncspc = Zs * Nrad * Ntan / (2.0 * self.qs * self.Npcpp)\n\n    return Ncspc",
        "sha1": "2705cc3c413acc404591728e13632b1d0c22f160",
        "id": 428665
    },
    {
        "content": "import torch\n\n\ndef _manhattan_fast(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Helper function to calculate manhattan distance between ``torch.Tensors`` ``x`` and ``y``: :math:`sum(|x-y|)`\n    Uses dimension expansion. Returns a 2D tensor of size :math:`m x n`.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        2D tensor of size :math:`m x f`\n    y : torch.Tensor\n        2D tensor of size :math:`n x f`\n    \"\"\"\n    d = torch.sum(torch.abs(x.unsqueeze(1) - y.unsqueeze(0)), dim=2)\n    return d",
        "sha1": "efeeea1161dea8cab92178d827ead639680ecce2",
        "id": 125220
    },
    {
        "content": "import re\n\n\ndef max_num1(x: str)->int:\n    \"\"\"\n    Input: String\n    Output: Integer\n    Finds the maximum integer in the string\n    \"\"\"\n    c = re.findall(\"\\d+\",x)\n    n = map(int,c)\n    return max(n)",
        "sha1": "e5a715a56ca16b265762ac10c7ef847d8bd89f21",
        "id": 670277
    },
    {
        "content": "def ref_str_to_tuple(ref):\n    \"\"\"String like ' a : b ' to tuple like ('a', 'b').\"\"\"\n    return tuple(x.strip() for x in ref.split(':'))",
        "sha1": "fc2e467f054d2b53a580f1d0917d01eda9ba1727",
        "id": 15067
    },
    {
        "content": "def subtract_baseline(cell, baseline_range, channel):\n    \"\"\"Subtract baseline from the selected channel of a Cell-like np.ndarray.\n\n    Arguments\n    ---------\n    cell: Cell-like\n        [c, t, s] array where c is channels, t is time (in timesteps), and s is\n        sweeps\n    baseline_range: slice\n        Baseline time slice in timesteps\n    channel: int\n        Index of channel from which to subtract baseline. Not guaranteed to\n        work with multiple channels.\n\n    Returns\n    -------\n    Copy of cell with baseline subtracted from the relevant channel.\n\n    \"\"\"\n    cell = cell.copy()\n\n    cell[channel, :, :] -= cell[channel, baseline_range, :].mean(axis=0)\n\n    return cell",
        "sha1": "c471f9074ebd02ec003656eeeb97db7b966ce1fb",
        "id": 462315
    },
    {
        "content": "import hashlib\n\n\ndef md5(s):\n  \"\"\"returns the md5 hash of string s as a hex string\"\"\"\n  return hashlib.md5(s.encode(\"utf-8\")).hexdigest()",
        "sha1": "c0d74c0e56b13874c14297cd784f3fbb4a57c813",
        "id": 583307
    },
    {
        "content": "def _isValidOpcodeByte(sOpcode):\n    \"\"\"\n    Checks if sOpcode is a valid lower case opcode byte.\n    Returns true/false.\n    \"\"\"\n    if len(sOpcode) == 4:\n        if sOpcode[:2] == '0x':\n            if sOpcode[2] in '0123456789abcdef':\n                if sOpcode[3] in '0123456789abcdef':\n                    return True;\n    return False;",
        "sha1": "dee516c1765aedfc9f7c0044cde29495d54f81f5",
        "id": 691379
    },
    {
        "content": "def split_list(inp_list, nr):\n    \"\"\"\n    Splits evenly a list\n    :param inp_list: list\n    :param nr: number of parts\n    :return: list of \"nr\" lists\n    \"\"\"\n    new_list = []\n    nr_el = 1.0 / nr * len(inp_list)\n    for i in range(nr):\n        start = int(round(i * nr_el))\n        end = int(round((i + 1) * nr_el))\n        new_list.append(inp_list[start:end])\n    return new_list",
        "sha1": "d51c94e47ac55b198a59a0271902a14c2030797d",
        "id": 559774
    },
    {
        "content": "def parse_package_status(release, package, status_text, filepath):\n    \"\"\"\n    parse ubuntu package status string format:\n          <status code> (<version/notes>)\n    :return: dict where\n          'status'        : '<not-applicable | unknown | vulnerable | fixed>',\n          'fix-version'   : '<version with issue fixed, if applicable>'\n    \"\"\"\n\n    # break out status code and detail\n    status_sections = status_text.strip().split(' ', 1)\n    code = status_sections[0].strip().lower()\n    detail = status_sections[1].strip('()') if len(status_sections) > 1 else None\n\n    status = 'unknown'\n    fix_version = None\n\n    if code == 'dne':\n        status = 'not-applicable'\n    elif code in ['ignored', 'pending', 'deferred', 'needed', 'needs-triage']:\n        status = 'vulnerable'\n    elif code == 'not-affected':\n        status = 'not-vulnerable'\n    elif code in ['released', 'released-esm']:\n        # if there isn't a release version, then just mark\n        # as vulnerable to test for package existence\n        if not detail:\n            status = 'vulnerable'\n        else:\n            status = 'fixed'\n            fix_version = detail\n    else:\n        print('Unsupported status \"{0}\" in {1}_{2} in \"{3}\". Setting to \"unknown\".'\n              .format(code, release, package, filepath))\n\n    result = {'status': status}\n    if fix_version is not None:\n        result['fix-version'] = fix_version\n    return result",
        "sha1": "a079fadfa1e0df1c5dcb849095bb80feebf4329c",
        "id": 520294
    },
    {
        "content": "import torch\n\n\ndef load_model(filename, model):\n    \"\"\"Load the model parameters in {filename}.\"\"\"\n    model_params = torch.load(str(filename))\n    model.load_state_dict(model_params)\n    return model",
        "sha1": "14cb058d61dedc1ad4b41b0aab09a4ce56db37a1",
        "id": 673612
    },
    {
        "content": "from math import exp\n\n\ndef sigmoid(x):\n    \"\"\"Returns the value mapped using a sigmoid.\n    This is the logistic function: ``1/(1+exp(-x))``\n    The input can be any number.\n    Results are in the range 0 to 1, with ``x=0 -> y=0.5``\n    \"\"\"\n    return 1.0/(1+exp(-x))",
        "sha1": "090e74d72df05ed897a323388a2030ff0ca9c44f",
        "id": 174423
    },
    {
        "content": "import re\n\n\ndef is_valid_cluster_name(name):\n    \"\"\"Validate that the cluster name against the pattern.\"\"\"\n    if len(name) > 25:\n        return False\n    if name[-1] == '.':\n        name = name[:-1]\n    allowed = re.compile(r\"(?!-)[A-Z\\d-]{1,63}(?<!-)$\", re.IGNORECASE)\n    return all(allowed.match(x) for x in name.split(\".\"))",
        "sha1": "e61bada409db834768a9eb793983b489b0cfca58",
        "id": 536663
    },
    {
        "content": "def removeLocations(lmA, prefix):\n    \"\"\"Removes locations with a given prefix\n        \n    Keyword arguments:\n    lmA -- Location map to delete location from\n    prefix -- Key or part of key in location map dictionary\n        \n    Creates a copy of provided location map lmA\n    Copy contains all key-value pairs from lmA in which the key does not start with the given prefix\n    \"\"\"\n    lmCopy = dict()\n    for key, value in lmA.items():\n        keyStartsWithPrefix = key.rfind(prefix, 0)\n        if(not keyStartsWithPrefix == 0):\n            lmCopy[key] = value\n    return lmCopy",
        "sha1": "ed83a14db23264cf49df81b44b6ce48a9f300c04",
        "id": 637018
    },
    {
        "content": "def dummy_api_env_file_content() -> dict:\n    \"\"\"\n    Return dummy env_data for ApiGatewayCognito class.\n    \"\"\"\n    dummy_env_data = {\n        'COGNITO_TOKEN_URL': 'https://test-token.url',\n        'COGNITO_CLIENT_ID': '12345',\n        'COGNITO_CLIENT_SECRET': 'my-secret',\n        'COGNITO_CUSTOM_SCOPES': 'verus-api/api-test',\n        'NOTIFICATION_API_URL': 'https://test-notification.url'\n    }\n    return dummy_env_data",
        "sha1": "d94bb1c8112e779b4f480583487c04d1ce2c1603",
        "id": 297878
    },
    {
        "content": "def load_lg_facts(conn, query):\n    \"\"\"\n    loads triple-based facts from a SemMedDB and builds the corresponding idx_pmid\n    idx_pmid stores a idx_subject and a idx_object for each pmid from the query result\n    the indices store which (subject -> [object1, ..., objectn] and which object -> [sub1, ..., subn]\n\n    @param conn: SemMedDB connection handle\n    @param query: the query which retrieves facts from the database (must project pmid, subj and obj)\n    @return: the idx_pmid (which stores a idx_subject and idx_object behind each pmid)\n    \"\"\"\n    cur = conn.cursor()\n    cur.execute(query)\n    rows = cur.fetchall()\n\n    idx_pmid = {}\n    for r in rows:\n        pmid, sub, obj = str(r[0]), str(r[1]), str(r[2])\n\n        if pmid not in idx_pmid:\n            idx_s = {}\n            idx_o = {}\n            idx_pmid[pmid] = {\"idx_subject\": idx_s, \"idx_object\": idx_o}\n        else:\n            idx_s = idx_pmid[pmid][\"idx_subject\"]\n            idx_o = idx_pmid[pmid][\"idx_object\"]\n\n        if sub not in idx_s:\n            idx_s[sub] = set()\n        idx_s[sub].add(obj)\n\n        if obj not in idx_o:\n            idx_o[obj] = set()\n        idx_o[obj].add(sub)\n\n    return idx_pmid",
        "sha1": "0be27157726c0629585e934ddc302833c5eb61f3",
        "id": 257647
    },
    {
        "content": "def has_any(data, keys):\n    \"\"\"\n    Checks any one of the keys present in the data given\n    \"\"\"\n    if data is None and not isinstance(data, dict):\n        return False\n\n    if keys is None and not isinstance(keys, list):\n        return False\n\n    for key in keys:\n        if key in data:\n            return True\n\n    return False",
        "sha1": "91f098ed4433ed0c25c74e234a2f81edc3cba689",
        "id": 231594
    },
    {
        "content": "def get_tool_names(tool_list):\n    \"\"\" SUMMARY:  creates a list (set) of unique tool names for searching, auto-suggestion, etc.\n          INPUT:  a list of two-item tuple (tool, platform)\n         OUTPUT:  a de-duplicated list (set) of tool names \"\"\"\n\n    tool_names = []\n    for x in tool_list:\n        tool_names.append(x[0])\n\n    return set(tool_names)",
        "sha1": "d872daa9f871ce3e5a79ba3ab7cb0d865051dad0",
        "id": 218906
    },
    {
        "content": "def quote_string(s: str, force:bool=False) -> str:\n    \"\"\"Sometimes wraps strings in double quotes, depending on the content and force parameter.\n    \n    Description:\n        This function provides conditional wrapping of strings inside double quotes. \n        If the input string contains a space, OR force is set to True, then the input string will \n        always be quoted.\n        If the input string does not contain a space, AND force is set to False, then the input \n        string is returned unmodified.\n\n    Args:\n        s: The string that needs to be wrapped in quotes (maybe)\n        force (optional): Whether to force quotes around the string even if not needed. Defaults to False.\n\n    Examples:\n        >>> quote_string(\"nevermore\", False)\n        'nevermore'\n        >>> quote_string(\"nevermore\")\n        'nevermore'\n        >>> quote_string(\"nevermore\", True)\n        '\"nevermore\"'\n        >>> quote_string(\"never more\", False)\n        '\"never more\"'\n    \n    Returns:\n        The string, maybe wrapped in double quotes\n    \"\"\"\n    return f'\"{s}\"' if force or ' ' in s else s",
        "sha1": "6624881986b7ed7b08e2f4e5b5cfb1b171907c90",
        "id": 568004
    },
    {
        "content": "def filters(request):\n    \"\"\"\n    Extracts the filters from the request string\n\n    Returns a dict of lists for the filters:\n\n    check=a&check=b&name=Bob&verbose=True&verbose=other\n\n    becomes\n\n    {'check': [u'a', u'b'], 'name': [u'Bob']}\n    \"\"\"\n    res = {}\n    for key in set(request.GET):\n        if key in ('verbose', 'fields'):\n            continue\n\n        values = [v for v in request.GET.getall(key) if v]\n        if values:\n            res[key] = values\n    return res",
        "sha1": "2a81a6f2ba53877bacaa43f3f309f1e960df7528",
        "id": 462663
    },
    {
        "content": "def build_plot_values(gdpinfo, gdpdata):\n    \"\"\"\n    Inputs:\n      gdpinfo - GDP data information dictionary\n      gdpdata - A single country's GDP stored in a dictionary whose\n                keys are strings indicating a year and whose values\n                are strings indicating the country's corresponding GDP\n                for that year.\n\n    Output: \n      Returns a list of tuples of the form (year, GDP) for the years\n      between \"min_year\" and \"max_year\", inclusive, from gdpinfo that\n      exist in gdpdata.  The year will be an integer and the GDP will\n      be a float.\n    \"\"\"\n    maxyr = gdpinfo['max_year'] \n    minyr = gdpinfo['min_year']\n    outer = [] #for list of tuples\n    inner = () #for tuples\n    gdp = {} # for the unsorted gdp data\n    \n    for dic in gdpdata:\n    #first try catch asses if year is a string that can be converted into an int\n        try:\n            key = int(dic)\n        except ValueError:\n            continue\n    #second try catch asses if value is a string that can be converted into a float\n        try:\n            gdp[key] = float(gdpdata[dic])\n        except ValueError:\n            continue\n    #if the value is empty that means there is no valid GDP data\n        if gdpdata[dic] == '' or gdpdata[dic] == \"\":\n            continue\n    #GDP cannot be negative\n        if float(gdpdata[dic]) < 0:\n            continue\n    #Sort the key's by least to greatest, also makes it a list of tuples\n    gdpsorted = sorted(gdp.items(), key = lambda t: t[0])\n    \n    for tup in gdpsorted:\n        #check again for empty value\n        if tup[1] == '' or  tup[1] == \"\":\n            continue\n        key = tup[0]\n        # value of key cannot be outside these bounds\n        if (key >= minyr) and (key <= maxyr):\n            inner = (key, float(tup[1]))\n            outer.append(inner)\n    return outer",
        "sha1": "c3586a6fd2b3ac0dce878c9aeea051f99afb02cc",
        "id": 284986
    },
    {
        "content": "def config_contains_tar(pipeline_config: dict, tags=None) -> bool:\n    \"\"\"\n    Check if the input file list contains a `.tar` archive.\n    \"\"\"\n    if tags is None:\n        tags = [\"archive\", \"inside_archive\"]\n\n    params = pipeline_config.get(\"params\", {})\n    ffiles = params.get(\"fetch_files\", [])\n\n    for ff in ffiles:\n        # For tar archives, we don't attempt to make a samplesSheet\n        ftags = ff.get(\"tags\", [])\n        if any([t in ftags for t in tags]):\n            return True\n\n    return False",
        "sha1": "65c38a5f953f8236b2886abd6c46aeb8b6f14d65",
        "id": 645476
    },
    {
        "content": "def clean_completion_name(name: str, char: str) -> str:\n    \"\"\"Clean the completion name, stripping bad surroundings\n\n    1. Remove all surrounding \" and '. For\n    \"\"\"\n    if char == \"'\":\n        return name.lstrip(\"'\")\n    if char == '\"':\n        return name.lstrip('\"')\n    return name",
        "sha1": "72e802c48f9dd3051def393f0fd9f3a348e37206",
        "id": 597241
    },
    {
        "content": "def is_closing_char(char, state):\n    \"\"\"Check if closing char should be processed.\"\"\"\n    return char == '}' and state['stack'] and state['stack'][-1] == '{'",
        "sha1": "b8bc51d54c0912054b62f3fb559209a4bb1655f3",
        "id": 110936
    },
    {
        "content": "def add_x(tabuleiro):\n    \"\"\" Function that returns a tuple equal to the given one but with x's instead of -1. \"\"\"\n    tab_to_print = ()\n    for line in tabuleiro:\n        for state in line:\n            if state == -1:\n                tab_to_print += ('x', )\n            else:\n                tab_to_print += (state, )\n    return tab_to_print",
        "sha1": "9e69b946594e64aee932175841c5ebd55dd216dc",
        "id": 372689
    },
    {
        "content": "def sort_batch_contributions(contributions):\n    \"\"\"Returns the list of contributions sorted by creation date (old -> young)\n    and score (high -> low).\n    \"\"\"\n    by_creation = sorted(contributions, key=lambda x: x[\"created\"])\n    by_score = sorted(by_creation, key=lambda x: x[\"score\"], reverse=True)\n\n    return by_score",
        "sha1": "d9cc0125813c158175e58198b64521db613a8e55",
        "id": 510223
    },
    {
        "content": "import ast\n\n\ndef get_attr(node, name):\n    \"\"\"get the attribute of node or None if the attribute doesn't exist\n\n    Args:\n        node (ast.AST): node whose attribute is going to be check.\n        name (str): name of the attribute whose existence is being tested.\n    Returns:\n        str: in case the docstring was requested.\n        ast.AST: in case the requested attribute is an instance of AST.\n        None: if the attibute was not found.\n        object: any type that the attribute may have.\n    \"\"\"\n    if name == 'doc':\n        return str(ast.get_docstring(node, clean=True))\n    if hasattr(node, name):\n        return getattr(node, name)",
        "sha1": "dc65dc72bb45ba7aec8d0be8c97439ec909b8f9f",
        "id": 248553
    },
    {
        "content": "def is_atom_response(req):\n    \"\"\"Returns True when the request wants an ATOM response, False otherwise\"\"\"\n    return \"Accept\" in req.headers and \"application/atom+xml\" in req.accept",
        "sha1": "dbf553594cda457de18543cebc4e87acf2aeaaa5",
        "id": 378726
    },
    {
        "content": "def get_length(response):\n    \"\"\"\n    Get length of response from HTTP response header.\n\n    Falls back to checking the length of the response content if value not\n    present in header. Also ensures that we convert from octets to bits for\n    use in the bandwidth estimation algorithm\n\n    \"\"\"\n    try:\n        length = int(response.headers.get('Content-Length'))\n    except TypeError:\n        length = len(response.content)\n    length = length * 8\n    return length",
        "sha1": "267edf24f709c6d405b794eb4473e7874befc6be",
        "id": 296585
    },
    {
        "content": "def merge_dicts(*args):\n    \"\"\"Given n dicts, merge them into a new dict as a shallow copy.\"\"\"\n    if len(args) < 1:\n        raise TypeError(\"merge_dicts(*args) takes 1 or more arguments (0 given)\")\n\n    z = args[0].copy()\n    for d in args[1:]:\n        z.update(d)\n\n    return z",
        "sha1": "765d9e1618e245c322f25adc97ff1bc82f86379b",
        "id": 294601
    },
    {
        "content": "def counter_clockwise(direction):\n    \"\"\"Rotates a direction counterclockwise, e.g. 'U' -> 'R'\n\n    Args:\n        direction (str): a valid direction ('U', 'R', 'D' or 'L')\n\n    Returns:\n        str: the direction rotated counterclockwise\n    \"\"\"\n    if direction == \"U\":\n        return \"L\"\n    if direction == \"L\":\n        return \"D\"\n    if direction == \"D\":\n        return \"R\"\n    return \"U\"",
        "sha1": "7c1084d35adc20c3896edf51a7210cd65e48bc39",
        "id": 309309
    },
    {
        "content": "def _truncate_string_left(strg, maxlen):\n    \"\"\"\n        \n    Helper function which truncates the left hand side of a string\n    to the given length and adds a continuation characters, \"...\".\n        \n    \"\"\"\n    if len(strg) > maxlen:\n        lhs = maxlen - 4\n        strg = \"... %s\" % strg[-lhs:]\n    else:\n        return strg",
        "sha1": "a4f5080ce3d8b7c87c437830835acf8f83992302",
        "id": 381757
    },
    {
        "content": "def split_textfile_line(text_line):\n    \"\"\"\n    Clear a line of text from spaces and linebreak characters the split to list\n    \"\"\"\n    out = text_line.rstrip('\\n')\n    while '  ' in out:\n        out = out.replace('  ', ' ')\n    out = out.split(' ')\n\n    return out[0][:-1], out[1:]",
        "sha1": "c79796b8a086cfabb27528463eb7c3c1cd1c97a8",
        "id": 141642
    },
    {
        "content": "def query_sort(resources, arguments):\n    \"\"\"Return the resources sorted\n\n    Args:\n        resources(list): List to sort\n        arguments(FormsDict): query arguments\n\n    Returns:\n        list: Sorted resource (asc or desc)\n    \"\"\"\n\n    if '_sort' not in arguments:\n        return resources\n\n    sort = arguments['_sort']\n    order = 'asc' if '_order' not in arguments else arguments['_order']\n    if order == 'asc':\n        return sorted(resources, key=lambda i: i[sort])\n\n    return sorted(resources, key=lambda i: i[sort], reverse=True)",
        "sha1": "2f74627826ba9b751fd2a20ee617f0a44fa78774",
        "id": 316616
    },
    {
        "content": "import string\n\n\ndef list_zero_alphabet() -> list:\n    \"\"\"Build a list: 0, a, b, c etc.\"\"\"\n    score_dirs = ['0']\n    for char in string.ascii_lowercase:\n        score_dirs.append(char)\n    return score_dirs",
        "sha1": "6cd9fc9e93257dcc7729235ac3cffa01dbd80c95",
        "id": 2598
    },
    {
        "content": "def _trajectory(line: str):\n  \"\"\"Returns parsed action trajectory.\"\"\"\n  actions = [int(x) for x in line.split(' ')]\n  return tuple(actions)",
        "sha1": "aa290956f1e507e2f5533752f6371ea895ddc471",
        "id": 389902
    },
    {
        "content": "def path_add_str(path_):\n    \"\"\" Format path_ for console printing \"\"\"\n    return '+ {}'.format(path_)",
        "sha1": "0f1edde223e432560482edd68f78cb2b42a6bc84",
        "id": 41347
    },
    {
        "content": "def concatenate_dirs(dir1, dir2):\n    \"\"\"\n    Appends dir2 to dir1. It is possible that either/both dir1 or/and dir2 is/are None\n    \"\"\"\n    result_dir = dir1 if dir1 is not None and len(dir1) > 0 else ''\n    if dir2 is not None and len(dir2) > 0:\n        if len(result_dir) == 0:\n            result_dir = dir2\n        else:\n            result_dir += '/' + dir2\n\n    return result_dir",
        "sha1": "576f2f0df6d7f54c659a1a315920e748d784f4be",
        "id": 646808
    },
    {
        "content": "import json\n\n\ndef search(es_object, index, search_params):\n    \"\"\" Search the Elasticsearch index. \"\"\"\n    json_search = json.dumps(search_params)\n    result = es_object.search(index=index, body=json_search)\n\n    return result",
        "sha1": "cd315f4b32ff364fb19e4bfaae70404e6eff19c6",
        "id": 88997
    },
    {
        "content": "import re\n\n\ndef get_main_desc(source):\n    \"\"\"\n    Retrieves the main description of the provided object `source` and returns\n    it.\n\n    The main description is defined as the first paragraph of its docstring.\n\n    Parameters\n    ----------\n    source : object\n        The object whose main description must be retrieved.\n\n    Returns\n    -------\n    main_desc : str or None\n        The main description string of the provided `source` or *None* if\n        `source` has not docstring.\n\n    \"\"\"\n\n    # Retrieve the docstring of provided source\n    doc = source.__doc__\n\n    # If doc is None, return None\n    if doc is None:\n        return(None)\n\n    # Obtain the index of the last character of the first paragraph\n    index = doc.find('\\n\\n')\n\n    # If index is -1, there is only 1 paragraph\n    if(index == -1):\n        index = len(doc)\n\n    # Gather everything up to this index\n    doc = doc[:index]\n\n    # Replace all occurances of 2 or more whitespace characters by a space\n    doc = re.sub(r\"\\s{2,}\", ' ', doc)\n\n    # Return doc\n    return(doc.strip())",
        "sha1": "4f66db7a4b559be8085419253e9dee3b8b44efe5",
        "id": 486490
    },
    {
        "content": "def level_and_fall_freq(\n    complete_lines: float,\n    base_speed: float=8.0,\n    speed_limit: float=0.1,\n) -> tuple:\n    \"\"\"\n    Return the level the player is on based on number of complete lines.\n\n    Args:\n        complete_lines: the number of lines cleared in the game\n        base_speed: the initial frequency at level 1 to scale down from\n\n    Returns:\n        a tuple of:\n        - the level the player is on\n        - the fall_frequency for piece (drop speed)\n\n    \"\"\"\n    # get the level that the player is on\n    level = int(complete_lines / 10) + 1\n    # get the frequency with which to move pieces down\n    fall_freq = base_speed / level\n    # reset the fall_frequency if it's below the speed limit\n    if fall_freq < speed_limit:\n        fall_freq = speed_limit\n\n    return level, fall_freq",
        "sha1": "80210cb4873abe78aa10c523f6aed054328f8f3c",
        "id": 447396
    },
    {
        "content": "def remove_gaps(sequences):\n    \"\"\"\n    Function that removes any gaps ('-') from the protein sequences in the input.\n    The descriptors cannot be calculated if a '-' value is passsed into their\n    respective funtions so gaps need to be removed. Removing the gaps has the same\n    effect as setting the value at the index of the sequence to 0 and has no effect\n    on the descriptors calculation. Input can be string, list of array of sequences.\n\n    Parameters\n    ----------\n    sequences : str/list/np.ndarray\n        string of 1 protein sequence or array/list of protein sequences.\n\n    Returns\n    -------\n    protein_seqs : np.ndarray\n        returns the same inputted protein sequences but with any gaps ('-') removed.\n    \"\"\"\n\n    is_string=False   #bool needed to ensure correct output format if input is str\n\n    if isinstance(sequences, str):\n      is_string = True\n      sequences = [sequences]     #convert single string into 1 element list\n\n    #concatenate multiple sequences into 1 iterable list\n    if isinstance(sequences, list) and \\\n      len(sequences)>1:\n      # for i in range(0,len(protein_seqs)):\n      #   protein_seqs[i] = ''.join(protein_seqs[i])\n      sequences = [''.join(sequences)]\n\n    #iterate through sequences, removing any gaps ('-')\n    for row in range(0, len(sequences)):\n        try:\n            sequences[row] = sequences[row].replace(\"-\",\"\")\n        except:\n            raise ValueError('Error removing gaps from sequences at index {} '.format(row))\n\n    #if input was str then join list of sequences into one str\n    if is_string:\n       sequences = ''.join(sequences)\n\n    return sequences",
        "sha1": "7a524ed9c699f52b540039c06c8ffd9fba5965cb",
        "id": 603060
    },
    {
        "content": "def reverse_bits(bits):\n    \"\"\"Reverse bits.\"\"\"\n    bit_to_reverse = {\n        '1': '0',\n        '0': '1'\n    }\n    return ''.join(\n        bit_to_reverse[bit]\n        for bit in bits\n    )",
        "sha1": "7d4b7f6476335d21102e3c74597b89bb82be1b99",
        "id": 206588
    },
    {
        "content": "def get_hamming_distance(M1, M2):\n    \"\"\"\n    Gets the number of dis-similar entries betweeen two numpy arrays.\n    Here the application is for boolean numpy array, whichis why this metric\n    is actually hamming distance. Since this comparison is element-wise, the\n    number of entries in each array needs to be equal.\n\n    Parameters\n    ----------\n    M1 : numpy.array\n        Boolean numpy array of arbitrary dimensions. Could be a 1-d, 2-d\n        array (matrix) or higher order (tensor).\n    M2 : numpy.array\n        Boolean numpy array of arbitrary dimensions. Could be a 1-d, 2-d\n        array (matrix) or higher order (tensor).\n\n    Returns\n    -------\n    ham : int\n        Number of unequal elements between the two boolean numpy arrays.\n\n    \"\"\"\n\n    ham = 0\n    vec1 = list(M1.ravel())\n    vec2 = list(M2.ravel())\n    assert len(vec1) == len(vec2)\n    for i in range(len(vec1)):\n        if vec1[i] != vec2[i]:\n            ham += 1\n    return ham",
        "sha1": "3d9777933107d07b252e8b92685a27dc8aeed53a",
        "id": 242008
    },
    {
        "content": "def get_value(settings, key):\n    \"\"\"\n    >>> from collections import namedtuple\n    >>> s = namedtuple(\"Settings\", [\"foo\"])\n    >>> s.foo = \"bar\"\n    >>> get_value(s, \"foo\")\n    bar\n    >>> s.foodict = {\"subkey\": \"subvalue\"}\n    >>> get_value(s, \"foodict.subkey\")\n    subvalue\n    >>> s.foodict2 = {\"subkey1\": {\"subkey2\": \"subvalue\"}}\n    >>> get_value(s, \"foodict2.subkey1.subkey2\")\n    subvalue\n    >>> get_value(s, \"absent\")\n    Traceback (most recent call last):\n        ...\n    AttributeError: type object 'Settings' has no attribute 'absent'\n    >>> get_value(s, \"foodict.absent\")\n    Traceback (most recent call last):\n        ...\n    KeyError: 'absent'\n    \"\"\"\n    value = settings\n    for k in key.split(\".\"):\n        if isinstance(value, dict):\n            value = value[k]\n        else:\n            value = getattr(value, k)\n    return value",
        "sha1": "d7c8c110761f171cf253256af5f0cf119a6a309c",
        "id": 438826
    },
    {
        "content": "def get_secondary_connections(network, user):\n    \"\"\"Get the 2nd level of connections of a given user.\n\n    Keyword arguments:\n    network -- a dictionary containing users' connections and games\n    user -- the name of a person in the network\n    \"\"\"\n    try:\n        return list(set([secondary for primary in network[user]['friends']\n                        for secondary in network[primary]['friends']]))\n    except KeyError:\n        return None",
        "sha1": "56050eb1a437fd4d4b6ab0b1c3815dae7f89426d",
        "id": 606760
    },
    {
        "content": "from typing import Tuple\n\n\ndef all_subclasses(cls: type) -> Tuple[type, ...]:\n    \"\"\"All subclasses of a class.\n\n    From: http://stackoverflow.com/a/17246726/2692780\n    \"\"\"\n    subclasses = []\n    for subclass in cls.__subclasses__():\n        subclasses.append(subclass)\n        subclasses.extend(all_subclasses(subclass))\n    return tuple(subclasses)",
        "sha1": "6e5d77854a540e59f657f02e28576409e366739c",
        "id": 656879
    },
    {
        "content": "def _score_for_model(meta):\n    \"\"\" Returns mean score between tasks in pipeline that can be used for early stopping. \"\"\"\n    mean_acc = list()\n    pipes = meta[\"pipeline\"]\n    acc = meta[\"accuracy\"]\n    if \"tagger\" in pipes:\n        mean_acc.append(acc[\"tags_acc\"])\n    if \"parser\" in pipes:\n        mean_acc.append((acc[\"uas\"] + acc[\"las\"]) / 2)\n    if \"ner\" in pipes:\n        mean_acc.append((acc[\"ents_p\"] + acc[\"ents_r\"] + acc[\"ents_f\"]) / 3)\n    return sum(mean_acc) / len(mean_acc)",
        "sha1": "dc08b1f69b3c45ddb35645d3e7f9869e11f19274",
        "id": 308283
    },
    {
        "content": "import pytz\nfrom datetime import datetime\n\n\ndef determine_current_datetime(**args) -> tuple:\n    \"\"\"\n    Return args['today_date'] as a timezone aware python datetime object\n    \"\"\"\n    tz = pytz.timezone('America/Vancouver')\n    args['today_date'] = datetime.now(tz)\n    return True, args",
        "sha1": "cd31a44133955da015f9fce2794d89dc4351ca6e",
        "id": 46073
    },
    {
        "content": "def _check_default(value, default_value):\n    \"\"\"Return true if the new value is a match for the default\n    parameter value\n    \"\"\"\n    default_present = False\n    if str(default_value) == str(value):\n        default_present = True\n    return default_present",
        "sha1": "85be27a97ed8a7d90274bc3d96c036b6ad30230e",
        "id": 169645
    },
    {
        "content": "from typing import List\nimport io\nimport csv\nimport base64\n\n\ndef to_csv_data_url(data: List[List], header: List[str]) -> str:\n    \"\"\" Return data url with base64-encoded csv.\n    \"\"\"\n    f = io.StringIO()\n    writer = csv.writer(f)\n    writer.writerow(header)\n    writer.writerows(data)\n    b64data = base64.b64encode(f.getvalue().encode('utf8')).decode('ascii')\n    return f'data:text/csv;base64,{b64data}'",
        "sha1": "0c8d3a57ca1446e75e3cd65f556e2617efc3ffba",
        "id": 554662
    },
    {
        "content": "def _get_morphometry_data_suffix_for_surface(surf):\n    \"\"\"\n    Determine FreeSurfer surface representation string.\n\n    Determine the substring representing the given surface in a FreeSurfer output curv file. For FreeSurfer's default surface 'white', the surface is not represented in the output file name pattern. For all others, it is represented by a dot followed by the name.\n\n    Parameters\n    ----------\n    surf: string\n        A string representing a FreeSurfer surface, e.g., 'white' or 'pial'.\n\n    Returns\n    -------\n    string\n        The empty string if `surf` is 'white'. A dot followed by the string in the input argument `surf` otherwise.\n\n    Examples\n    --------\n    >>> import brainload.freesurferdata as fsd\n    >>> print fsd._get_morphometry_data_suffix_for_surface('pial')\n    .pial\n    \"\"\"\n    if surf == 'white':\n        return ''\n    return '.' + surf",
        "sha1": "fcc9a4611bd056a7159d88e5b8706efa40e2ab67",
        "id": 687688
    },
    {
        "content": "def _remove_equivs(string_to_fix):\n    \"\"\"\n    Removes the substring \"-equiv\" from strings. For use in unit conversion\n    Parameter\n    ---------\n    string_to_fix: str\n        The string to strip of \"-equiv\".\n    Returns\n    -------\n    str\n    \"\"\"\n    return string_to_fix.replace(\"-equiv\", \"\")",
        "sha1": "6bb2beceac3979ac675ce859fb82fbc2af1ccbe2",
        "id": 340852
    },
    {
        "content": "def runner_kwargs(options):\n    \"\"\"Given the command line options, return the arguments to\n    :py:class:`EMRJobRunner`\n    \"\"\"\n    kwargs = options.__dict__.copy()\n    for non_runner_kwarg in ('quiet', 'verbose', 'list_all', 'find',\n                             'terminate'):\n        del kwargs[non_runner_kwarg]\n\n    return kwargs",
        "sha1": "91059dc36b33ab41a92138045ecdcce77536101b",
        "id": 256492
    },
    {
        "content": "import re\n\n\ndef letter_case_to_delimiter(_str_in):\n    \"\"\" Converts TestAbcM to test_abc_m \"\"\"\n    return re.sub('(?<!^)(?=[A-Z])',\n                  '_',\n                  _str_in).lower()",
        "sha1": "64fdef1c0c778f0ec6bf24c7e76dfd052761920b",
        "id": 473397
    },
    {
        "content": "def identity(image, **kwargs):\n    \"\"\"Return the original image, ignoring any kwargs.\"\"\"\n    return image",
        "sha1": "24140cb0998b224d70135d645925ae93e64976fa",
        "id": 541389
    },
    {
        "content": "def millis_to_srt(millis):\n    \"\"\"Convert milliseconds time to the SRT subtitles format time string.\"\"\"\n    result = ''\n    # milliseconds\n    milliseconds = millis % 1000\n    result = ('%03d' % milliseconds) + result\n    millis = (millis - milliseconds) / 1000\n    # seconds\n    seconds = millis % 60\n    result = ('%02d,' % seconds) + result\n    millis = (millis - seconds) / 60\n    # minutes\n    minutes = millis % 60\n    result = ('%02d:' % minutes) + result\n    millis = (millis - minutes) / 60\n    # hours\n    result = ('%02d:' % millis) + result\n    # ready\n    return result",
        "sha1": "cfcf2376ee9a0f12150e0f6cd960cdf8b1c497a0",
        "id": 208494
    },
    {
        "content": "def color_string(color_number, text):\n    \"\"\"Return the text with color codes for the given color.\"\"\"\n    return '\\x1b[{0}m{1}\\x1b[0m'.format(color_number, text)",
        "sha1": "1e955a8c14df0f3e6de41d0b232202093f0ca152",
        "id": 278677
    },
    {
        "content": "def str_to_bool(boolstr, default=False):\n    \"\"\"Convert a string to a boolean.\"\"\"\n    boolstr = boolstr.lower()\n    if boolstr in (\"true\", \"yes\", \"on\", \"1\"):\n        return True\n    elif boolstr in (\"false\", \"no\", \"off\", \"0\"):\n        return False\n    else:\n        return default",
        "sha1": "dc2d6eed69973f72af03e93f84b6dd6f1e9449b7",
        "id": 505140
    },
    {
        "content": "import bz2\n\n\ndef decompress(data):\n    \"\"\"\n    Helper function to decompress data (using bz2)\n    \"\"\"\n    c = bz2.BZ2Decompressor()\n    return c.decompress(data)",
        "sha1": "18dee25febbc1e3ce9e709d19129169e60fb928a",
        "id": 369071
    },
    {
        "content": "from typing import Optional\n\n\ndef get_qname(uri: Optional[str], name: str) -> str:\n    \"\"\"\n    Returns an expanded QName from URI and local part. If any argument has boolean value\n    `False` or if the name is already an expanded QName, returns the *name* argument.\n\n    :param uri: namespace URI\n    :param name: local or qualified name\n    :return: string or the name argument\n    \"\"\"\n    if not uri or not name or name[0] in ('{', '.', '/', '['):\n        return name\n    else:\n        return '{%s}%s' % (uri, name)",
        "sha1": "6c9e0cf3dff8d78f5ada5aa35135314cbfb41ad5",
        "id": 636202
    },
    {
        "content": "def gccdicosnap(dicosnap,dicoedgeccm):\n    \"\"\"\n    Selects all time-stamped edges with nodes belonging to the maximal connected\n    component.\n\n    Parameters:\n        dicosnap (dict): Dictionary indexed by timestamps, whose keys are\n        edgelists for a given time-slice.\n        dicoedgeccm (dict): Dictionary indexed by all edges in the aggregated\n        network.\n\n    Returns:\n        dicosnapccm (dict): Dictionary indexed by timestamps, whose keys are\n        edgelists for a given time-slice, with only edges in the c.c.\n    \"\"\"\n    dicosnapccm = {}\n    for t in dicosnap.keys():\n        dicosnapccm[t] = {}\n        dicosnapccm[t][\"edgelist\"] = []\n        for edge in dicosnap[t][\"edgelist\"]:\n            if edge in dicoedgeccm.keys():\n                dicosnapccm[t][\"edgelist\"].append(edge)\n\n    return dicosnapccm",
        "sha1": "5c9cb173eee2bffa2f82edbf03ed05f5e1a7d2f0",
        "id": 575731
    },
    {
        "content": "def location(text, index):\n    \"\"\"\n    Location of `index` in the `text`. Report row and column numbers when\n    appropriate.\n\n    \"\"\"\n    if isinstance(text, str):\n        line, start = text.count('\\n', 0, index), text.rfind('\\n', 0, index)\n        column = index - (start + 1)\n        return \"{}:{}\".format(line + 1, column + 1)\n    else:\n        return str(index + 1)",
        "sha1": "5f8026d3d30267833c6014a6d338b9d7fb2e5294",
        "id": 61890
    },
    {
        "content": "def parse_seed(seed_str):\n    \"\"\"converts 3-4 char seed string to integer value\"\"\"\n    return int(seed_str[1:3])",
        "sha1": "81afb0124653b1b41757a8747625306b271be346",
        "id": 574141
    },
    {
        "content": "def _size_in_list(same_arr_size_list, size):\n    \"\"\"\n    This function, for a given list of ArraysSameSize objects, checks\n    if a certain size in inside that list\n    \"\"\"\n    for elem in same_arr_size_list:\n        if elem.size == size:\n            return True\n    return False",
        "sha1": "1f185bd0e637168b5757b63b73523966bad60687",
        "id": 494370
    },
    {
        "content": "def _MergeLinkedMembers(cnxn, user_service, user_ids):\n  \"\"\"Remove any linked child accounts if the parent would also be shown.\"\"\"\n  all_ids = set(user_ids)\n  users_by_id = user_service.GetUsersByIDs(cnxn, user_ids)\n  result = [uid for uid in user_ids\n            if users_by_id[uid].linked_parent_id not in all_ids]\n  return result",
        "sha1": "075138804b464438227e82baf1f084ddb2fb32ff",
        "id": 233456
    },
    {
        "content": "def calculate_steps(time):\n    \"\"\"\n    Function to calculate the amount of steps for each trayectory.\n    \"\"\"\n    return int(time * 100)",
        "sha1": "d418c852d831cc4aad04ccc001e57da0867abb96",
        "id": 419954
    },
    {
        "content": "def stringify(obj):\n    \"\"\"Helper method which converts any given object into a string.\"\"\"\n\n    if isinstance(obj, list):\n        return ''.join(obj)\n    else:\n        return str(obj)",
        "sha1": "b244bd17d378299dc5303545bf635066a92b738e",
        "id": 199995
    },
    {
        "content": "def clean(elems):\n    \"\"\"\n    This method takes a list of scraped selenium web elements\n    and filters/ returns only the hrefs leading to publications.\n    \"\"\"\n\n    urls = []\n    for elem in elems:\n        url = elem.get_attribute(\"href\")\n        if 'article' in url and 'pdf' not in url\\\n                            and 'search' not in url\\\n                            and 'show=' not in url:\n            urls.append(url)\n    return urls",
        "sha1": "8703797a615c8662def5cea40c7805c51694da75",
        "id": 542126
    },
    {
        "content": "def BM3_EOS_energy (V, V0, E0, K0, Kp0):\n    \"\"\"Calculate the energy from a 3rd order BM EOS\"\"\"\n\n    E = E0 + ((9.0*V0*K0)/16.0) * ( (((V0/V)**(2.0/3.0)-1.0)**3.0)*Kp0 +\n             (((V0/V)**(2.0/3.0) - 1.0)**2.0 * (6.0-4.0*(V0/V)**(2.0/3.0))))\n    return E",
        "sha1": "b0a4371e9710a3be08541dae6f2d1650ada61d8b",
        "id": 245908
    },
    {
        "content": "def evaluate_rc(rc_definition, cv_list):\n    \"\"\"\n    Evaluate the RC value given by RC definition for the given list of CV values given by cv_list.\n\n    Parameters\n    ----------\n    rc_definition : str\n        A reaction coordinate definition formatted as a string of python-readable code with \"CV[X]\" standing in for the\n        Xth CV value (zero-indexed); e.g., \"CV2\" has value \"4\" in the cv_list [1, -2, 4, 6]\n    cv_list : list\n        A list of CV values whose indices correspond to the desired values in rc_definition\n\n    Returns\n    -------\n    rc_value : float\n        The value of the reaction coordinate given the values in cv_list\n\n    \"\"\"\n\n    # Fill in CV[X] slots with corresponding values from cv_list\n    for i in reversed(range(len(cv_list))):     # reversed so that e.g. CV10 doesn't get interpreted as '[CV1]0'\n        rc_definition = rc_definition.replace('CV' + str(i + 1), str(cv_list[i]))\n\n    # Evaluate the filled-in rc_definition and return the result\n    return eval(rc_definition)",
        "sha1": "3fff4efdaa497b721f62b0ddad4344ecf8f2e230",
        "id": 141410
    },
    {
        "content": "def flatten(_list):\n    \"\"\"Flatten nested list.\"\"\"\n    return [item for sublist in _list for item in sublist]",
        "sha1": "696c10ff1d18b0721cf4a36aac0e148483f898d5",
        "id": 265352
    },
    {
        "content": "def _html_template(site_name, author, js_snippet, css_snippet):\n    \"\"\"Create a HTML Template given a site name,\n    author, snippet to include JS files and CSS files\n    \"\"\"\n    template_data = {\n        'site_name': site_name,\n        'author': author,\n        'js': js_snippet,\n        'css': css_snippet\n    }\n    with open('module43.template') as filehandle:\n        template = filehandle.read()\n    return template.format_map(template_data)",
        "sha1": "16726c344a441c43e097fbb0f5445f08fb767ecb",
        "id": 671117
    },
    {
        "content": "def remove_nones_from_dict(orig_dict):\n    \"\"\"Returns a copy of a dictionary with any none values removed\"\"\"\n    d = {}\n    for k, v in orig_dict.items():\n        if isinstance(v, dict):\n            d[k] = remove_nones_from_dict(v)\n        else:\n            if v is not None:\n                d[k] = v\n    return d",
        "sha1": "20a94113bdcc6ac78973e96b35b81e676485b890",
        "id": 425266
    },
    {
        "content": "import re\n\n\ndef get_nc_set_id(xml_str):\n    \"\"\"get netconf set-id value\"\"\"\n\n    result = re.findall(r'<rpc-reply.+?set-id=\\\"(\\d+)\\\"', xml_str)\n    if not result:\n        return None\n    return result[0]",
        "sha1": "b0127e78dee1bf790015776041639af6e1873467",
        "id": 180729
    },
    {
        "content": "def pts_from_rect_outside(r):\n  \"\"\" returns start_pt, end_pt where end_pt is _outside_ the rectangle \"\"\"\n  return (r[0], r[1]), ((r[0] + r[2]), (r[1] + r[3]))",
        "sha1": "615a0c7a546a72a6375a8ae94fe0052922633d12",
        "id": 579205
    },
    {
        "content": "def g7_8_enrollment(parcels, k12):\n    \"\"\"\n    Enrollment for grades 7-8 (middle).\n\n    \"\"\"\n    k = k12.local\n    return k.groupby(\n        'parcel_id')['G7_8'].sum().reindex(parcels.index).fillna(0)",
        "sha1": "d7ffae61a21a64a08dffd063dbbf008e71435294",
        "id": 442110
    },
    {
        "content": "from bs4 import BeautifulSoup\n\n\ndef get_tables_from_html_page(html_page):\n    \"\"\"Given an html page, return all html tables on the page and\n    their contents, i.e., all <td ..>, <tr ..>\"\"\"\n  \n    #tables = (BeautifulSoup(html_page)).findChildren('table')\n    tables = (BeautifulSoup(html_page)).find_all('table')\n  \n    return tables",
        "sha1": "218a541c8beab9ae7b909a430d2a88bf1edad677",
        "id": 315421
    },
    {
        "content": "import torch\n\n\ndef argmin(x):\n    \"\"\"Deterministic argmin.\n\n    Different from torch.argmin, which may have undetermined result if the are\n    multiple elements equal to the min, this argmin is guaranteed to return the\n    index of the first element equal to the min in each row.\n\n    Args:\n        x (Tensor): only support rank-2 tensor\n    Returns:\n        rank-1 int64 Tensor represeting the column of the first element in each\n        row equal to the minimum of the row.\n    \"\"\"\n    assert x.ndim == 2\n    m, _ = x.min(dim=1, keepdim=True)\n    r, c = torch.nonzero(x == m, as_tuple=True)\n    r, num_mins = torch.unique(r, return_counts=True)\n    i = torch.cumsum(num_mins, 0)\n    i = torch.cat([torch.tensor([0]), i[:-1]])\n    return c[i]",
        "sha1": "6ff5546b96b964c93d92a1b2d507e5ada703c2fc",
        "id": 670020
    },
    {
        "content": "import torch\n\n\ndef rescale_torch_tensor(\n    tensor: torch.Tensor,\n    new_min: torch.Tensor,\n    new_max: torch.Tensor,\n    prev_min: torch.Tensor,\n    prev_max: torch.Tensor,\n):\n    \"\"\"\n    Rescale column values in N X M torch tensor to be in new range.\n    Each column m in input tensor will be rescaled from range\n    [prev_min[m], prev_max[m]] to [new_min[m], new_max[m]]\n    \"\"\"\n    assert tensor.shape[1] == new_min.shape[1] == new_max.shape[1]\n    assert tensor.shape[1] == prev_min.shape[1] == prev_max.shape[1]\n    prev_range = prev_max - prev_min\n    new_range = new_max - new_min\n    return ((tensor - prev_min) / prev_range) * new_range + new_min",
        "sha1": "50057d4c4925002a8e3548e98054452ead05693d",
        "id": 324640
    },
    {
        "content": "def recursive_len(x):\n    \"\"\"Return the len of a string without using len but recursively\n    \n        Example:\n        >>> a = \"hola\"\n        >>> recursive_len(a)\n        >>> 4\n    \"\"\"\n    if x == \"\":\n        return 0\n\n    return 1 + recursive_len(x[1:])",
        "sha1": "bded0e9fa261e1ce9189424c671cb60a6e00de35",
        "id": 343537
    },
    {
        "content": "from typing import List\nfrom typing import Dict\nfrom typing import Any\n\n\ndef build_urls_dict(regions_list: list, services_list: list, unique_id) -> List[Dict[str, Any]]:\n    \"\"\"Builds a URL dictionary with the relevant data for each service\n\n    Args:\n        regions_list: list of regions\n        services_list: list of services\n        unique_id: unique uuid\n\n    Returns:\n        URLs services list\n    \"\"\"\n    urls_list = []\n    for region in regions_list:\n        for service in services_list:\n            if service == 'All':\n                url = f'https://endpoints.office.com/endpoints/{region}?ClientRequestId={unique_id}'\n            else:\n                url = f'https://endpoints.office.com/endpoints/{region}?ServiceAreas={service}' \\\n                      f'&ClientRequestId={unique_id}'\n            urls_list.append({\n                'Region': region,\n                'Service': service,\n                'FeedURL': url\n            })\n    return urls_list",
        "sha1": "89c9c16bdded1f17d2fdcf7a22b3852728057a63",
        "id": 529921
    },
    {
        "content": "def lap(gr, u):\n    \"\"\" compute the Laplacian of u, including the first ghost cells \"\"\"\n\n    lapu = gr.scratchArray()\n\n    ib = gr.ilo-1\n    ie = gr.ihi+1\n\n    lapu[ib:ie+1] = (u[ib-1:ie] - 2.0*u[ib:ie+1] + u[ib+1:ie+2])/gr.dx**2\n\n    return lapu",
        "sha1": "22614436ff22adae74e13b19fb5188c76a899e4e",
        "id": 432225
    },
    {
        "content": "def map_image_values(image, old_range, new_range):\n    \"\"\"\n    Maps the values of an image to a new range and datatype.\n\n    Parameters:\n        image (array)             -- input array\n        old_range (tuple)         -- the value range of the current image\n        new_range (tuple)         -- the desired new value range of the image\n    \"\"\"\n    old_space_size = old_range[1] - old_range[0]\n    new_space_size = new_range[1] - new_range[0]\n    multiplier = new_space_size / old_space_size\n\n    image -= old_range[0]\n    image *= multiplier\n    image += new_range[0]\n\n    return image",
        "sha1": "e28cd7936808d40cc48e99c872e92006943c8850",
        "id": 349949
    },
    {
        "content": "import torch\n\n\ndef _load_model_from_state_dict(model, fname):\n    \"\"\"Load trained model based on instance and path to state dictionary.\n    \"\"\"\n    state_dict = torch.load(fname, map_location={'cuda:0': 'cpu'})\n    model.load_state_dict(state_dict)\n    return model",
        "sha1": "1385f947beeb3fec58da44872ea6c15a46c3eda7",
        "id": 403789
    },
    {
        "content": "from pathlib import Path\n\n\ndef get_paths(dir_name, glob):\n    \"\"\" returns a generator of the recursive paths on input glob \"\"\"\n    return Path(f'./{dir_name}').rglob(glob)",
        "sha1": "6803981e40397d000900dd0a8fc8ee32eddc6bc4",
        "id": 700487
    },
    {
        "content": "def repr_author(Author):\n  \"\"\"\n  Get string representation an Author namedtuple in the format:\n  von Last, jr., First.\n\n  Parameters\n  ----------\n  Author: An Author() namedtuple\n     An author name.\n\n  Examples\n  --------\n  >>> from bibmanager.utils import repr_author, parse_name\n  >>> names = ['Last', 'First Last', 'First von Last', 'von Last, First',\n  >>>          'von Last, sr., First']\n  >>> for name in names:\n  >>>     print(f\"{name!r:22}: {repr_author(parse_name(name))}\")\n  'Last'                : Last\n  'First Last'          : Last, First\n  'First von Last'      : von Last, First\n  'von Last, First'     : von Last, First\n  'von Last, sr., First': von Last, sr., First\n  \"\"\"\n  name = Author.last\n  if Author.von != \"\":\n      name = \" \".join([Author.von, name])\n  if Author.jr != \"\":\n      name += f\", {Author.jr}\"\n  if Author.first != \"\":\n      name += f\", {Author.first}\"\n  return name",
        "sha1": "f15bba99a1c6466a6b3e0fbd30ac32109f6447b5",
        "id": 28679
    },
    {
        "content": "def mm_as_m(mm_value):\n    \"\"\"Turn a given mm value into a m value.\"\"\"\n    if mm_value == 'None':\n        return None\n    return float(mm_value) / 1000",
        "sha1": "7c5525b1e16801f67ee76484a2471ac3f6c17c40",
        "id": 679091
    },
    {
        "content": "def _firstspin(bands):\n    \"\"\"Get only the bands for the first spin if multiple are contained.\"\"\"\n    if bands.ndim not in [2, 3]:\n        raise ValueError('invalid input')\n    if bands.ndim == 3:\n        bands = bands[0]\n    return bands",
        "sha1": "d412c864cc15ad0e4a3b357c085a0acb010b5e45",
        "id": 89191
    },
    {
        "content": "def affixed(text, prefix=None, suffix=None, normalize=None):\n    \"\"\"\n    Args:\n        text (str | None): Text to ensure prefixed\n        prefix (str | None): Prefix to add (if not already there)\n        suffix (str | None): Suffix to add (if not already there)\n        normalize (callable | None): Optional function to apply to `text`\n\n    Returns:\n        (str | None): `text' guaranteed starting with `prefix` and ending with `suffix`\n    \"\"\"\n    if text is not None:\n        if normalize:\n            text = normalize(text)\n\n        if prefix and not text.startswith(prefix):\n            text = prefix + text\n\n        if suffix and not text.endswith(suffix):\n            text = text + suffix\n\n    return text",
        "sha1": "2ebd8d734ae3cd641212ae173e7e804e359ffffa",
        "id": 245057
    },
    {
        "content": "def is_prime(n):\n    \"\"\"Determine whether the given integer n is prime.\"\"\"\n    result = True\n    for i in range(n - 1, 2, -1):\n        if n % i == 0:\n            # If n is divisible by a smaller integer, then n is not prime.\n            result = False\n            break\n\n    return result",
        "sha1": "bbd39ab5f65fcf59d8ff73934f6cacdaaef22b90",
        "id": 294948
    },
    {
        "content": "import math\n\n\ndef shock_speed(p2, d1, p1, g):\n\n\t\"\"\"\n\tCalculates the shock speed in the upstream reference frame\n\tInput:\n\tp2 - Downstream pressure\n\td1 - Upstream density\n\tp1 - Upstream pressure\n\tg - Adiabatic index\n\t\"\"\"\n\t\n\t\n\treturn math.sqrt(-p1 + g*p1 + p2 + g*p2)/(math.sqrt(2)*math.sqrt(d1))",
        "sha1": "cbb68f2ba862a0006f79312507f73b993dade2fa",
        "id": 383896
    },
    {
        "content": "from typing import Optional\nimport re\n\n\ndef isolate_git_commit_in_release(release: str) -> Optional[str]:\n    \"\"\"\n    Given a release field, determines whether is contains\n    .git.<commit> information or .g<commit> (new style). If it does, it returns the value\n    of <commit>. If it is not found, None is returned.\n    \"\"\"\n    match = re.match(r'.*\\.git\\.([a-f0-9]+)(?:\\.+|$)', release)\n    if match:\n        return match.group(1)\n\n    match = re.match(r'.*\\.g([a-f0-9]+)(?:\\.+|$)', release)\n    if match:\n        return match.group(1)\n\n    return None",
        "sha1": "f88da2d7430ec1d7ef4c7cc7a4dcd1cfd860ec2f",
        "id": 254173
    },
    {
        "content": "import math\n\n\ndef floor_to_odd_integer(x: float) -> int:\n    \"\"\"\n    Round down to the nearest odd number.\n\n    >>> floor_to_odd_integer(3)\n    3\n    >>> floor_to_odd_integer(4)\n    3\n\n    :param x: Value to be rounded.\n    :return: Nearest lower odd number no larger than the provided value.\n    \"\"\"\n    x_floor = math.floor(x)\n    return x_floor if x_floor % 2 == 1 else x_floor - 1",
        "sha1": "4f525cf29d289695c035b3c1b7d1c7f5104fd752",
        "id": 196345
    },
    {
        "content": "def choose_max_efficiency(efficiencies):\n    \"\"\"\n    Given a single or list of DOM efficiencies choose the highest\n    \"\"\"\n    if type(efficiencies) == list or type(efficiencies) == tuple:\n       return max(map(float,efficiencies)) \n    else:\n       return float(efficiencies)",
        "sha1": "736327b6d33d84125aa8df1f1b31a018d0ec8691",
        "id": 649907
    },
    {
        "content": "from warnings import warn\nfrom functools import reduce\n\n\ndef add_track_prop_dicts(track_prop_dicts):\n    \"\"\"Adds together multiple track properties dicts of with the same\n    properties.\n\n    Args:\n        track properties_dicts: a list of track properties dicts with\n            the same properties.\n\n    Returns:\n        A track properties dictionary with the lists of values of each\n        track properties dict in the input list concatenated.\n\n    Raises:\n        ValueError: if there is no property shared by all of the track\n            property dicts.\n    \"\"\"\n\n    def add_two_track_prop_dicts(tp_so_far, tp_to_add):\n        \"\"\"Adds two track properties dicts together as per rules in\n        parent function. Returns the sum.\"\"\"\n\n        props_in_common = set(tp_so_far.keys()).intersection(\n                set(tp_to_add.keys()))\n\n        if props_in_common != set(tp_to_add.keys()):\n            warn(\"Track property dicts have differing value lists. \"\n                    \"Will add only properties in common: {}\"\n                    .format(props_in_common), UserWarning)\n\n        if not len(props_in_common):\n            raise ValueError(\"Track property dicts to add have no properties \"\n                    \"in common.\")\n\n        return dict(map(lambda track_property:\n            (track_property,\n                tp_so_far[track_property] + tp_to_add[track_property]),\n            props_in_common))\n\n    return reduce(add_two_track_prop_dicts, track_prop_dicts)",
        "sha1": "f365515c4c6b08a9d7c02ad1d086835b7f89aca9",
        "id": 382071
    },
    {
        "content": "import torch\n\n\ndef reps_dot(sent1_reps: torch.Tensor, sent2_reps: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    calculate representation dot production\n    :param sent1_reps: (N, sent1_len, reps_dim)\n    :param sent2_reps: (N, sent2_len, reps_dim)\n    :return: (N, sent1_len, sent2_len)\n    \"\"\"\n    return torch.bmm(sent1_reps, torch.transpose(sent2_reps, -1, -2))",
        "sha1": "ce8790433820f573b7c8c5ccd1f388abd917f513",
        "id": 47765
    },
    {
        "content": "def wrap_seq_string(seq_string, n=60):\n    \"\"\"Wrap nucleotide string (seq_string) so that each line of fasta has length n characters (default n=60)\n \n    Args:\n        seq_string (str): String of DNA/protein sequence\n        n (int): Maximum number of characters to display per line\n\n    Returns:\n        (str): String of DNA/protein sequence wrapped to n characters\n\n    \"\"\"\n    chunks = [seq_string[i:i + n] for i in range(0, len(seq_string), n)]\n    return('\\n'.join(chunks))",
        "sha1": "6987d3066b8907ad6ec44a374a3d86c6a2bb7b4c",
        "id": 672757
    },
    {
        "content": "def check_value(value: bytearray, size_value: int) -> bool:\n    \"\"\"\n    Check the correctness of the variable.\n\n    This function checks the type ('bytes' or 'bytearray') and whether\n    the size of the 'value' variable matches the 'size_value' value.\n\n    Args:\n        value: The variable that you want to check.\n        saize_value: The required size of the variable.\n\n    Returns:\n        Check result.\n    \"\"\"\n    result = True\n    if (not isinstance(value, (bytes, bytearray))) or len(value) != size_value:\n        result = False\n    return result",
        "sha1": "4a80852f366cf96e7742cf68fcbf58ef51d86b40",
        "id": 62323
    },
    {
        "content": "import itertools\n\n\ndef strip_qualifiers(typename):\n    \"\"\"Remove const/volatile qualifiers, references, and pointers of a type\"\"\"\n    qps = []\n\n    while True:\n        typename = typename.rstrip()\n        qual = next(itertools.dropwhile(lambda q: not typename.endswith(q), ('&', '*', 'const', 'volatile', '')))\n        if not qual: break\n        typename = typename[:-len(qual)]\n        qps.append(qual)\n\n    while True:\n        typename = typename.lstrip()\n        qual = next(itertools.dropwhile(lambda q: not typename.startswith(q), ('const', 'volatile', '')))\n        if not qual: break\n        typename = typename[len(qual):]\n        qps.append(qual)\n\n    return typename, qps[::-1]",
        "sha1": "1e6d2eec2b8b192f779efc236b54fd250ecca574",
        "id": 76898
    },
    {
        "content": "def lrc(nibble1, nibble2, nibble3):\n    \"\"\"Returns the \"Longitudinal Redundancy Check\" nibble.\"\"\"\n    return 0xf ^ nibble1 ^ nibble2 ^ nibble3",
        "sha1": "ae32086124d7e1272b8ab7b22c8a4a8f7db803b2",
        "id": 317449
    },
    {
        "content": "import json\n\n\ndef get_target_names(json_label_decode):\n    \"\"\"Get encode of label\n\n    Args: \n        json_label_decode (string): path to json file\n\n    Returns:\n        [dict] encode of label\n    \"\"\"\n    if json_label_decode:\n        with open(json_label_decode) as f:\n            label_decode = json.load(f)\n\n        target_names = []\n        for i in label_decode:\n                target_names.append(label_decode[i])\n\n        return target_names\n        \n    else: raise ValueError('[ERROR]: {} is not found'.format(json_label_decode))",
        "sha1": "556dc3837fea9271b886ebcf02d6c6323061405c",
        "id": 656346
    },
    {
        "content": "def get_instance_pci_devs(inst, request_id=None):\n    \"\"\"Get the devices allocated to one or all requests for an instance.\n\n    - For generic PCI request, the request id is None.\n    - For sr-iov networking, the request id is a valid uuid\n    - There are a couple of cases where all the PCI devices allocated to an\n      instance need to be returned. Refer to libvirt driver that handles\n      soft_reboot and hard_boot of 'xen' instances.\n    \"\"\"\n    pci_devices = inst.pci_devices\n    if pci_devices is None:\n        return []\n    return [device for device in pci_devices if\n                   device.request_id == request_id or request_id == 'all']",
        "sha1": "716ca1164e19f0ad052ebdaab38e603864e8ee10",
        "id": 395610
    },
    {
        "content": "def nthwords2int(nthword):\n    \"\"\"Takes an \"nth-word\" (eg 3rd, 21st, 28th) strips off the ordinal ending\n    and returns the pure number.\"\"\"\n\n    ordinal_ending_chars = 'stndrh'  # from 'st', 'nd', 'rd', 'th'\n\n    try:\n        int_output = int(nthword.strip(ordinal_ending_chars))\n    except Exception as e:\n        raise Exception('Illegal nth-word: ' + nthword)\n\n    return int_output",
        "sha1": "10338beff5fdafb612efc090e6d0edd39c757c1b",
        "id": 486925
    },
    {
        "content": "def get_site_root(context):\n    \"\"\"\n    Returns the root page by looking in the request for the current site.\n    \"\"\"\n    return context['request'].site.root_page",
        "sha1": "a945f691706eb52c85bb4d1bb04c7894a408cc0a",
        "id": 264474
    },
    {
        "content": "def shorten_str(text: str, width: int = 30, suffix: str = \"[...]\") -> str:\n    \"\"\"Custom string shortening (`textwrap.shorten` collapses whitespace).\"\"\"\n    if len(text) <= width:\n        return text\n    else:\n        return text[: width - len(suffix)] + suffix",
        "sha1": "9b41b35222bdbdc528ab64aae05bb52851561ebb",
        "id": 604503
    },
    {
        "content": "def name_to_tag(name):\n    \"\"\"\n    Returns sanitized str `name`: no more than 10 characters,\n    with spaces replaced with `_`\n    \"\"\"\n    if len(name) > 10:\n        name = name[:10]\n    return name.replace(' ', '_').strip()",
        "sha1": "7ccc6b117e554b24cb8e3470a61499f12bbc2f66",
        "id": 435443
    },
    {
        "content": "def appropriate_partition(distance):\n    \"\"\"Find appropriate partition of a distance into parts.\n\n    Parameters\n    ----------\n    distance : float\n        Traveled distance in meters\n\n    Returns\n    -------\n    segment_distance : float\n        Appropriate length of segments in which we split the total distance\n    \"\"\"\n    if distance < 5000:\n        return 400\n    elif distance < 20000:\n        return 1000\n    elif distance < 40000:\n        return 2000\n    elif distance < 100000:\n        return 5000\n    else:\n        return 10000",
        "sha1": "35d7888166ea416d470e106fbbfc3e07af6671e1",
        "id": 34227
    },
    {
        "content": "def SerializeAttributesToJsonDict(json_dict, instance, attributes):\n  \"\"\"Adds the |attributes| from |instance| to a |json_dict|.\n\n  Args:\n    json_dict: (dict) Dict to update.\n    instance: (object) instance to take the values from.\n    attributes: ([str]) List of attributes to serialize.\n\n  Returns:\n    json_dict\n  \"\"\"\n  json_dict.update({attr: getattr(instance, attr) for attr in attributes})\n  return json_dict",
        "sha1": "1a8a915c350e11397e89a58a77184c960758b548",
        "id": 424005
    },
    {
        "content": "import re\n\n\ndef assembly_contig_coverage(name):\n    \"\"\"From a contig name return the coverage (from the fasta name)\n    Example:\n        NODE-1-length-34650-cov-6.786732\n    will return 6.786732 or 0 if failed.\n    \"\"\"\n    match = re.match(r'.*cov[-|_](?P<cov>\\d*\\.?\\d*)', name)\n    if match:\n        return match.group('cov')\n    return 0",
        "sha1": "608d752f8fa5d82bc3943f6c5950d98da18934a1",
        "id": 408778
    },
    {
        "content": "def decode_rgb565(val):\n    \"\"\"Decode a RGB565 uint16 into a RGB888 tuple.\"\"\"\n    r5 = (val & 0xf800) >> 11\n    g6 = (val & 0x7e0) >> 5\n    b5 = val & 0x1f\n    return (\n        int((r5 * 255 + 15) / 31),\n        int((g6 * 255 + 31) / 63),\n        int((b5 * 255 + 15) / 31)\n    )",
        "sha1": "0c9a67016df686eb23282de74f663493caa305a9",
        "id": 687811
    },
    {
        "content": "import math\n\n\ndef distance_calculation(homeLattitude, homeLongitude, destinationLattitude, destinationLongitude):\n\n    \"\"\"\n\n    This function returns the distance between two geographiclocations using\n    the haversine formula.\n\n    Inputs:\n        1.  homeLattitude          -   Home or Current Location's  Latitude\n        2.  homeLongitude          -   Home or Current Location's  Longitude\n        3.  destinationLattitude   -   Destination Location's  Latitude\n        4.  destinationLongitude   -   Destination Location's  Longitude\n\n    \"\"\"\n\n    # Radius of earth in metres\n    R = 6371e3\n\n    rlat1, rlon1 = homeLattitude * (math.pi/180), homeLongitude * (math.pi/180)\n    rlat2, rlon2 = destinationLattitude * (math.pi/180), destinationLongitude * (math.pi/180)\n    dlat = (destinationLattitude - homeLattitude) * (math.pi/180)\n    dlon = (destinationLongitude - homeLongitude) * (math.pi/180)\n\n    # Haversine formula to find distance\n    a = (math.sin(dlat/2) * math.sin(dlat/2)) + (math.cos(rlat1) * math.cos(rlat2) * (math.sin(dlon/2) * math.sin(dlon/2)))\n    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1-a))\n\n    # Distance (in meters)\n    distance = R * c\n\n    return distance",
        "sha1": "8cbe8df1e771f180cc61293c44f3ed46f98a42d2",
        "id": 648399
    },
    {
        "content": "def deg2tenths_of_arcminute(deg):\n    \"\"\"\n    Return *deg* converted to tenths of arcminutes (i.e., arcminutes *\n    10).\n    \"\"\"\n    return 10 * deg * 60",
        "sha1": "2f8ab77642d91eb4e061ad30727ed0098257501c",
        "id": 76686
    },
    {
        "content": "def get_index_with_default(header, column_name, default_value=None):\n    \"\"\"Helper function to extract the index of a column.\"\"\"\n    return header.index(column_name) if column_name in header else default_value",
        "sha1": "0a2fefc8def6e6d91c4852d42da0db5ca4813a8c",
        "id": 47233
    },
    {
        "content": "def get_file_content(file_path):\n    \"\"\"Returns file content.\"\"\"\n    with open(file_path, 'r+') as f:\n        file_content = f.read()\n    return file_content",
        "sha1": "ed5655c80538fa9f0615c5b53f7b2ec7cc39b99a",
        "id": 82527
    },
    {
        "content": "def annotation_filter(annotations, condition):\n    \"\"\"\n    Filter annotations.\n    `annotations`: the annotations to filter\n    `condition`: the filter callback\n    return: the filtered annotations\n    \"\"\"\n    filtered = dict()\n    for (key, value) in annotations.items():\n        if condition(key, value):\n            filtered[key] = value\n    return filtered",
        "sha1": "517e19ab9888964d77326a8bec8f4d469e22bc10",
        "id": 651272
    },
    {
        "content": "def all_ped_combos_lsts(num_locs=4, val_set=(\"0\", \"1\")):\n    \"\"\"Return a list of all pedestrian observation combinations (in list format) for a vehicle under the 4 location scheme\"\"\"\n    res = []\n    if num_locs == 0:\n        return []\n    if num_locs == 1:\n        return [[flag] for flag in val_set]\n\n    for comb in all_ped_combos_lsts(num_locs - 1, val_set):\n        # append a flag for all possible flags\n        for flag in val_set:\n            appended = comb + [flag]\n            res.append(appended)\n            \n    return res",
        "sha1": "e65594b9917b7d170b8ae433e35d55dfa6cf867b",
        "id": 540235
    },
    {
        "content": "import logging\nimport requests\n\n\ndef _get(url, headers):\n    \"\"\" GET request with the proper headers \"\"\"\n\n    logging.info(\"Sending request to base URL...\")\n    ret = requests.get(url, headers=headers)\n    if ret.status_code != 200:\n        logging.error('Status code {status} for url {url}\\n{content}'.format(\n            status=ret.status_code, url=url, content=ret.text))\n\n    logging.info(f\"Website returned status code : {ret.status_code}\")\n    return ret",
        "sha1": "921575d784b7720779c27b15c55bc82ac5ab02c8",
        "id": 316098
    },
    {
        "content": "def create_dict_neighbors(H):\n    \"\"\"\n    Given our undirected graph, Create a dictionary where value==node and key==set of its neighbours.\n    \"\"\"\n    G = H.to_undirected()\n    G_nodes = list(G.nodes())\n    neighbors_dict = {}\n    for i in range(len(G_nodes)):\n        node = G_nodes[i]\n        neighbors_dict.update({node: set(G[node])})\n    return neighbors_dict",
        "sha1": "70e273fc3593e8dce75b4f2d2c3b37647e001895",
        "id": 327011
    },
    {
        "content": "def docstring_parameter(*sub):\n    \"\"\"\n    Inject variables into docstrings of functions.\n\n    This is used in below main function to get the version and description\n    of the package to the click help screen.\n    \"\"\"\n    # decorator definition\n    def dec(obj):\n        obj.__doc__ = obj.__doc__.format(*sub)\n        return obj\n\n    return dec",
        "sha1": "9a307dee4fd5842add5dbf27e96d601ed7b81b47",
        "id": 330038
    },
    {
        "content": "def identity(x):\n    \"\"\"The identity activation function.\n    Shortcut is ``linear``.\n\n    Parameters\n    ----------\n    x : Tensor\n        input.\n\n    Returns\n    -------\n    Tensor\n        A ``Tensor`` in the same type as ``x``.\n\n    \"\"\"\n    return x",
        "sha1": "6815fd8d46c0ec3921a39ff588ee7d9b19ed0e67",
        "id": 507345
    },
    {
        "content": "import gzip\nimport base64\nimport json\n\n\ndef decode_event(event):\n    \"\"\"\n    Unzip and decode given event.\n\n    :param event: CloudWatch Log event.\n    :type event: dict\n\n    :return: Unzipped and decoded event.\n    :rtype: dict\n    \"\"\"\n    decoded_json_event = gzip.decompress(base64.b64decode(event[\"awslogs\"][\"data\"]))\n    decoded_event = json.loads(decoded_json_event)\n\n    return decoded_event",
        "sha1": "3f14ee9a19537940acc39a2bf79b9f34cfd9bcdf",
        "id": 281548
    },
    {
        "content": "def check_strings(text: str, pattern: str) -> int:\n    \"\"\"\n        Helper function for checking conditions on both the text and pattern\n        to see if searching is needed at all.\n\n        Arguments:\n            text - The text we're searching through\n            pattern - The pattern we're searching for\n\n        Return:\n            int that indicates a status.\n            0  - Text is empty and so is the pattern, one match\n            -1 - Text is empty but there is a pattern, no match\n            1  - pattern is empty, meaning that there is a match\n            2  - Text and pattern are both strings >= 1 character\n    \"\"\"\n    if not text and not pattern:\n        return 0\n    elif not text:\n        return -1\n    elif not pattern:\n        return 1\n    else:\n        return 2",
        "sha1": "2fc9873859e301dcf243d9f5615068ae3dba997b",
        "id": 188292
    },
    {
        "content": "import time\n\n\ndef get_nearest_year_for_day(day):\n    \"\"\"\n    Returns the nearest year to now inferred from a Julian date.\n\n    >>> freezer = getfixture('freezer')\n    >>> freezer.move_to('2019-05-20')\n    >>> get_nearest_year_for_day(20)\n    2019\n    >>> get_nearest_year_for_day(340)\n    2018\n    >>> freezer.move_to('2019-12-15')\n    >>> get_nearest_year_for_day(20)\n    2020\n    \"\"\"\n    now = time.gmtime()\n    result = now.tm_year\n    # if the day is far greater than today, it must be from last year\n    if day - now.tm_yday > 365 // 2:\n        result -= 1\n    # if the day is far less than today, it must be for next year.\n    if now.tm_yday - day > 365 // 2:\n        result += 1\n    return result",
        "sha1": "693511e033864011c11b83a6522c651dc51a4f9f",
        "id": 352578
    },
    {
        "content": "def adjust_age_groups(age_labels):\n    \"\"\"\n    for each pair of cols to aggregate, takes the first number of the first element, and the last number for the last element\n    for instance: [\"0-4\",'5-10'] -> ['0-10']\n    \"\"\"\n    i=0\n    new_age_labels=[]\n    label=\"\"\n    for element in age_labels:\n        if i%2==0:\n            label+=element.split('-')[0]\n            i+=1\n        elif i%2==1:\n            label=label+'-'+element.split('-')[-1]\n            new_age_labels.append(label)\n            label=\"\"\n            i+=1\n    #making the last agegroup based on the first number +\n    new_age_labels[-1]= new_age_labels[-1].split(\"-\")[0]+\"+\"\n    return(new_age_labels)",
        "sha1": "521a2f6779ae8fa3f3a53801e0f935844245cffc",
        "id": 705541
    },
    {
        "content": "def is_char(object):\n    \"\"\"Check if the given object is a character\"\"\"\n    if isinstance(object, str):\n        if len(object) == 1:\n            return True\n    return False",
        "sha1": "80b9b89023036ee51c302ed4e86dc6106cf932f7",
        "id": 354687
    },
    {
        "content": "import pickle\n\n\ndef load_pickle(file_path):\n    \"\"\"Wrapper for loading data from a pickle file.\n\n    Args:\n        file_path: string, path to the pickle file.\n\n    Returns:\n        A dictionary containing the loaded data.\n    \"\"\"\n    with open(file_path, 'rb') as handle:\n        data = pickle.load(handle)\n    return data",
        "sha1": "f6f06c75a4c5fffbc5033ae677ea98c30d92161a",
        "id": 117176
    },
    {
        "content": "def bool_from(obj, default=False):\n    \"\"\"\n    Returns:\n        True if obj is not None and its string representation is not 0 or False (case-insensitive). If obj is None,\n        'default' is used.\n    \"\"\"\n    return str(obj).lower() not in ('0', 'false') if obj is not None else bool(default)",
        "sha1": "2d09ffac8eaa5f05079ce4f43e56e07a4073e028",
        "id": 434638
    },
    {
        "content": "def and_list(items):\n  \"\"\" Create a comma-separated list of strings.\n  \"\"\"\n  assert isinstance(items, list)\n  match len(items):\n    case 0: return ''\n    case 1: return items[0]\n    case 2: return f'{items[0]} and {items[1]}'\n    case _: return ', '.join(items[0:-1]) + f', and {items[-1]}'",
        "sha1": "b0cea95bfe95d613986b6130fd7acabaef6a1d7c",
        "id": 24477
    },
    {
        "content": "import re\nimport ast\n\n\ndef parse_spec(spec):\n  \"\"\"Parses a list of key values pairs.\n\n  Args:\n    spec: A comma separated list of strings of the form `<key>=<value>`.\n\n  Raises:\n    ValueError: If `spec` is malformed or contains duplicate keys.\n\n  Returns:\n    A dict.\n  \"\"\"\n  params = {}\n  if spec:\n    # The regex represents a key-value pair.\n    regex = r\"\"\"([^=,]+) # The key. Comma and = are excluded.\n                =\n                (\\[[^\\[\\]=]*\\] # The value as a list. Nested lists unsupported.\n                 |[^=,\\[\\]]*) # Or a simple value.\n                (,|$) # The end of a key-value pair.\n            \"\"\"\n    tuples = re.findall(regex, spec, flags=re.X)\n    if not tuples:\n      raise ValueError('Spec \"%s\" doesn\\'t contain any key value pair.' % spec)\n    for item in tuples:\n      key = item[0]\n      value = item[1]\n      if not value: raise ValueError('Empty value for key %s' % key)\n      if key in params: raise ValueError('Duplicate key %s' % key)\n      try:\n        params[key] = ast.literal_eval(value)\n      except ValueError:\n        params[key] = value\n\n  return params",
        "sha1": "2a87fe9fbfdf3cb1990ccf9a7144a448ced20ae6",
        "id": 540115
    },
    {
        "content": "def get_mask_names_dict(mask_names, tracer_names):\n    \"\"\"\n    Return a dictionary with the mask names assotiated to the fields to be\n    correlated\n\n    Parameters:\n    -----------\n        mask_names (dict):  Dictionary of the masks names assotiated to the\n        fields to be correlated. It has to be given as {1: name1, 2: name2, 3:\n        name3, 4: name4}, where 12 and 34 are the pair of tracers that go into\n        the first and second Cell you are computing the covariance for; i.e.\n        <Cell^12 Cell^34>. In fact, the tjpcov.mask_names.\n        tracer_names (dict):  Dictionary of the tracer names of the same form\n        as mask_name.\n\n    Returns:\n    --------\n        masks_names_dict (dict):  Dictionary with the mask names assotiated to the\n        fields to be correlated.\n\n    \"\"\"\n    mn  = {}\n    for i in [1, 2, 3, 4]:\n        mn[i] = mask_names[tracer_names[i]]\n    return mn",
        "sha1": "d6d393fa7cd78233c6a3899ae1740d822381e6c1",
        "id": 616589
    },
    {
        "content": "def precision(classifier, testset):\n    \"\"\"\n    Runs the classifier for each example in `testset`\n    and verifies that the classification is correct\n    using the `target`.\n\n    Returns a number between 0.0 and 1.0 with the\n    precision of classification for this test set.\n    \"\"\"\n\n    hit = 0\n    total = 0\n    for example in testset:\n        if classifier.classify(example)[0] == classifier.target(example):\n            hit += 1\n        total += 1\n    if total == 0:\n        raise ValueError(\"Empty testset!\")\n    return hit / float(total)",
        "sha1": "24fed4005dbf5dab2c31a588e9aa384b9ebae51b",
        "id": 532197
    },
    {
        "content": "def prepend_a_an(name):\n    \"\"\"Add a/an to a name\"\"\"\n    if name[0] in [\"a\", \"e\", \"i\", \"o\", \"u\"]:\n        return \"an \" + name\n    else:\n        return \"a \" + name",
        "sha1": "201b4d23ab87931f849bb25bd5b4a631fc655852",
        "id": 171713
    },
    {
        "content": "def square_area(side):\n    \"\"\"\n    2. Function with one input and one output\n    This function demonstrates how a function returns a processed output \n    based on the received input\n    This function calculates the area of a square\n    side: the side of the square, must be a positive number\n    area: the area of the square, must be a positive number\n    \"\"\"\n    area = side * side\n    return area",
        "sha1": "bec6587fb3de8d638ff3e1de8b3ca40a067923d0",
        "id": 484607
    },
    {
        "content": "def is_latitude_norther_than(lat_a, lat_b):\n    \"\"\"\n      Check if lat_a is norther than lat_b\n    \"\"\"\n\n    is_latitude_a_negative = lat_a < 0\n    is_latitude_b_negative = lat_b < 0\n\n    same_sign = is_latitude_a_negative == is_latitude_b_negative\n\n    if not is_latitude_a_negative and is_latitude_b_negative:\n        return True\n\n    if is_latitude_a_negative and not is_latitude_b_negative:\n        return False\n\n    if same_sign:\n        return lat_a > lat_b\n\n    return lat_a < lat_b",
        "sha1": "290c0a4f298290bb375170c6748515b7a7218d95",
        "id": 16392
    },
    {
        "content": "from typing import Dict\nfrom typing import Any\nimport requests\nimport json\n\n\ndef get_person_by_id(id_number: int, auth_dict: Dict[str, Any]) -> Any:\n    \"\"\"\n    This function just fetches a person by the id number from coresignal.\n\n    Args\n    -------\n        id_number: int\n            the coresignal id number. Should be aquired from a coresignal query.\n        auth_dict: auth_dict\n            the authorization header. Check here for instructions on how to make this\n\n    Returns\n    ----------\n\n        person_data: dict\n            the full response from coresignal\n    \"\"\"\n    url = \"https://api.coresignal.com/dbapi/v1/collect/member/{}\".format(id_number)\n    response = requests.get(url, headers=auth_dict)\n    if response.status_code == 200:\n        data = json.loads(response.text)\n        return data\n    else:\n        raise ValueError(\n            \"Bad Response Code. Response code: {}\".format(response.status_code)\n        )",
        "sha1": "0eab682a03e12c95c5f80aaddf72ae82cdc1a4da",
        "id": 607252
    },
    {
        "content": "def _ConcurrencyValue(value):\n  \"\"\"Returns True if value is an int > 0 or 'default'.\"\"\"\n  try:\n    return value == 'default' or int(value) > 0\n  except ValueError:\n    return False",
        "sha1": "f042cd492e8f237751950cffb51aae844a2784a0",
        "id": 320894
    },
    {
        "content": "def percent_used(used, total, decimal=2):\n    \"\"\"\n    Return percent used by giving total and used value.\n\n    :param used: Used value.\n    :type used: Integer\n    :param total: Total value.\n    :type total: Integer\n    :return: Float for percent used.\n    :rtype: Float\n    \"\"\"\n    pused = round((100. / total) * used, decimal)\n    return pused",
        "sha1": "0c8b694d36af2dcaf903b307c34beb794bc391ff",
        "id": 605932
    },
    {
        "content": "def point_touches_rectangle(point, rect):\n    \"\"\"\n    Returns True if the point is in the rectangle or touches it's boundary.\n\n    point_touches_rectangle(point, rect) -> bool\n\n    Parameters\n    ----------\n    point : Point or Tuple\n    rect  : Rectangle\n\n    Examples\n    --------\n    >>> rect = Rectangle(0,0,10,10)\n    >>> a = Point((5,5))\n    >>> b = Point((10,5))\n    >>> c = Point((11,11))\n    >>> point_touches_rectangle(a,rect)\n    1\n    >>> point_touches_rectangle(b,rect)\n    1\n    >>> point_touches_rectangle(c,rect)\n    0\n    \"\"\"\n    chflag = 0\n    if point[0] >= rect.left and point[0] <= rect.right:\n        if point[1] >= rect.lower and point[1] <= rect.upper:\n            chflag = 1\n    return chflag",
        "sha1": "aeee8515f041a8e1bd6a45f3da375805495224c3",
        "id": 120746
    },
    {
        "content": "def dict_get(_dict, keys):\n    \"\"\"Get dict values by keys.\"\"\"\n    return [_dict[key] for key in keys]",
        "sha1": "9ce636167f6f4ada73e32533e78a8b16ede83f2c",
        "id": 70441
    },
    {
        "content": "def find_group_single_candidate(square: tuple, squares: list, grid: list) -> list:\n    \"\"\"\n    Checks for a single option a square can have based on no other square in a group having it as an option\n\n    :param square: A tuple (row, column) coordinate of the square\n    :param squares: A list of squares (tuples) to check the options of\n    :param grid: The sudoku grid as a 3D list\n    :return: A list of options\n    \"\"\"\n    if square in squares:\n        squares.remove(square)\n    square_options = set(grid[square[0]][square[1]][1])\n    for sq in squares:\n        if grid[sq[0]][sq[1]][0] == 0:\n            square_options = square_options - set(grid[sq[0]][sq[1]][1])\n        else:\n            square_options = square_options - {grid[sq[0]][sq[1]][0]}\n    return list(square_options)",
        "sha1": "1983720f4540b0bd964a2ba535455ca0510ef369",
        "id": 72890
    },
    {
        "content": "def initialize_counts_2(shots, hex_counts=True):\n    \"\"\"Initialize test circuits reference counts.\"\"\"\n    targets = []\n    if hex_counts:\n        # Initialize 0 to |1> from |++>\n        targets.append({'0x1': shots / 2, '0x3': shots / 2})\n        # Initialize 1 to |1> from |++>\n        targets.append({'0x2': shots / 2, '0x3': shots / 2})\n    else:\n        # Initialize 0 to |1> from |++>\n        targets.append({'01': shots / 2, '11': shots / 2})\n        # Initialize 1 to |1> from |++>\n        targets.append({'10': shots / 2, '11': shots / 2})\n    return targets",
        "sha1": "d9163c70010adaf8e230806cdc27728da9f5cda7",
        "id": 585918
    },
    {
        "content": "import torch\n\n\ndef sample_and_group_all(xyz, points, use_xyz=True):\n    \"\"\"\n    Args:\n        xyz: Tensor, (B, 3, nsample)\n        points: Tensor, (B, f, nsample)\n        use_xyz: boolean\n\n    Returns:\n        new_xyz: Tensor, (B, 3, 1)\n        new_points: Tensor, (B, f|f+3|3, 1, nsample)\n        idx: Tensor, (B, 1, nsample)\n        grouped_xyz: Tensor, (B, 3, 1, nsample)\n    \"\"\"\n    b, _, nsample = xyz.shape\n    device = xyz.device\n    new_xyz = torch.zeros((1, 3, 1), dtype=torch.float, device=device).repeat(b, 1, 1)\n    grouped_xyz = xyz.reshape((b, 3, 1, nsample))\n    idx = torch.arange(nsample, device=device).reshape(1, 1, nsample).repeat(b, 1, 1)\n    if points is not None:\n        if use_xyz:\n            new_points = torch.cat([xyz, points], 1)\n        else:\n            new_points = points\n        new_points = new_points.unsqueeze(2)\n    else:\n        new_points = grouped_xyz\n\n    return new_xyz, new_points, idx, grouped_xyz",
        "sha1": "8d6b53b6d0deb012d7f9e1282f49cadf2f4a1553",
        "id": 402679
    },
    {
        "content": "def get_cpe_version(cpe: str):\n    \"\"\"Return the version entry of the given CPE\"\"\"\n    split_cpe = cpe.split(\":\")\n    if len(split_cpe) > 4:\n        return split_cpe[4]\n    return \"\"",
        "sha1": "11cfc2073e61294e30041619c87323e6d75b1223",
        "id": 649763
    },
    {
        "content": "import re\n\n\ndef escapeWithWildcards(term):\n    \"\"\"\n    Escapes special characters of the Lucene Query Syntax except wildcards.\n\n    @param term: The term to be escaped.\n    @return the term with all the special characters escaped.\n    \"\"\"\n    # First escape everything except \\\n    term = re.sub(r'[\\+\\-\\&\\|\\!\\(\\)\\{\\}\\[\\]\\^\\\"\\:]', r'\\\\\\g<0>', term)\n    # Then escape \\\n    return re.sub(r'\\\\([^\\+\\-\\&\\|\\!\\(\\)\\{\\}\\[\\]\\^\\\"\\:\\*\\?\\~])', r'\\\\\\\\\\g<1>',\n                  term)",
        "sha1": "f429157ea8978327c63a95c8bf54788c369aa828",
        "id": 170308
    },
    {
        "content": "import heapq\n\n\ndef _simple_chooser(queue, remaining):\n    \"\"\"Default contraction chooser that simply takes the minimum cost option.\n    \"\"\"\n    cost, k1, k2, k12 = heapq.heappop(queue)\n    if k1 not in remaining or k2 not in remaining:\n        return None  # candidate is obsolete\n    return cost, k1, k2, k12",
        "sha1": "5bb92184767ba68247b124d4a935ea9dab327f96",
        "id": 38263
    },
    {
        "content": "def get_positive_non_zero_int_input(prompt):\n    \"\"\"\n    Prompts the user for a positive int input\n\n    :param prompt: (String)\n        Prompts user for input\n\n    :return: (int)\n        a positive, non-zero int\n    \"\"\"\n    while True:\n        try:\n            value = int(input(prompt))\n            if value > 0:\n                return value\n            else:\n                print(\"Input must be positive, try again.\")\n        except ValueError:\n            print(\"Input must be numeric\")",
        "sha1": "12585abde15c11fe3a76514a7ac56a850ad5ee5e",
        "id": 549204
    },
    {
        "content": "def sortUsingList(tosort, reflist):\n\t\"\"\"\n\tSorts tosort by order of reflist.\n\tExample: tosort: ['a', 'b', 'c'], reflist: [1, 3, 2]\n\tReturn: ['a', 'c', 'b']\n\t:param tosort:\n\t:param reflist:\n\t:return:\n\t\"\"\"\n\treturn [x for (y, x) in sorted(zip(reflist, tosort))]",
        "sha1": "ae8f7103e112d7d028934e59a56b4daea8c9aab5",
        "id": 498897
    },
    {
        "content": "def ilc_index(ndim):\n    \"\"\"Returns einsum indexing given ndim of cinv.\n    If covmat of 1d powers, return single index, else\n    return 2 indices for 2D kspace matrix.\"\"\"\n    if ndim==3:\n        return \"p\"\n    elif ndim==4:\n        return \"ij\"\n    else:\n        raise ValueError",
        "sha1": "61c53834eebd6595c58d8efd22485e342aec371f",
        "id": 472728
    },
    {
        "content": "import re\n\n\ndef get_exclude_patterns_as_regex_list(exclude_patterns=None):\n    \"\"\"\n    Takes a list of strings are returns a list of compiled regexes.\n    Strips enclosing quotes if present.\n\n    :type exclude_patterns: list[str]\n    :return:\n    :rtype: list[re.__Regex]\n    \"\"\"\n    exclude_regexes = []\n    if exclude_patterns:\n\n        for regex in exclude_patterns:\n            # strip matching quotes around regex if present\n            if regex.startswith('\"') and regex.endswith('\"'):\n                regex = regex[1:-1]\n            elif regex.startswith(\"'\") and regex.endswith(\"'\"):\n                regex = regex[1:-1]\n            exclude_regexes.append(re.compile(regex))\n\n    return exclude_regexes",
        "sha1": "4de3c0e34540e326e5d95e00fc5dc738b7fe3f17",
        "id": 640578
    },
    {
        "content": "def make_number_formatter(decimal_places):\n    \"\"\"\n    Given a number of decimal places creates a formatting string that will\n    display numbers with that precision.\n    \"\"\"\n    fraction = '0' * decimal_places\n\n    return ''.join(['#,##0.', fraction, ';-#,##0.', fraction])",
        "sha1": "9d8cfbfb56d01a170f6754925f26f1433a9627e1",
        "id": 33719
    },
    {
        "content": "def staticTunnelTemplate(user, device, ip, aaa_server, group_policy):\n    \"\"\"\n        Template for static IP tunnel configuration for a user.\n        This creates a unique address pool and tunnel group for a user.\n\n        :param user: username id associated with static IP\n        :type user: str\n        :param device: hostname of device\n        :type device: str\n        :param ip_start: first IP address in address pool\n        :type ip_start: str\n        :param aaa_server: name of authentication server for tunnel group\n        :type aaa_server: str\n        :param group_policy: name of group policy to attach tunnel groups to\n        :type group_policy: str\n\n        :return: configuration for the ASA\n        :rtype: str\n    \"\"\"\n\n    config = \"\"\n\n    # ip local pool <USER> X.X.X.X mask 255.255.255.255\n    config += f\"ip local pool {user} {str(ip)} mask 255.255.255.255\\n\"\n\n    # tunnel-group <USER> type remote-access\n    config += f\"tunnel-group {user} type remote-access\\n\"\n\n    # tunnel-group <USER> general-attributes\n    config += f\"tunnel-group {user} general-attributes\\n\"\n\n    # address-pool <USER>\n    config += f\"address-pool {user}\\n\"\n\n    # authentication-server-group <AAA_SERVER>\n    config += f\"authentication-server-group {aaa_server}\\n\"\n\n    # default-group-policy <GROUP-POLICY>\n    config += f\"default-group-policy {group_policy}\\n\"\n\n    # tunnel-group <USER> webvpn-attributes\n    config += f\"tunnel-group {user} webvpn-attributes\\n\"\n\n    # group-url https://<DEVICE>/<USER> enable\n    config += f\"group-url https://{device}/{user} enable\\n\"\n\n    return config",
        "sha1": "0367205462157fe8b242095f19cf936a24b9afd8",
        "id": 672315
    },
    {
        "content": "from typing import Any\n\n\ndef equals(check_value: Any, item: Any) -> bool:\n    \"\"\"Check if two values are equal.\n\n    :param check_value: Value to check.\n    :type check_value: Any\n    :param item: Item to check against.\n    :type item: Any\n    :return: Bool of comparison.\n    :rtype: bool\n    \"\"\"\n    return check_value == item",
        "sha1": "960fa68702f0ead68d6c64d55c863546db06e92a",
        "id": 315668
    },
    {
        "content": "from datetime import datetime\nimport math\n\n\ndef calc_travel_3d(current_plane, lead_s: float):\n    \"\"\"Extrapolate the 3D position of the aircraft\n\n    Arguments:\n        lat {float} -- Starting latitude (degrees)\n        lon {float} -- Starting longitude (degrees)\n        alt {float} -- Starting altitude (meters)\n        lat_lon_time {datetime} -- Last time lat / lon was updated\n        altitude_time {datetime} -- Last time altitude was updated\n        speed_mps {float} -- Speed (meters per second)\n        heading {float} -- Heading (degrees)\n        climb_rate {float} -- climb rate (meters per second) \n        \n    Returns:\n        Tuple[float, float, float] -- The new latitude (deg)/longitude (deg)/alt (meters) as a tuple\n    \"\"\"\n    lat = current_plane[\"lat\"]\n    lon = current_plane[\"lon\"]\n    alt = current_plane[\"altitude\"]\n\n    # The time values sometimes do not have a microsecond component if it falls exactly on the second\n    # we need to parse both formats to avoid crashing.\n    try:\n        lat_lon_time = datetime.strptime(current_plane[\"latLonTime\"], '%Y-%m-%d %H:%M:%S.%f')\n    except ValueError:\n        lat_lon_time = datetime.strptime(current_plane[\"latLonTime\"], '%Y-%m-%d %H:%M:%S')\n    try:\n        altitude_time = datetime.strptime(current_plane[\"altitudeTime\"], '%Y-%m-%d %H:%M:%S.%f')\n    except ValueError:\n        altitude_time = datetime.strptime(current_plane[\"altitudeTime\"], '%Y-%m-%d %H:%M:%S')\n    speed_mps = current_plane[\"groundSpeed\"]\n    heading = current_plane[\"track\"]\n    climb_rate = current_plane[\"verticalRate\"]\n\n    lat_lon_age = datetime.utcnow() - lat_lon_time\n    lat_lon_age_s = lat_lon_age.total_seconds() + lead_s\n\n    alt_age = datetime.utcnow() - altitude_time\n    alt_age_s = alt_age.total_seconds() + lead_s\n\n    R = float(6371) # Radius of the Earth\n    brng = math.radians(heading) # Bearing is 90 degrees converted to radians.\n    d = float((lat_lon_age_s * speed_mps) / 1000.0) # Distance in km\n\n    lat1 = math.radians(lat) # Current lat point converted to radians\n    lon1 = math.radians(lon) # Current long point converted to radians\n\n    lat2 = math.asin(math.sin(lat1)*math.cos(d/R) + math.cos(lat1)*math.sin(d/R)*math.cos(brng))\n    lon2 = lon1 + math.atan2(math.sin(brng)*math.sin(d/R)*math.cos(lat1), math.cos(d/R)-math.sin(lat1)*math.sin(lat2))\n\n    lat2 = math.degrees(lat2)\n    lon2 = math.degrees(lon2)\n        \n    alt2 = alt+climb_rate*alt_age_s\n    \n    return (lat2, lon2, alt2)",
        "sha1": "7ff9471c3fb79e6351bca33c999c877ef6792b1a",
        "id": 128342
    },
    {
        "content": "def pluralize(count: int) -> str:\n    \"\"\"Util function to put a string in plural when the count is bigger than 1\n\n    Args:\n        count (int): the number of time the string to pluralize is counted\n\n    Returns:\n        str: return s if the string should be put to plural\n    \"\"\"\n    if count > 1:\n        return \"s\"\n    else:\n        return \"\"",
        "sha1": "d746ec1c0631d11b8020d18f36f21629b8a10c95",
        "id": 284932
    },
    {
        "content": "def divide_lists(lst_numer, lst_denom):\n    \"\"\"\n    Divides each element of the nested list 'lst_numer' by 'lst_denom'.\n\n    The division is done by taking each element of 'lst_numer' and for each\n    index of that element, dividing the item at that index by the item at the\n    same index of 'lst_denom'. See example below:\n\n        >>> numer = [[1., 2.],\n        ...          [3., 4.]]\n        >>> denom = [0.5, 0.5]\n        >>> divide_lists(numer, denom)\n        [[2., 4.], [6., 8.]]\n\n    NOTE: It is assumed that each element of 'lst_numer' has the same length\n    as 'lst_denom' as shown in the dimensions of the arguments below.\n\n    Arguments:\n        lst_numer: nested list of list(dimensions N x M)\n        lst_denom: list of denominators(dimensions 1 x M)\n\n    Returns:\n        A new list formed by dividing each element of 'lst_numer' by\n        'lst_denom' according to the division process described above.\n    \"\"\"\n    indexes = range(len(lst_denom))\n    return [[n[i] / float(lst_denom[i]) for i in indexes] for n in lst_numer]",
        "sha1": "d040c159ca79eedfe74b5067affd67d42d928879",
        "id": 155318
    },
    {
        "content": "def _GenerateBeginningCode(required_namespaces, namespace):\n    \"\"\"\n    First method called to generate the c# wrapper code.\n    This generates the using statements and the namespace declaration.\n    \"\"\"\n    code = []\n    for name in required_namespaces:\n        code.append(\"using {0};\\n\".format(name))\n\n    code.append(\"\\n\\n\")\n    code.append(\"namespace {0}\\n{{\".format(namespace))\n    code.append(\"\\n\\n\")\n\n    return \"\".join(code)",
        "sha1": "9f022edcbde74c3fec04ccf7fd709b0be1abb39e",
        "id": 436624
    },
    {
        "content": "def refine_search(\n        ra: list, dec: list, oid: list,\n        id_out: list, names: list, types: list) -> list:\n    \"\"\" Create a final table by merging coordinates of objects found on the\n    bibliographical database, with those objects which were not found.\n\n    Parameters\n    ----------\n    ra: list of float\n        List of RA\n    dec: list of float\n        List of Dec of the same size as ra.\n    oid: list of str\n        List of object ID (custom)\n    id_out: list of str\n        List of object ID returned by the xmatch with CDS\n    names: list of str\n        For matches, names of the celestial objects found\n    types: list of str\n        For matches, astronomical types of the celestial objects found\n\n    Returns\n    ----------\n    out: List of Tuple\n        Each tuple contains (objectId, ra, dec, name, type).\n        If the object is not found in Simbad, name & type\n        are marked as Unknown. In the case several objects match\n        the centroid of the alert, only the closest is returned.\n    \"\"\"\n    out = []\n    for ra_in, dec_in, id_in in zip(ra, dec, oid):\n        # cast for picky Spark\n        ra_in, dec_in = float(ra_in), float(dec_in)\n        id_in = str(id_in)\n\n        # Discriminate with the objectID\n        if id_in in id_out:\n            # Return the closest object in case of many\n            # (smallest angular distance)\n            index = id_out.index(id_in)\n            out.append((\n                id_in, ra_in, dec_in,\n                str(names[index]), str(types[index])))\n\n        else:\n            # Mark as unknown if no match\n            out.append((id_in, ra_in, dec_in, \"Unknown\", \"Unknown\"))\n\n    return out",
        "sha1": "afaae8e9e1404e6e13cfe77c25d5bffb2dd1c91b",
        "id": 119296
    },
    {
        "content": "def get_kpoints(content):\n    \"\"\"Read a list of kpoints and their weights from kgrid.x output file.\"\"\"\n    lines = content.splitlines()[2:]\n    kpoints = list()\n    weights = list()\n    for line in lines:\n        k = [ float(ki) for ki in line.split()[:3] ]\n        w = float(line.split()[-1])\n        kpoints.append(k)\n        weights.append(w)\n    return kpoints, weights",
        "sha1": "ca049c42808c39a0ef1d28ab39f6bb661e695c6a",
        "id": 564155
    },
    {
        "content": "def _get_projection(el):\n    \"\"\"\n    Get coordinate reference system from non-auxiliary elements.\n    Return value is a tuple of a precedence integer and the projection,\n    to allow non-auxiliary components to take precedence.\n    \"\"\"\n    return (int(el._auxiliary_component), el.crs) if hasattr(el, 'crs') else None",
        "sha1": "94da883c3aba5647d0619670bf1bbdc14fa36299",
        "id": 154809
    },
    {
        "content": "def remove_extra_space_from_args(args):\n    \"\"\"\n    Remove leading and trailing spaces from all the arguments and remove empty arguments.\n\n    :param args: Dictionary of arguments\n\n    :return: Dictionary of arguments\n    :rtype: ``Dict``\n    \"\"\"\n    return {key: value.strip() for (key, value) in args.items() if value and len(value.strip()) > 0}",
        "sha1": "fa5412e3ba2b6996439de441d631612cb04f7c54",
        "id": 577315
    },
    {
        "content": "from typing import Counter\nimport glob\n\n\ndef load_files(folder):\n    \"\"\"Load all text files in folder and return character histogram data and\n    files contaning blocked characters.\"\"\"\n\n    print(\"Working on %s\" % folder)\n\n    # List files containing these chars\n    blockchars = \"\u00a9\u00a3\u20ac\u00bb\u00fe\"\n\n    blockfiles = {}\n    for char in blockchars:\n        blockfiles[char] = []\n\n\n    # count chars in txt files\n    c = Counter()\n\n    for file in glob.glob(folder + \"/*/*.txt\"):\n        with open(file) as f:\n            for line in f:\n                c += Counter(line)\n                for char in line:\n                    if char in blockchars:\n                        blockfiles[char].append(file)\n\n    return c, blockfiles",
        "sha1": "c00722ce5c89ba6b7502c061ff893ce740fe356e",
        "id": 509611
    },
    {
        "content": "def _test_int_pickling_compare(int_1, int_2):\n    \"\"\"Add the two given ints.\"\"\"\n    return int_1 + int_2",
        "sha1": "db0c37f6c4a764fc3c46221fca8877e747417af1",
        "id": 666611
    },
    {
        "content": "def autocomplete_field(bfield, **opts):\n    \"\"\"\n    Returns ``<script>`` tag with jquery-ui autocompletion for the given\n    field. In order to get this working field have to specify ``queryset``\n    attribute. You may pass options to autocomplete plugin and if you specify\n    ``attr`` option, it would be used to represent available choices (default is\n    ``__unicode__``).\n\n    Syntax::\n\n        {% autocomplete_field field [options] %}\n\n    For example, lets say our form contains ``owner`` field which is a\n    ``ModelChoiceField`` related with ``User`` model. If we pass form into\n    the context, we can autocomplete it's ``owner`` field by putting following\n    snippet somewhere in a template::\n\n        {% autocomplete_field form.owner delay=0 %}\n\n    \"\"\"\n    field_id = bfield.auto_id\n    attr = opts.pop('attr', '__unicode__')\n    assert hasattr(bfield.field.queryset.model, attr)\n    choices = ( callable(item) and item() or item for item in\n        (getattr(obj, attr) for obj in bfield.field.queryset))\n\n    opts['source'] = '[' + ', '.join(('\"%s\"' % choice\n        for choice in choices)) + ']'\n    opts.setdefault('delay', 50)\n\n    options = ', '.join((\"%s: %s\" % (key, val) for key, val in opts.items()))\n\n    script_body = \\\n    \"\"\"<script type=\"text/javascript\">\n        $(document).ready(function(){\n            $('#%(field_id)s').autocomplete({%(options)s});\n        });\n    </script>\n    \"\"\" % {'field_id': field_id, 'options': options}\n\n    return script_body",
        "sha1": "97b94ed5d687b5e482975ec65424b9fc8bedf304",
        "id": 506261
    },
    {
        "content": "import re\nimport inspect\n\n\ndef parse_pyvars(code: str, frame_nr: int = 2):\n    \"\"\"Looks through call stack and finds values of variables.\n\n    Parameters\n    ----------\n    code : str\n        SuperCollider command to be parsed\n    frame_nr : int, optional\n        on which frame to start, by default 2 (grandparent frame)\n\n    Returns\n    -------\n    dict\n        {variable_name: variable_value}\n\n    Raises\n    ------\n    NameError\n        If the variable value could not be found.\n    \"\"\"\n    matches = re.findall(r\"\\s*\\^[A-Za-z_]\\w*\\s*\", code)\n\n    pyvars = {match.split(\"^\")[1].strip(): None for match in matches}\n    missing_vars = list(pyvars.keys())\n\n    stack = inspect.stack()\n    frame = None\n    try:\n        while missing_vars and frame_nr < len(stack):\n            frame = stack[frame_nr][0]\n            for pyvar in pyvars:\n                if pyvar not in missing_vars:\n                    continue\n                # check for variable in local variables\n                if pyvar in frame.f_locals:\n                    pyvars[pyvar] = frame.f_locals[pyvar]\n                    missing_vars.remove(pyvar)\n                # check for variable in global variables\n                elif pyvar in frame.f_globals:\n                    pyvars[pyvar] = frame.f_globals[pyvar]\n                    missing_vars.remove(pyvar)\n            frame_nr += 1\n    finally:\n        del frame\n        del stack\n    if missing_vars:\n        raise NameError(\"name(s) {} not defined\".format(missing_vars))\n    return pyvars",
        "sha1": "381d698d3905ee306f75ad888d07c1107e398251",
        "id": 27374
    },
    {
        "content": "import struct\n\n\ndef word(b):\n    \"\"\"Combine two bytes into a 16-bit word.\"\"\"\n\n    return struct.unpack('=H', struct.pack('=BB', b[0], b[1]))[0]",
        "sha1": "dfa045712228c97fb1ce6712339fd39ff8e587a4",
        "id": 398283
    },
    {
        "content": "from typing import Any\n\n\ndef is_null(value: Any) -> bool:\n    \"\"\"Check if a value is equivalent to null in Dynamo\"\"\"\n    return value is None or (isinstance(value, (set, frozenset)) and len(value) == 0)",
        "sha1": "4ad43961673b7e8ae7c9eff8739299fedaae9a4c",
        "id": 349791
    },
    {
        "content": "def read_file(name):\n    \"\"\"Given the path/name of the file, return nthe jobs list with jobs[][0]: weight and jobs[][1]: length.\n    \"\"\"\n    \n    file = open(name,'r')\n    data = file.readlines()\n    \n    jobs = []\n    \n    for line in data[1:]:\n        items = line.split()\n        jobs.append([int(items[0]),int(items[1])])\n        \n    return jobs",
        "sha1": "3e6e5b51913f272da80943a2acaa68165bbc0af0",
        "id": 622278
    },
    {
        "content": "def mock_logger(mocker):\n    \"\"\"\n    Mock log exception\n    \"\"\"\n    return mocker.patch(\"course_catalog.api.log.exception\")",
        "sha1": "3a7824ac2342a52faa91c6ca8de4e10b3d3f2c0c",
        "id": 558471
    },
    {
        "content": "def truncate_float(f):\n    \"\"\"\n    Truncate a float to an int.\n\n    This filter is useful because:\n    1. The default `floatformat` template filter only does rounding,\n    not truncation\n    2. f.__int__ in the template gets a TemplateSyntaxError:\n    \"Variables and attributes may not begin with underscores\"\n    \"\"\"\n    return int(f)",
        "sha1": "d9e0932e52fea9112d4154bb1b836e923b5cfce3",
        "id": 319042
    },
    {
        "content": "def get_second_smallest(values):\n    \"\"\"\n    returns the second lowest value in a list of numbers\n\n    Args:\n        values: a list of floats\n    \n    Returns:\n        the second lowst number in values\n    \"\"\"\n    smallest, second_smallest = float(\"inf\"), float(\"inf\")\n    for value in values:\n        if value <= smallest:\n            smallest, second_smallest = value, smallest\n        elif value < second_smallest:\n            second_smallest = value\n    return second_smallest",
        "sha1": "9daee10aa13205bf941078e715e1ca6f40c3905d",
        "id": 540499
    },
    {
        "content": "def _find_neighborhoods_ids(input_list, neighborhoods_df):\n    \"\"\"\n    Read in a list of names of neighborhood names. Returns a list of\n    neighbhorhood ids\n\n    Args:\n        input_list: a list containing the names of the neighborhoods\n            in the treatment group.\n        neighborhoods_df: a dataframe containing neighborhood names\n            and object_id for each neighborhood.\n\n    Returns:\n        a list of neighborhood ids corresponding wiht the given names.\n\n    \"\"\"\n\n    id_list = []\n    for i, _ in enumerate(input_list):\n        id_list = id_list + \\\n            [int(neighborhoods_df[neighborhoods_df['S_HOOD'] == input_list[i]]['OBJECTID'])]\n\n    return id_list",
        "sha1": "71ec994ce2e4fe28ec20a1e5604eea4159f8c172",
        "id": 628228
    },
    {
        "content": "import struct\n\n\ndef bu16(value):\n    \"\"\"Convert a 16-bit integer to bytes (LSB first).\n\n    Example:\n            bu16(0x1234) == b'\\x34\\x12'\n    \"\"\"\n    return struct.pack('<H', value)",
        "sha1": "1ad19ffb1ffbc0cd86261b441788fabf938e7cae",
        "id": 539922
    },
    {
        "content": "import logging\n\n\ndef TruncateStr(text, max_len=500):\n  \"\"\"Truncates strings to the specified maximum length or 500.\n\n  Args:\n    text: Text to truncate if longer than max_len.\n    max_len: Maximum length of the string returned by the function.\n\n  Returns:\n    A string with max_len or less letters on it.\n  \"\"\"\n  if len(text) > max_len:\n    logging.warning(\n        'Text length of %d is greater than the max length allowed. '\n        'Truncating to a length of %d. Text: %s', len(text), max_len, text)\n  return text[:max_len]",
        "sha1": "2401d6d56c6f99bd64cfd825aca0ab5badca1964",
        "id": 35622
    },
    {
        "content": "from datetime import datetime\n\n\ndef get_timestamp() -> int:\n    \"\"\"Returns current timestamp as seconds since epoch.\"\"\"\n    return int(datetime.utcnow().strftime(\"%s\"))",
        "sha1": "794a16c3e5425c526d4b9855a3fcb7762e101481",
        "id": 474851
    },
    {
        "content": "import torch\n\n\ndef load_checkpoint(checkpoint_path, net, optimizer=None, strict=True):\n    \"\"\" \n    Load checkpoint for network and optimizer.\n    Args:\n        checkpoint_path: str\n        net: torch.nn.Module\n        optimizer(optional): torch.optim.Optimizer or None\n    Returns:\n        net: torch.nn.Module\n        optimizer: torch.optim.Optimizer\n        start_epoch: int\n    \"\"\"\n    checkpoint = torch.load(checkpoint_path)\n    net.load_state_dict(checkpoint['model_state_dict'], strict=strict)\n    if optimizer is not None:\n        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    start_epoch = checkpoint['epoch']\n    print(\"-> loaded checkpoint %s (epoch: %d)\"%(checkpoint_path, start_epoch))\n    return net, optimizer, start_epoch",
        "sha1": "f7e5b19203bfdba342c3ea3fdadd8e31640a9ff5",
        "id": 204615
    },
    {
        "content": "import random\n\n\ndef keep_fraction(l, f):\n  \"\"\"Return fraction `f` of list `l`. Shuffles `l`.\"\"\"\n  random.shuffle(l)\n  count = int(f * len(l))\n  return l[:count]",
        "sha1": "b499d9c892bc17f4397b45adafaf67dfb9f6cd17",
        "id": 659211
    },
    {
        "content": "def get_zero_idx(res_array):\n    \"\"\"\n    Returns list with indexes of zeros in res_array\n\n    Parameters\n    ----------\n    res_array : np.array\n        Numpy results array\n\n    Returns\n    -------\n    list_idx : list (of ints)\n        List holding zero indexes\n    \"\"\"\n\n    list_idx = []\n\n    for i in range(len(res_array)):\n        if res_array[i] == 0:\n            list_idx.append(i)\n\n    return list_idx",
        "sha1": "d6dd989faf4a12d215a0d8e0e34a3d59087fd8dd",
        "id": 292238
    },
    {
        "content": "import re\n\n\ndef old_to_new_tracks(old_tracks: str) -> str:\n    \"\"\"\n    >>> old_to_new_tracks(EXAMPLE_INPUT)\n    'make_tracks li1 -x_offset 0.23 -x_pitch 0.46 -y_offset 0.17 -y_pitch 0.34\\\\nmake_tracks met1 -x_offset 0.17 -x_pitch 0.34 -y_offset 0.17 -y_pitch 0.34\\\\nmake_tracks met2 -x_offset 0.23 -x_pitch 0.46 -y_offset 0.23 -y_pitch 0.46\\\\nmake_tracks met3 -x_offset 0.34 -x_pitch 0.68 -y_offset 0.34 -y_pitch 0.68\\\\nmake_tracks met4 -x_offset 0.46 -x_pitch 0.92 -y_offset 0.46 -y_pitch 0.92\\\\nmake_tracks met5 -x_offset 1.70 -x_pitch 3.40 -y_offset 1.70 -y_pitch 3.40\\\\n'\n    \"\"\"\n    old_tracks_lines = old_tracks.split(\"\\n\")\n    layers = {}\n\n    for line in old_tracks_lines:\n        if re.match(r\"^\\s*$\", line):\n            continue\n        layer, cardinal, offset, pitch = re.split(r\"\\s+\", line)\n        layers[layer] = layers.get(layer) or {}\n        layers[layer][cardinal] = (offset, pitch)\n\n    final_str = \"\"\n    for layer, data in layers.items():\n        x_offset, x_pitch = data[\"X\"]\n        y_offset, y_pitch = data[\"Y\"]\n        final_str += f\"make_tracks {layer} -x_offset {x_offset} -x_pitch {x_pitch} -y_offset {y_offset} -y_pitch {y_pitch}\\n\"\n\n    return final_str",
        "sha1": "9bc45cc3ea1e51a6fa52dd53d6e4af6dc6adef6e",
        "id": 658272
    },
    {
        "content": "def construct_wikidata_link(identifier: str) -> str:\n    \"\"\"\n    Constructs a wikidata link from the given identifier.\n\n    Args:\n        identifier: The identifier to construct the wikidata link from.\n\n    Returns:\n        The constructed wikidata link.\n    \"\"\"\n    if isinstance(identifier, str):\n        return f\"https://www.wikidata.org/wiki/{identifier}\"\n    else:\n        raise ValueError(\n            f\"identifier must be of type `str`. Got {type(identifier)} instead.\"\n        )",
        "sha1": "5fb015620945789873ba160b923154abbcabd2e8",
        "id": 230475
    },
    {
        "content": "def to_camel(string: str) -> str:\n    \"\"\"Convert snake_case to camelCase.\"\"\"\n    words = []\n    for num, word in enumerate(string.split(\"_\")):\n        if num > 0:\n            word = word.capitalize()\n        words.append(word)\n    return \"\".join(words)",
        "sha1": "af9967e065573d61b01381aad8c866a402d9e5a0",
        "id": 644702
    },
    {
        "content": "def getNameOnly(filename):\n    \"\"\"get file name without extension from a path\"\"\"\n    nameonly = filename.split('.')[0]\n    return nameonly",
        "sha1": "590178fe7516a831431600d64a9b0df6c73ff30a",
        "id": 380526
    },
    {
        "content": "def protobuf_namespace_alias() -> str:\n    \"\"\"Get the protobuf namespace alias.\"\"\"\n    return \"pb\"",
        "sha1": "b6735cdeaec11f4e6b14cf40929efddf106ce3c1",
        "id": 609637
    },
    {
        "content": "def legendre(a: int, p: int) -> int:\n    \"\"\"\n    Calculate value of Legendre symbol (a/p).\n\n    :param a: Value of a in (a/p).\n    :param p: Value of p in (a/p).\n    :returns: Value of (a/p).\n    \"\"\"\n    return pow(a, (p - 1) // 2, p)",
        "sha1": "b1966692e738921b463fa13fc5b5b9fc814fe0bf",
        "id": 457403
    },
    {
        "content": "def pproc_command(commands):\n    \"\"\"\n    Creates a pproc command from a list of command strings.\n    \"\"\"\n    commands = \" \".join([\n        \"\\\"{}\\\"\".format(command) for command in commands\n    ])\n    return \"pproc {}\".format(commands)",
        "sha1": "c8dd6335d2bbee254b914534d0110102a5cdcfab",
        "id": 567229
    },
    {
        "content": "def transform_misses(record):\n    \"\"\"Format the missed datasets record we got from the database to adhere to the response schema.\"\"\"\n    \n    response = {}\n    response[\"datasetId\"] = dict(record).get(\"stableId\")  \n    response[\"internalId\"] = dict(record).get(\"datasetId\")\n    response[\"exists\"] = False\n    # response[\"datasetId\"] = ''  \n    response[\"variantCount\"] = 0\n    response[\"callCount\"] = 0\n    response[\"sampleCount\"] = 0\n    response[\"frequency\"] = 0 \n    response[\"numVariants\"] = 0 \n    response[\"info\"] = {\"access_type\": dict(record).get(\"accessType\")}\n\n    return response",
        "sha1": "a3e02cedf76147154e4929c6313ba67d29f45eb7",
        "id": 56568
    },
    {
        "content": "def get_rec_names(study_folder):\n    \"\"\"\n    Get list of keys of recordings.\n    Read from the 'names.txt' file in study folder.\n\n    Parameters\n    ----------\n    study_folder: str\n        The study folder.\n\n    Returns\n    ----------\n\n    rec_names: list\n        LIst of names.\n    \"\"\"\n    with open(study_folder / 'names.txt', mode='r', encoding='utf8') as f:\n        rec_names = f.read()[:-1].split('\\n')\n    return rec_names",
        "sha1": "ab9c7eddc0ce593ddcea129c2122e753a94432bf",
        "id": 648436
    },
    {
        "content": "def timestr_to_seconds(s):\n    \"\"\"\n    Convert a string in the format 00:00:00 into seconds.\n    \"\"\"\n    hours, minutes, seconds = s.split(\":\")\n    hours = int(hours) * 3600\n    minutes = int(minutes) * 60\n    seconds = int(seconds)\n    return hours + minutes + seconds",
        "sha1": "08b079369d2685491b71a9fe9556391b2735722d",
        "id": 206807
    },
    {
        "content": "def apple_url_fix(url):\n    \"\"\"\n    Fix Apple URL.\n\n    :param url: URL to fix\n    :return: fixed URL\n    \"\"\"\n    if url.startswith(\"webcal://\"):\n        url = url.replace('webcal://', 'http://', 1)\n    return url",
        "sha1": "e3576984708089e5932a45675ed5af7bc622b3fa",
        "id": 613295
    },
    {
        "content": "def is_float(x):\n    \"\"\"Return true if X can be coerced to a float. Otherwise, return false.\"\"\"\n    try:\n        float(x)\n        return True\n    except ValueError:\n        return False",
        "sha1": "0450c3c6b2024de07e6ea9ec09c5a3955b744ef9",
        "id": 54681
    },
    {
        "content": "import re\n\n\ndef validate_date(date):\n    \"\"\"Validate date format.\n\n    :param date: The date that will be entered into the database.\n    :type date: str\n    :return: True if the date format is well formed with valid characters, false if not.\n    :rtype: bool\n    \"\"\"\n    result = None\n    date_format = \"([0-9]{4}-[0-9]{2}-[0-9]{2} [0-9]{2}:[0-9]{2}:[0-9]{2})\"\n    result = re.match(date_format, date)\n    if result is None:\n        return False\n    else:\n        return True",
        "sha1": "c9bf560c6e25246e8713f93b4a85ac87a7693337",
        "id": 591580
    },
    {
        "content": "def segment_spectrum_batch(spectra_mat, w=50, dw=25):\n    \"\"\"\n    Segment multiple raman spectra into overlapping windows\n\n    Args:\n        spectra_mat (2D numpy array): array of input raman spectrum\n        w (int, optional): length of window. Defaults to 50.\n        dw (int, optional): step size. Defaults to 25.\n\n    Returns:\n        list of numpy array: list containing arrays of segmented raman spectrum\n    \"\"\"\n    \n    return [spectra_mat[:,i:i+w] for i in range(0,spectra_mat.shape[1]-w,dw) ]",
        "sha1": "956791e2957f4810726d1d94549f79f3d83c8d21",
        "id": 13684
    },
    {
        "content": "def convert_mug_to_cup(value):\n    \"\"\"Helper function to convert a string from mug to cup\"\"\"\n    if isinstance(value, str) and value.lower() == 'mug':\n        return 'cup'\n    else:\n        return value",
        "sha1": "9bc2831702e6a8223c3597f581d58f1c4adaf5cf",
        "id": 53455
    },
    {
        "content": "def call_if_attribute(obj, attr, *args, **kwargs):\n    \"\"\"\n    Call object method, if it exists.\n\n    Arguments:\n        obj (object): The object.\n        attr (str): The name of the object method.\n        *args (optional): Arguments for method.\n        **kwargs (optional): Keyword arguments for method.\n\n    Returns:\n        Return value of calling method, or None if object does not have method.\n    \"\"\"\n    op = getattr(obj, attr, None)\n    if callable(op):\n        return op(*args, **kwargs)",
        "sha1": "12a4942bae78422c049567c414309c43c51434e9",
        "id": 367112
    },
    {
        "content": "from typing import List\n\n\ndef get_percentage_of_points_changed(previous_labels: List, labels: List) -> float:\n    \"\"\"Get the percentage of points that changed clusters.\n\n    Args:\n        previous_labels: The labels of the points before the update.\n        labels: The labels of the points after the update.\n\n    Returns:\n        The percentage of points that changed clusters.\n    \"\"\"\n    num_points = len(labels)\n    num_changed = 0\n    for i in range(num_points):\n        if previous_labels[i] != labels[i]:\n            num_changed += 1\n    return num_changed / num_points * 100.0",
        "sha1": "d7075aad3560f4c9bd62bd70e5866bfe8abdf291",
        "id": 566235
    },
    {
        "content": "def prevcrc32(invtab, crctab, nxt, byte):\n    \"\"\"\n    return value such that  crc32(prev, byte) == nxt\n\n    next = crctab[(prev^byte)&0xff] ^ (prev>>8)\n    \"\"\"\n    i = invtab[nxt>>24]              # i == (prev^byte)&0xff\n    return (( (crctab[i] ^ nxt)<<8 ) | (i^byte)) & 0xFFFFFFFF",
        "sha1": "fb56be40b84bdd72566cb40bbbff4e60f2f0102c",
        "id": 538270
    },
    {
        "content": "def sorted_element_targets(element_target_list):\n    \"\"\"Sort element_targets in reverse based on the their depth in the\n    inheritance hierarchy.\n\n    This will make sure any general target, like text will be applied by\n    a specific target like axis_text_x.\n\n    \"\"\"\n    def key(element_target_):\n        return len(element_target_.__class__.__mro__)\n\n    return sorted(element_target_list, key=key, reverse=True)",
        "sha1": "10a9091469393f555ee230202b149786c3812237",
        "id": 404473
    },
    {
        "content": "def chunk_text(corpus, length, step):\n    \"\"\"\n        Cut the given corpus into a series of offset sequences.\n    \"\"\"\n    # A list of sequences\n    sequences = []\n    # For each sequence, record the next character in the corpus\n    next_chars = []\n    for i in range(0, len(corpus) - length, step):\n        sequences.append(corpus[i:i + length])\n        next_chars.append(corpus[i + length])\n    return sequences, next_chars",
        "sha1": "b0efedd89b4139bb207cbe964b8ce2e18e973126",
        "id": 471051
    },
    {
        "content": "def get_num_articles_words(issue):\n    \"\"\"\n    Give an issue, gets a tuple with the number of articles and total\n    number of words in the articles.\n\n    :param issue: issue\n    :type issue: defoe.papers.issue.Issue\n    :return: (1, num_articles, num_words)\n    :rtype: tuple(int, int, int)\n    \"\"\"\n\n    num_words_per_article = [len(article.words) for article in issue.articles]\n\n    num_words = sum(num_words_per_article)\n\n    return (1, len(issue.articles), num_words)",
        "sha1": "088ae2eebb9877c6963db7f4f51677c02726ff0b",
        "id": 463697
    },
    {
        "content": "def entry_gt_comp(entryA, entryB, sampleA=None, sampleB=None):\n    \"\"\"\n    Compare the genotypes of two entries\n\n    :param `entryA`: first entry\n    :type `entryA`: :class:`pysam.VariantRecord`\n    :param `entryB`: second entry\n    :type `entryB`: :class`pysam.VariantRecord`\n    :param `sampleA`: sample of entryA to check\n    :type `sampleA`: string, optional\n    :param `sampleB`: sample of entryB to check\n    :type `sampleB`: string, optional\n\n    :return: True if the genotypes are concordant\n    :rtype: bool\n\n    Example\n        >>> import truvari\n        >>> import pysam\n        >>> v = pysam.VariantFile('repo_utils/test_files/input1.vcf.gz')\n        >>> a = next(v)\n        >>> b = next(v)\n        >>> truvari.entry_gt_comp(a, b)\n        True\n    \"\"\"\n    if not sampleA:\n        sampleA = entryA.samples.keys()[0]\n    if not sampleB:\n        sampleB = entryB.samples.keys()[0]\n    return entryA.samples[sampleA][\"GT\"] == entryB.samples[sampleB][\"GT\"]",
        "sha1": "dbeb1ef3c5db4b3ce62456071434483ec4971cdc",
        "id": 192269
    },
    {
        "content": "import torch\n\n\ndef softmax(v):\n    \"\"\"\n    Computes the softmax function along dimension 0 in a torch.tensor.\n    \"\"\"\n    exps = torch.exp(v)\n\n    return exps / torch.sum(exps, dim=0).unsqueeze(0)",
        "sha1": "7b04e0780f893265f388331f9d76e2c1e52f8efa",
        "id": 315875
    },
    {
        "content": "from typing import List\nimport torch\n\n\ndef cat_tensor_with_nones(\n        tensors: List[torch.Tensor]\n):\n    \"\"\"\n    Concatenate a list of tensors. If any are None, they are excluded\n    \"\"\"\n    tensors = [t for t in tensors if t is not None]\n    if len(tensors) == 1:\n        return tensors[0]\n    elif len(tensors) == 2:\n        return torch.cat((tensors[0], tensors[1]), dim=1)\n    elif len(tensors) == 0:\n        return None\n    else:\n        raise Exception(\"not implemented\")",
        "sha1": "dcf3a38129e54ef33c27baebd95a228c82dacc2d",
        "id": 586665
    },
    {
        "content": "from typing import List\n\n\ndef calculate_diff_occurrence(joltages: List[int]) -> int:\n    \"\"\"Calculate diff occurrences for differences of 1 and 3.\n\n    Args:\n        joltages (List[int]): the joltages of the adapters on hand\n\n    Returns:\n        int: the product of numbers of diff of 1 and diff of 3\n\n    \"\"\"\n    lowest = 1\n    highest = 3\n    device_joltage = max(joltages) + highest\n    # Add both the starting (outlet) and ending (device) joltages.\n    joltages.append(0)\n    joltages.append(device_joltage)\n    joltages = sorted(joltages)\n    diffs = [\n        joltages[i + 1] - joltage for i, joltage in enumerate(joltages[:-1])\n        ]\n    diffs = {diff: diffs.count(diff) for diff in range(lowest, highest + 1)}\n    return diffs[lowest] * diffs[highest]",
        "sha1": "f331fce8319d5cc367fb2e75c282955d93364fb5",
        "id": 627375
    },
    {
        "content": "import random\n\n\ndef random_positive_external_id() -> int:\n    \"\"\"\n    Generate a random integer ID that's 15-digits long.\n    \"\"\"\n    return random.SystemRandom().randint(10 ** 14, (10 ** 15) - 1)",
        "sha1": "c8b5e2d43643729b10af3919a3db261261ba2aad",
        "id": 76550
    },
    {
        "content": "import ctypes\n\n\ndef _load_cdll(sdk_path: str) -> ctypes.CDLL:\n  \"\"\"Loads SDK dynamic library.\"\"\"\n  handle = ctypes.cdll.LoadLibrary(sdk_path)\n  handle.WTQInit.restype = ctypes.c_bool\n  handle.WTQGetDeviceInformation.restype = ctypes.c_bool\n  handle.WTQGetHWInformation.restype = ctypes.c_bool\n  handle.WTQGetPPGDeviceInformation.restype = ctypes.c_bool\n  handle.WTQGetPPGHWInformation.restype = ctypes.c_bool\n  handle.WTQGetSerialNumber.restype = ctypes.c_char_p\n  handle.WTQGetPPGSerialNumber.restype = ctypes.c_char_p\n  return handle",
        "sha1": "bb5a38026413fff11288a12012e31377499e67a3",
        "id": 379382
    },
    {
        "content": "import attr\n\n\ndef DictParameter(default=None, factory=dict, **kwargs):\n    \"\"\"Adds a dict parameter:\n\n    Args:\n        default (optional, dict): Default value. Default: None.\n        base (optional, callabe): Factory function, e.g. dict\n            or collections.OrderedDict. Default: dict.\n    \"\"\"\n    if default is not None:\n        return attr.ib(default, **kwargs)\n    return attr.ib(factory=factory, **kwargs)",
        "sha1": "023b58244cd7152bdd25d35211afd189f3eaf136",
        "id": 260497
    },
    {
        "content": "def feature_norm_ldc(df):\n    \n    \"\"\"\n    Process the features to obtain the standard metrics in LDC mode.\n    \"\"\"\n    \n    df['HNAP'] = df['HNAC']/df['ICC_abs']*100\n    df['TCC'] = (df['ICC_abs']+df['DCC_abs'])/df['VOL']\n    df['ICC'] = df['ICC_abs']/df['VOL']\n    df['DCC'] = df['DCC_abs']/df['VOL']\n    return df",
        "sha1": "60e3ef31c0be07179854de3191c2c75f4ec2cb4d",
        "id": 3557
    },
    {
        "content": "import torch\n\n\ndef calculate_linear_lrp_fast(in_layer_activations: torch.Tensor, weights: torch.Tensor, out_layer_relevance: torch.Tensor, epsilon: torch.Tensor = torch.Tensor([0.0])) -> torch.Tensor:\n    \"\"\"\n    @in_layer_activations: the activations from each neuron from the \"previous\" layer\n    @weights: the weights between the previous and the latter layer. torch.Tensor of shape [out_layer_relevance.shape, in_layer_activations.shape]\n    @out_layer_relevance: The relevance values for each neuron from the latter layer.\n    @epsilon: The epsilon value for the LRP_epsilon algorithm\n    @return: The relevance values for each neuron from the @in_layer_activations\n    \"\"\"\n\n    R = torch.nn.ReLU()(torch.mul(weights, in_layer_activations))\n    R = torch.mul(R.T, out_layer_relevance)\n    denominator = torch.add(torch.sum(R,dim=0),epsilon)\n    R = torch.divide(R,denominator)\n    R = torch.sum(R, dim=1)\n    return R",
        "sha1": "e3165ed321d9844d85661a8ef51c9fbdaa2012f3",
        "id": 356385
    },
    {
        "content": "def vcv_recursive(putative_root, vcv_matrix, finished):\n    \"\"\"\n    This is a fast recursive function to calculate the variance co-variance matrix. \n    \n    Input(s):\n    putative_root - the root node (initially) and subsequently, other clades from a Bio.Phylo tree object\n    vcv_matrix - the matrix which will store everything. Needs to be n x n where n is the \n            total number of terminals in the tree object where putative_root belongs. Should be zeroes to start\n    finished - a list of all called nodes to keep track of everything for record keeping\n\n    Output(s):\n    vcv_matrix - the updated vcv_matrix described above\n    finished - the updated finished list described above\n    \"\"\"\n    terminals = putative_root.get_terminals()\n    if not set(terminals).issubset(set(finished)):\n        vcv_matrix[len(finished):len(finished)+len(terminals), len(finished):len(finished)+len(terminals)] += putative_root.branch_length\n    if len(putative_root.clades) == 2:\n            vcv_matrix, finished = vcv_recursive(putative_root.clades[0], vcv_matrix, finished)\n            vcv_matrix, finished = vcv_recursive(putative_root.clades[1], vcv_matrix, finished)\n    elif len(putative_root.clades) == 0:\n        finished.append(putative_root)\n    return vcv_matrix, finished",
        "sha1": "91ffee8a8a6c53359a1dc7511eaec3f5018c23ad",
        "id": 402810
    },
    {
        "content": "def write_peak_bedtool_string(cluster):\n    \"\"\"\n    Format Peak into NarrowBed format\n    :param cluster: Peak object\n    :return: str, format to NarrowBed format with tab-delimited, [chrom, start, stop, name, pval, strand, thick_start, thick_stop]\n    \"\"\"\n    cluster_info_list = [\n        cluster.chrom,\n        cluster.genomic_start,\n        cluster.genomic_stop,\n        cluster.gene_name + \"_\" + str(cluster.peak_number) + \"_\" + str(cluster.number_reads_in_peak),\n        cluster.final_p_value,\n        cluster.strand,\n        cluster.thick_start,\n        cluster.thick_stop,\n    ]\n    cluster_bedtool_string = \"\\t\".join([str(info) for info in cluster_info_list])\n    return cluster_bedtool_string",
        "sha1": "2890e478549a2a0e63d3056ce92397669697ce7b",
        "id": 656476
    },
    {
        "content": "def get_error_description(code: int) -> str:\n    \"\"\"\n    \u041f\u043e\u043b\u0443\u0447\u0435\u043d\u0438\u0435 \u043e\u043f\u0438\u0441\u0430\u043d\u0438\u044f \u043e\u0448\u0438\u0431\u043a\u0438.\n\n    :param code: \u041a\u043e\u0434 \u043e\u0448\u0438\u0431\u043a\u0438\n    :return: \u041e\u043f\u0438\u0441\u0430\u043d\u0438\u0435 \u043e\u0448\u0438\u0431\u043a\u0438\n    \"\"\"\n\n    errors = {\n        -100: '\u0417\u0430\u0434\u0430\u043d\u043d\u044b\u0439 MFN \u0432\u043d\u0435 \u043f\u0440\u0435\u0434\u0435\u043b\u043e\u0432 \u0411\u0414',\n        -101: '\u041e\u0448\u0438\u0431\u043e\u0447\u043d\u044b\u0439 \u0440\u0430\u0437\u043c\u0435\u0440 \u043f\u043e\u043b\u043a\u0438',\n        -102: '\u041e\u0448\u0438\u0431\u043e\u0447\u043d\u044b\u0439 \u043d\u043e\u043c\u0435\u0440 \u043f\u043e\u043b\u043a\u0438',\n        -140: 'MFN \u0432\u043d\u0435 \u043f\u0440\u0435\u0434\u0435\u043b\u043e\u0432 \u0411\u0414',\n        -141: '\u041e\u0448\u0438\u0431\u043a\u0430 \u0447\u0442\u0435\u043d\u0438\u044f',\n        -200: '\u0423\u043a\u0430\u0437\u0430\u043d\u043d\u043e\u0435 \u043f\u043e\u043b\u0435 \u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u0435\u0442',\n        -201: '\u041f\u0440\u0435\u0434\u044b\u0434\u0443\u0449\u0430\u044f \u0432\u0435\u0440\u0441\u0438\u044f \u0437\u0430\u043f\u0438\u0441\u0438 \u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u0435\u0442',\n        -202: '\u0417\u0430\u0434\u0430\u043d\u043d\u044b\u0439 \u0442\u0435\u0440\u043c\u0438\u043d \u043d\u0435 \u043d\u0430\u0439\u0434\u0435\u043d (\u0442\u0435\u0440\u043c\u0438\u043d \u043d\u0435 \u0441\u0443\u0449\u0435\u0441\u0442\u0432\u0443\u0435\u0442)',\n        -203: '\u041f\u043e\u0441\u043b\u0435\u0434\u043d\u0438\u0439 \u0442\u0435\u0440\u043c\u0438\u043d \u0432 \u0441\u043f\u0438\u0441\u043a\u0435',\n        -204: '\u041f\u0435\u0440\u0432\u044b\u0439 \u0442\u0435\u0440\u043c\u0438\u043d \u0432 \u0441\u043f\u0438\u0441\u043a\u0435',\n        -300: '\u0411\u0430\u0437\u0430 \u0434\u0430\u043d\u043d\u044b\u0445 \u043c\u043e\u043d\u043e\u043f\u043e\u043b\u044c\u043d\u043e \u0437\u0430\u0431\u043b\u043e\u043a\u0438\u0440\u043e\u0432\u0430\u043d\u0430',\n        -301: '\u0411\u0430\u0437\u0430 \u0434\u0430\u043d\u043d\u044b\u0445 \u043c\u043e\u043d\u043e\u043f\u043e\u043b\u044c\u043d\u043e \u0437\u0430\u0431\u043b\u043e\u043a\u0438\u0440\u043e\u0432\u0430\u043d\u0430',\n        -400: '\u041e\u0448\u0438\u0431\u043a\u0430 \u043f\u0440\u0438 \u043e\u0442\u043a\u0440\u044b\u0442\u0438\u0438 \u0444\u0430\u0439\u043b\u043e\u0432 MST \u0438\u043b\u0438 XRF (\u043e\u0448\u0438\u0431\u043a\u0430 \u0444\u0430\u0439\u043b\u0430 \u0434\u0430\u043d\u043d\u044b\u0445)',\n        -401: '\u041e\u0448\u0438\u0431\u043a\u0430 \u043f\u0440\u0438 \u043e\u0442\u043a\u0440\u044b\u0442\u0438\u0438 \u0444\u0430\u0439\u043b\u043e\u0432 IFP (\u043e\u0448\u0438\u0431\u043a\u0430 \u0444\u0430\u0439\u043b\u0430 \u0438\u043d\u0434\u0435\u043a\u0441\u0430)',\n        -402: '\u041e\u0448\u0438\u0431\u043a\u0430 \u043f\u0440\u0438 \u0437\u0430\u043f\u0438\u0441\u0438',\n        -403: '\u041e\u0448\u0438\u0431\u043a\u0430 \u043f\u0440\u0438 \u0430\u043a\u0442\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438',\n        -600: '\u0417\u0430\u043f\u0438\u0441\u044c \u043b\u043e\u0433\u0438\u0447\u0435\u0441\u043a\u0438 \u0443\u0434\u0430\u043b\u0435\u043d\u0430',\n        -601: '\u0417\u0430\u043f\u0438\u0441\u044c \u0444\u0438\u0437\u0438\u0447\u0435\u0441\u043a\u0438 \u0443\u0434\u0430\u043b\u0435\u043d\u0430',\n        -602: '\u0417\u0430\u043f\u0438\u0441\u044c \u0437\u0430\u0431\u043b\u043e\u043a\u0438\u0440\u043e\u0432\u0430\u043d\u0430 \u043d\u0430 \u0432\u0432\u043e\u0434',\n        -603: '\u0417\u0430\u043f\u0438\u0441\u044c \u043b\u043e\u0433\u0438\u0447\u0435\u0441\u043a\u0438 \u0443\u0434\u0430\u043b\u0435\u043d\u0430',\n        -605: '\u0417\u0430\u043f\u0438\u0441\u044c \u0444\u0438\u0437\u0438\u0447\u0435\u0441\u043a\u0438 \u0443\u0434\u0430\u043b\u0435\u043d\u0430',\n        -607: '\u041e\u0448\u0438\u0431\u043a\u0430 autoin.gbl',\n        -608: '\u041e\u0448\u0438\u0431\u043a\u0430 \u0432\u0435\u0440\u0441\u0438\u0438 \u0437\u0430\u043f\u0438\u0441\u0438',\n        -700: '\u041e\u0448\u0438\u0431\u043a\u0430 \u0441\u043e\u0437\u0434\u0430\u043d\u0438\u044f \u0440\u0435\u0437\u0435\u0440\u0432\u043d\u043e\u0439 \u043a\u043e\u043f\u0438\u0438',\n        -701: '\u041e\u0448\u0438\u0431\u043a\u0430 \u0432\u043e\u0441\u0441\u0442\u0430\u043d\u043e\u0432\u043b\u0435\u043d\u0438\u044f \u0438\u0437 \u0440\u0435\u0437\u0435\u0440\u0432\u043d\u043e\u0439 \u043a\u043e\u043f\u0438\u0438',\n        -702: '\u041e\u0448\u0438\u0431\u043a\u0430 \u0441\u043e\u0440\u0442\u0438\u0440\u043e\u0432\u043a\u0438',\n        -703: '\u041e\u0448\u0438\u0431\u043e\u0447\u043d\u044b\u0439 \u0442\u0435\u0440\u043c\u0438\u043d',\n        -704: '\u041e\u0448\u0438\u0431\u043a\u0430 \u0441\u043e\u0437\u0434\u0430\u043d\u0438\u044f \u0441\u043b\u043e\u0432\u0430\u0440\u044f',\n        -705: '\u041e\u0448\u0438\u0431\u043a\u0430 \u0437\u0430\u0433\u0440\u0443\u0437\u043a\u0438 \u0441\u043b\u043e\u0432\u0430\u0440\u044f',\n        -800: '\u041e\u0448\u0438\u0431\u043a\u0430 \u0432 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0430\u0445 \u0433\u043b\u043e\u0431\u0430\u043b\u044c\u043d\u043e\u0439 \u043a\u043e\u0440\u0440\u0435\u043a\u0442\u0438\u0440\u043e\u0432\u043a\u0438',\n        -801: 'ERR_GBL_REP',\n        -801: 'ERR_GBL_MET',\n        -1111: '\u041e\u0448\u0438\u0431\u043a\u0430 \u0438\u0441\u043f\u043e\u043b\u043d\u0435\u043d\u0438\u044f \u0441\u0435\u0440\u0432\u0435\u0440\u0430 (SERVER_EXECUTE_ERROR)',\n        -2222: '\u041e\u0448\u0438\u0431\u043a\u0430 \u0432 \u043f\u0440\u043e\u0442\u043e\u043a\u043e\u043b\u0435 (WRONG_PROTOCOL)',\n        -3333: '\u041d\u0435\u0437\u0430\u0440\u0435\u0433\u0438\u0441\u0442\u0440\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0439 \u043a\u043b\u0438\u0435\u043d\u0442 (\u043e\u0448\u0438\u0431\u043a\u0430 \u0432\u0445\u043e\u0434\u0430 \u043d\u0430 \u0441\u0435\u0440\u0432\u0435\u0440) ' +\n               '(\u043a\u043b\u0438\u0435\u043d\u0442 \u043d\u0435 \u0432 \u0441\u043f\u0438\u0441\u043a\u0435)',\n        -3334: '\u041a\u043b\u0438\u0435\u043d\u0442 \u043d\u0435 \u0432\u044b\u043f\u043e\u043b\u043d\u0438\u043b \u0432\u0445\u043e\u0434 \u043d\u0430 \u0441\u0435\u0440\u0432\u0435\u0440 (\u043a\u043b\u0438\u0435\u043d\u0442 \u043d\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u0442\u0441\u044f)',\n        -3335: '\u041d\u0435\u043f\u0440\u0430\u0432\u0438\u043b\u044c\u043d\u044b\u0439 \u0443\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0439 \u0438\u0434\u0435\u043d\u0442\u0438\u0444\u0438\u043a\u0430\u0442\u043e\u0440 \u043a\u043b\u0438\u0435\u043d\u0442\u0430',\n        -3336: '\u041d\u0435\u0442 \u0434\u043e\u0441\u0442\u0443\u043f\u0430 \u043a \u043a\u043e\u043c\u0430\u043d\u0434\u0430\u043c \u0410\u0420\u041c',\n        -3337: '\u041a\u043b\u0438\u0435\u043d\u0442 \u0443\u0436\u0435 \u0437\u0430\u0440\u0435\u0433\u0438\u0441\u0442\u0440\u0438\u0440\u043e\u0432\u0430\u043d',\n        -3338: '\u041d\u0435\u0434\u043e\u043f\u0443\u0441\u0442\u0438\u043c\u044b\u0439 \u043a\u043b\u0438\u0435\u043d\u0442',\n        -4444: '\u041d\u0435\u0432\u0435\u0440\u043d\u044b\u0439 \u043f\u0430\u0440\u043e\u043b\u044c',\n        -5555: '\u0424\u0430\u0439\u043b \u043d\u0435 \u0441\u0443\u0449\u0435\u0441\u0442\u0432\u0443\u0435\u0442',\n        -6666: '\u0421\u0435\u0440\u0432\u0435\u0440 \u043f\u0435\u0440\u0435\u0433\u0440\u0443\u0436\u0435\u043d. \u0414\u043e\u0441\u0442\u0438\u0433\u043d\u0443\u0442\u043e \u043c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u043e\u0435 \u0447\u0438\u0441\u043b\u043e ' +\n               '\u043f\u043e\u0442\u043e\u043a\u043e\u0432 \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0438',\n        -7777: '\u041d\u0435 \u0443\u0434\u0430\u043b\u043e\u0441\u044c \u0437\u0430\u043f\u0443\u0441\u0442\u0438\u0442\u044c/\u043f\u0440\u0435\u0440\u0432\u0430\u0442\u044c \u043f\u043e\u0442\u043e\u043a \u0430\u0434\u043c\u0438\u043d\u0438\u0441\u0442\u0440\u0430\u0442\u043e\u0440\u0430 ' +\n               '(\u043e\u0448\u0438\u0431\u043a\u0430 \u043f\u0440\u043e\u0446\u0435\u0441\u0441\u0430)',\n        -8888: '\u041e\u0431\u0449\u0430\u044f \u043e\u0448\u0438\u0431\u043a\u0430',\n    }\n\n    if code >= 0:\n        return '\u041d\u043e\u0440\u043c\u0430\u043b\u044c\u043d\u043e\u0435 \u0437\u0430\u0432\u0435\u0440\u0448\u0435\u043d\u0438\u0435'\n\n    if code not in errors:\n        return '\u041d\u0435\u0438\u0437\u0432\u0435\u0441\u0442\u043d\u0430\u044f \u043e\u0448\u0438\u0431\u043a\u0430'\n\n    return errors[code]",
        "sha1": "56b68e58698e93c681cd7dd3d537b3a3270fb3e8",
        "id": 110716
    },
    {
        "content": "import torch\n\n\ndef db_to_amplitude(x, ref=1.0):\n    \"\"\"\n    Decibel-to-amplitude conversion (exponential mapping with base=10)\n\n    Args:\n        x (Tensor): Input in decibel to be converted\n        ref (float): Amplitude value that is equivalent to 0 decibel\n\n    Returns:\n        (Tensor): same size of x, after conversion\n    \"\"\"\n    power_spec = torch.pow(10.0, x / 10.0 + torch.log10(torch.tensor(ref,\n                                                        device=x.device,\n                                                        requires_grad=False,\n                                                        dtype=x.dtype)))\n    return power_spec.pow(0.5)",
        "sha1": "9fd14bb5eecf39d7bad15175f9a6969beebee543",
        "id": 540912
    },
    {
        "content": "import sqlite3\n\n\ndef connect_to_database(database_name):\n    \"\"\"\n    Get connection object by database name\n    :param database_name: database name\n    :return: connection object\n    \"\"\"\n    connection = sqlite3.connect(database_name)\n    return connection",
        "sha1": "f289077afb121851d27db67f64b4677d694c7b99",
        "id": 222597
    },
    {
        "content": "def seconds_from(t, t_reference, unit_conversion=int(1e9)):\n    \"\"\"\n    Helper function which concerts times into relative times in\n    specified unit.\n\n    :param t: Time\n    :param t_reference: Reference time e.g. start of an event or first\n        peak in event.\n    :param unit_conversion: Conversion factor for time units e.g. 10**3\n        for micro seconds.\n    \"\"\"\n    return (t - t_reference) / unit_conversion",
        "sha1": "d9bd8f41ae4d90991cd83c52cdf1399e1db88cd1",
        "id": 323616
    },
    {
        "content": "import random\n\n\ndef getRandomWord (wordDict):\n    \"\"\"\n    Returns a random string from the passed dictionary of lists of strings, and \n    the key also.\n    \"\"\"\n    # Randomly select a key from the dictionary\n    wordKey = random.choice(list(wordDict.keys()))\n\n    # Randomly select a word from the key's list in the dictionary\n    wordIndex = random.randint(0, len(wordDict[wordKey]) - 1)\n    \n    return [wordDict[wordKey][wordIndex], wordKey]",
        "sha1": "015be984a5173954ac071a676a1d17912e88d550",
        "id": 97364
    },
    {
        "content": "def computeBinarySum(s):\n    \"\"\"Compute a sum of binary numbers from a string expression\n\n    Args:\n    \n        s (str): string representing a sum of binary numbers,\n            e.g. '101+1+10011'\n\n    Returns: \n\n        int: the sum of the binary numbers in s\n\n    \"\"\"\n    nums = [int(x, 2) for x in s.split(\"+\")]\n    return sum(nums)",
        "sha1": "1d2b678f1f0c9bdf25339a0dae1a8577a7247fa4",
        "id": 353567
    },
    {
        "content": "import uuid\n\n\ndef get_random_filename(ext, prefix=\"\"):\n    \"\"\"\n    Returns a randomly generated string with the specified\n    extension `ext` appended at the end. `ext` should have a dot.\n    A str `prefix` can also be prefixed to the filename.\n    \"\"\"\n    # hex returns a string with no dashes\n    return prefix + str(uuid.uuid4().hex) + ext",
        "sha1": "e148ba7f0f87cffdcbdb6bff30b4a12d399844f5",
        "id": 489390
    },
    {
        "content": "def fit_fooof_group_3d(fg, freqs, power_spectra, freq_range=None, n_jobs=1):\n    \"\"\"Run FOOOFGroup across a 3D collection of power spectra.\n\n    Parameters\n    ----------\n    fg : FOOOFGroup\n        Fitting object, pre-initialized with desired settings, to fit with.\n    freqs : 1d array\n        Frequency values for the power spectra, in linear space.\n    power_spectra : 3d array\n        Power values, in linear space, as [n_conditions, n_power_spectra, n_freqs].\n    freq_range : list of [float, float], optional\n        Desired frequency range to fit. If not provided, fits the entire given range.\n    n_jobs : int, optional, default: 1\n        Number of jobs to run in parallel.\n        1 is no parallelization. -1 uses all available cores.\n\n    Returns\n    -------\n    fgs : list of FOOOFGroups\n        Collected FOOOFGroups after fitting across power spectra, length of n_conditions.\n    \"\"\"\n\n    fgs = []\n    for cond_spectra in power_spectra:\n        fg.fit(freqs, cond_spectra, freq_range, n_jobs)\n        fgs.append(fg.copy())\n\n    return fgs",
        "sha1": "bd229f1df62c5502252190839891ef3976f468a9",
        "id": 133411
    },
    {
        "content": "def pull_force_by_name(system, force_name = 'NonbondedForce', **kwargs):\n    \"\"\"\n    pull a force object from a system for querying\n\n    arguments\n        system : simtk.openmm.openmm.System\n            system that will be queried\n        force_name : str, default 'NonbondedForce'\n            force name that will be matched\n\n    returns\n        force : simtk.openmm.openmm.Force\n            matched force\n    \"\"\"\n    forces = system.getForces()\n    force_dict = {force.__class__.__name__: force for force in forces}\n    try:\n        force = force_dict[force_name]\n    except Exception as e:\n        raise Exception(f\"{e}\")\n    return force",
        "sha1": "840a064c15442c138f258794ecfdb401a552857a",
        "id": 317868
    },
    {
        "content": "def func_DN(TH:float,\n            SH:float,\n            Sinh:float)->float:\n    \"\"\"\u6cd5\u7dda\u9762\u76f4\u9054\u65e5\u5c04\u91cf\u306e\u5f0f\n    Args:\n      TH(float): \u6c34\u5e73\u9762\u5168\u5929\u65e5\u5c04\u91cf(MJ/m2)\n      SH(float): \u6c34\u5e73\u9762\u5929\u7a7a\u65e5\u5c04\u91cf(MJ/m2)\n      Sinh(float): \u592a\u967d\u9ad8\u5ea6\u89d2\u306e\u30b5\u30a4\u30f3(-)\n\n    Returns:\n      DN(float)): \u6cd5\u7dda\u9762\u76f4\u9054\u65e5\u5c04\u91cf(MJ/m2)\n    \"\"\"\n    return ( TH - SH ) / Sinh",
        "sha1": "adc5e4f8169560faaa71c6125fc07d98c4998050",
        "id": 466956
    },
    {
        "content": "def bfs_shortest_path_print(graph, x, y):\n    \"\"\"Return shortest path between nodes x and y.\"\"\"\n    visited = []\n    q = [[x]]\n\n    while q:\n        path = q.pop(0)\n        node = path[-1]\n        if node not in visited:\n            for adjacent in graph[node]:\n                new_path = list(path)\n                new_path.append(adjacent)\n                if adjacent == y:\n                    return new_path\n                q.append(new_path)\n            visited.append(node)\n    return f'No path from {x} to {y}.'",
        "sha1": "65c6b276cf09b6d37de69c317a8c012eb82ffe89",
        "id": 63236
    },
    {
        "content": "import itertools\n\n\ndef group_by(iterable, iteratee):\n    \"\"\"\n     Splits an iterable into sets, grouped by the result of running each value through iteratee. \n     If iteratee is a string instead of a function, groups by the property named by iteratee on each of the values. \n\n     params: iterable, iteratee\n        iterable -> a list, tuple, iterator, generator\n        iteratee -> a function or a lambda, taking single value as input and returning a transformed value on which iterable will be grouped\n\n    Returns a dictionary\n\n    Examples:\n    >>> _.group_by([1.3, 2.1, 2.4], lambda x:math.floor(x))\n    >>> {1: [1.3], 2: [2.1, 2.4]}\n    \"\"\"\n    grouped_iterators = itertools.groupby(iterable, iteratee)\n    return dict((x,list(y)) for x,y in grouped_iterators)",
        "sha1": "79371a848d3ff290765e34df45f56d65aa74170e",
        "id": 582228
    },
    {
        "content": "def createPointWkt(coords):\n    \"\"\"Create WKT POINT string.\n\n    Args:\n        coords (list): Two item list representing single point coordinate.\n\n    Returns:\n        str: WKT POINT string.\n    \"\"\"\n    return 'POINT(' + str(coords[0]) + ' ' + str(coords[1]) + ')'",
        "sha1": "13b1a4864bb9cccc56e3c34bc340f1a2f4d4c3ee",
        "id": 689761
    },
    {
        "content": "def StrToListInts(data: str) -> list:\n\t\"\"\" Converts a string to a list of integers. \"\"\"\n\t# RETURNS: list<int>\n\tif data.replace(\" \",\"\") == \"[]\":\n\t\treturn []\n\treturn [int(entry) for entry in data.replace(\"[\",\"\").replace(\"]\",\"\").replace(\" \",\"\").split(\",\")]",
        "sha1": "fc29f56dbca21e75a67658260d33a9538c551029",
        "id": 549140
    },
    {
        "content": "from typing import List\n\n\ndef get_monolingual_default_bucket_key(buckets: List[int]) -> int:\n    \"\"\"\n    Returns the default bucket from a list of buckets, i.e. the largest bucket.\n\n    :param buckets: List of buckets.\n    :return: The largest bucket in the list.\n    \"\"\"\n    return max(buckets)",
        "sha1": "f7d02d1f46f8c9e3cea48c861d326ac29d6b67a3",
        "id": 580829
    },
    {
        "content": "def net_addr(addr):\n    \"\"\"Get network address prefix and length from a given address.\"\"\"\n    if addr is None:\n        return (None, None)\n    nw_addr, nw_len = addr.split('/')\n    nw_len = int(nw_len)\n    return nw_addr, nw_len",
        "sha1": "11d4b798338d88a1348ef8efd82fbc3b49b9eb7f",
        "id": 625985
    },
    {
        "content": "def rotate_list(numbers, cursor):\n    \"\"\"Rotate list such that the current start moves to the position\n    indicated by the cursor.\"\"\"\n    return numbers[len(numbers) - cursor:] + numbers[:len(numbers) - cursor]",
        "sha1": "691a4ba53445e67409b5faa2760c6a0eca967437",
        "id": 73399
    },
    {
        "content": "def fmt_utilization(util):\n    \"\"\"Format utilization.\"\"\"\n    return '{0:.4f}'.format(util)",
        "sha1": "225f69353ec4ebf291858ac76b99926d018ea710",
        "id": 378717
    },
    {
        "content": "def makeFuncMappable(func, *args, **kwargs):\n    \"\"\"Generally, `map` doesn't take scalar arguments (Pandas `map` is more\n    restrictive, but even Python's `map` is restricted to all arguments having\n    same length as the first argument -- which is the iterable being mapped).\n\n    This function returns a version of the passed `func` that only takes one\n    argument, running the original `func` with that argument and all other args\n    and kwargs specified to makeFuncMappable.\n    \"\"\"\n    def mappableFunc(arg0):\n        return func(arg0, *args, **kwargs)\n    return mappableFunc",
        "sha1": "ec6d80e1d345bb1b3243682d8cb9417de23cb058",
        "id": 138633
    },
    {
        "content": "import copy\n\n\ndef _instances_by_namespace(data):\n    \"\"\"Rebuild instance data so we can look it up by namespace.\n\n    Note that the `representation` is added into the instance's\n    data with a `representation` key.\n\n    Args:\n        data (dict): scene build data\n\n    Returns:\n        dict\n\n    \"\"\"\n    result = {}\n    # Add new assets\n    for representation_id, instances in data.items():\n\n        # Ensure we leave the source data unaltered\n        instances = copy.deepcopy(instances)\n        for instance in instances:\n            instance['representation'] = representation_id\n            result[instance['namespace']] = instance\n\n    return result",
        "sha1": "094fdb39011688cf7b7c5e6e3608f46c615c4de6",
        "id": 391375
    },
    {
        "content": "import torch\n\n\ndef act_last_qubit_tensor(\n        single_qubit_operator: torch.Tensor,\n        state_tensor: torch.Tensor,\n) -> torch.Tensor:\n    \"\"\"Apply an operator to the last qubit of a state in tensor layout.\n\n        Args:\n            single_qubit_operator: Tensor with size (2, 2, 2) or (2, 2) in\n                the complex and real cases respectively. For the complex case,\n                the first dimension is for real and imaginary parts.\n\n            state_tensor: State or batch of states in tensor layout.\n        \"\"\"\n    def act(matrix, tensor):\n        \"\"\"Contract matrix with the last index of tensor.\"\"\"\n        return torch.einsum('ab, ...b -> ...a', matrix, tensor)\n\n    return act(single_qubit_operator, state_tensor)",
        "sha1": "5da00981e6f26d4836821add77c628532072a9d5",
        "id": 533118
    },
    {
        "content": "import logging\n\n\ndef fileobj_hdlr(f, **kwargs):\n    \"\"\"A file object log handler\n\n    Args:\n        f (obj): A file like object.\n\n    Returns:\n        New instance of :class:`logging.StreamHandler`\n\n    Examples:\n        >>> from io import StringIO\n        >>> fileobj_hdlr(StringIO())  # doctest: +ELLIPSIS\n        <logging.StreamHandler object at 0x...>\n    \"\"\"\n    return logging.StreamHandler(f)",
        "sha1": "d296c3852323195c8ec87f1a36885f22665b0eb2",
        "id": 343816
    },
    {
        "content": "def bahai_month(date):\n    \"\"\"Return 'month' element of a  Bahai date, date.\"\"\"\n    return date[3]",
        "sha1": "73685c4c71fa57c0d6cd6b456d8273176cdcf6f8",
        "id": 196009
    },
    {
        "content": "def map_hostname_info(hostname, nmap_store):\n    \"\"\"Map hostname if there is one to the database record.\"\"\"\n    if hostname is not None:\n        nmap_store[\"hostname\"] = hostname.get('name')\n        return nmap_store\n    nmap_store[\"hostname\"] = None\n    return nmap_store",
        "sha1": "ecab1c241f1785dbc52f1dbc9ad6a3a8fddf618b",
        "id": 690013
    },
    {
        "content": "def rF(count, total):\n\t\"\"\"Returns the relative frequency.\"\"\"\n\treturn float(count)/float(total)",
        "sha1": "a91c97088e5a2e0a5a86048cfd3c28bb9db8eecc",
        "id": 165936
    },
    {
        "content": "def is_set(obj) -> bool:\n    \"\"\"Checks if the given object is either a set or a frozenset.\"\"\"\n    return isinstance(obj, (set, frozenset))",
        "sha1": "6c968f282439aafe09e4bab4dbda4b3a475a2b81",
        "id": 686384
    },
    {
        "content": "def get_setval_path(module_or_path_data):\n    \"\"\" Build setval for path parameter based on playbook inputs\n        Full Command:\n          - path {name} depth {depth} query-condition {query_condition} filter-condition {filter_condition}\n        Required:\n          - path {name}\n        Optional:\n          - depth {depth}\n          - query-condition {query_condition},\n          - filter-condition {filter_condition}\n    \"\"\"\n    if isinstance(module_or_path_data, dict):\n        path = module_or_path_data\n    else:\n        path = module_or_path_data.params[\"config\"][\"sensor_groups\"][0].get(\n            \"path\"\n        )\n    if path is None:\n        return path\n\n    setval = \"path {name}\"\n    if \"depth\" in path.keys():\n        if path.get(\"depth\") != \"None\":\n            setval = setval + \" depth {depth}\"\n    if \"query_condition\" in path.keys():\n        if path.get(\"query_condition\") != \"None\":\n            setval = setval + \" query-condition {query_condition}\"\n    if \"filter_condition\" in path.keys():\n        if path.get(\"filter_condition\") != \"None\":\n            setval = setval + \" filter-condition {filter_condition}\"\n\n    return setval",
        "sha1": "12b3b8dbd748440bdae7e0232c0e05f66c3dc3f8",
        "id": 167101
    },
    {
        "content": "def cut_out(s, phrase):\n    \"\"\"\n    Returns the input string <s> but with all occurrences of <phrase> deleted\n\n    <phrase> should be one or more words, separated by\n    whitespace. Effort is made to preserve one space between words,\n    which makes it better than s.replace(phrase, '')\n\n    >>> s = 'the quick brown fox, which is the brownest ever, jumped over the lazy dog'\n    >>> cut_out(s, 'the')\n    'quick brown fox, which is brownest ever, jumped over lazy dog'\n    >>> s.replace('the', '')\n    ' quick brown fox, which is  brownest ever, jumped over  lazy dog'\n\n    Note the extra spaces in the s.replace version\n\n    \"\"\"\n    return ' '.join(map(str.strip, s.split(phrase))).strip()",
        "sha1": "4f7cc45f6a384087665120e821ca569059273150",
        "id": 435869
    },
    {
        "content": "from typing import Dict\n\n\ndef are_dicts_matching_structure(dict_1: Dict, dict_2: Dict) -> bool:\n    \"\"\"Recursively checks if the keys of the first dictionary match the keys of the second, as well\n    as their value types.\n\n    Args:\n        dict_1 (Dict): first dict to compare\n        dict_2 (Dict): second dict to compare\n\n    Returns:\n        bool: True if the Dict keys and value types match, else False\n    \"\"\"\n    # If the set of keys isn't matching, these dicts don't \"match\"\n    if dict_1.keys() != dict_2.keys():\n        return False\n\n    for key in dict_1.keys():\n        # If the type of key_x in dict_1 doesn't match the type of the same key in dict_2, the\n        # dicts don't match\n        if not isinstance(dict_2[key], type(dict_1[key])):\n            return False\n\n        # If it's a dict type, recurse\n        if isinstance(dict_1[key], dict):\n            if not are_dicts_matching_structure(dict_1[key], dict_2[key]):\n                return False\n\n    return True",
        "sha1": "4f05ce45dde64437bb6a546afa08a12e831865e0",
        "id": 181391
    },
    {
        "content": "def max_diff_w_nan(array):\n    \"\"\"Return the maximum different between element in an array\"\"\"\n    \n    arr_no_nan=[a for a in array if a==a]\n    arr_no_nan.sort()\n    \n    return arr_no_nan[-1]-arr_no_nan[0]",
        "sha1": "47610dff4d5452ae8a623403d481906e9c30cd44",
        "id": 130416
    },
    {
        "content": "def is_there_any_common_element(first: set, second: set) -> int:\n    \"\"\"\n    The function checks if there are common elements in two sets and if there are any then it returns 1, else 0.\n\n    Parameters\n    ----------\n    first : set\n            unique items from the set A\n\n    second : set\n             unique items from the set B\n\n    Returns\n    -------\n    bool\n        True if there is any common element between sets A and B; 0 otherwise\n    \"\"\"\n\n    return int(bool(first & second))",
        "sha1": "8f14e12ebebc058ddea2430540d625f0533738d1",
        "id": 193335
    },
    {
        "content": "def datesuffix(n):\n    \"\"\"Given day of month return English Ordinal Suffix\"\"\"\n    if (n == 1 or n == 21 or n == 31):\n        suffix = \"st\"\n    elif (n == 2 or n == 22):\n        suffix = \"nd\"\n    elif (n == 3 or n == 23):\n        suffix = \"rd\"\n    else:\n        suffix = \"th\"\n    return suffix",
        "sha1": "f0cd5f14e8700e9df3be28bd99577802458d5342",
        "id": 432765
    },
    {
        "content": "def r2h(rgb):\n    \"\"\"\n    Convert an RGB-tuple to a hex string.\n    \"\"\"\n    return '#%02x%02x%02x' % tuple(rgb)",
        "sha1": "b9a6c0d5fda41d2824d9f99b22851def4ba0903a",
        "id": 606424
    },
    {
        "content": "import re\nimport json\n\n\ndef read_json (filename) :\n    \"\"\"\n    Comments in the form of\n        # rest of line\n    are stripped from json before parsing\n\n    use like this::\n\n        import pprint\n        pprint.pprint (read_json (sys.argv[1]))\n\n    \"\"\"\n\n    with open (filename) as f:\n\n        content = ''\n\n        # weed out comments\n        for line in f.readlines () :\n            content += re.sub (r'^\\s*#.*', '', line)\n\n        return json.loads (content)",
        "sha1": "8bd89325e8f3b486bb13790fc4c827427a435515",
        "id": 16657
    },
    {
        "content": "def start_text(**kwargs):\n    \"\"\"Return a start text.\"\"\"\n    name = kwargs.get(\"name\", \"undefined\")\n    parameters = kwargs.get(\"parameters\", \"undefined\")\n\n    message = (\n        \"Start of generation: {name}\"\n        + '\\n'\n        + \"Parameters: {parameters}\"\n    ).format(\n        name=name,\n        parameters=parameters\n    )\n\n    return message",
        "sha1": "278c0cfc8295f544544adfa2174219c4c7a223e2",
        "id": 51691
    },
    {
        "content": "def get_ears_pos(e):\n    \"\"\"\n    Get position of the ears from the app slider\n    args:\n        e   input from app slider\n    returns:\n        the angular position of the ear\n    \"\"\"\n\n    # define range of ears\n    ears_range = 50\n    # define center of ears\n    ears_offset = 100\n\n    # interpolate from slider range (0-100)\n    e_pos = (e-50)*(ears_range/50.0)+ears_offset\n    # print(e_pos)\n    return e_pos",
        "sha1": "aab7b357d9209ca7fffc2fd91597964e5e19240c",
        "id": 164265
    },
    {
        "content": "def choose_film() -> str:\n    \"\"\"\n    Returns the title of the chosen film.\n    \"\"\"\n    film = input(\"Enter a film to see the books which served for the film adaptation.\\n\\\nIf the film is not an adaptation of any book, you will receive recommendations of \\\nsimilar to the film books.\\n\")\n    return film",
        "sha1": "1d5f06d164612a0b101e36c1deb674bd8accf052",
        "id": 187436
    },
    {
        "content": "def error_503(_):\n    \"\"\"Maintenance.\"\"\"\n    return 'PixyShip is down for maintenance', 503",
        "sha1": "3acecece1da3191d699fd58ebcd840494cf31b4c",
        "id": 695605
    },
    {
        "content": "def replace_semicolons(sentences):\n    \"\"\"\n    Spanish GSD and AnCora have different standards for semicolons.\n\n    GSD has semicolons at the end of sentences, AnCora has them in the middle as clause separators.\n    Consecutive sentences in GSD do not seem to be related, so there is no combining that can be done.\n    The easiest solution is to replace sentence final semicolons with \".\" in GSD\n    \"\"\"\n    new_sents = []\n    count = 0\n    for sentence in sentences:\n        for text_idx, text_line in enumerate(sentence):\n            if text_line.startswith(\"# text\"):\n                break\n        else:\n            raise ValueError(\"Expected every sentence in GSD to have a # text field\")\n        if not text_line.endswith(\";\"):\n            new_sents.append(sentence)\n            continue\n        count = count + 1\n        new_sent = list(sentence)\n        new_sent[text_idx] = text_line[:-1] + \".\"\n        new_sent[-1] = new_sent[-1].replace(\";\", \".\")\n        count = count + 1\n        new_sents.append(new_sent)\n    print(\"Updated %d sentences to replace sentence-final ; with .\" % count)\n    return new_sents",
        "sha1": "e8952e93b510662b5d7615284ba047c434fd798f",
        "id": 370068
    },
    {
        "content": "def status(client, access_key, status=True):\n    \"\"\"Set security credentials status.\"\"\"\n    status = client.user.credentials.status(\n        method=\"POST\", accessKey=access_key, isActive=status\n    )\n    return status",
        "sha1": "b7047540307b72523e02795a7debe43667f6b276",
        "id": 203129
    },
    {
        "content": "def modpower(x,e,n):\n  \"\"\"returns the value of x to the e-th power mod n \n  computed by the method of successive squaring\"\"\"\n  result = 1 # to get started\n  s,q = x,e  # s=current square, q=current quotient\n  while q > 0:\n    if q%2 == 1: \n      result = (s * result) % n\n    s = (s * s) % n  # compute the next square\n    q = q//2         # compute the next quotient\n  return result",
        "sha1": "1e0d9176e0c7de339a894e8d8ea6a7b87d238dec",
        "id": 196299
    },
    {
        "content": "from typing import Callable\nfrom typing import Tuple\n\n\ndef retry_fn(fn: Callable, allowable_exceptions: Tuple, retry_count: int=5):\n    \"\"\"\n    Call fn, retrying if exception type in allowable_exceptions is raised up to retry_count times\n    \"\"\"\n    for i in range(0, retry_count):\n        try:\n            return fn()\n        except allowable_exceptions:\n            if i == retry_count - 1:\n                raise",
        "sha1": "d531751d7e3b1706667c3d2a66c57844f0334888",
        "id": 619034
    },
    {
        "content": "def concatenate_script(list_ur_commands):\n    \"\"\"\n    Internal function that concatenates generated UR script into one large script file. Usually used to combine\n    scripts generated by the GrasshopperPython components\n\n    Args:\n        list_ur_commands: A list of formatted UR Script strings\n\n    Returns:\n        ur_script: The concatenated script\n    \"\"\"\n\n    ur_script = \"\\ndef my_script():\\n\"\n    #ur_script += '\\tpopup(\"running my_script\")\\n'\n\n    combined_script = \"\"\n    for ur_cmd in list_ur_commands:\n        combined_script += ur_cmd\n\n    #format combined script\n    lines =  combined_script.split(\"\\n\")\n    for l in lines:\n        ur_script += \"\\t\" + l + \"\\n\"\n\n    ur_script += 'end\\n'\n    ur_script += '\\nmy_script()\\n'\n    return ur_script",
        "sha1": "0fd3be6d60b467b89644fbbaea3e5790f45cf94d",
        "id": 278015
    },
    {
        "content": "import token\n\n\ndef _get_definition_tokens(tokens):\n    \"\"\" Given the tokens, extracts the definition tokens.\n\n    Parameters\n    ----------\n    tokens : iterator\n        An iterator producing tokens.\n\n    Returns\n    -------\n    A list of tokens for the definition.\n    \"\"\"\n    # Retrieve the trait definition.\n    definition_tokens = []\n    first_line = None\n\n    for type, name, start, stop, line_text in tokens:\n        if first_line is None:\n            first_line = start[0]\n\n        if type == token.NEWLINE:\n            break\n\n        item = (\n            type,\n            name,\n            (start[0] - first_line + 1, start[1]),\n            (stop[0] - first_line + 1, stop[1]),\n            line_text,\n        )\n\n        definition_tokens.append(item)\n\n    return definition_tokens",
        "sha1": "f5ffe1b5757828742777d8678fdbd4738d227aa8",
        "id": 51231
    },
    {
        "content": "def postprocess_token_labels(\n    labels,\n    input_mask,\n    label_map=None,\n    remove_trailing_word_pieces=False,\n    trailing_token_mask=None,\n):\n    \"\"\"\n    Postprocesses token classification output:\n        1) Removes predictions on padded tokens.\n        2) If label_map is provided, maps predicted numerical labels\n            back to original labels.\n        3) If remove_trailing_word_pieces is True and trailing_token_mask\n            is provided, remove the predicted labels on trailing word pieces\n            generated by WordPiece tokenizer.\n\n    Args:\n        labels (list): List of lists of predicted token labels.\n        input_mask (list): List of lists. Each sublist contains the attention\n            mask of the input token list, 1 for input tokens and 0\n            for padded tokens.\n        label_map (dict, optional): A dictionary mapping original labels\n            (which may be string type) to numerical label ids. If\n            provided, it's used to map predicted numerical labels back to\n            original labels. Default value is None.\n        remove_trailing_word_pieces (bool, optional): Whether to remove\n            predicted labels of trailing word pieces generated by WordPiece\n            tokenizer. For example, \"criticize\" is broken into \"critic\" and\n            \"##ize\". After removing predicted label for \"##ize\",\n            the predicted label for \"critic\" is assigned to the original word\n            \"criticize\". Default value is False.\n        trailing_token_mask (list, optional): list of boolean values, True for\n            the first word piece of each original word, False for trailing\n            word pieces, e.g. ##ize. If remove_trailing_word_pieces is\n            True, this mask is used to remove the predicted labels on\n            trailing word pieces, so that each original word in the input\n            text has a unique predicted label.\n    \"\"\"\n    if label_map:\n        reversed_label_map = {v: k for k, v in label_map.items()}\n        labels_org = [[reversed_label_map[l_i] for l_i in l] for l in labels]\n    else:\n        labels_org = labels\n\n    labels_org_no_padding = [\n        [label for label, mask in zip(label_list, mask_list) if mask == 1]\n        for label_list, mask_list in zip(labels_org, input_mask)\n    ]\n\n    if remove_trailing_word_pieces and trailing_token_mask:\n        # Remove the padded values in trailing_token_mask first\n        token_mask_no_padding = [\n            [token for token, padding in zip(t_mask, p_mask) if padding == 1]\n            for t_mask, p_mask in zip(trailing_token_mask, input_mask)\n        ]\n\n        labels_no_trailing_pieces = [\n            [label for label, mask in zip(label_list, mask_list) if mask]\n            for label_list, mask_list in zip(\n                labels_org_no_padding, token_mask_no_padding\n            )\n        ]\n        return labels_no_trailing_pieces\n    else:\n        return labels_org_no_padding",
        "sha1": "b7fd5d981ab1ec5c2f3fe4fe95c1c086ede48d00",
        "id": 150140
    },
    {
        "content": "import torch\n\n\ndef move_to_device(item, device):\n    \"\"\"\n    Move tensor onto device. Works on individual tensors, and tensors contained/nested in lists, tuples, and dicts.\n    Parameters:\n        item: tensor to move or (possibly nested) container of tensors to move.\n        device: target device\n\n    Returns:\n        None\n    \"\"\"\n    if torch.is_tensor(item):\n        return item.to(device)\n    elif isinstance(item, list):\n        return [move_to_device(v, device) for v in item]\n    elif isinstance(item, tuple):\n        return tuple([move_to_device(v, device) for v in item])\n    elif isinstance(item, dict):\n        return {k: move_to_device(v, device) for k, v in item.items()}\n    else:\n        return item",
        "sha1": "a9c73672eeabcc8a14d9391bfdfaf5cdf39d5b12",
        "id": 336003
    },
    {
        "content": "def inherits_from(obj, a_class):\n    \"\"\"returns true if obj is a subclass of a_class, otherwise false\"\"\"\n    return(issubclass(type(obj), a_class) and type(obj) != a_class)",
        "sha1": "2c5852ad42f6eb9a4614a29e0fae1c156e2a9295",
        "id": 340454
    },
    {
        "content": "def table_from_bool(ind1, ind2):\n    \"\"\"\n    Given two boolean arrays, return the 2x2 contingency table\n\n    ind1, ind2 : array-like\n        Arrays of the same length\n    \"\"\"\n    return [\n            sum(ind1 & ind2),\n            sum(ind1 & ~ind2),\n            sum(~ind1 & ind2),\n            sum(~ind1 & ~ind2),\n        ]",
        "sha1": "497ce6ad1810386fedb6ada9ba87f0a5baa6318a",
        "id": 4818
    },
    {
        "content": "def parse_row(row):\n    \"\"\"\n    Takes a row of MOS data and returns the variable and a list of values.\n   \n    Parameters\n    -----------\n    row : string\n        The rows where the data collected is stored.\n    \n    Returns\n    --------\n    var : string\n        Value spaces that are part of the rows\n    vals : list\n        Value that are part of the rows\n    \"\"\"\n    var = row[:5].strip()\n    row = row.rstrip('\\n')\n    vals = []\n    for i in range(5, len(row)-1,3):\n        val = row[i:i+3].strip()\n        if ('/' in val):\n            vals[-1] = None\n            vals.append(row[i-3:i+3].strip())\n        elif val != '':\n            vals.append(row[i:i+3].strip())\n        else:\n            vals.append(None)\n    return var, vals",
        "sha1": "fca90f49e7fb8a3eaf1bb9b164812daa2a01a5bd",
        "id": 609161
    },
    {
        "content": "def format_month(month):\n    \"\"\"Formats a month to first 3 characters of the month input\n\n    Args:\n        month: user input month\n    Returns:\n        A ValueError if the input is not a month, or a 3 character month.\n    \"\"\"\n    months = ['Jan','Feb','Mar','Apr','May','Jun',\n              'Jul','Aug','Sep','Oct','Nov','Dec']\n    if (month.isdigit()):\n        if(len(month) > 2):\n            raise ValueError\n        else:\n            month = int(month)\n            if((month > 12) | (month <= 0)): raise ValueError\n            return months[month - 1]\n    elif not(month.istitle() | month.islower()| month.isupper()):\n        raise ValueError\n    elif(month.capitalize() in months):\n        return month.capitalize()\n    else:\n        raise ValueError",
        "sha1": "da7ffd8bc801377ecebcc76e972219633ae21566",
        "id": 12951
    },
    {
        "content": "import re\n\n\ndef get_cached_path(cogs_dir, sheet_title):\n    \"\"\"Return the path to the cached version of a sheet based on its title.\"\"\"\n    filename = re.sub(r\"[^A-Za-z0-9]+\", \"_\", sheet_title.lower())\n    return f\"{cogs_dir}/tracked/{filename}.tsv\"",
        "sha1": "2d456030a44529f0fa7418f8025f7fb52411f22b",
        "id": 178645
    },
    {
        "content": "import itertools\n\n\ndef __build_dimensions(**dimensions) -> str:\n    \"\"\"Builds correct format for custom metric dimensions from kwargs\n\n    Parameters\n    ----------\n    dimensions: dict, optional\n        additional dimensions\n\n    Returns\n    -------\n    str\n        Dimensions in the form of \"key=value,key2=value2\"\n    \"\"\"\n    MAX_DIMENSIONS = 10\n    dimension = \"\"\n\n    # CloudWatch accepts a max of 10 dimensions per metric\n    # We include service name as a dimension\n    # so we take up to 9 values as additional dimensions\n    # before we convert everything to a string of key=value\n    dimensions_partition = dict(itertools.islice(dimensions.items(), MAX_DIMENSIONS))\n    dimensions_list = [dimension + \"=\" + value for dimension, value in dimensions_partition.items() if value]\n    dimension = \",\".join(dimensions_list)\n\n    return dimension",
        "sha1": "d38e61c8757402a910d3a3c3d61973a7dc4c3bba",
        "id": 554576
    },
    {
        "content": "def sfl(L):\n    \"\"\" (list) -> bool\n    Precondition: len(L) >= 2\n\n    Return True if and only if first item of the list is same as last.\n\n    >>> sfl([3, 4, 2, 8, 3])\n    True\n\n    >>> sfl([a, b, c])\n    False\n\n    \"\"\"\n    return (L[0] == L[-1])",
        "sha1": "e89e3a6b8aa3c17ebfee553b044d536227eeabe5",
        "id": 667491
    },
    {
        "content": "def manhattan_heuristic(position, problem, info={}):\n    \"\"\"Return Manhattan distance heuristic for a PositionSearchProblem.\"\"\"\n    xy1 = position\n    xy2 = problem.goal\n    return abs(xy1[0] - xy2[0]) + abs(xy1[1] - xy2[1])",
        "sha1": "247200a19c30357e674b89b111ce92097275d055",
        "id": 527254
    },
    {
        "content": "def get_examples_from_kb(kb, example_type='train'):\n    \"\"\"Extract all the examples of a type (i.e. train, dev, test) from the knowledge base\n\n    Args:\n        kb: Knowledge base\n        example_type: Name of example type (i.e. train, dev, test)\n\n    Returns:\n        Lists of the rows, columns, and targets from the dataset\n    \"\"\"\n    # prepare the training set\n    batch = list(kb.get_all_facts([example_type]))\n    rows = list()\n    cols = list()\n    targets = list()\n\n    for i in range(len(batch)):\n        example = batch[i]\n        cols.append(kb.get_id(example[0][0], 0))\n        rows.append(kb.get_id(example[0][1], 1))\n        targets.append(example[1])\n\n    return rows, cols, targets",
        "sha1": "364430ddf6482d3fc616995d731da09acb607adb",
        "id": 496888
    },
    {
        "content": "def rename_gs_to_az(src):\n    \"\"\"String formatting to update the prefix of the CMIP6 gs store location to Azure\"\"\"\n    tgt = \"az://cmip6/\" + src.split(\"CMIP6/\")[1]\n    return tgt",
        "sha1": "67cfd01976a259ad4ac94610db9c7f83c12b937c",
        "id": 561275
    },
    {
        "content": "import requests\n\n\ndef _get(endpoint, method=\"\", query={}):\n    \"\"\"GET request to the SpaceX API\n\n        Sends HTTP request to the SpaceX API given a\n        set of parameters. Should only be used by the\n        spacex_py module\n\n    Parameters\n    ----------\n        endpoint : str\n            The endpoint for the request\n        method : str\n            The method used for the request\n        query : dict\n            A dictionary representation of query string options\n\n    Returns\n    -------\n        tuple\n            returns the response body and headers\n    \"\"\"\n    request_url = \"https://api.spacexdata.com/v2/{end}/{meth}\".format(\n        end=endpoint, meth=method)\n    res = requests.get(request_url, params=query)\n\n    if not res.ok:\n        res.raise_for_status()\n\n    return res.json(), res.headers",
        "sha1": "da4878a9e8c80e320321ac6f1ecfd1ea4cf0da1a",
        "id": 469949
    },
    {
        "content": "def sym_pairwise(arg1, arg2):\n    \"\"\"Calculates the symmetric difference of two\n    sets.\n\n    Arguments:\n        arg1 -- the first set\n        arg2 -- the second set\n\n    Returns: the symmetric difference of two sets\n    \"\"\"\n    result = []\n\n    # Add each element in arg1 to result as long\n    # as the element is not in arg2 and not\n    # already in result.\n    for element in arg1:\n        if element not in arg2 and element not in result:\n            result.append(element)\n\n    # Add each element in arg2 to result as long\n    # as the element is not in arg1 and not\n    # already in result.\n    for element in arg2:\n        if element not in arg1 and element not in result:\n            result.append(element)\n\n    return result",
        "sha1": "5dedefee7a711f8817fa52cc7dd7043e283309c3",
        "id": 430882
    },
    {
        "content": "import base64\n\n\ndef featB64encode(feat):\n  \"\"\"Base64 encode feature.\n\n  :param feat: feature\n  :type feat: :class:`numpy.ndarray`\n  :return: str\n  \"\"\"\n  return base64.b64encode(feat)",
        "sha1": "af509e6f3c67739f374c2762735d25b4ab80185c",
        "id": 52291
    },
    {
        "content": "def rotate3(a):\n    \"\"\" rotate 3x3 array clockwise by one\n    0   1   2\n    3   4   5\n    6   7   8\n    \"\"\"\n    save = a[0]\n    a[0] = a[1]\n    a[1] = a[2]\n    a[2] = a[5]\n    a[5] = a[8]\n    a[8] = a[7]\n    a[7] = a[6]\n    a[6] = a[3]\n    a[3] = save\n    return a",
        "sha1": "498f329cfa3c673c5921ce71ea81cec23e9c0532",
        "id": 141492
    },
    {
        "content": "def to_gb(num_bytes):\n    \"\"\" Turn bytes into GB and round to 2 decimal places.\n    \"\"\"\n    return round(float(num_bytes) / 1000000000, 2)",
        "sha1": "4663b28aa5351131ee80df95c286dfa15cc6431c",
        "id": 364778
    },
    {
        "content": "def ode(F_x, F_y, position, dt):\n    \"\"\"\n    This function performs a one time step for the ODE\n    \\dot z = F\n    in a two-dimensional domain\n    by the explicit Euler forward method with time step dt\n    \n    :param F_x: float. The x-component of the vector field\n    :param F_y: float. The y-component of the vector field\n    :param position: list of two elements. It is the initial position in R^2\n    :param dt: float. The time step.\n\n    :output (new_position_x, new_position_y): tuple of two floats. It represents\n                          the position at final time \n    \"\"\"\n    new_position_x = position[0] + dt* F_x\n    new_position_y = position[1] + dt* F_y\n\n    return (new_position_x, new_position_y)",
        "sha1": "c9f18984a30da5316a044dff6780b6ba213851c1",
        "id": 218131
    },
    {
        "content": "def get_naip_tnm_url(xmin, ymin, xmax, ymax,\n                     bbox_epsg=3857,\n                     img_epsg=3857,\n                     img_format='jpgpng',\n                     width=500, height=500):\n  \"\"\"\n  Returns the URL for making a request to get an image from The National Map's\n  NAIP REST service.\n\n  Parameters\n  ----------\n  minx, miny, maxx, maxy : numeric\n    the coordinates of the bounding box of the image to return, assumed to be\n    in coordinate reference system EPSG:3857\n  img_format : string\n    image format to request, valid options are 'jpg', 'jpgpng', 'png', 'png8',\n    'png24', 'png32', 'bmp', 'gif', 'tiff'.\n\n\n  Returns\n  -------\n  url : string\n    the url that will return the NAIP image\n  \"\"\"\n\n  url = ''.join(['https://services.nationalmap.gov/arcgis/rest/services/',\n  'USGSNAIPImagery/ImageServer/exportImage?',\n  'bbox={}%2C{}%2C{}%2C{}&'.format(xmin, ymin, xmax, ymax),\n  'bboxSR={}&'.format(bbox_epsg),\n  'size={}%2C{}&'.format(width, height),\n  'imageSR={}&'.format(img_epsg),\n  'time=&',\n  'format={}&'.format(img_format),\n  'pixelType=U8&',\n  'noData=&',\n  'noDataInterpretation=esriNoDataMatchAny&',\n  'interpolation=+RSP_BilinearInterpolation&'\n  'compression=&',\n  'compressionQuality=&',\n  'bandIds=&',\n  'mosaicRule=&',\n  'renderingRule=&',\n  'f=image'])\n\n  return url",
        "sha1": "07b96b49d421bdf72ca45d3837d2411e509a9a1c",
        "id": 182682
    },
    {
        "content": "def pack(x: int, y: int, z: int) -> int:\n    \"\"\"\n    Pack x, y, and z into fields of an 8-bit unsigned integer.\n    x: bits 4..7 (4 bits)\n    y: bits 2..3 (2 bits)\n    z: bits 0..1 (2 bits)\n    \"\"\"\n    word = (x << 4) | (y << 2) | z\n    return word",
        "sha1": "88ec4e3940a55bd10f0f1f12c968000ea945a529",
        "id": 662708
    },
    {
        "content": "def count(values):\n    \"\"\"\n    Returns a dict of counts for each value in the iterable.\n    \"\"\"\n    counts = dict()\n    for v in values:\n        if v not in counts:\n            counts[v] = 0\n        counts[v] += 1\n    return counts",
        "sha1": "0e4febef6dbfefb2e04b103f177af3dd3d3bfc59",
        "id": 114786
    },
    {
        "content": "def initialize_models_itemwise(model, U, suffix='model'):\n    \"\"\"Initializes classifier/regressor per item to be predicted\n\n    Parameters:\n        model : model object to use to fit the data\n        U (DataFrame) : utilily matrix (rows are users, columns are items) \n        suffix (str) : suffix for keys in output dictionary\n\n    Returns:\n        models (dict): dictionary of models, keys correspond to columns/items \n        in the utility matrix and values are the model objects\n    \"\"\"\n    models = {f'{item}{suffix}': model for item in U.columns}\n    return models",
        "sha1": "9e4ed69249201b923d9708af40da3cd476b0cb9e",
        "id": 162451
    },
    {
        "content": "def convert_tag(tag: str) -> str:\n    \"\"\"Convert nltk pos tag to wodnet tag.\"\"\"\n    res = tag[0].lower()\n\n    if res in [\"j\"]:\n        res = \"a\"\n    if res not in [\"a\", \"v\", \"n\", \"r\"]:\n        res = \"n\"\n\n    return res",
        "sha1": "ef185f5107f7c4b55b73de833957ad3149828387",
        "id": 500050
    },
    {
        "content": "def from_size(n):\n    \"\"\"\n    Constructs a zeroed, *n* sized vector clock.\n    \"\"\"\n    return (0,) * n",
        "sha1": "64de5493f441c077f63d5416108dd8962e932a9b",
        "id": 56769
    },
    {
        "content": "from typing import List\n\n\ndef comment_lines(lines: List[str]) -> List[str]:\n    \"\"\"\n    Adds a multi line comment to a List of source lines.\n\n    :param lines: The source lines to be commented.\n    :returns: The commented List of of source lines.\n    \"\"\"\n\n    lines[0] = f'\"\"\"{lines[0]}'\n    if not lines[-1].endswith(\"\\n\"):\n        lines[-1] += \"\\n\"\n    lines[-1] = f'{lines[-1]}\"\"\"'\n    return lines",
        "sha1": "dfa8425f810f40c5aea474a2543d88e9f151ee25",
        "id": 513359
    },
    {
        "content": "def ignore_ctrlc(method):\n    \"\"\"Decorator to make a ProjectKey command ignore Ctrl-C.\n       - For a commands that runs programs that want to handle\n         Ctrl-C themselves - ipython for instance.\"\"\"\n    method.ignore_ctrlc = True\n    return method",
        "sha1": "9b873ad5818cbc9054d4983ab1671bf7064745bb",
        "id": 644903
    },
    {
        "content": "def to_list(obj):\n    \"\"\"\n    Checks if obj is a list and returns it. If not, returns an empty list.\n    :param obj:\n    :return:\n    \"\"\"\n    if isinstance(obj, list):\n        return obj\n    else:\n        return []",
        "sha1": "a672cdc51150365950a4c35657d873770f13d042",
        "id": 179392
    },
    {
        "content": "def make_id_set(feature_list):\n    \"\"\"\n    Helper function to make id lookup sets for a feature list\n    \"\"\"\n    return set((x['id'] for x in feature_list))",
        "sha1": "7ef9d048639fc74dfc5ff329e6e937cb380eea06",
        "id": 532691
    },
    {
        "content": "import torch\n\n\ndef accuracy_MNIST(output, target):\n    \"\"\"\n    Tests the accuracy of the prediction to the target.\n    :param output: The prediction of the model.\n    :param target: The gold target of the prediction.\n    :return: The percent of correct predictions from the whole.\n    \"\"\"\n    with torch.no_grad():\n        pred = torch.argmax(output, dim=1)\n        assert pred.shape[0] == len(target)\n        correct = 0\n        correct += torch.sum(pred == target).item()\n\n    return correct / len(target)",
        "sha1": "781ac5cc3bd9ec6691d5d149a729fe6d2f8ce357",
        "id": 424711
    },
    {
        "content": "def relationship_types() -> str:\n\t\"\"\"\n\tReturns a statement to get all the relationship types in the database.\n\n\tReturns\n\t-------\n\tout: str\n\t\tNeo4j statement\n\n\t\"\"\"\n\n\treturn 'CALL db.relationshipTypes()'",
        "sha1": "cebfff5f89e7673f5227ec1a5bf39105d6d2ef0d",
        "id": 345503
    },
    {
        "content": "def get_orig(term):\n    \"\"\"\n    Follows the 'orig' attribute until it comes to an object without that attribute.\n    \"\"\"\n    while hasattr(term, 'orig'):\n        term = term.orig\n    return term",
        "sha1": "8f16db416cac14d937f89c710ac6ad849adca467",
        "id": 105584
    },
    {
        "content": "def get_el_config(charge):\n    \"\"\" Returns the electronic shell structure associated with a nuclear charge \"\"\"\n\n    # Electronic shells: 1s, 2s, 2p, 3s, 3p, 4s, 3d, 4p, 5s, 4d, 5p, 6s, 4f, 5d, 6p, 7s, 5f, 6d, 7p\n    el_shell = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n\n    # Maximum number of electrons for each shell\n    max_el = [2, 2, 6, 2, 6, 2, 10, 6, 2, 10, 6, 2, 14, 10, 6, 2, 14, 10, 6]\n\n    # Set atomic charge as number of remaining electrons\n    rest_electrons = charge\n\n    # Start with first shell (1s)\n    shell = 0\n\n    # Until all electrons are depleted:\n    while rest_electrons != 0:\n\n        # Compare actual shell with maximum occupation number of current shell\n        if el_shell[shell] != max_el[shell]:\n\n            # If current shell is not full, add one electron and deplete it\n            el_shell[shell] += 1\n            rest_electrons -= 1\n\n        else:  # If the shell is full go to the next one\n\n            shell += 1\n\n    # Return value(s) \n    return el_shell",
        "sha1": "db8a06ab5e277ce5468b0e6d43670d92d9a57580",
        "id": 693923
    },
    {
        "content": "from typing import List\n\n\ndef read_table(input_path: str, delimiter: str = \"\\t\") -> List[List[str]]:\n    \"\"\"\n    read table file (like tsv, csv)\n\n    change delimiter to parse another format.\n    \"\"\"\n    with open(input_path) as f:\n        return [line.strip().split(delimiter) for line in f]",
        "sha1": "a68ca75ac019e8203f1c3299d3c0273ea98b326c",
        "id": 388807
    },
    {
        "content": "import ssl\n\n\ndef default_ssl_context() -> ssl.SSLContext:\n    \"\"\"Creates an SSL context suitable for use with HTTP/2. See\n    https://tools.ietf.org/html/rfc7540#section-9.2 for what this entails.\n    Specifically, we are interested in these points:\n\n        \u00a7 9.2: Implementations of HTTP/2 MUST use TLS version 1.2 or higher.\n        \u00a7 9.2.1: A deployment of HTTP/2 over TLS 1.2 MUST disable compression.\n\n    The h2 project has its own ideas about how this context should be\n    constructed but the resulting context doesn't work for us in the standard\n    Python Docker images (though it does work under macOS). See\n    https://python-hyper.org/projects/h2/en/stable/negotiating-http2.html#client-setup-example\n    for more.\n    \"\"\"\n    ctx = ssl.create_default_context(purpose=ssl.Purpose.SERVER_AUTH)\n\n    # OP_NO_SSLv2, OP_NO_SSLv3, and OP_NO_COMPRESSION are already set by default\n    # so we just need to disable the old versions of TLS.\n    ctx.options |= (ssl.OP_NO_TLSv1 | ssl.OP_NO_TLSv1_1)\n\n    # ALPN and NPN allow upgrades from HTTP/1.1, but these extensions are only\n    # supported by recent versions of OpenSSL. Try to set them up, but don't cry\n    # if they fail.\n    try:\n        ctx.set_alpn_protocols([\"h2\", \"http/1.1\"])\n    except NotImplementedError:\n        pass\n\n    try:\n        ctx.set_npn_protocols([\"h2\", \"http/1.1\"])\n    except NotImplementedError:\n        pass\n\n    return ctx",
        "sha1": "4d1d46fef9f2350f2a10add1890332fcdb66f0ea",
        "id": 588891
    },
    {
        "content": "import re\n\n\ndef _parse_strand_label(s:str) -> list[str]:\n    \"\"\"Breaks a strand label specifiction in to a list of labels\n\n    Args:\n        s (str): see get_strand_ids() for details\n    Returns:\n        list[str]: list of strand labels.\n    \"\"\"    \n    clean_s = re.sub(\"[(]+\", \"(\", re.sub(\"[)]+\", \")\", s))\n    is_combo = False\n    output = []\n    current = \"\"\n    for c in clean_s:\n        if is_combo:\n            if c == \")\":\n                output.append(current)\n                current = \"\"\n                is_combo = False\n            else:\n                current = current + c\n        else:\n            if c == \"(\":\n                is_combo = True\n            else:\n                output.append(c)\n    return output",
        "sha1": "697389ef2d3d4aaaf582192ef945340c27d27536",
        "id": 599620
    },
    {
        "content": "def merge(A, B):\n    \"\"\"Merge two sorted lists.\"\"\"\n    C = []\n    # Take lesser of two elements\n    while A and B:\n        if A[0] < B[0]:\n            C.append(A.pop(0))\n        else:\n            C.append(B.pop(0))\n    # Grab the rest\n    if A:\n        C.extend(A)\n    elif B:\n        C.extend(B)\n    return C",
        "sha1": "b5f015e0ac1410ae3260563798cba74aa98b9ab8",
        "id": 276030
    },
    {
        "content": "def merge_dfs(df_list, keys_for_merge):\n  \"\"\"Joins all the dataframes on the keys.\n\n  Does an outer join, does not check that all rows exist in all dataframes.\n\n  Args:\n    df_list: A list of pandas dataframes. Each needs to have the keys_for_merge\n      as columns.\n    keys_for_merge: A string list of column names to merge on. Typically\n      ['plate', 'well'] aka WELL_MERGE_KEYS.\n\n  Returns:\n    The joined pandas dataframe.\n  \"\"\"\n  if len(df_list) < 1:\n    raise ValueError('I need at least 1 df to merge')\n\n  for k in keys_for_merge:\n    if k not in df_list[0]:\n      raise ValueError('df missing column %s. Has: %s' %\n                       (k, df_list[0].columns))\n  merged_df = df_list[0]\n\n  for df in df_list[1:]:\n    cols_to_add = keys_for_merge + [c for c in df.columns if c not in merged_df]\n    for k in keys_for_merge:\n      if k not in df:\n        raise ValueError('df missing column %s. Has: %s' % (k, df[0].columns))\n    merged_df = merged_df.merge(df[cols_to_add], on=keys_for_merge, how='outer')\n\n  return merged_df",
        "sha1": "f5b0ee9250f7d0c2a855c7b62684d12371a34ec5",
        "id": 129422
    },
    {
        "content": "def getComponents(board, references):\n    \"\"\"\n    Return a list of components based on designator\n    \"\"\"\n    return [f for f in board.GetFootprints() if f.GetReference() in references]",
        "sha1": "761ca2610165260dfba1c80c803ba21820746dbd",
        "id": 435260
    },
    {
        "content": "def escape_file_name(name):\n    \"\"\"No filesystems can handle a null or a slash in a file or directory\n    name so we replace those with a string saying what it is.  Note that\n    any names with any other (unicode) characters will still be preserved in\n    the file name which may nor may not cause you problems.\"\"\"\n\n    return name.replace(\"\\x00\", \"[null]\").replace(\"/\", \"[slash]\")",
        "sha1": "0faf0fdd63a880a2defc7e74f12757f1fb518051",
        "id": 181753
    },
    {
        "content": "def grafana(data, headers):\n    \"\"\"Pretty-print a grafana notification.\"\"\"\n    text = \"\"\n    if \"title\" in data:\n        text = \"#### \" + data[\"title\"] + \"\\n\"\n    if \"message\" in data:\n        text = text + data[\"message\"] + \"\\n\\n\"\n    if \"evalMatches\" in data:\n        for match in data[\"evalMatches\"]:\n            text = text + \"* \" + match[\"metric\"] + \": \" + str(match[\"value\"]) + \"\\n\"\n    data[\"body\"] = text\n    return data",
        "sha1": "2680b7395ec9a903d49cef216259b91567adae3c",
        "id": 448721
    },
    {
        "content": "def loop(env, agent, training=False):\n    \"\"\"\n    Basic training loop; given an environment, calls Agent.act() until the \n    game is finished.\n    \"\"\"\n    reward = 0\n    done = False\n    score = 0\n    special_data = {}\n    special_data['ale.lives'] = 3\n    ob = env.reset()\n    while not done:\n        \n        action = agent.act(ob, reward, done, training=training)\n        ob, reward, done, _ = env.step(action)\n        score += reward\n        # env.render()\n     \n    # Close the env and write monitor result info to disk\n    # print (\"Your score: %d\" % score)\n    return score",
        "sha1": "3a1cb1ba7184d35115717abbf97ba008459ca9dc",
        "id": 352914
    },
    {
        "content": "def color_gains_loss(val):\n    \"\"\"\n    Colours positive gains as green and negative as green\n    \"\"\"\n    color = 'red' if val < 0 else 'green'\n\n    return 'color: %s' % color",
        "sha1": "92196dcda6ac0de4ee1bc169790e6298035424d5",
        "id": 378834
    },
    {
        "content": "import struct\n\n\ndef get_array_of_string(num, length, data):\n    \"\"\"Read subset of data as an array of strings\n\n    Parameters\n    ----------\n    num : int\n        Number of entries in the array\n    length : int\n        Length of the strings in the array\n    data : str\n        4C binary file\n\n    Returns\n    -------\n    str\n        Truncated 4C binary data file\n    list\n        List of strings from data file\n\n    \"\"\"\n    results = []\n    pos = 0\n    for i in range(num):\n        val = struct.unpack('{}s'.format(length),\n                            data[pos:pos + length])[0].decode()\n        results.append(val.strip())\n        pos += length\n    new_data = data[pos:]\n    return new_data, results",
        "sha1": "073722515cc4797e4039ad6b478582be9c15bf0c",
        "id": 88892
    },
    {
        "content": "def _calculate_logging_step(tasks, logging_freq, batch_size, task_tr_samples):\n    \"\"\"Calculate for each task how many steps to log information and write to tensorboard.\n\n    Args:\n         tasks: a list of task names\n         logging_freq: int. How many loggings per epoch.\n         batch_size:\n         task_tr_samples: a dictionary mapping task name to training samples.\n    \"\"\"\n    task_logging_steps = dict()\n    for task in tasks:\n        total_steps = task_tr_samples[task] // batch_size\n        logging_steps = total_steps // logging_freq\n        task_logging_steps[task] = logging_steps\n    return task_logging_steps",
        "sha1": "c07d10eaada0e65870a3b14f254c9c3d2d57059e",
        "id": 416830
    },
    {
        "content": "def time_to_num(time):\n    \"\"\"\n    time: a string representing a time, with hour and minute separated by a colon (:)\n    Returns a number\n\n    e.g. time_to_num(\"9:00\") -> 9 * 2 = 18\n         time_to_num(\"21:00\") -> 21 * 2 = 42\n         time_to_num(\"12:30\") -> 12.5 * 2 = 25\n    \"\"\"\n    time_comps = time.split(\":\")\n\n    assert len(time_comps) == 2;\n    assert int(time_comps[1]) == 0 or int(time_comps[1]) == 30\n\n    result = int(time_comps[0])*2\n    return result if int(time_comps[1]) == 0 else result + 1",
        "sha1": "4112b8c59140c81a901d871f2cb0be4313817155",
        "id": 585747
    },
    {
        "content": "def sliceExists(ctx, slice):\n    \"\"\"Queries the federation to see if the given slice exists\n    \"\"\"\n    slice_id = (\n        \"urn:publicid:IDN+emulab.net:{}+slice+{}\"\n    ).format(ctx.project, slice)\n    if slice_id in ctx.cf.listSlices(ctx):\n        return True\n    return False",
        "sha1": "e6e67f2dbe3403d3ec386421d9042ad7f8c9e070",
        "id": 463137
    },
    {
        "content": "import re\n\n\ndef parse_show_vlan_internal(raw_result):\n    \"\"\"\n    Parse the 'show vlan internal' command raw output.\n\n    :param str raw_result: vtysh raw result string.\n    :rtype: dict\n    :return: The parsed result of the show vlan internal command in a \\\n        dictionary of the form. Returns None if no internal vlan found or \\\n        empty dictionary:\n\n     ::\n\n        {\n            '1024': { 'interface': '1',\n                      'vlan_id': '1024'\n            },\n            '1025': { 'interface': '10',\n                      'vlan_id': '1025'\n            }\n        }\n    \"\"\"\n\n    show_re = (\n        r'\\s+(?P<vlan_id>\\d+)\\s+(?P<interface>\\S+)'\n    )\n\n    result = {}\n\n    for line in raw_result.splitlines():\n        re_result = re.search(show_re, line)\n        if re_result:\n            partial = re_result.groupdict()\n            result[partial['vlan_id']] = partial\n    if result == {}:\n        return None\n    else:\n        return result",
        "sha1": "f3f7bec8d4afc8d1d65e5eef0f60f772128e8530",
        "id": 19649
    },
    {
        "content": "def escape_lg(s):\n    \"\"\"Escape the > and < in the string, which are special in rst\"\"\"\n    return s.replace('>','\\>').replace('<','\\<')",
        "sha1": "5ebee9ec50e5260e9fed929fbd5d17531d415062",
        "id": 592062
    },
    {
        "content": "import re\n\n\ndef name_suffix_from_file(filename):\n    \"\"\"\n    extracts name and suffix from a filename\n    \"\"\"\n    name = re.match(r\"(.*)\\.\", filename)\n    suffix = re.search(r\"[^.]+$\", filename)\n    if name is not None and suffix is not None:\n        return name.group(1), suffix.group()\n\n    return filename, \"\"",
        "sha1": "16c1270398238e483e6ba2f70657a8a92254cf0d",
        "id": 462349
    },
    {
        "content": "def chop(seq, size):\n    \"\"\"Chop a sequence into chunks of the given size.\"\"\"\n    chunk = lambda ii: seq[ii:ii + size]\n    return map(chunk,range(0, len(seq), size))",
        "sha1": "958e6523fba9ec8097c242fbe8f7c419c1dce8b8",
        "id": 183807
    },
    {
        "content": "import struct\n\n\ndef get_uint32(s: bytes) -> int:\n    \"\"\"\n    Get unsigned int32 value from bytes\n    :param s: bytes array contains unsigned int32 value\n    :return: unsigned int32 value from bytes array\n    \"\"\"\n    return struct.unpack('I', s)[0]",
        "sha1": "69f499ad9001f3313681b48a951cac4fd3c0b406",
        "id": 436310
    },
    {
        "content": "def already_visited(string):\n    \"\"\"\n    Helper method used to identify if a subroutine call or definition has\n    already been visited by the script in another instance\n    :param string: The call or definition of a subroutine/function\n    :return: a boolean indicating if it has been visited already or not\n    \"\"\"\n    separated = string.partition('(')[2]\n    if separated.replace(' ', '').replace('(', '')[:2] == 'gr':\n        visited = True\n    else:\n        visited = False\n\n    return visited",
        "sha1": "7a9d84b6e04cdf7edb27bb7cf49cf1021130ab07",
        "id": 703889
    },
    {
        "content": "def dimer_true(dataframe, col_num, dimer_list):\n    \"\"\"\n    Boolean masks to let us know which primers from original df\n    form dimers. If so they are dropped.\n    Args:\n        dataframe (pd.DataFrame): the primer dataframe\n        col_num (int): the column number to check for primer match\n        dimer_list (list): the list containing primer dimer info\n    Returns:\n        out_series (pd.Series): boolean masked series, True if primer is dimer, else False\n    \"\"\"\n    out_series = dataframe.iloc[:, col_num].isin([seq[1] or seq[2] for seq in dimer_list])\n    return out_series",
        "sha1": "138bcbaf01ed93bfb195e89900b37ab682bd6dea",
        "id": 28496
    },
    {
        "content": "def make_word_schedule(block):\n    \"\"\"\n    This function takes the initial 512 bit block and divides it up into 16 x 32 bit words.\n    It then appends a further 48 x 16 bit words (all set to zero) to bring the word schedule\n    up to 64 words each of 32 bits.\n    :param block:\n    :return:\n    \"\"\"\n    words = []\n\n    for i in range(16):\n        word = ''\n        for j in range(i * 32, i * 32 + 32):\n            word += block[j]\n        words.append(word)\n\n    for i in range(48):\n        word = ''\n        for j in range(32):\n            word += '0'\n        words.append(word)\n\n    return words",
        "sha1": "448e327ed91b6c4ec4c876a05c774de47669b0f2",
        "id": 100888
    },
    {
        "content": "import re\n\n\ndef _get_git_ref_from_chartpress_based_version(version):\n    \"\"\"\n    Get a git ref from a chartpress set version of format like\n    1.2.3-beta.1.n123.h1234567, 1.2.3-n123.h1234567, or 1.2.3.\n    \"\"\"\n    tag_hash_split = re.split(\"[\\.|-]n\\d\\d\\d\\.h\", version)\n    if len(tag_hash_split) == 2:\n        return tag_hash_split[1]\n    else:\n        return tag_hash_split[0]",
        "sha1": "4eb00071157403f905bbeafd1b32449cbb48848d",
        "id": 546939
    },
    {
        "content": "import math\n\n\ndef cost_default_edge_weight(_: float) -> float:\n    \"\"\"\n    For a cost default, we assume the edge does not exist: this results in a\n    cost of positive infinity.\n    \"\"\"\n    return math.inf",
        "sha1": "5ad762e12e8bc163c17dd2a8cc024ec8b71a0d4d",
        "id": 569948
    },
    {
        "content": "def get_cal_name(cal_id, service, http):\n    \"\"\"\n    Retrieve the name (summary) for a calendar.\n\n    @param cal_id: str, calendar id\n    @return: str, calendar name\n    \"\"\"\n\n    response = service.calendars().get(calendarId=cal_id).execute(http=http)\n    return response['summary']",
        "sha1": "a57a0ea045914abcf3840bf82f0ecccf2ff540fb",
        "id": 63314
    },
    {
        "content": "def remove_dict_fields(d, fields):\n    \"\"\"Remove multiple keys from a dictionary.\n    Args:\n        d (dict): The dictionary to clean.\n        fields (list): A list of keys (str) to remove.\n    \"\"\"\n    for key in fields:\n        if key in d:\n            del d[key]\n    return d",
        "sha1": "365eed3908a43b9e802494c849bf399408e0ff29",
        "id": 540961
    },
    {
        "content": "def get_perm_from_formats(from_, to_):\n  \"\"\" Get perm from data formats.\n  For example:\n    get_perm_from_formats('NHWC', 'NCHW') = [0, 3, 1, 2]\n\n  :param from_: From data format string.\n  :param to_: To data format string.\n  :return: Perm. Int list.\n  \"\"\"\n  return list(map(lambda x: from_.find(x), to_))",
        "sha1": "6f842ce60d37b48eafdf03bfa6bd8b7536aaa5a9",
        "id": 237341
    },
    {
        "content": "import torch\n\n\ndef create_scheduler_step_lr(optimizer, step_size=30, gamma=0.1):\n    \"\"\"\n    Create a learning rate scheduler. Every `step_size`, the learning late will be multiplied by `gamma`\n\n    Args:\n        optimizer: the optimizer\n        step_size: every number of epochs composing one step. Each step the learning rate will be decreased\n        gamma: apply this factor to the learning rate every time it is adjusted\n\n    Returns:\n        a learning rate scheduler\n    \"\"\"\n    return torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)",
        "sha1": "efca3365e4dcf0b2bd44d5cc81c1605e38d922c8",
        "id": 284722
    },
    {
        "content": "def get_parser_options(fast: bool = False, preproc: bool = False) -> int:\n  \"\"\"Set and retrieve parser options based on user input.\n\n  Args:\n      fast (bool, optional): Flag for ultra-fast parsing at the expense of accuracy. Defaults to False.\n      preproc (bool, optional): Include preprocessor macros in parsing. Defaults to False.\n\n  Returns:\n      An integer bitfield representing the selected options.\n  \"\"\"\n\n  # clang.cindex.TranslationUnit does not have all latest flags\n  CXTranslationUnit_None = 0x0\n  CXTranslationUnit_DetailedPreprocessingRecord = 0x01\n  CXTranslationUnit_Incomplete = 0x02\n  CXTranslationUnit_PrecompiledPreamble = 0x04\n  CXTranslationUnit_CacheCompletionResults = 0x08\n  CXTranslationUnit_ForSerialization = 0x10\n  CXTranslationUnit_CXXChainedPCH = 0x20\n  CXTranslationUnit_SkipFunctionBodies = 0x40\n  CXTranslationUnit_IncludeBriefCommentsInCodeCompletion = 0x80\n  CXTranslationUnit_CreatePreambleOnFirstParse = 0x100\n  CXTranslationUnit_KeepGoing = 0x200\n  CXTranslationUnit_SingleFileParse = 0x400\n  CXTranslationUnit_LimitSkipFunctionBodiesToPreamble = 0x800\n  CXTranslationUnit_IncludeAttributedTypes = 0x1000\n  CXTranslationUnit_VisitImplicitAttributes = 0x2000\n  CXTranslationUnit_IgnoreNonErrorsFromIncludedFiles = 0x4000\n  CXTranslationUnit_RetainExcludedConditionalBlocks = 0x8000\n\n  default_parser_options = (CXTranslationUnit_VisitImplicitAttributes  # only way to get class methods parsed correctly\n                            | CXTranslationUnit_CacheCompletionResults  # potentially speeds up parsing of large baselines\n                            | CXTranslationUnit_SkipFunctionBodies  # skips parsing inside of functions, much faster\n                            | CXTranslationUnit_LimitSkipFunctionBodiesToPreamble  # only skips function preambles\n                            | CXTranslationUnit_RetainExcludedConditionalBlocks  # keep includes inside ifdef blocks\n                            | CXTranslationUnit_KeepGoing)  # don't stop on errors\n\n  # Don't parse include files recursively - major speedup\n  if fast:\n    default_parser_options |= CXTranslationUnit_SingleFileParse\n\n  # Needed to parse preprocessor macros\n  if preproc:\n    default_parser_options |= CXTranslationUnit_DetailedPreprocessingRecord\n\n  return default_parser_options",
        "sha1": "d0d33fb28e177aef010aca4069231b9fa66a4eaa",
        "id": 336555
    },
    {
        "content": "import pickle\n\n\ndef read_pickle(filepath):\n    \"\"\"Read ``data`` from a file with .pickle extension\"\"\"\n    with open(filepath, \"rb\") as f:\n        data = pickle.load(f)\n    return data",
        "sha1": "d8bcbdb79a2ddf8a8390dee4c4bc66ceb4ae4146",
        "id": 496619
    },
    {
        "content": "def calculate_average_price_sl_percentage_long(sl_price, average_price):\n    \"\"\"Calculate the SL percentage based on the average price for a long deal\"\"\"\n\n    return round(\n        ((sl_price / average_price) * 100.0) - 100.0,\n        2\n    )",
        "sha1": "4a0651e77454a2581e3ba31a800d813fc3bbff79",
        "id": 356632
    },
    {
        "content": "def str_to_range(value):\n    \"\"\"Generate year range based on year string range segment input.\n\n    Args:\n        value (str): year string range segment (see parse_year_string()\n                     documentation for year string details)\n    Returns:\n        year range (List[int]):\n    \"\"\"\n    if not isinstance(value, str):\n        raise Exception(\"str_to_range: input must be str\")\n\n    range_split = value.split(':')\n\n    if len(range_split) != 2:\n        raise Exception(\"str_to_range: result of split must be 2 items\")\n\n    try:\n        start = int(range_split[0])\n        end = int(range_split[1]) + 1\n    except:\n        raise Exception(\"str_to_range: invalid years\")\n\n    return range(start, end)",
        "sha1": "43e3eb2e911963ed3dfd5961c11453666b4bb6d4",
        "id": 448504
    },
    {
        "content": "import json\n\n\ndef extract_summary(summary_file, extra_info):\n    \"\"\"Get dict from summary file\n\n    Args:\n        summary_file: name of summary file\n        extra_info: dict of extra info to add to each entry\n\n    Return:\n        new dict\n    \"\"\"\n\n    # Switched to json load for speed\n    temp_results = json.load(open(summary_file, \"r\"))\n    temp_results.update(extra_info)\n    return temp_results",
        "sha1": "574f106c0c02d01f1fbcc58b7d164813a6d49971",
        "id": 508141
    },
    {
        "content": "import torch\n\n\ndef to_one_hot(indexes, output_dim):\n    \"\"\"\n    :param indexes: list of numbers in the range [0, output_dim)\n    :param output_dim: size of a single one-hot tensor\n    :return: tensor containing one_hot representation of indexes\n    \"\"\"\n    assert output_dim >= 2\n    assert output_dim > max(indexes)\n    assert min(indexes) >= 0\n\n    return torch.eye(output_dim)[indexes]",
        "sha1": "bebf3a084459a95d7b7bf693691a6b0daf9c466d",
        "id": 592499
    },
    {
        "content": "def fix_line_breaks(text):\n    \"\"\" Convert Win line breaks to Unix\n    \"\"\"\n    return text.replace(\"\\r\\n\", \"\\n\")",
        "sha1": "c4c698fce80d7c3820f689a163d0df19ea682573",
        "id": 679240
    },
    {
        "content": "def fill_slicer(slicer, in_len):\n    \"\"\" Return slice object with Nones filled out to match `in_len`\n\n    Also fixes too large stop / start values according to slice() slicing\n    rules.\n\n    The returned slicer can have a None as `slicer.stop` if `slicer.step` is\n    negative and the input `slicer.stop` is None. This is because we can't\n    represent the ``stop`` as an integer, because -1 has a different meaning.\n\n    Parameters\n    ----------\n    slicer : slice object\n    in_len : int\n        length of axis on which `slicer` will be applied\n\n    Returns\n    -------\n    can_slicer : slice object\n        slice with start, stop, step set to explicit values, with the exception\n        of ``stop`` for negative step, which is None for the case of slicing\n        down through the first element\n    \"\"\"\n    start, stop, step = slicer.start, slicer.stop, slicer.step\n    if step is None:\n        step = 1\n    if start is not None and start < 0:\n        start = in_len + start\n    if stop is not None and stop < 0:\n        stop = in_len + stop\n    if step > 0:\n        if start is None:\n            start = 0\n        if stop is None:\n            stop = in_len\n        else:\n            stop = min(stop, in_len)\n    else:  # step < 0\n        if start is None:\n            start = in_len - 1\n        else:\n            start = min(start, in_len - 1)\n    return slice(start, stop, step)",
        "sha1": "ed2bcad1246f8f5e238878bf2fc46edfa871a445",
        "id": 359280
    },
    {
        "content": "def _validate_log_level_choices(level: int) -> int:\n    \"\"\"Custom validation function for Pydantic to ensure that a valid\n    logging level is configured.\n\n    Args:\n        level: Log level choice to be validated.\n\n    Returns:\n        Unmodified `level` value if validation succeeds.\n\n    Raises:\n        ValueError: Raised if validation fails.\n    \"\"\"\n    choices = [0, 10, 20, 30, 40, 50]\n    if level not in choices:\n        raise ValueError(\"illegal log level specified\")\n    return level",
        "sha1": "636aa63a2f2a7777ac99ed033bc351ffbbbddc40",
        "id": 525536
    },
    {
        "content": "def calc_bolometric_luminosity(cont_lwav, cont_wav, reference='Shen2011'):\n    \"\"\"Calculate the bolometric luminosity from the monochromatic continuum\n    luminosity (erg/s/A) using bolometric correction factors from the\n    literature.\n\n    The following bolometric corrections are available\n    cont_wav = 1350, reference = Shen2011\n    cont_wav = 3000, reference = Shen2011\n    cont_wav = 5100, reference = Shen2011\n\n    The Shen et al. 2011 (ApJ, 194, 45) bolometric corrections are based on the\n    composite spectral energy distribution (SED) in Richards et al. 2006 (\n    ApJ, 166,470).\n\n    :param cont_lwav: Monochromatic continuum luminosity in erg/s/A.\n    :type cont_lwav: astropy.units.Quantity\n    :param cont_wav: Wavelength of the monochromatic continuum luminosity in A.\n    :type cont_wav: astropy.units.Quantity\n    :param reference: A reference string to select from the available \\\n        bolometric corrections.\n    :type reference: string\n    :return: Returns a tuple of the bolometric luminosity in erg/s and a \\\n        reference string indicating the publication and continuum wavelength \\\n        of the bolometric correction.\n    :rtype: astropy.units.Quantity, string\n\n    \"\"\"\n\n    if cont_wav.value == 1350 and reference == 'Shen2011':\n        reference = 'Shen2011_1350'\n        return cont_lwav * cont_wav * 3.81, reference\n\n    if cont_wav.value == 3000 and reference == 'Shen2011':\n        reference = 'Shen2011_3000'\n        return cont_lwav * cont_wav * 5.15, reference\n\n    if cont_wav.value == 5100 and reference == 'Shen2011':\n        reference = 'Shen2011_5100'\n        return cont_lwav * cont_wav * 9.26, reference\n    else:\n        raise ValueError('[ERROR] No bolometric correction available for the '\n                         'supplied combination of continuum wavelength and '\n                         'reference.')",
        "sha1": "e5ecc7b602a8467961cdde7040ae2e45bd0cfe3a",
        "id": 188573
    },
    {
        "content": "def cwt_synthesis(wavelet_matrix, mean = 0):\n    \"\"\"Synthesizing a signal given a wavelet dataset\n\n    Parameters\n    ----------\n    wavelet_matrix: ndarray\n        The wavelet data matrix.\n    mean: float\n        The mean to translate the signal.\n\n    Returns\n    -------\n    arraylike\n    \tThe generated signal\n\n    \"\"\"\n    return sum(wavelet_matrix[:])+mean",
        "sha1": "62ecc30a190dd5be0b03406ef839ceb3f09ba647",
        "id": 75344
    },
    {
        "content": "def _getLogHandlers(logToFile=True, logToStderr=True):\n    \"\"\"Get the appropriate list of log handlers.\n\n    :param bool logToFile: If ``True``, add a logfile handler.\n    :param bool logToStderr: If ``True``, add a stream handler to stderr.\n    :rtype: list\n    :returns: A list containing the appropriate log handler names from the\n        :class:`logging.config.dictConfigClass`.\n    \"\"\"\n    logHandlers = []\n    if logToFile:\n        logHandlers.append('rotating')\n    if logToStderr:\n        logHandlers.append('console')\n    return logHandlers",
        "sha1": "31294b99e1db76a3392a3f4b40d87e73e4c12407",
        "id": 85929
    },
    {
        "content": "def json_config(json_sample_path):\n    \"\"\"Return a list containing the key and the sample path for a json config.\"\"\"\n    return [\"--config\", json_sample_path]",
        "sha1": "e609ad7aae09fc0ea3acd19d8b22e08a0f362789",
        "id": 640720
    },
    {
        "content": "import re\n\n\ndef remove_hyperlinks(text):\n    \"\"\"\n    Remove all hyperlinks and URLs from the raw text\n    Args:\n        text(str) -- raw text\n    Returns:\n        text(str) -- text clean from hyperlinks\n    \"\"\"\n    text = re.sub(r'https?://\\S+', '', text)\n\n    return text",
        "sha1": "a5042e83e5891a71dac274b0a6fdf4dc6513ae8c",
        "id": 205775
    },
    {
        "content": "def load_file(filename):\n    \"\"\" Load content of a file.  Useful \n    for setting metadata to content of a text file\"\"\"\n    f = open(filename, \"rb\") # py3, added rb mode\n    content = f.read()\n    f.close()\n    return content",
        "sha1": "4ffb0b3c9b28dec68ebd5b6b3464681701b85df2",
        "id": 171895
    },
    {
        "content": "from typing import Dict\n\n\ndef substitute_tags(tag_map: Dict[str, str], text: str) -> str:\n    \"\"\"Substitute tags from the text for corresponding values in the map\"\"\"\n    for tag, value in tag_map.items():\n        text = text.replace('\"' + tag + '\"', value)\n    return text",
        "sha1": "2f6f019416411a537f30e09db0c18f1bddbb9412",
        "id": 186169
    },
    {
        "content": "def get_user(user_id=None):\n    \"\"\"\n    Return object representing the logged in user\n\n    Keyword Parameters:\n    user_id  -- String, identifier representing the logged in user\n      (Default: None, representing an public/anonymous user session)\n\n    >>> # Check public/Anonymous user\n    >>> from pprint import pprint\n    >>> anonymous_user = get_user()\n    >>> pprint(anonymous_user)\n    {'user': {'description': 'Anonymous user.', 'id': None}}\n    >>> anonymous_user = get_user(None) #public/Anonymous user\n    >>> pprint(anonymous_user)\n    {'user': {'description': 'Anonymous user.', 'id': None}}\n    >>> # Check logged in user\n    >>> user = get_user('uid=bob.newhart,ou=People,o=bobnewhart.com')\n    >>> pprint(user)\n    {'user': {'description': 'Authenticated user.',\n              'id': 'uid=bob.newhart,ou=People,o=bobnewhart.com'}}\n    \"\"\"\n    description = \"Authenticated user.\"\n    if user_id is None:\n        description = \"Anonymous user.\"\n    attributes = {'id': user_id, 'description': description}\n    user_object = {'user': attributes}\n    return user_object",
        "sha1": "5ce58eb5a4b1c88b55eee74d0a65e7d3ade27bd4",
        "id": 514144
    },
    {
        "content": "def mean(values):\n    \"\"\"\n    returns mean value of a list of numbers\n\n    >>> mean([1, 2, 3, 4, 1, 2, 3, 4])\n    2.5\n    >>> round(mean([1, 2, 3, 4, 1, 2, 3, 4.1, 1000000]), 4)\n    111113.3444\n    \"\"\"\n    return sum(values) / float(len(values))",
        "sha1": "a61b1991b2f7acc75d53089289d1fadcac559b5a",
        "id": 399227
    },
    {
        "content": "def key_value_string_parser(itemsep=\";\", kvsep=\":\"):\n    \"\"\"\n    Parses a string into a dict.\n\n    Arguments:\n        itemsep - character which separates items\n        kvsep - character which separates the key and value within an item\n\n    Returns:\n        a function which takes the string to be parsed as the sole argument\n        and returns a dict.\n\n    Example:\n\n        >>> parse = key_value_string_parser(itemsep=\";\", kvsep=\":\")\n        >>> parse(\"a:2;b:3\")\n        {'a': 2, 'b': 3}\n    \"\"\"\n\n    def parser(s):\n        items = s.split(itemsep)\n        return dict(item.split(kvsep, 1) for item in items if item)\n\n    return parser",
        "sha1": "94a4f7293ea02bceff7f3073a6ab12424200f404",
        "id": 413373
    },
    {
        "content": "def valid_cards_syntax(cards_str):\n    \"\"\" Confirm that only numeric values separated by periods was given as input \"\"\"\n    cards = cards_str.split('.')\n    for c in cards:\n        if not c.isnumeric():\n            return 'Cards must contain only digits 0-9 separated by periods'\n    return None",
        "sha1": "8e3811505075269d2b1a37751c14017e107ce69b",
        "id": 41597
    },
    {
        "content": "def create_error_payload(exception, message, endpoint_id):\n  \"\"\"\n  Creates an error payload to be send as a response in case of failure\n  \"\"\"\n\n  print(f'{exception}: {message}')\n  error_payload = {\n    'status': 'MESSAGE_NOT_SENT',\n    'endpointId': endpoint_id if endpoint_id else 'NO_ENDPOINT_ID',\n    'message': f'{exception}: {message}'        \n  }  \n  return error_payload",
        "sha1": "90f266d22429d385e828dcdd92fca3d7b2e6df48",
        "id": 4728
    },
    {
        "content": "from typing import List\nimport re\n\n\ndef _split_line(line: str) -> List[str]:\n    \"\"\"\n    >>> _split_line('%%testcell test_cell1 # hello')\n    ['test_cell1']\n    >>> _split_line('%%testcell -n # test_cell1')\n    []\n    \"\"\"\n    positional_args = [\n        arg for arg in re.split(r\" +\", line) if not re.match(r\"^(%|-)\", arg) and arg\n    ]\n    if \"#\" in positional_args:\n        del positional_args[positional_args.index(\"#\") :]\n\n    return positional_args",
        "sha1": "8555a6ca8c9355d79f90798b867cccc0e1ec6743",
        "id": 495987
    },
    {
        "content": "def get_fromtos_line(linestring_coords):\n    \"\"\"\n    Converts a list of linestring coordinates to a list of 'from-to'\n    dictionaries.\n    \"\"\"\n    line_coordinates_list_froms = linestring_coords[:-1]\n    line_coordinates_list_tos = linestring_coords[1:]\n\n    i = 0\n    fromtos = []\n\n    for i, val in enumerate(line_coordinates_list_froms):\n        fromto_dict = {}\n        fromto_dict['from'] = val\n        fromto_dict['to'] = line_coordinates_list_tos[i]\n\n        fromtos.append(fromto_dict)\n\n    return fromtos",
        "sha1": "2a05362815a71a718c300b4c05e2a3d4ef8796d0",
        "id": 634258
    },
    {
        "content": "def opt_bool(opt):\n    \"\"\" Convert bool ini strings to actual boolean values\n    \"\"\"\n\n    return opt.lower() in ['yes', 'y', 'true', '1']",
        "sha1": "9f321363cb96b08a122c9437c028cbe142de5881",
        "id": 37815
    },
    {
        "content": "async def _get_feed_price(db):\n    \"\"\"Get a steemd-style ratio object representing feed price.\"\"\"\n    price = await db.query_one(\"SELECT usd_per_steem FROM hive_state\")\n    return {\"base\": \"%.3f SBD\" % price, \"quote\": \"1.000 STEEM\"}",
        "sha1": "cdf165add5a442f261e52827c6a087ef8912d43a",
        "id": 613254
    },
    {
        "content": "import re\n\n\ndef get_s3_location(s3_host):\n    \"\"\"Get S3 region information from ``s3_store_host``.\n\n    :param s3_host: S3 endpoint url\n    :returns: string value; region information which user wants to use on\n              Amazon S3, and if user wants to use S3 compatible storage,\n              returns ''\n    \"\"\"\n    locations = {\n        's3.amazonaws.com': '',\n        's3-us-east-1.amazonaws.com': 'us-east-1',\n        's3-us-east-2.amazonaws.com': 'us-east-2',\n        's3-us-west-1.amazonaws.com': 'us-west-1',\n        's3-us-west-2.amazonaws.com': 'us-west-2',\n        's3-ap-east-1.amazonaws.com': 'ap-east-1',\n        's3-ap-south-1.amazonaws.com': 'ap-south-1',\n        's3-ap-northeast-1.amazonaws.com': 'ap-northeast-1',\n        's3-ap-northeast-2.amazonaws.com': 'ap-northeast-2',\n        's3-ap-northeast-3.amazonaws.com': 'ap-northeast-3',\n        's3-ap-southeast-1.amazonaws.com': 'ap-southeast-1',\n        's3-ap-southeast-2.amazonaws.com': 'ap-southeast-2',\n        's3-ca-central-1.amazonaws.com': 'ca-central-1',\n        's3-cn-north-1.amazonaws.com.cn': 'cn-north-1',\n        's3-cn-northwest-1.amazonaws.com.cn': 'cn-northwest-1',\n        's3-eu-central-1.amazonaws.com': 'eu-central-1',\n        's3-eu-west-1.amazonaws.com': 'eu-west-1',\n        's3-eu-west-2.amazonaws.com': 'eu-west-2',\n        's3-eu-west-3.amazonaws.com': 'eu-west-3',\n        's3-eu-north-1.amazonaws.com': 'eu-north-1',\n        's3-sa-east-1.amazonaws.com': 'sa-east-1'\n    }\n    # strip off scheme and port if present\n    key = re.sub(r'^(https?://)?(?P<host>[^:]+[^/])(:[0-9]+)?/?$',\n                 r'\\g<host>',\n                 s3_host)\n    return locations.get(key, '')",
        "sha1": "9d68dc433429a5736379200490cffa458856300c",
        "id": 547226
    },
    {
        "content": "import torch\n\n\ndef train_model(train_loader, model, optimizer, criterion, device):\n    \"\"\"\n    Note: train_loss and train_acc is accurate only if set drop_last=False in loader\n\n    :param train_loader: y: one_hot float tensor\n    :param model:\n    :param optimizer:\n    :param criterion: set reduction='sum'\n    :param device:\n    :return:\n    \"\"\"\n    model.train(mode=True)\n    train_loss = 0\n    correct = 0\n    for batch_idx, (x, y) in enumerate(train_loader):\n        x, y = x.to(device), y.to(device)\n        global_prob = model(x)[0]\n        if isinstance(criterion, torch.nn.CrossEntropyLoss):\n            _, yi = y.max(dim=1)\n            loss = criterion(global_prob, yi)\n        else:\n            loss = criterion(global_prob, y)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        train_loss += loss.item()\n        with torch.no_grad():\n            pred = global_prob.max(1, keepdim=True)[1]  # get the index of the max log-probability\n            _, y_idx = y.max(dim=1)\n            correct += pred.eq(y_idx.view_as(pred)).sum().item()\n\n    train_loss /= len(train_loader.dataset)\n    train_acc = correct / len(train_loader.dataset)\n    return {'loss': train_loss, 'acc': train_acc}",
        "sha1": "43621d0a6a0285960ffb2dad8f19ca3a4eebf29d",
        "id": 17001
    },
    {
        "content": "from typing import Any\nfrom typing import Union\n\n\ndef check_is_union(data_type: Any) -> bool:\n    \"\"\"Check if `data_type` is based on a `typing.Union`.\"\"\"\n    return hasattr(data_type, '__origin__') and \\\n        data_type.__origin__ is Union",
        "sha1": "ea6da23d864cde40a3862bc12fd0ab20a7ae3ea0",
        "id": 219611
    },
    {
        "content": "def _metatile_contents_equal(zip_1, zip_2):\n    \"\"\"\n    Given two open zip files as arguments, this returns True if the zips\n    both contain the same set of files, having the same names, and each\n    file within the zip is byte-wise identical to the one with the same\n    name in the other zip.\n    \"\"\"\n\n    names_1 = set(zip_1.namelist())\n    names_2 = set(zip_2.namelist())\n\n    if names_1 != names_2:\n        return False\n\n    for n in names_1:\n        bytes_1 = zip_1.read(n)\n        bytes_2 = zip_2.read(n)\n\n        if bytes_1 != bytes_2:\n            return False\n\n    return True",
        "sha1": "3b5ec1cfbea5cef52a24450ca4b22bc382c2d6be",
        "id": 571920
    },
    {
        "content": "def testdat(testdir):\n    \"\"\"Path to the testdat directory\"\"\"\n    return testdir / \"testdat\"",
        "sha1": "d7c278fba718164d50863e3fb353155a1ff00eee",
        "id": 13483
    },
    {
        "content": "from typing import Callable\nfrom typing import Any\nfrom typing import Sequence\n\n\ndef foldr(fun: Callable[[Any, Any], Any], acc: Any, seq: Sequence[Any]) -> Any:\n    \"\"\"Implementation of foldr in Python3.\n\n    This is an implementation of the right-handed\n    fold function from functional programming.\n\n    If the list is empty, we return the accumulator\n    value. Otherwise, we recurse by applying the\n    function which was passed to the foldr to\n    the head of the iterable collection\n    and the foldr called with fun, acc, and\n    the tail of the iterable collection.\n\n    Below are the implementations of the len\n    and sum functions using foldr to\n    demonstrate how foldr function works.\n\n    >>> foldr((lambda _, y: y + 1), 0, [0, 1, 2, 3, 4])\n    5\n    >>> foldr((lambda x, y: x + y), 0, [0, 1, 2, 3, 4])\n    10\n\n    foldr takes the second argument and the\n    last item of the list and applies the function,\n    then it takes the penultimate item from the end\n    and the result, and so on.\n    \"\"\"\n    return acc if not seq else fun(seq[0], foldr(fun, acc, seq[1:]))",
        "sha1": "5648d8ce8a2807270163ebcddad3f523f527986e",
        "id": 3484
    },
    {
        "content": "def format_native_name(owner, project):\n    \"\"\"\n    Formates the package to the owner used in elm native.\n    >>> format_native_name('elm-lang', 'navigation')\n    '_elm_lang$navigation'\n    \"\"\"\n\n    underscored_owner = owner.replace(\"-\", \"_\")\n    underscored_project = project.replace(\"-\", \"_\")\n    return \"_{owner}${repo}\".format(owner=underscored_owner, repo=underscored_project)",
        "sha1": "492f2b3ab2f84effb1a2511a4c4111f697ee1bfb",
        "id": 525753
    },
    {
        "content": "def _julian_ephemeris_day(jd, deltat):\n    \"\"\"Calculate the Julian Ephemeris Day from the Julian Day and delta-time = (terrestrial time - universal time) in seconds\"\"\"\n    return jd + deltat / 86400.0",
        "sha1": "cc3c8e2c53576bbb3ca6b928f6c5d0ca91197d85",
        "id": 487792
    },
    {
        "content": "def search_mains(xml_root):\n    \"\"\"\n    Searches an xml root for the main activities of an app\n    Probably not the best way to do it, but it works\n    \n    :param xml_root: the root node of the xml file\n    :return: a list of main activities\n    \"\"\"\n    mains = []\n    for application in xml_root:\n        if application.tag == 'application':\n            for activity in application:\n                if activity.tag == 'activity':\n                    for intent in activity:\n                        if intent.tag == 'intent-filter':\n                            for action in intent:\n                                if action.tag == 'action':\n                                    if 'android.intent.action.MAIN' in action.attrib.values():\n                                        mains.append(\n                                            activity.attrib['{http://schemas.android.com/apk/res/android}name'])\n    return mains",
        "sha1": "75a53360f061622b5d811040ff0f805a900bf655",
        "id": 536816
    },
    {
        "content": "def decomposition(x):\n    \"\"\"\n    Return the decomposition of ``x``.\n\n    EXAMPLES::\n\n        sage: M = matrix([[2, 3], [3, 4]])\n        sage: M.decomposition()\n        [\n        (Ambient free module of rank 2 over the principal ideal domain Integer Ring, True)\n        ]\n\n        sage: G.<a,b> = DirichletGroup(20)\n        sage: c = a*b\n        sage: d = c.decomposition(); d\n        [Dirichlet character modulo 4 of conductor 4 mapping 3 |--> -1,\n        Dirichlet character modulo 5 of conductor 5 mapping 2 |--> zeta4]\n        sage: d[0].parent()\n        Group of Dirichlet characters modulo 4 with values in Cyclotomic Field of order 4 and degree 2\n    \"\"\"\n    return x.decomposition()",
        "sha1": "9ce34f0e57c2729a8ffe631230c1da71d9676cd1",
        "id": 665660
    },
    {
        "content": "def get_dependencies(modules, module_name):\n    \"\"\"Return a set of all the dependencies in deep of the module_name.\n    The module_name is included in the result.\"\"\"\n    result = set()\n    for dependency in modules.get(module_name, {}).get('depends', []):\n        result |= get_dependencies(modules, dependency)\n    return result | set([module_name])",
        "sha1": "f280f6c7452a8ed2553bb48272d781e11d3e4e2f",
        "id": 190389
    },
    {
        "content": "def sanitize_mac_for_api(mac):\n    \"\"\"Converts a generalized mac address to one for the API.\n\n    Takes any standard mac (case-insensitive, with or without colons) and\n    formats it to uppercase and removes colons.  This is the format for\n    the API.\n    :param mac: The input mac.\n    :returns: The sanitized mac.\n    \"\"\"\n    return mac.replace(':', '').upper()",
        "sha1": "544c8799db481068ae6b221cbea4dd75a38df307",
        "id": 131653
    },
    {
        "content": "from typing import Tuple\n\n\ndef _yymmdd2ymd(yymmdd: int) -> Tuple[int, int, int]:\n    \"\"\"yymmdd -> (year, month, day)\n\n    Examples:\n        >>> _yymmdd2ymd(321123)\n        (32, 11, 23)\n        >>> _yymmdd2ymd(320323)\n        (32, 3, 23)\n    \"\"\"\n    year, mmdd = divmod(yymmdd, 10000)\n    month, day = divmod(mmdd, 100)\n    return year, month, day",
        "sha1": "9e9d3fa20b4684b603a203c5cc8c8284a8f45dd7",
        "id": 696008
    },
    {
        "content": "import struct\nimport socket\n\n\ndef IpStringToNativeInt(ip4addr):\n  \"\"\"Convert w.x.y.z IP4 address to *native* byte order integer.\n\n  IP addresses are conventionally in network byte order, which is\n  big-endian. We instead want an integer in native byte order where\n  we can do comparisons to see if an IP address is within a range.\n\n  Arguments:\n    ip4addr: a dotted quad string.\n  Returns:\n    an integer in *native* byte order, not network byte order.\n  \"\"\"\n  try:\n    return struct.unpack('!I', socket.inet_pton(socket.AF_INET, ip4addr))[0]\n  except socket.error:\n    return 0",
        "sha1": "2c76f09a47a2c0a70c66b86ecb04ce3b43162433",
        "id": 502816
    },
    {
        "content": "def dict_get_nested(d, keys, alt_ret):\n    \"\"\"Get a nested dictionary entry given a list of keys.\n\n    Equivalent to d[keys[0]][keys[1]]...etc.\n\n    Args:\n        d (dict): nested dictionary to search.\n        keys (list): keys to search one by one.\n        alt_ret: returned if the specified item is not found.\n\n    Returns:\n          item matching the chain of keys in d.\n\n    \"\"\"\n    item = d.get(keys[0], alt_ret)\n    for k in keys[1:]:\n        item = item.get(k, alt_ret)\n    return item",
        "sha1": "5e67b5ab2f6134835eb622e9feb0cffa28e60fa2",
        "id": 365713
    },
    {
        "content": "def _rescale_0_1(batch):\n    \"\"\"\n    Rescale all image from batch, per channel, between 0 and 1\n    \"\"\"\n    for image_id in range(batch.size(0)):\n        for channel_id in range(batch[image_id].size(0)):\n            pix_min = batch[image_id][channel_id].min()\n            pix_range = batch[image_id][channel_id].max() - pix_min\n            batch[image_id][channel_id].sub_(pix_min).div_(pix_range)\n    return batch",
        "sha1": "65e70cb6b3779f9ec776568a65fee13f56f0ca21",
        "id": 47031
    },
    {
        "content": "def dequeue(self):\n    \"\"\"Remove and return the item at the front of this queue\"\"\"\n    if self.is_empty() == True:\n        raise ValueError\n\n    head_data = self.front()\n\n    self.list.delete(self.list.get_at_index(0))\n\n    return head_data",
        "sha1": "335d341d77c79680ba3adb2b074a0595edd2dca4",
        "id": 560117
    },
    {
        "content": "import re\n\n\ndef make_single_line(text):\n    \"\"\" Replaces newlines, spaces and tabs with a single space \"\"\"\n    return re.sub(r'[\\n\\s\\t]+', ' ', text).strip()",
        "sha1": "89e8a8cab402a95056fe31879b207c4a462829cd",
        "id": 90085
    },
    {
        "content": "import json\n\n\ndef job_as_json(job):\n    \"\"\"\n    Serializes a job into JSON\n    \"\"\"\n    return json.dumps(job, indent=2)",
        "sha1": "0d97351e5f8aadd0b32a5f88b16212a9e37f23a3",
        "id": 529221
    },
    {
        "content": "from typing import List\nfrom typing import Dict\n\n\ndef _add_annotation(objs: List[Dict], annotation_key: str) -> List[Dict]:\n    \"\"\"\n    Adds a boolean annotation (set to True) to a list of rich text objects\n    (https://developers.notion.com/reference/rich-text)\n    \"\"\"\n    for obj in objs:\n        if 'annotations' in obj:\n            obj['annotations'][annotation_key] = True\n        else:\n            obj['annotations'] = {annotation_key: True}\n    return objs",
        "sha1": "9f3f465cd88ebdfa6e5adbc909c0be9a77836bf7",
        "id": 637525
    },
    {
        "content": "def append_readable_time(readable_time: str, time: int, duration: str) -> str:\n    \"\"\"Appends to the readable_time string with the specified time and duration.\"\"\"\n\n    if readable_time:\n        # Checks if its just one or multiple for proper wording,\n        # luckily the -s ending works out for every case.\n        return (\n            f\"{readable_time}, {time} {duration}\"\n            if time == 1\n            else f\"{readable_time}, {time} {duration}s\"\n        )\n\n    return f\"{time} {duration}\" if time == 1 else f\"{time} {duration}s\"",
        "sha1": "ed20402f2bbfc3e72bb919122414bacf2d758610",
        "id": 378744
    },
    {
        "content": "import hashlib\n\n\ndef get_sha1(file):\n    \"\"\"Return the SHA1 hash of <file>\"\"\"\n    m = hashlib.sha1()\n    with open(file, 'rb') as f:\n        for chunk in iter(lambda: f.read(4096), b\"\"):\n            m.update(chunk)\n    return m.hexdigest().zfill(40)",
        "sha1": "5e66be88dd7552faed6471b28c2674aef1a72831",
        "id": 160704
    },
    {
        "content": "def send_gmail_email(service, user_id: str, message):\n    \"\"\"Send a message using the GMail API service.\n\n    Args:\n        service: GMail API service.\n        user_id: GMail API service user id.\n        message: An GMail API compatible email object.\n\n    Returns:\n        Sent email information if successful.\n    \"\"\"\n    try:\n        sent_gmail_message_info = (\n            service.users().messages().send(userId=user_id, body=message).execute()\n        )\n        return sent_gmail_message_info\n    except Exception as gmail_exception:\n        raise gmail_exception",
        "sha1": "85fc26e7cb0e1defd6a413ab356750fdc465522c",
        "id": 412076
    },
    {
        "content": "def wrap_image(img_path, trailing_br=False, **kwargs):\n    \"\"\"Return a <img> markup, linking to the image in a URL\"\"\"\n    _width = kwargs.get('width', '')\n    _style = kwargs.get('style', '')\n    \n    if _style != '':\n        _style = ' style=\"' + _style + '\"'\n    \n    if str(_width) != '' : _width = ' width='+str(_width)\n    else                 : _width = ''\n    \n    txt = '<img src=\"' + img_path +'\" ' + _width + _style + '></img>'\n    if trailing_br:\n        txt = txt + '<br>'\n    return txt",
        "sha1": "860fc64c29010d386a981a178001962d2fb851d7",
        "id": 661007
    },
    {
        "content": "def prepare_query(query, params):\n    \"\"\"Replace template query {parameter}s with the\n    values provided in the dictionary\n    \"\"\"\n    return query.format(**params)",
        "sha1": "df88a1d90fa42e2ab7181764cb454b8f1e60ae1e",
        "id": 197946
    },
    {
        "content": "def callable_or_raise(obj):\n    \"\"\"Check that an object is callable, else raise a :exc:`ValueError`.\n    \"\"\"\n    if not callable(obj):\n        raise ValueError('Object {0!r} is not callable.'.format(obj))\n    return obj",
        "sha1": "cb6dd8c03ea41bb94a8357553b3f3998ffcc0d65",
        "id": 709264
    },
    {
        "content": "def get_test_name(testcase_func, _, param):\n    \"\"\"\n    A function to create test names for parameterized\n    For example, a test case named test_list will generate names\n    of the form test_list_professor, test_list_head_ta, etc.\n    \"\"\"\n\n    return f\"{testcase_func.__name__}_{param.args[0]}\"",
        "sha1": "0b077a0d5717c52c4aaaff49552809dff555bc54",
        "id": 324557
    },
    {
        "content": "import logging\n\n\ndef loglevel(level):\n    \"\"\"\n    Converts a string representing the logging level to corresponding number.\n\n    This is primarily used as the type of an argument in argparse. See\n    https://docs.python.org/3/library/logging.html#logging-levels for a list\n    of logging levels.\n\n    Args:\n        level (str): logging level\n\n    Returns:\n        int: Numeric logging level\n\n    Raises:\n        ValueError: If `level` is not a recognized logging level.\n    \"\"\"\n    numeric_level = getattr(logging, level.upper(), None)\n    if not isinstance(numeric_level, int):\n        raise ValueError('Invalid log level: %s' % loglevel)\n    return numeric_level",
        "sha1": "9cec4bb7c31a43edefa5b660e4da3889112ede74",
        "id": 390974
    },
    {
        "content": "def mass_equation(pos, vel, force, mass_params):\n    \"\"\" Mass equation\"\"\"\n    d2x = -force/mass_params.mass + mass_params.g\n    return d2x",
        "sha1": "ae09087e3285cc83e6a191f593c7cba2cb04e3c8",
        "id": 337688
    },
    {
        "content": "def mm2m(millimeters):\n    \"\"\"millimeters -> meters\"\"\"\n    return millimeters/1000",
        "sha1": "4c31ed9df60b76ab0f7c8f0393c72f804da9aab1",
        "id": 678226
    },
    {
        "content": "def cluster(data, maxgap):\n  \"\"\"\n  This function is from https://stackoverflow.com/a/14783998,\n    which arranges data into groups where successive elements\n    differ by no more than *maxgap*.\n  \"\"\"\n  data.sort()\n  groups = [[data[0]]]\n  for x in data[1:]:\n      if abs(x - groups[-1][-1]) <= maxgap:\n          groups[-1].append(x)\n      else:\n          groups.append([x])\n  return groups",
        "sha1": "9c74cd3dc52d65a426b0460a2fdddf158e9b600a",
        "id": 439488
    },
    {
        "content": "def _popanykey(dct, *keys, strict=False):\n    \"\"\"\n    Returns the first of the listed keys to be found in the dict\n    :param dct: a dict\n    :param keys:\n    :param strict: [False] if True, raise KeyError if we get to the end of the list and nothing was found\n    :return:\n    \"\"\"\n    for key in keys:\n        if key.lower() in dct:\n            return dct.pop(key.lower())\n    if strict:\n        raise KeyError(keys)\n    return None",
        "sha1": "dc895b8a2f7b4c42b8f46557a48a0f1fb048eb96",
        "id": 492293
    },
    {
        "content": "def invert(record):\n    \"\"\" Invert (ID, tokens) to a list of (token, ID)\n    Args:\n        record: a pair, (ID, token vector)\n    Returns:\n        pairs: a list of pairs of token to ID\n    \"\"\"\n    return [(k,record[0]) for k,v in record[1].iteritems()]",
        "sha1": "d4deb41f1572824d2a5c6dfcde12d68df8944f70",
        "id": 673402
    },
    {
        "content": "import math\n\n\ndef VecLen(a):\n    \"\"\"Return the Euclidean length of the argument vector.\n\n    Args:\n      a: n-tuple of floats\n    Returns:\n      float: the 2-norm of a\n    \"\"\"\n\n    s = 0.0\n    for v in a:\n        s += v * v\n    return math.sqrt(s)",
        "sha1": "b8f1f962ac71b4c1598bcdefed899d761fb1fbec",
        "id": 501479
    },
    {
        "content": "def expTaylor(n, x):\n    \"\"\"Calculate exp(x) with n terms.\n\n    10.3us <- Run `%timeit expTaylor(100, 5)` in iPython\n\n    Rationale\n    ---------\n    Term 1. x**1 / fact(1) = x / 1\n    Term 2. x**2 / fact(2) = x*x / (1*2) = Term 1 * x/2\n    Term 3. x**3 / fact(3) = x*x*x / (1*2*3) = Term 2 * x/3\n    Term 4. x**4 / fact(4) = x*x*x*x / (1*2*3*4) = Term 3 * x/n\n    ...\n    Term k. x**k / fact(k) = Term (k-1) * x/k\n    \"\"\"\n    term   = 1\n    output = term  # Start with the first term, to avoid ZeroDivisionError\n    for k in range(1, n):\n        term   *= x / k\n        output += term\n    return output",
        "sha1": "97884a382e872fd6637beca24c3f5e6a79c2c1cb",
        "id": 565368
    },
    {
        "content": "def points_2_xywh(box):\n    \"\"\" Converts [xmin, ymin, xmax, ymax] to [xmin, ymin, width, height]. \"\"\"\n\n    box = [box[0], box[1], box[2] - box[0], box[3] - box[1]]\n    box = [int(round(x)) for x in box]\n    return box",
        "sha1": "24873f02ae4b828c0e43f139eda5b7eca360d9f9",
        "id": 686102
    },
    {
        "content": "def no_donations(effect):\n    \"\"\"no (successful) donations have been made\"\"\"\n    return not effect.instance.donations.filter(status='succeeded').count()",
        "sha1": "d951c6d32d901eba18a38fe8567c9ee71ba31648",
        "id": 561571
    },
    {
        "content": "def snapname(nsnap):\n    \"\"\"snapname(17) -> snap_0000017 \"\"\"\n    prefix=\"snap_\"\n    suffix=\"%07d\" % nsnap\n    name = prefix + suffix\n    return name",
        "sha1": "f6caf4be28cdf46da22aeed7995202e6c390c35b",
        "id": 263886
    },
    {
        "content": "def get_season_archive_url(base_url: str) -> str:\n    \"\"\"Creates the URL for the season archive endpoint.\"\"\"\n    return f\"{base_url}/season/archive\"",
        "sha1": "5ca241d304e0ba5a3175aed025b6851289fddda3",
        "id": 397524
    },
    {
        "content": "from itertools import chain\n\n\ndef flatten(listOfLists):\n    \"\"\"\n    Flatten one level of nesting given a list of lists. That is, convert\n    [[1, 2, 3], [4, 5, 6]] to [1, 2, 3, 4, 5, 6].\n\n    :param listOfLists: a list of lists, obviously\n    :return: the flattened list\n    \"\"\"\n\n    return list(chain.from_iterable(listOfLists))",
        "sha1": "05edac25559dc344458da38e84420822e46601df",
        "id": 217768
    },
    {
        "content": "import re\n\n\ndef _match_trainid(value):\n    \"\"\"Return the trainID in value, None if no trainID.\"\"\"\n    mat = re.search(r\"TR(\\d{4})\", value)\n    if mat is not None and len(mat.groups()) > 0:\n        return mat.groups()[0]\n    return None",
        "sha1": "be5676b31ead30e6220bbde0058ef63110f0a53c",
        "id": 659627
    },
    {
        "content": "def convert_binary(df, binary_column, positive, negative):\n    \"\"\"Converts a column with binary variable codes as 0 and 1 to understandable strings.\n\n    Args:\n        df: dataframe with binary column\n        binary_column: column name of binary variable\n        positive: string to encode 1 as\n        negative: string to encode 0 as\n\n    Returns:\n        Input dataframe with converted binary column\n    \"\"\"\n    replace_dict = {0: negative, 1: positive}\n    df[binary_column] = df[binary_column].replace(replace_dict)\n    return df",
        "sha1": "79aa4b7a120c2c8a4254dd335d7743b8f3505c37",
        "id": 388395
    },
    {
        "content": "def check_stats(cbpro_client, logger, product = 'BTC-USD'):\n    \"\"\" Check stats of a pair\n\n    Params: \n        - product (pair): string default BTC-USD\n    Return: \n        - price response\n    \n    {\n        \"open\": \"6745.61000000\", \n        \"high\": \"7292.11000000\", \n        \"low\": \"6650.00000000\", \n        \"volume\": \"26185.51325269\", \n        \"last\": \"6813.19000000\", \n        \"volume_30day\": \"1019451.11188405\"\n    }\n\n\n    \"\"\"\n    logger.info(\"Getting price\")\n    btc_stats = cbpro_client.get_product_24hr_stats(product)\n    logger.info(\"Last Price: {}\".format(btc_stats.get(\"last\")))\n\n    resp = btc_stats\n\n    if 'message' in resp.keys():\n        logger.warning(\"message in keys?\")\n    else:\n        logger.info(\"Stats Response: {}\".format(resp))\n    \n    return resp",
        "sha1": "b1e074cd3a124eb9c7759067c99449e5dc0d49c5",
        "id": 230790
    },
    {
        "content": "def nextLiteral(cnf):\n    \"\"\"returns the first literal in the first clause.\n    \"\"\"\n    return list(cnf[0].keys())[0]",
        "sha1": "9b3e7afcc8fcd14f6ee792bc0c0561233a589d2e",
        "id": 390071
    },
    {
        "content": "import re\n\n\ndef spaces_only(text):\n    \"\"\"Return text with all whitespace reduced to single spaces (trimmed).\"\"\"\n    return re.sub(r\"\\s+\", \" \", text).strip()",
        "sha1": "e9140c56d34b9f38641b73db1fadcf84b099f475",
        "id": 338490
    },
    {
        "content": "def _mutation_drop(expr, expolim, tfuncs, nvars, random_state, **kwargs):\n    \"\"\"Randomly removes one IT term from the IT expression\"\"\"\n\n    index = random_state.randint(0, len(expr))\n\n    return [expr[i] for i in range(len(expr)) if i!= index]",
        "sha1": "23c45c7d3f967e916f9bb629a37fec090318c0dd",
        "id": 483288
    },
    {
        "content": "def clean_code(code):\n    \"\"\" Remove duplicate declarations of std function definitions.\n    \n    Use this if you build a JS source file from multiple snippets and\n    want to get rid of the function declarations like ``_truthy`` and\n    ``sum``.\n    \n    Parameters:\n        code (str): the complete source code.\n    \n    Returns:\n        new_code (str): the code with duplicate definitions filtered out.\n    \"\"\"\n    known_funcs = {}\n    \n    lines = []\n    for line in code.splitlines():\n        line2 = line.lstrip()\n        indent = len(line) - len(line2)\n        if line2.startswith('var ') and ' = function ' in line2:\n            prev_indent = known_funcs.get(line2, 999)\n            if prev_indent == 0 :\n                continue\n            if indent == 0:\n                known_funcs[line2] = indent\n        lines.append(line)\n    return '\\n'.join(lines)",
        "sha1": "0e4266005f11c76336788bd5b6971f15ecf38b10",
        "id": 456318
    },
    {
        "content": "def increment(clock, index):\n    \"\"\"\n    Increment the clock at *index*.\n    \"\"\"\n    return clock[:index] \\\n        + (clock[index] + 1,) \\\n        + clock[index+1:]",
        "sha1": "94684ce7e5d69deec64aa320151ff0710e90411a",
        "id": 376601
    },
    {
        "content": "def _asymptotic_decay(x: float, t: int, max_t: int) -> float:\n    \"\"\"\n    Asymptotic decay function. Can be used for both the learning_rate or the neighborhood_radius.\n\n    :param x: float: Initial x parameter\n    :param t: int: Current iteration\n    :param max_t: int: Maximum number of iterations\n    :return: float: Current state of x after t iterations\n    \"\"\"\n    return x / (1 + t / (max_t / 2))",
        "sha1": "b57ccd576a448298e919d8172c8a53ec05f8aff9",
        "id": 296078
    },
    {
        "content": "def findCmdLineSwitch(argList, switch, ignoreCase=True):\n    \"\"\"\n    Argument List\uc5d0\uc11c command switch\ub97c \ucc3e\ub294\ub2e4.\n\n    optViewFields = '/view_fields:'\n    optDeletedRecords = '/deleted_records'\n    argv = 'SQLiteParser.py external.db files /view_fields:_data,date_modified,date_added /deleted_records'.split()\n    v1 = findCmdLineSwitch(argv, optViewFields)       # _data,date_modified,date_added\n    v2 = findCmdLineSwitch(argv, optDeletedRecords)   # True\n  \"\"\"\n    argc = len(argList)\n    for i in range(1, argc):\n        if type(argList[i]) is not str: continue\n        if ignoreCase:\n            argv = argList[i].lower()\n            switch = switch.lower()\n        else:\n            argv = argList[i]\n        if argv == switch:\n            return True\n        elif argv.startswith(switch):\n            value = argv[len(switch):]\n            if value == '':\n                return True\n            else:\n                return value\n        else:\n            False",
        "sha1": "693d160bc21002b3d48775d70d6fa50a570b0fa5",
        "id": 661307
    },
    {
        "content": "def point_in_polygon(polyx, polyy, testx, testy):\n    \"\"\"\n    check whether or not a given coordinate is inside or outside of a polygon\n\n    Parameters\n    ----------\n    polyx : np.ndarray\n            x-coordinates of the polygon\n\n    polyy : np.ndarray\n            y-coordinates of the polygon\n\n    testx : float\n            x-coordinate of the point\n\n    testy : float\n            y-coordinate of the point\n\n    Returns\n    -------\n    bool\n            True, if the point is inside of the polygon\n    \"\"\"\n    res = False\n    j = polyx.shape[0] - 1\n    for i in range(polyx.shape[0]):\n        if ((polyy[i] > testy) != (polyy[j] > testy)) \\\n                and (testx < (polyx[j] - polyx[i]) * (testy - polyy[i]) / (polyy[j] - polyy[i]) + polyx[i]):\n            res = not res\n        j = i\n    return res",
        "sha1": "e7fdbde112919017da3363788e35dadf9ec76972",
        "id": 553864
    },
    {
        "content": "from typing import Set\nimport json\n\n\ndef _get_item_db(locale: str) -> Set[str]:\n    \"\"\"Fetches the item database for a given locale, with caching.\"\"\"\n    with open('items/%s.json' % locale, encoding='utf-8') as fp:\n        return set(json.load(fp))",
        "sha1": "6de037d129f6b345e8cfe49aab5c793b13433dd9",
        "id": 440697
    },
    {
        "content": "def _compute_union(w1, w2):\n    \"\"\"Compute the union of two windows\"\"\"\n    col_off = min(w1.col_off, w2.col_off)\n    row_off = min(w1.row_off, w2.row_off)\n    width = max(w1.col_off+w1.width, w2.col_off+w2.width) - col_off\n    height = max(w1.row_off+w1.height, w2.row_off+w2.height) - row_off\n    return col_off, row_off, width, height",
        "sha1": "eb80025a85b83fa0bd73d7766e760e3b61dabd29",
        "id": 459583
    },
    {
        "content": "def obj_func(xl, arg):\n    \"\"\"Wraps a spreadsheet computation as a Python function.\"\"\"\n    # Copy argument values to input range\n    xl.Range('Inputs').Value = [(float(x), ) for x in arg]\n\n    # Calculate after changing the inputs\n    xl.Calculate()\n\n    # Return the value of the output cell\n    result = xl.Range(\"Output\").Value\n    return result",
        "sha1": "a1d8b2f3384449fcef0b0f88b21946692c7270f7",
        "id": 631058
    },
    {
        "content": "import typing\n\n\ndef validate_list(expected, lst: list) -> typing.Union[str, bool]:\n    \"\"\"\n    Validate a list against our expected schema.\n\n    Returns False if the list is valid.\n    Returns error in string format if invalid.\n    \"\"\"\n\n    if not isinstance(lst, list):\n        return \"Expected argument of type `%s`, got `%s`\" % (\n            str(expected).replace(\"typing.\", \"\"),\n            type(lst).__name__,\n        )\n\n    each_arg_type = typing.get_args(expected)[0]\n\n    for item in lst:\n        if not isinstance(item, each_arg_type):\n            return \"Not all list items are of expected value, `%s`, found `%s`\" % (\n                each_arg_type.__name__,\n                type(item).__name__,\n            )\n\n    return False",
        "sha1": "1318eaa1b8b4b1730b356475d5b2a3bdc037405f",
        "id": 673211
    },
    {
        "content": "def calculate_loan_to_value_ratio(loan_amount, home_value):\n    \"\"\"Calculates users loan to value ratio based on inputs.\n\n    Args:\n        loan_amount (int): The requested loan amount.\n        home_value (int): The home value.\n\n    Returns:\n        The loan-to-value ratio.\n    \"\"\"\n    loan_to_value_ratio = int(loan_amount) / int(home_value)\n    return loan_to_value_ratio",
        "sha1": "c5ea07d01a4898c1006e4d54f536b8e33379aa62",
        "id": 55891
    },
    {
        "content": "def stitch_values(values_and_indices_list):\n  \"\"\"Stitch values together according to their indices.\n\n  Args:\n    values_and_indices_list: a list of tuples of values and indices indicating\n      the values and positions in the returned list.\n\n  Returns:\n    a stitched list of values.\n  \"\"\"\n  length = 0\n  for values_and_indices in values_and_indices_list:\n    length += len(values_and_indices[0])\n\n  result = [None] * length\n  for values_and_indices in values_and_indices_list:\n    if values_and_indices and values_and_indices[0]:\n      for v, i in zip(*values_and_indices):\n        assert result[i] is None\n        result[i] = v\n  return result",
        "sha1": "f760b39d831a58d49b303e1647f33c857754065e",
        "id": 385227
    },
    {
        "content": "import re\n\n\ndef _dashify_uppercase(name):\n    \"\"\"convert somethingWithUppercase into something-with-uppercase\"\"\"\n    first_cap_re = re.compile('(.)([A-Z][a-z]+)')  # better to define this once\n    all_cap_re = re.compile('([a-z0-9])([A-Z])')\n    s1 = first_cap_re.sub(r'\\1-\\2', name)\n    return all_cap_re.sub(r'\\1-\\2', s1).lower()",
        "sha1": "798e670388f318c7c4949eb034ddf1b86913faa2",
        "id": 374069
    },
    {
        "content": "from typing import OrderedDict\n\n\ndef sorted_dict(d: dict):\n    \"\"\"\n    Returns OrderedDict sorted by ascending key\n    :param d: dict\n    :return: OrderedDict\n    \"\"\"\n    return OrderedDict(sorted(d.items()))",
        "sha1": "46e550f428841d66601a4e36c654dd450f0231ed",
        "id": 473221
    },
    {
        "content": "def time_to_offset(starttime, endtime, middletime):\n    \"\"\"Transform middle time to offset.\"\"\"\n    nominator = (middletime - starttime).total_seconds()\n    denominator = (endtime - starttime).total_seconds()\n    return float(nominator) / float(denominator)",
        "sha1": "d1fecfb06172b6f7a871a94478dcb954fc301e86",
        "id": 416609
    },
    {
        "content": "import torch\n\n\ndef batch_fn(fn):\n    \"\"\"Transforms a function :fn: to act on each element of a batch individually.\"\"\"\n    def batched_fn(x, **kwargs):\n        return torch.stack([fn(xi, **kwargs) for xi in x])\n    return batched_fn",
        "sha1": "ae54a0e37cce7f76abcaa954d2f5a4bad9ba0e3b",
        "id": 501477
    },
    {
        "content": "def binary_search(array: list, target: int) -> bool:\n    \"\"\" searches through a sorted list to find a target integer \"\"\"\n    mid = len(array) // 2\n    if len(array) < 1:\n        return False\n    if len(array) == 1:\n        return array[0] == target\n    if array[mid] < target:\n        return binary_search(array[mid:], target)\n    elif array[mid] > target:\n        return binary_search(array[:mid], target)\n    else:\n        return True",
        "sha1": "29ecabf13b3a5bdb394958504d0766659add7965",
        "id": 470944
    },
    {
        "content": "import re\n\n\ndef isdatauri(value):\n    \"\"\"\n    Return whether or not given value is base64 encoded data URI such as an image.\n    If the value is base64 encoded data URI, this function returns ``True``, otherwise ``False``.\n\n    Examples::\n\n        >>> isdatauri('data:text/plain;base64,Vml2YW11cyBmZXJtZW50dW0gc2VtcGVyIHBvcnRhLg==')\n        True\n\n        >>> isdatauri('dataxbase64data:HelloWorld')\n        False\n\n    :param value: string to validate base64 encoded data URI\n    \"\"\"\n    data_uri = re.compile(r\"\\s*data:([a-zA-Z]+/[a-zA-Z0-9\\-+]+(;[a-zA-Z\\-]+=[a-zA-Z0-9\\-]+)?)?(;base64)?,[a-zA-Z0-9!$&',()*+,;=\\-._~:@/?%\\s]*\\s*$\")\n    return bool(data_uri.match(value))",
        "sha1": "43a50554c17b1fd180a9234f2a4631689f14c823",
        "id": 17431
    },
    {
        "content": "import base64\n\n\ndef pictourl(pic):\n    \"\"\"\n    turn bytes into data URL\n    \"\"\"\n    if pic.startswith(b'\\x89\\x50\\x4e\\x47\\r\\n\\x1a\\n'): # PNG signature\n        return b\"data:image/png;base64,\" + base64.b64encode(pic)\n    if pic.startswith(b'\\xff\\xd8'):\n        return b\"data:image/jpeg;base64,\" + base64.b64encode(pic)\n    return b\"data:,Unknown%20file%20format\"",
        "sha1": "f660b2d63e582c045eb8a257b064d2ba59b38f1a",
        "id": 391749
    },
    {
        "content": "def query_transform(context, include_page=False, **kwargs):\n    \"\"\"\n\n    Returns the URL-encoded querystring for the current page,\n    updating the params with the key/value pairs passed to the tag.\n\n    E.g: given the querystring ?foo=1&bar=2\n    {% query_transform bar=3 %} outputs ?foo=1&bar=3\n    {% query_transform foo='baz' %} outputs ?foo=baz&bar=2\n    {% query_transform foo='one' bar='two' baz=99 %}\n    outputs ?foo=one&bar=two&baz=99\n\n    A RequestContext is required for access to the current querystring.\n\n    from: https://gist.github.com/benbacardi/d6cd0fb8c85e1547c3c60f95f5b2d5e1\n\n    if page is true, we will return the page number tag too, if it is\n    false, we want to strip it out and reset our filters to page 1.\n    This allows the same template tag to be used in paginators and\n    'refinement' widgets.  Without, refinement widgets may point to a\n    page that doesn't exist after the new filter has been applied.\n\n    \"\"\"\n\n    query = context[\"request\"].GET.copy()\n    for k, v in kwargs.items():\n        query[k] = v\n\n    if query.get(\"page\") and not include_page:\n        query.pop(\"page\")\n    return query.urlencode()",
        "sha1": "4e1361b174a910dd5e2fceef2eaec5bbbf89387b",
        "id": 650926
    },
    {
        "content": "import functools\n\n\ndef methodcache(f):\n    \"\"\"Utility decorator that ensures the passed function is only called once and the result \n    is cached and reused on future calls. This saves the cached values in the method __cache\n    attribute (only works for methods).\n    \"\"\"\n\n    name = f.__name__\n\n    @functools.wraps(f)\n    def cached(self, *args, **kwargs):\n        if not hasattr(self, \"__cache\"):\n            self.__cache = {}\n\n        if name not in self.__cache:\n            self.__cache[name] = {\"called\": False, \"result\": None}\n\n        if self.__cache[name][\"called\"]:\n            return self.__cache[name][\"result\"]\n\n        result = f(self, *args, **kwargs)\n        self.__cache[name] = {\"called\": True, \"result\": result}\n        return result\n\n    return cached",
        "sha1": "8b538245836431a914ca2042b1ae2d502675e99a",
        "id": 319781
    },
    {
        "content": "def _get_approx_string_width(text, font_width, fixed_width=False):\n    \"\"\"\n    Get the approximate width of a string using a specific average font width.\n\n    Args:\n        text(str): Text string to calculate width of.\n        font_width(int): Average width of font characters.\n        fixed_width(bool): Indicates that the font is fixed width.\n\n    Returns:\n        int: Width of string in pixels.\n\n    Examples:\n\n        Call the function with a string and the maximum character width of the font you are using:\n\n            >>> int(_get_approx_string_width('hello', 10))\n            29\n\n        This example shows the comparison of simplistic calculation based on a fixed width.\n        Given a test string and a fixed font width of 10, we can calculate the width\n        by multiplying the length and the font character with:\n\n            >>> test_string = 'GOOGLE|ijkl'\n            >>> _get_approx_string_width(test_string, 10, fixed_width=True)\n            110\n\n        Since some characters in the string are thinner than others we expect that the\n        apporximate text width will be narrower than the fixed width calculation:\n\n            >>> _get_approx_string_width(test_string, 10)\n            77\n\n    \"\"\"\n    if fixed_width:\n        return len(text) * font_width\n\n    size = 0.0\n\n    # A dictionary containing percentages that relate to how wide\n    # each character will be represented in a variable width font.\n    # These percentages can be calculated using the ``_get_character_percentage_dict`` function.\n    char_width_percentages = {\n        \"lij|' \": 40.0,\n        '![]fI.,:;/\\\\t': 50.0,\n        '`-(){}r\"': 60.0,\n        '*^zcsJkvxy': 70.0,\n        'aebdhnopqug#$L+<>=?_~FZT0123456789': 70.0,\n        'BSPEAKVXY&UwNRCHD': 70.0,\n        'QGOMm%W@': 100.0\n    }\n\n    for s in text:\n        percentage = 100.0\n        for k in char_width_percentages.keys():\n            if s in k:\n                percentage = char_width_percentages[k]\n                break\n        size += (percentage / 100.0) * float(font_width)\n\n    return int(size)",
        "sha1": "f1efee8d86e5882ba36b277244f4d6b65742da04",
        "id": 158676
    },
    {
        "content": "def moleculeRepre(atoms):\n    \"\"\"\n    Return the list of all representations of one molecule\n    \"\"\"\n    listeRepre = []\n    for atom in atoms:\n        if atom[20] not in listeRepre:\n            listeRepre.append(atom[20])\n    return listeRepre",
        "sha1": "796792c23d86e5fc462d24170b7c8976b6238351",
        "id": 344570
    },
    {
        "content": "def tsub(tup1, tup2):\n    \"\"\" Subtracts tup1 elements from tup2 elements. \"\"\"\n    return (tup1[0]-tup2[0], tup1[1]-tup2[1])",
        "sha1": "d44dd291a79fbbbfdb11e1a114f915c6b9036b3c",
        "id": 551270
    },
    {
        "content": "import pathlib\nimport shutil\n\n\ndef repository(tmp_path: pathlib.Path, sessionrepository: pathlib.Path) -> pathlib.Path:\n    \"\"\"Fixture for a Mercurial repository.\"\"\"\n    path = tmp_path / \"repository\"\n    shutil.copytree(sessionrepository, path)\n\n    return path",
        "sha1": "2acfb5f4d12a62abe24eb72bf9d2799d03ad342b",
        "id": 453380
    },
    {
        "content": "import math\n\n\ndef get_frost_point_c(t_air_c, dew_point_c):\n    \"\"\"Compute the frost point in degrees Celsius\n    :param t_air_c: current ambient temperature in degrees Celsius\n    :type t_air_c: float\n    :param dew_point_c: current dew point in degrees Celsius\n    :type dew_point_c: float\n    :return: the frost point in degrees Celsius\n    :rtype: float\n    \"\"\"\n    dew_point_k = 273.15 + dew_point_c\n    t_air_k = 273.15 + t_air_c\n    frost_point_k = dew_point_k - t_air_k + 2671.02 / \\\n        ((2954.61 / t_air_k) + 2.193665 * math.log(t_air_k) - 13.3448)\n    return round(frost_point_k - 273.15, 1)",
        "sha1": "0e46dce3d3b41b7c870e8cfb6410b9c9ea202c07",
        "id": 136493
    },
    {
        "content": "def _tests_in_suite(suite_name, tests):\n    \"\"\"Check if the suite includes tests.\n\n    :param suite_name: Name of the suite to be checked.\n    :param tests: Set of tests\n    :type suite_name: str\n    :type tests: pandas.Series\n    :returns: True if the suite includes tests.\n    :rtype: bool\n    \"\"\"\n\n    for key in tests.keys():\n        if suite_name == tests[key][u\"parent\"]:\n            return True\n    return False",
        "sha1": "53d3e1a1cc3e12d698f8a2a5eb4e246ce4e53ea9",
        "id": 669417
    },
    {
        "content": "def track_length(tracks):\n    \"\"\"\n    Calculate the number of points in each of a set of trajectories.\n\n    args\n    ----\n        tracks      :   pandas.DataFrame\n\n    returns\n    -------\n        same dataframe with \"track_length\" column \n\n    \"\"\"\n    if \"track_length\" in tracks.columns:\n        tracks = tracks.drop(\"track_length\", axis=1)\n    tracks = tracks.join(\n        tracks.groupby(\"trajectory\").size().rename(\"track_length\"),\n        on=\"trajectory\"\n    )\n    return tracks",
        "sha1": "ff39b24e50371288146f8e5f40b927387aa8821f",
        "id": 445384
    },
    {
        "content": "def flatten_object(obj):\n\t\"\"\"flatten an object into paths for easier enumeration. \n\t\tArgs: an object\n\t\tReturns:\n\t\t\ta flat object with a list of pairs \n\t\t\t[(p_1, v_1), ..., (p_k, v_k)], (paths and values)\n\t\tExample:\n\t\t\t{x: 1, y : {z: [a, b], w: c}} will be flatten into\n\t\t\t[(\"/x\", 1), (\"/y/z/0\", a), (\"/y/z/1\", b), (\"/y/w\", c)]\n\t\"\"\"\n\tpaths = []\n\n\tif isinstance(obj, (dict,)):\n\t\tfor f in obj:\n\t\t\tsub_paths = flatten_object(obj[f])\n\t\t\tfor p in sub_paths:\n\t\t\t\tpaths.append((\"/{}{}\".format(f, p[0]), p[1]))\n\telif isinstance(obj, (list,)):\n\t\tfor i, x in enumerate(obj):\n\t\t\tsub_paths = flatten_object(x)\n\t\t\tfor p in sub_paths:\n\t\t\t\tpaths.append((\"/{}{}\".format(i, p[0]), p[1]))\n\telse:\n\t\tpaths = [(\"\", obj)]\n\n\treturn paths",
        "sha1": "8ff5777ef4d3b8a7a5883b557791a58b82d50c6f",
        "id": 420347
    },
    {
        "content": "from typing import Union\nimport json\n\n\ndef get_object(path: str) -> Union[list, dict]:\n    \"\"\"\n    This function takes a path to a .json file and returns a json object.\n    \"\"\"\n    file = open(path, 'r', encoding='utf-8')\n    data = json.load(file)\n    return data",
        "sha1": "ae6d84b38ae551e688860078a773598829662b25",
        "id": 123294
    },
    {
        "content": "def get_value_from_dict(key_path, input_dict):\n    \"\"\"\n    Returns the value of a key in input_dict\n    key_path must be given in string format with dots\n    Example: result.dir\n    \"\"\"\n    if not isinstance(key_path, str) or not isinstance(input_dict, dict):\n        return None\n    for key in key_path.split('.'):\n        input_dict = input_dict.get(key)\n        if not input_dict:\n            return None\n    return input_dict",
        "sha1": "c76ed21bcf4aa8a9884d6660a235ff92ebeae1ac",
        "id": 72076
    },
    {
        "content": "def jsModuleDeclaration(name):\n    \"\"\"\n    Generate Javascript for a module declaration.\n    \"\"\"\n    var = ''\n    if '.' not in name:\n        var = 'var '\n    return '%s%s = {\"__name__\": \"%s\"};' % (var, name, name)",
        "sha1": "e53119d96fe87edbc7e52766b46c9ed2a33fa4ae",
        "id": 391337
    },
    {
        "content": "import torch\n\n\ndef _num_elements(losses):\n    \"\"\"Computes the number of elements in `losses` tensor.\"\"\"\n    return torch.tensor(losses.size()[0]).type(losses.dtype)",
        "sha1": "10f22419120d62d9556cc38de99b0ea0ebf37a8e",
        "id": 131736
    },
    {
        "content": "def parse_comma_sep_list(csl):\n    \"\"\"Parse comma separated list of integers.\"\"\"\n    return [int(x) for x in csl if x != \"\"]",
        "sha1": "df4396fd46749b6bb7cec03c1cc98062454f05a1",
        "id": 456816
    },
    {
        "content": "import re\nimport fnmatch\n\n\ndef multiglob_compile(globs, prefix=False):\n    \"\"\"Generate a single \"A or B or C\" regex from a list of shell globs.\n\n    :param globs: Patterns to be processed by :mod:`fnmatch`.\n    :type globs: iterable of :class:`~__builtins__.str`\n\n    :param prefix: If ``True``, then :meth:`~re.RegexObject.match` will\n        perform prefix matching rather than exact string matching.\n    :type prefix: :class:`~__builtins__.bool`\n\n    :rtype: :class:`re.RegexObject`\n    \"\"\"\n    if not globs:\n        # An empty globs list should only match empty strings\n        return re.compile('^$')\n    elif prefix:\n        globs = [x + '*' for x in globs]\n    return re.compile('|'.join(fnmatch.translate(x) for x in globs))",
        "sha1": "276ca5039c9fea621c27cee1713c7e263c7adbf1",
        "id": 165383
    },
    {
        "content": "def create_document_metadata(file_name, last_updated, last_updated_ts, file_path, file_dir):\n    \"\"\"Create a dict of metadata for each file; \n    a list of these dict are stored in updated_docs_metadata\"\"\"\n    f = {\n        \"last_updated\": last_updated,\n        \"last_updated_ts\": last_updated_ts,\n        \"file_name\": file_name,\n        \"file_path\": file_path,\n        \"file_dir\": file_dir\n\n    }\n    return f",
        "sha1": "cfead48c93b5a518d47f95a6f487da977cc2cc72",
        "id": 569602
    },
    {
        "content": "def upsample(arr, mode):\n    \"\"\"Upsample an 2D array.\n\n    Arguments:\n        arr {2d numpy array} -- The target array.\n        mode {1 or 2 or 4} -- Upsample ratio (4:mode).\n\n    Returns:\n        2d numpy array -- Upsampled array.\n    \"\"\"\n    if mode not in {1, 2, 4}:\n        raise ValueError(f'Mode ({mode}) must be 1, 2 or 4.')\n\n    if mode == 4:\n        return arr\n    return arr.repeat(3 - mode, axis=0).repeat(2, axis=1)",
        "sha1": "b71a9a73ba640f6628493a2945a3a67829216fae",
        "id": 328867
    },
    {
        "content": "import pickle\n\n\ndef load_pickle(filepath_to_load):\n    \"\"\"Load pickle file\"\"\"\n    with open(filepath_to_load, \"rb\") as inp:\n        return pickle.load(inp)",
        "sha1": "3635b9d58919f9a43bd53bb0145e297baa831ed1",
        "id": 677506
    },
    {
        "content": "import requests\n\n\ndef get_number_of_asa_service_groups(asa):\n    \"\"\"\n    Returns the number of service object-groups the given ASA object has configured.\n    :param asa: ASA device to count network object-groups.\n    :return: Integer which describes how many object-groups is found on ASA.\n    \"\"\"\n    url = asa.url() + \"/api/objects/networkservicegroups?limit=1\"\n\n    headers = {\n        'Content-Type': 'application/json',\n        'User-agent': 'REST API Agent',\n        'X-Auth-Token': asa.token\n    }\n\n    response = requests.request(\"GET\", url, headers=headers, verify=False).json()\n\n    return response['rangeInfo']['total']",
        "sha1": "a175ea823dfad4bf791da8ad3198ee9c002f0134",
        "id": 653754
    },
    {
        "content": "import re\n\n\ndef text_is_all_uppercase(text: str) -> bool:\n    \"\"\"Return True if every character is uppercase.\n    Excludes punctuation, spaces, and digits.\n    \"\"\"\n    return all([char.isupper() for char in re.sub(r\"[^A-Za-z]\", \"\", text)])",
        "sha1": "21f4b01f65820df1628ba993b6c9bd62f953e4c2",
        "id": 589940
    },
    {
        "content": "import random\n\n\ndef gen_null(entry, threshold=0.8):\n    \"\"\"Randomly return null or the input as is\"\"\"\n    if random.random() > threshold:\n        return ''\n    return entry",
        "sha1": "89c8a2bd0ac896b77ede114945424621ef8312b1",
        "id": 168578
    },
    {
        "content": "def is_hyperopt(x):\n    \"\"\"Check whether a given object is a hyperopt argument\n\n    The format expected is ('hp_NAME_OF_FUNCTION', 'name for hyperopt', remaining, arguments)\n\n    \"\"\"\n\n    if isinstance(x, (tuple, list)):\n        if isinstance(x[0], str):\n            s = x[0].split(\"_\", 1)\n            if s[0] == \"hp\":\n                return True\n\n    return False",
        "sha1": "9d65a04f9ccc694063d44aea8b5b48f5ec272cb7",
        "id": 599497
    },
    {
        "content": "def merge_dicts(*dicts):\n    \"\"\"\n    Merges an arbitrary number of dicts.\n\n    Note:\n        Updates left to right, so will override existing\n        attributes!\n    \"\"\"\n\n    merged = {}\n    for dictionary in dicts:\n        merged.update(dictionary)\n    return merged",
        "sha1": "cab964fbb2d068c0e0e4ce0ae7a7b22682937b2f",
        "id": 224068
    },
    {
        "content": "def linereader_int(file):\n    \"\"\"Read input file and return a tuple with content converted to ints.\"\"\"\n    file = open(file, \"r\")\n    return tuple(map(int, [line.rstrip(\"\\n\") for line in file.readlines()]))",
        "sha1": "7c9cd7b9b1a568aebc5740d8da36499a98ae812c",
        "id": 616173
    },
    {
        "content": "def _convert_float32_to_float64(data):\n    \"\"\"\n        Converts DataArray values of float32 to float64\n        :param data: Xarray dataset of coverage data\n\n        :returns: Xarray dataset of coverage data\n        \"\"\"\n\n    for var_name in data.variables:\n        if data[var_name].dtype == 'float32':\n            og_attrs = data[var_name].attrs\n            data[var_name] = data[var_name].astype('float64')\n            data[var_name].attrs = og_attrs\n\n    return data",
        "sha1": "1a66c6de0de7ff2c79d7e03c017b79b78cb43639",
        "id": 20256
    },
    {
        "content": "def getNorm(name, p=None):\n    \"\"\"Get an instance of a fuzzy norm with given name.\n    Normally looks into the fuzzy.norm package for a suitable class.\n    \"\"\"\n    m = __import__(\"fuzzy.norm.\"+name, fromlist=[name])\n    c = m.__dict__[name]\n    if p is None:\n        return c()\n    else:\n        return c(p)",
        "sha1": "eee640d29792f950ac959676241be8f7bf134bd0",
        "id": 605708
    },
    {
        "content": "from itertools import islice\n\n\ndef head(f,n):\n    \"\"\"Returns either the first n of lines of f or f if fewer lines\n    \"\"\"\n    with open(f) as ff:\n        head=list(islice(ff,n))\n    return head",
        "sha1": "6fd94ff423a860c24eeedccadfb421bb5bbb1c0a",
        "id": 476704
    },
    {
        "content": "def parse_maddr_str(maddr_str):\n    \"\"\"\n    The following line parses a row like:\n      {/ip6/::/tcp/37374,/ip4/151.252.13.181/tcp/37374}\n    into\n      ['/ip6/::/tcp/37374', '/ip4/151.252.13.181/tcp/37374']\n    \"\"\"\n    return maddr_str.replace(\"{\", \"\").replace(\"}\", \"\").split(\",\")",
        "sha1": "1a1ca1d846c650a3c01dca04a3debf921186d1a7",
        "id": 79392
    },
    {
        "content": "from datetime import datetime\n\n\ndef parse_instantaneous_date(date, time):\n    \"\"\"Parse a date string of the form 'yyyy-mm-dd' and a time string of the\n    form 'mm:ss\\tEDT' and return a datetime object.\n\n    :param: date\n    :type date: string\n    :param: time\n    :type date: string\n    :rtype: datetime.datetime\n\n    >>> instantaneous_date = parse_instantaneous_date(date = \"2015-08-01\", time = \"00:15\\tEDT\")\n    >>> instantaneous_date\n    datetime.datetime(2015, 8, 1, 0, 15)\n    \"\"\"\n    time = time.split(\"\\t\")[0]\n    date_str = \" \".join((date, time))\n\n    return datetime.strptime(date_str, \"%Y-%m-%d %H:%M\")",
        "sha1": "9fdfda8c7c78b79a9c79b456b5637ab598f6dcba",
        "id": 168616
    },
    {
        "content": "from typing import List\nfrom typing import Dict\n\n\ndef format_meas_map(meas_map: List[List[int]]) -> Dict[int, List[int]]:\n    \"\"\"\n    Return a mapping from qubit label to measurement group given the nested list meas_map returned\n    by a backend configuration. (Qubits can not always be measured independently.) Sorts the\n    measurement group for consistency.\n\n    Args:\n        meas_map: Groups of qubits that get measured together, for example: [[0, 1], [2, 3, 4]]\n    Returns:\n        Measure map in map format\n    \"\"\"\n    qubit_mapping = {}\n    for sublist in meas_map:\n        sublist.sort()\n        for q in sublist:\n            qubit_mapping[q] = sublist\n    return qubit_mapping",
        "sha1": "3d28b763622563269eec2fe76ca5f163053db9ed",
        "id": 404929
    },
    {
        "content": "import requests\n\n\ndef get_file_size(url, params, timeout=10):\n  \"\"\"Get file size from a given URL in bytes.\n\n  Args:\n    url: str.\n      URL string.\n    timeout: int, optional.\n      Timeout in seconds.\n\n  Returns:\n    int. File size in bytes.\n\n  #### Examples\n\n  ```python\n  get_file_size(url)\n  ## 178904\n  ```\n  \"\"\"\n  try:\n    response = requests.get(url, params={}, stream=True)\n  except (requests.exceptions.HTTPError) as e:\n    print(e)\n    return 0\n  try:\n    file_size = int(response.headers[\"Content-Length\"])\n  except (IndexError, KeyError, TypeError):\n    return 0\n\n  return file_size",
        "sha1": "642e733a1385423f0f8a894e8e3bfb96ea15dda4",
        "id": 90630
    },
    {
        "content": "def _find_value(dictionary, key, default=None):\n  \"\"\"\n  Helper to find a value in a dictionary\n  \"\"\"\n  if key not in dictionary:\n    return default\n\n  return dictionary[key]",
        "sha1": "3ec10f2ab7acaabb9b7d08ac62d1abda38df8605",
        "id": 479498
    },
    {
        "content": "def _mangle_name(internal_name, class_name):\n    \"\"\"Transform *name* (which is assumed to be an \"__internal\" name)\n    into a \"_ClassName__internal\" name.\n\n    :param str internal_name: the assumed-to-be-\"__internal\" member name\n    :param str class_name: the name of the class where *name* is defined\n    :return: the transformed \"_ClassName__internal\" name\n    :rtype: str\n\n    \"\"\"\n    return \"_%s%s\" % (class_name.lstrip('_'), internal_name)",
        "sha1": "6a97a729437e08f510f2eefc8210c8063d1648a5",
        "id": 398238
    },
    {
        "content": "import warnings\n\n\ndef create_gobnilp_settings(score_type, palim=None, alpha=None):\n    \"\"\"\n    Creates a string of Gobnilp settings given the allowed arities and parent size.\n\n    Parameters\n    ----------\n    score_type : int\n        The scoring function used to learn the network\n        0 for BDeu or 2 for BIC\n    palim: int\n        The maximum number of parents a node can have.\n    alpha : float\n        The Equivalent Sample Size of the BDeu Score.\n    Returns\n    -------\n    gobnilp_settings : str\n        A string describing the provided constraints.\n    \"\"\"\n\n    if palim is None:\n        palim = 3\n        warnings.warn('Maximum number of parents (palim) not defined. Defaulting to palim=3.')\n    if score_type=='BDeu' and alpha is None:\n        alpha = 1.0\n        warnings.warn('ESS (alpha) not defined. Defaulting to alpha=1.0.')\n\n    output_dot_location = 'sol.mat'\n\n    gobnilp_settings = ''\n    gobnilp_settings += 'gobnilp/scoring/palim = {}\\n'.format(palim)\n    gobnilp_settings += 'gobnilp/scoring/initpalim = {}\\n'.format(palim)\n    gobnilp_settings += 'gobnilp/delimiter = \" \"\\n'\n    gobnilp_settings += 'gobnilp/mergedelimiters = TRUE\\n'\n    gobnilp_settings += 'gobnilp/outputfile/solution = \"golb.bn\"\\n'\n    gobnilp_settings += 'gobnilp/outputfile/adjacencymatrix = \"sol.mat\"\\n'\n    gobnilp_settings += 'gobnilp/scoring/alpha = {}\\n'.format(format(alpha,'f'))\n    gobnilp_settings += 'limits/time = 3600\\n'\n    if score_type in [\"BIC\", \"BDeu\"]:\n        gobnilp_settings += 'gobnilp/scoring/score_type = \"{}\"\\n'.format(score_type)\n\n    return gobnilp_settings",
        "sha1": "c78cdb0c4d5351adeca894c8869a4429f3c34578",
        "id": 673222
    },
    {
        "content": "def restore_scale(expr, mean, std):\n    \"\"\"\n    Makes each gene j have mean_j and std_j\n    :param expr: matrix of gene expressions. Shape=(nb_samples, nb_genes)\n    :param mean: vector of gene means. Shape=(nb_genes,)\n    :param std: vector of gene stds. Shape=(nb_genes,)\n    :return: Rescaled gene expressions\n    \"\"\"\n    return expr * std + mean",
        "sha1": "58175da2d8b3543d1eac6e70f04d91f2cb10f542",
        "id": 505305
    },
    {
        "content": "def runge_kutta_fourth_xy(rhs, h, x, y):\n    \"\"\"\n    Solves one step using a fourth-order Runge-Kutta method. RHS expects both x and y variables.\n\n    Moin, P. 2010. Fundamentals of Engineering Numerical Analysis. 2nd ed.\n    Cambridge University Press. New York, New York.\n\n    :param rhs: \"Right-hand Side\" of the equation(s). Everything but the derivative. (e.g dy/dx = f(x, y))\n    :param h: step size\n    :param x: step dimension\n    :param y: output dimension\n    :return:\n    \"\"\"\n\n    k_1 = rhs(x, y)\n    k_2 = rhs(x + h / 2.0, y + k_1 / 2.0)\n    k_3 = rhs(x + h / 2.0, y + k_2 / 2.0)\n    k_4 = rhs(x + h, y + k_3)\n\n    return y + (k_1 + 2 * (k_2 + k_3) + k_4) / 6.0 * h",
        "sha1": "22b8d042376501b6910ddb1511c61ed7d2282896",
        "id": 689045
    },
    {
        "content": "def remove_whitespace_tokens(tokens):\n    \"\"\"Remove any whitespace tokens from a list of tokens.\"\"\"\n    return [token for token in tokens if token.type != 'space']",
        "sha1": "db98386cbcc24193d4bfe9dce608dcd4aa6da1eb",
        "id": 296761
    },
    {
        "content": "def _load_type(type_name):\n    \"\"\" _load_type('exceptions.KeyError') -> KeyError \"\"\"\n    module_name, name = type_name.rsplit('.', 1)\n    mod = __import__(module_name, fromlist = [str(name)])\n    return getattr(mod, name)",
        "sha1": "0e0e4684566da1b4b2f6550cb520591efd5a9901",
        "id": 294999
    },
    {
        "content": "def two_lists_get_intersect(l1,l2):\n    \"\"\"\n    Given two lists, return list with common elements.\n\n    >>> l1 = [1,5,10,20,30]\n    >>> l2 = [5,20,40]\n    >>> two_lists_get_intersect(l1, l2)\n    [5, 20]\n    >>> l3 = [50]\n    >>> two_lists_get_intersect(l1, l3)\n    []\n\n    \"\"\"\n\n    assert l1, \"given l1 empty\"\n    assert l2, \"given l2 empty\"\n    l3 = [element for element in l1 if element in l2]\n    return l3",
        "sha1": "fa8ada89ace351bc126562b4d608e8741056bd36",
        "id": 167397
    },
    {
        "content": "def task_15_list_customers_with_any_order_or_not(cur):\n    \"\"\"\n    List all customers, whether they placed any order or not.\n\n    Args:\n        cur: psycopg cursor\n\n    Returns: 213 records\n    \"\"\"\n    cur.execute(\"\"\"SELECT customername, contactname, country, orderid\n                   FROM customers\n                   FULL JOIN orders\n                   ON customers.customerid=orders.customerid\"\"\")\n    return cur.fetchall()",
        "sha1": "c3c8c66d4c244fb8fdc0c09bb67560e07c143546",
        "id": 576912
    },
    {
        "content": "from pathlib import Path\n\n\ndef create_toml_data_directory(run_name: str) -> Path:\n    \"\"\"\n    create the directory that we will store our timestamped toml data in\n    Parameters\n    ----------\n    run_name: str\n        The run name\n\n    Returns\n    -------\n    Path\n        The path to the directory that we are writing the data into\n    \"\"\"\n    directory_path = Path(run_name).resolve()\n    if not directory_path.exists():\n        directory_path.mkdir(exist_ok=True, parents=True)\n    return directory_path",
        "sha1": "ecefc7aaebb93e8142a10f3eddb2b0b04d7033e5",
        "id": 466964
    },
    {
        "content": "from typing import List\n\n\ndef _k_hot_to_sparse(k_hot: List[int]) -> List[int]:\n  \"\"\"Converts k-hot embedding to sparse representation.\"\"\"\n  return [idx for idx, val in enumerate(k_hot) if val != 0]",
        "sha1": "3dc156a6e610c51ab0288a31e67b1cac38448b58",
        "id": 180962
    },
    {
        "content": "def get_dataset_filename(ds_dict):\n    \"\"\"Figure out the downloaded filename for a dataset entry\n\n    if a `file_name` key is present, use this,\n    otherwise, use the last component of the `url`\n\n    Returns the filename\n\n    Examples\n    --------\n    >>> ds_dict = {'url': 'http://example.com/path/to/file.txt'}\n    >>> get_dataset_filename(ds_dict)\n    'file.txt'\n    >>> ds_dict['file_name'] = 'new_filename.blob'\n    >>> get_dataset_filename(ds_dict)\n    'new_filename.blob'\n    \"\"\"\n\n    file_name = ds_dict.get('file_name', None)\n    url = ds_dict.get('url', [])\n    if file_name is None:\n        file_name = url.split(\"/\")[-1]\n    return file_name",
        "sha1": "4fc91641d80323266f924423cae6a1d06f64584e",
        "id": 79472
    },
    {
        "content": "def convert_to_obj(vert, tri):\n    \"\"\"\n    Convert verts and faces to an OBJ file.\n\n    Arguments:\n        vert (int[]): List of verts\n        tri (int[]): List of faces\n    \"\"\"\n    res = \"\"\n    for vert in vert:\n        res += (\"v {} {} {}\\n\".format(vert[0], vert[1], vert[2]))\n    for face in tri:\n        res += (\"f {} {} {}\\n\".format(*(face + 1)))\n    return res",
        "sha1": "a3a99c59d97cfac6e91595e304ca331bbb6cede9",
        "id": 418658
    },
    {
        "content": "import json\n\n\ndef mock_hue_put_response(request, context):\n    \"\"\"Callback for mocking a Philips Hue API response using the requests-mock library,\n    specifically for a put request.\n\n    This mock response assumes that the system has a single light with light_id of '1',\n    and the expected request is to set the 'on' state as well as 'hue' state.\n\n    See https://requests-mock.readthedocs.io/en/latest/response.html for usage details.\n\n    Args:\n        request: The requests.Request object that was provided. The request method is\n        assumed to be a put request.\n        context: An object containing the collected known data about this response\n        (headers, status_code, reason, cookies).\n\n    Returns:\n        The response text with confirmation of the arguments passed in.\n    \"\"\"\n    expected_path = '/lights/1/state'\n    if expected_path not in request.url:\n        context.status_code = 400\n        return 'invalid Philips Hue url'\n\n    try:\n        body_dict = json.loads(request.body)\n    except json.JSONDecodeError:\n        context.status_code = 400\n        return 'put request body should be a JSON-encoded string'\n\n    try:\n        on = body_dict['on']\n        hue = body_dict['hue']\n    except KeyError:\n        context.status_code = 400\n        return 'missing keys in put request body'\n\n\n    context.status_code = 200\n\n    response = []\n    if on:\n        response.append({'success':{'/lights/1/state/on': 'true'}})\n    else:\n        response.append({'success':{'/lights/1/state/on': 'false'}})\n\n    response.append({'success':{'/lights/1/state/hue': f'{hue}'}})\n    return str(response)",
        "sha1": "cc54a09d9595ac6e0868f7143bd663d2da78c3e4",
        "id": 675349
    },
    {
        "content": "def get_args(params):\n    \"\"\"\n    Converting a list of arguments to a string, separated by commas\n    \"\"\"\n    return ', '.join(map(lambda x: \"'\" + x + \"'\", params))",
        "sha1": "f402ed1a1bd2b24a4949620f2695ae4319aa3c05",
        "id": 448034
    },
    {
        "content": "def parse_dimensions(root):\n    \"\"\"Returns a dictionary of dimensionalities.\"\"\"\n\n    dims = {}\n    for node in root.find(\"{*}unitDimensionSet\"):\n\n        dimension = node.find('{*}dimension').text\n        dims[dimension] = dict(\n            name = node.find('{*}name').text,\n            baseForConversion = node.find('{*}baseForConversion').text,\n            canonicalUnit = node.find('{*}canonicalUnit').text,\n        )\n    return dims",
        "sha1": "17c0cafd7341b9761e58aec4e6f6294f57d2d9a9",
        "id": 216205
    },
    {
        "content": "import hashlib\n\n\ndef dt_encode_topic(topic: str) -> str:\n    \"\"\"\n    Encode any string to be accepted by Digital Twin setSource. Use byte encoding and sha256-hashing.\n\n    :param topic: Topic name to be encoded.\n\n    :return: Hashed-encoded topic name\n\n    \"\"\"\n\n    return f\"0x{hashlib.sha256(topic.encode('utf-8')).hexdigest()}\"",
        "sha1": "8813d152dbdcb24e6ab0319ddee54d7591de4197",
        "id": 619454
    },
    {
        "content": "import math\n\n\ndef fitness(expected, output):\n    \"\"\"Calculates the similarity score between expected and output.\"\"\"\n    s = 0\n    for i in range(4):\n        s += (expected[i] - output[i])**2\n    return 1/(1 + math.sqrt(s))",
        "sha1": "e2b100c5617d44a5242c7ec2bd393e3404c8c614",
        "id": 558639
    },
    {
        "content": "def jinja_hasattr(obj, string):\n    \"\"\"Template filter checking if the provided object at the provided\n    string as attribute\n    \"\"\"\n    return hasattr(obj, string)",
        "sha1": "bd97caa7caf3e1e00ec7dc60a06ff0eb9f5d1f59",
        "id": 154287
    },
    {
        "content": "def binary_to_decimal(number):\n    \"\"\"\n        Calculates the decimal of the given binary number.\n\n        :param number: decimal number in string or integer format\n        :return integer of the equivalent decimal number\n    \"\"\"\n    decimal = []\n    number = list(str(number)[::-1])\n    for i in range(len(number)):\n        decimal.append(int(number[i]) * (2 ** i))\n\n    return sum(decimal)",
        "sha1": "6d615b9bc5a50cc9d2a970fa77c989fa95d0d77e",
        "id": 11608
    },
    {
        "content": "def discard_inserted_documents(error_documents, original_documents):\n    \"\"\"Discard any documents that have already been inserted which are violating index constraints\n       such documents will have an error code of 11000 for a DuplicateKey error\n       from https://github.com/mongodb/mongo/blob/master/src/mongo/base/error_codes.yml#L467\n\n       Parameters:\n           error_documents (List[Dict]): list of documents that failed to insert in original transaction\n           original_documents (List[Dict]): list of documents from original transaction that failed\n           error_code (Int): error status code to filter on\n\n       Returns:\n           List[Dict]: list of documents with matching error code entries removed\n    \"\"\"\n    # doc['op'] returns the actual document from the previous transaction\n    errors = list(doc['op'] for doc in error_documents if doc['code'] == 11000)\n    return list(doc for doc in original_documents if doc not in errors)",
        "sha1": "9d7d47a0ade2300449a7f1a4a20c3a70f6dce583",
        "id": 703051
    },
    {
        "content": "def _find_pattern(pattern, buf, iterator, start=0):\n    \"\"\"Find pattern in buf, appending new data from iterator to buf if\n    necessary\n    \"\"\"\n\n    while len(buf) <= start + len(pattern):\n        buf.extend(next(iterator))\n    while True:\n        pos = buf.find(pattern, start)\n        if pos >= 0:\n            assert pos >= start\n            return pos\n        else:\n            start = len(buf) - len(pattern) - 1\n            buf.extend(next(iterator))",
        "sha1": "3ed7b6e8c3f6f298f51dd799952ae6621e371487",
        "id": 462150
    },
    {
        "content": "def is_bitcode_file(path):\n  \"\"\"\n  Returns True if path contains a LLVM bitcode file, False if not.\n  \"\"\"\n  with open(path, 'rb') as f:\n    return f.read(4) == b'BC\\xc0\\xde'",
        "sha1": "acfd17eee949f42994b2bc76499ee58c710eb388",
        "id": 9898
    },
    {
        "content": "def isGlideinHeldNTimes(jobInfo, factoryConfig=None, n=20):\n    \"\"\"This function looks at the glidein job's information and returns if the\n    CondorG job is held for more than N(defaults to 20) iterations\n\n    This is useful to remove Unrecoverable glidein (CondorG job) with forcex option.\n\n    Args:\n        jobInfo (dict): Dictionary containing glidein job's classad information\n\n    Returns:\n        bool: True if job is held more than N(defaults to 20) iterations, False if otherwise.\n    \"\"\"\n    if factoryConfig is None:\n        factoryConfig = globals()[\"factoryConfig\"]\n\n    greater_than_n_iterations = False\n    nsysholds = jobInfo.get(\"NumSystemHolds\")\n    if nsysholds > n:\n        greater_than_n_iterations = True\n\n    return greater_than_n_iterations",
        "sha1": "e608ada46f1571b9e96531bc3cca3692f940846a",
        "id": 694035
    },
    {
        "content": "def read_file(filepath):\n\t\"\"\"\n\tfor a given file, change it into list of words and numbers, will call this in build_trie\n\n\tParameters\n\t----------\n\tfilepath: Name of the file we will use\n\n\tReturn\n\t------\n\ta list of words and a list of corresponding weights\n\n\n\t\"\"\"\n\tfile = open(filepath, 'r',encoding = \"utf-8\")\n\tdata = file.readlines()\n\tdata_list = []\n\tfor i in range(len(data)):\n\t\tif i != 0:\n\t\t\tdata_list.append(data[i])\n\tnum_list = []\n\tword_list = []\n\tfor l in data_list:\n\t\tif l != '\\n':\n\t\t\tentry = l.split('\\t')\n\t\t\tnum_list.append(int(entry[0]))\n\t\t\tword_list.append(entry[1][:-1])\n\treturn num_list,word_list",
        "sha1": "7e9a54bfdd54f1382221f2dc179e2a76c68fdc82",
        "id": 548085
    },
    {
        "content": "import ipaddress\n\n\ndef cidr_stix_pattern_producer(data):\n    \"\"\"Convert a CIDR from TC to a STIX pattern.\"\"\"\n    if isinstance(ipaddress.ip_network(data.get('summary'), strict=False), ipaddress.IPv6Network):\n        return f\"[ipv6-addr:value = '{data.get('summary')}']\"\n\n    return f\"[ipv4-addr:value = '{data.get('summary')}']\"",
        "sha1": "56f7d94fef6d913d6c2bd3afa48d747dc5a4c549",
        "id": 110115
    },
    {
        "content": "def _lower(key):\n    \"\"\"Transforms a string to lowercase, leaves other types alone.\"\"\"\n    keyfn = getattr(key, 'lower', None)\n    return keyfn() if keyfn else key",
        "sha1": "cf5f9c88c46c4feb5d9daf27c34927e8fa076092",
        "id": 400327
    },
    {
        "content": "def arithmetic_sum_n(a_1, d, n):\n    \"\"\"Calculate the sum of an arithmetic series.\n\n    Parameters:\n        a_1     The first element of the series\n        d       The difference between elements\n        n       The number of elements in the series to sum\n\n    Return:\n        The sum of n numbers in the arithmetic series.\n    \"\"\"\n    a_n = a_1 + d*(n - 1)\n    return n*(a_1 + a_n) // 2",
        "sha1": "3a391886a0ecab60b35632c1bf429131d3f08373",
        "id": 626827
    },
    {
        "content": "def limit(x, y, d, nx, ny):\n  \"\"\" limit x,y values to edge of canvas. \"\"\"\n  if x < 0:\n    x, d = 0, 0\n  if x > nx - 1:\n    x, d = nx - 1, 2\n  if y < 0:\n    y, d = 0, 3\n  if y > ny - 1:\n    y, d = ny - 1, 1\n  return x, y, d",
        "sha1": "862f31c0e7d30553b04d7658d5f6a7187434dbde",
        "id": 129090
    },
    {
        "content": "def indexget(obj, i, default=None):\n  \"\"\"\n  Return obj[i] if possible, else default.\n\n  Similar to dict.get() but for integer indexed objects.\n  \"\"\"\n  try:\n    return obj[i]\n  except (IndexError, TypeError, KeyError):\n    return default",
        "sha1": "2ab7b79854cb1e6008b5f8f09f06d9d59a7d5e84",
        "id": 236636
    },
    {
        "content": "def clean_str(\n    s: str,\n    l: list,\n    r: list,\n    ) -> str:\n    \"\"\"Replace substrings within a given string\n\n    Parameters\n    ----------\n    s : str\n        The string\n    l : list\n        The list of substrings\n    r : list\n        The list of replacement substrings\n\n    Returns\n    -------\n    str\n        The string with the substrings removed\n    \"\"\"    \n\n    #   Loop through every substring in the list\n    for i in range(0, len(l)):\n\n        #   Remove all occurrences of the substring\n        s = s.replace(l[i], r[i])\n\n    return s",
        "sha1": "1011679d3ad1f600d5bb5b3b6e937001aa59fcc4",
        "id": 608347
    },
    {
        "content": "def split_time(duration: float) -> tuple[int, int, int]:\n    \"\"\"Deremine how many hours, minutes, and seconds are in a number\n    of seconds.\n\n    :param duration: The number of seconds.\n    :return: A :class:tuple object.\n    :rtype: tuple\n    \"\"\"\n    s = duration % 60\n    duration -= s\n    m = duration % 3600\n    duration -= m\n    h = int(duration / 3600)\n    m = int(m / 60)\n    s = int(s)\n    return h, m, s",
        "sha1": "e85298e0899e9a5819f6afbb89cb97afdb51d979",
        "id": 255683
    },
    {
        "content": "def subtree_induced_by_subset(tree, s):\n    \"\"\" Returns the subtree of tree induced by the nodes containing the set s.\n\n    Args:\n       tree (NetworkX graph): A junction tree.\n       s (set): Subset of the node in the underlying graph of T.\n\n    Example:\n        >>> t = jtlib.sample(5)  \n        >>> t.nodes\n        NodeView((frozenset([0, 4]), frozenset([3]), frozenset([1, 2, 4])))\n        >>> t.edges\n        EdgeView([(frozenset([0, 4]), frozenset([1, 2, 4])), (frozenset([3]), frozenset([1, 2, 4]))])\n        >>> subt = jtlib.subtree_induced_by_subset(t, frozenset([1]))\n        >>> subt.nodes\n        NodeView((frozenset([1, 2, 4]),))\n        >>> t.edges\n        EdgeView([(frozenset([0, 4]), frozenset([1, 2, 4])), (frozenset([3]), frozenset([1, 2, 4]))])\n    \"\"\"\n    if len(s) == 0:\n        return tree.copy()\n    v_prime = {c for c in tree.nodes() if s <= c}\n    return tree.subgraph(v_prime).copy()",
        "sha1": "51794d12299c832d69262258a1d4f69896fa6e86",
        "id": 100200
    },
    {
        "content": "def is_list(obj: object):\n    \"\"\"Returns true if object is a list. Principle use is to determine if a\n    field has undergone validation: unvalidated field.errors is a tuple,\n    validated field.errors is a list.\"\"\"\n    return isinstance(obj, list)",
        "sha1": "e340cd28b2bc027ac528efc91c02c3150f3f546f",
        "id": 222834
    },
    {
        "content": "def get_words_label(words_data: list) -> list:\n    \"\"\"\n    \u5f97\u5230\u5f53\u524d\u6570\u636e\u96c6\u4e0b\u7684\u8bcd\u6c47\u8868\n    :param words_data: \u8bfb\u53d6\u5230\u7684\u8bcd\u8bed\u6570\u636e\n    :return: \u8bcd\u6c47\u8868\n    \"\"\"\n    # \u4f7f\u7528 set \u53bb\u91cd\n    words_label = set({})\n    for words in words_data:\n        words_label.update(words[1])\n    res = list(words_label)\n    res.sort()\n    return res",
        "sha1": "d9ce0701c3c1baff1067d5c96a7730dc42b027f9",
        "id": 8416
    },
    {
        "content": "def sum_digits(s):\n    \"\"\"\n    assumes s a string\n    Returns an int that is the sum of all of the digits in s.\n    If there are no digits in s it raises a ValueError exception.\n    \"\"\"\n    sum_of_digits = 0\n    count = 0\n    for item in s:\n        if item in list('0123456789'):\n            sum_of_digits += int(item)\n            count += 1\n    if not count:\n        raise ValueError\n    return sum_of_digits",
        "sha1": "40fb5868eccf3d6840177273ce2084da5238d3cd",
        "id": 493503
    },
    {
        "content": "def convert_matrix_to_text_2d(matrix, axis=0):\n    \"\"\" Converts a text Matrix back to String\n\n    Given a text matrix, convert it back into a single string.\n\n    :param numpy.matrix matrix: Matrix of text to convert back to string\n    :param int axis: [Optional] Which axis to limit (0 for Column, 1 for Rows;\n                     defaults to 0)\n\n    :return: String converted back from matrix\n    :rtype: str\n\n    \"\"\"\n    ret = ''\n    for i in range(matrix.shape[axis]):\n        if axis == 0:\n            ret += ''.join(matrix[i, :].tolist()[0])\n        else:\n            ret += ''.join(matrix[:, i].transpose().tolist()[0])\n\n    return ret",
        "sha1": "38655ae8fe286cfa98624f69e3027e43d4fb5581",
        "id": 240357
    },
    {
        "content": "def str_repeat(space, s, repeat):\n    \"\"\"Repeat a string.\"\"\"\n    return space.newstr(s * repeat)",
        "sha1": "3e947da1fa3bf403b0836bd4e7ae0052d310636e",
        "id": 5242
    },
    {
        "content": "import pytz\nfrom datetime import datetime\n\n\ndef float_to_datetime(timestamp, tzinfo=None):\n    \"\"\"\n    Convert a timestamp to a datetime instance.\n    If tzinfo is passed, interpret the timestamp in the given timezone.\n    If tzinfo isn't passed, interpret the timestamp as UTC.\n\n    For example, epoch starts at 1am CET (midnight UTC, as CET = UTC + 1h). So 1h from that time is\n    2am CET.\n    >> cet_tz = timezone('CET')\n    >> float_to_datetime(3600, tzinfo=cet_tz)\n    datetime.datetime(1970, 1, 1, 2, 0, tzinfo=<DstTzInfo 'CET' CET+1:00:00 STD>)\n\n    Without timezone give, 3600s from the epoch start is just 1h into the epoch:\n    >> float_to_datetime(3600)\n    datetime.datetime(1970, 1, 1, 1, 0)\n\n    See tests for more examples.\n\n    Args:\n        timestamp (float): e.g. 123456.123, seconds from the epoch, can include milliseconds\n        tzinfo (timezone): optional timezone object\n\n    Returns:\n        datetime: if no timezone given - a timezone-naive datetime.\n                  Otherwise - a datetime object in the given timezone.\n    \"\"\"\n    _tz = tzinfo if tzinfo else pytz.UTC\n    dt = datetime.fromtimestamp(timestamp, tz=_tz)\n\n    if not tzinfo:\n        dt = dt.replace(tzinfo=None)\n    return dt",
        "sha1": "f74dadce58664115e7f29c92723540be4e5f71e9",
        "id": 657135
    },
    {
        "content": "def range_check(df, maximum, minimum):\n    \"\"\"\n    range_check adds a column to data frame with label if data are out of range.\n    Arguments:\n        df: data frame with a column 'raw' of raw data.\n        maximum: maximum acceptable value - above this value, data are anomalous\n        minimum: minimum acceptable value - below this value, data are anomalous\n    Returns:\n        df: data frame with an added column 'anomaly' with boolean where 1 = True for anomalous\n        range_count: total number of anomalies from this check\n    \"\"\"\n    # could do some sort of look up table with the values for each sensor\n    # could also add seasonal checks\n    df = df.eval('anomaly = raw > @maximum or raw < @minimum')\n    range_count = sum(df['anomaly'])\n\n    return df, range_count",
        "sha1": "d58bcc332e61b252ee3dac9fe500d29fe546cb35",
        "id": 650762
    },
    {
        "content": "from typing import OrderedDict\n\n\ndef strip_prefix_if_present(state_dict, prefix='module.'):\n    \"\"\"\n    This function is taken from the maskrcnn_benchmark repo.\n    It can be seen here:\n    https://github.com/facebookresearch/maskrcnn-benchmark/blob/master/maskrcnn_benchmark/utils/model_serialization.py\n    \"\"\"\n    keys = sorted(state_dict.keys())\n    if not all(key.startswith(prefix) for key in keys):\n        return state_dict\n    stripped_state_dict = OrderedDict()\n    for key, value in state_dict.items():\n        if key.startswith(prefix):\n            stripped_state_dict[key[len(prefix):]] = value\n            # stripped_state_dict[key.replace(prefix, \"\")] = value\n        else:\n            pass\n    return stripped_state_dict",
        "sha1": "e87d8a29317648aad81e654223fbaca47f79a986",
        "id": 401348
    },
    {
        "content": "def comment_scalar(a_dict, key):\n    \"\"\"Comment out a scalar in a ConfigObj object.\n\n    Convert an entry into a comment, sticking it at the beginning of the section.\n\n    Returns: 0 if nothing was done.\n             1 if the ConfigObj object was changed.\n    \"\"\"\n\n    # If the key is not in the list of scalars there is no need to do anything.\n    if key not in a_dict.scalars:\n        return 0\n\n    # Save the old comments\n    comment = a_dict.comments[key]\n    inline_comment = a_dict.inline_comments[key]\n    if inline_comment is None:\n        inline_comment = ''\n    # Build a new inline comment holding the key and value, as well as the old inline comment\n    new_inline_comment = \"%s = %s %s\" % (key, a_dict[key], inline_comment)\n\n    # Delete the old key\n    del a_dict[key]\n\n    # If that was the only key, there's no place to put the comments. Do nothing.\n    if len(a_dict.scalars):\n        # Otherwise, put the comments before the first entry\n        first_key = a_dict.scalars[0]\n        a_dict.comments[first_key] += comment\n        a_dict.comments[first_key].append(new_inline_comment)\n\n    return 1",
        "sha1": "f2121caa4e58ec88527ae128a0ac9669efa066d7",
        "id": 28089
    },
    {
        "content": "def always_true(result, previous_state):\n    \"\"\" Not really necessary since no filters results in an always true result,\n    but this is useful to show an example of what a filter is without actually\n    doing anything.\n    \"\"\"\n    return True",
        "sha1": "157fe783cfe14049d6c9bf742eb41064fc9b1ef7",
        "id": 387684
    },
    {
        "content": "def get_proto_arguments(protos, genfiles_dir_path):\n    \"\"\"Get the protoc arguments specifying which protos to compile.\"\"\"\n    arguments = []\n    for proto in protos:\n        massaged_path = proto.path\n        if massaged_path.startswith(genfiles_dir_path):\n            massaged_path = proto.path[len(genfiles_dir_path) + 1:]\n        arguments.append(massaged_path)\n    return arguments",
        "sha1": "f00decab9c96d8b33e594f20e06239de0dc7971a",
        "id": 474052
    },
    {
        "content": "def none(**_):\n    \"\"\"\tInput:\tanything\n        Return:\t0.0       (float)\n        Descr.: Dummy method to handle no temperature correction\"\"\"\n    return 0.0",
        "sha1": "e06b22f91d5a73450ddb4ca53fbb2569d567dcf1",
        "id": 706698
    },
    {
        "content": "def bigend_2_int(p_bytes):\n    \"\"\" Convert bigending bytestring to int\n    \"\"\"\n    l_ix = 0\n    l_int = 0\n    while l_ix < len(p_bytes):\n        l_b = int(p_bytes[l_ix])\n        l_int = l_int * 256 + l_b\n        l_ix += 1\n    return l_int",
        "sha1": "5c51a3752eec30804ab45185fb51c70be240a5b6",
        "id": 19906
    },
    {
        "content": "def moving_average_filter(val, filtered_val_prev, zeta):\n    \"\"\"\n    Basic moving average filter\n    zeta = 1 -> ignore prev. vals\n    zeta = 0 -> ignore current val\n    \"\"\"\n    filtered_val = (1-zeta)*filtered_val_prev + zeta*val\n    return filtered_val",
        "sha1": "388b02589efcbf10b791c00341432ba08f947b8b",
        "id": 412722
    },
    {
        "content": "import random\n\n\ndef weighted_choice(mapping):\n    \"\"\"Return a single element from a weighted sample.\n\n    The input is a dictionary of items with weights as values.\n    \"\"\"\n    # use roulette method\n    rnd = random.random() * sum(mapping.values())\n    for k, w in mapping.items():\n        rnd -= w\n        if rnd < 0:\n            return k",
        "sha1": "182a5586ef74f627e5187a8bb0342424762099d3",
        "id": 190160
    },
    {
        "content": "def happens_before(G, first, second):\n    \"\"\"Returns true if first happens before second in sequential order.\"\"\"\n    first_doc = G.nodes[first]['doc']\n    second_doc = G.nodes[second]['doc']\n    return first_doc['end_time'] <= second_doc['start_time']",
        "sha1": "db7b02ff2c93fbbc91026db011ab1fab7fdafff3",
        "id": 398292
    },
    {
        "content": "def get_traceback(message):\n    \"\"\"\n    Extract traceback from pytest output\n\n    Parameters\n    ----------\n    message : str\n        The pytest output\n\n    Returns\n    -------\n    str\n        The traceback found in the given pytest output\n\n    Raises\n    ------\n    ValueError\n        If message does not contain a traceback\n    \"\"\"\n    try:\n        index = message.index(\"= FAILURES \")\n    except ValueError:\n        raise ValueError(\"Given message does not contain a traceback\")\n\n    for _ in range(3):\n        index = message.index(\"\\n\", index + 1)\n\n    end_index = message.rindex(\"short test summary info\")\n    end_index = message.rindex(\"\\n\", 0, end_index)\n\n    return message[index:end_index]",
        "sha1": "0a69a8604c905bfcbc0065c9562fe82d64c55ea2",
        "id": 341018
    },
    {
        "content": "def _extract_stack_info(stack):\n  \"\"\"\n  Given a stack, save the static info for each frame (basically\n  everything except for the reference to the frame).\n  \"\"\"\n\n  stack_info = []\n  for frame in stack: stack_info.append(frame[1:])\n  return stack_info",
        "sha1": "1219c56f214ef9633504b0eebae4604f4b1f40bc",
        "id": 587403
    },
    {
        "content": "def get_segment_output_path(output_path, muxing_output_path):\n    \"\"\"\n    Returns the output path for a stream segment.\n    \"\"\"\n    segment_path = muxing_output_path\n    substr = muxing_output_path[0:len(output_path)]\n\n    if substr == output_path:\n        return muxing_output_path[len(output_path):]\n\n    return segment_path",
        "sha1": "7c35c5becf6b6eb4f20399f6fa5b3367bce5af79",
        "id": 27018
    },
    {
        "content": "import requests\nimport tempfile\nimport gzip\n\n\ndef get_cif(code, mmol_number, outfile=None):\n    \"\"\"\n    Parameters\n    ----------\n    code : str\n        PDB code.\n    mmol_number : int\n        mmol number (biological assembly number) of file to download. Numbers from PDBe.\n        If None, defaults to the preferred biological assembly listed for code on the PDBe.\n    outfile : str\n        Filepath. Writes returned value to this file.\n\n    Returns\n    -------\n    cif_string : str, or None\n        Content of the cif file as a string.\n        None if unable to download the cif_file from the pdbe site.\n    \"\"\"\n    pdbe_url = \"http://www.ebi.ac.uk/pdbe/static/entry/download/{0}-assembly-{1}.cif.gz\".format(code, mmol_number)\n    r = requests.get(pdbe_url)\n    if r.status_code == 200:\n        temp_gz = tempfile.NamedTemporaryFile()\n        temp_gz.write(r.content)\n        with gzip.open(temp_gz.name, 'rb') as foo:\n            cif_string = foo.read().decode()\n    else:\n        print(\"Could not download cif file for {0}\".format(code))\n        return None\n    # Write to file.\n    if outfile and cif_string:\n        with open(outfile, 'w') as foo:\n            foo.write(cif_string)\n    return cif_string",
        "sha1": "b76ff7bfddaf64df862cff1773cdf82deee99ea1",
        "id": 671938
    },
    {
        "content": "def remove_query_string(url):\n    \"\"\"\n    Returns url without any query string parameters.\n    \"\"\"\n    return url.split(\"?\")[0]",
        "sha1": "f99a607e68e9e086f3c0f3296806dba8979a45fd",
        "id": 80897
    },
    {
        "content": "def kelvin_to_rankine(temp):\n    \"\"\"\n    From Kelvin (K) to Rankine (R)\n    \"\"\"\n    return temp*9/5",
        "sha1": "1815d3f5d3aaf2e50a2bf1cb8bc9af3bee22914c",
        "id": 554567
    },
    {
        "content": "def escapeXMLChars(text):\n    \"\"\"Controls characters that need to be escaped (to obtain a well-formed\n    XML document)\n\n    Args:\n        text: The text that will be escaped (string)\n\n    Returns:\n        text: new text containing XML entities instead of characters (string)\n    \"\"\"\n    text = text.replace(\"&\", \"&amp;\")\n    #text = text.replace(\"\\\"\", \"&quot;\")\n    #text = text.replace(\"'\", \"&apos;\")\n    text = text.replace(\"<\", \"&lt;\")\n    #text = text.replace(\">\", \"&gt;\")\n    return text",
        "sha1": "284bfefb400ebe5381a2b9454a2a8b15f199f36d",
        "id": 73157
    },
    {
        "content": "def f1 (p, r):\n    \"\"\"\n    Computes F1\n\n    Args:\n        p (float): precision\n        r (float): recall\n\n    Returns:\n        float: F1\n        \"\"\"\n    try:\n        return 2 * (p*r) / (p+r)\n    except ZeroDivisionError:\n        return 0",
        "sha1": "59c91941a86fa8e91f6b5ce83d2e966004b4f0b9",
        "id": 207499
    },
    {
        "content": "import inspect\n\n\ndef get_function_name() -> str:\n    \"\"\"Returns the current function name in which get_function_name is called\"\"\"\n    frame = inspect.currentframe()\n    function_name = inspect.getouterframes(frame)[4].function\n\n    if function_name == '<module>':\n        return '__root__'\n    else:\n        return function_name + '()'",
        "sha1": "d31295dcedb8b4cebff41034f3d83d705c8b7ae6",
        "id": 368607
    },
    {
        "content": "def flatten_json(input, delimeter=\"_\"):\n    \"\"\"Returns a flat dictionary by flattening nested objects using the given delimeter\"\"\"\n    output = {}\n\n    def flatten(element, name=\"\"):\n        if type(element) is dict:\n            for item in element:\n                flatten(element[item], name + item + delimeter)\n        elif type(element) is list:\n            i = 0\n            for item in element:\n                flatten(item, name + str(i) + delimeter)\n                i += 1\n        else:\n            output[name[:-1]] = element\n\n    flatten(input)\n    return output",
        "sha1": "28a3a4595d0fd859e294eec8f6cddee71cf7feea",
        "id": 389039
    },
    {
        "content": "def get_idxs_in_correct_order(idx1, idx2):\n    \"\"\"First idx must be smaller than second when using upper-triangular arrays (matches, keypoints)\"\"\"\n    if idx1 < idx2: return idx1, idx2\n    else: return idx2, idx1",
        "sha1": "05963c4f4b692b362980fde3cf6bb1de1b8a21e0",
        "id": 29453
    },
    {
        "content": "import ctypes\n\n\ndef _malloc_char_array(n):\n\n    \"\"\"\n    Return a pointer to allocated UTF8 (C char) array of length `n'.\n    \"\"\"\n\n    t = ctypes.c_char * n\n    return t()",
        "sha1": "a67c6101e243392388fcba723dd37e4fe00cc0a1",
        "id": 675156
    },
    {
        "content": "from functools import reduce\nimport operator\n\n\ndef factorial(n):\n    \"\"\"Calculate n factorial\"\"\"\n    return reduce(operator.mul, range(2, n+1), 1)",
        "sha1": "a58e0aad4e3a8baf06bbd1a6929a3aab2ab4e66e",
        "id": 698685
    },
    {
        "content": "def comma_separated_list(x):\n    \"\"\"\n    Parse a restructured text option as a comma-separated list of strings.\n    \"\"\"\n    return x.split(',')",
        "sha1": "c75c80c5f19fe1843c8ed8e8cc64e4233533bf20",
        "id": 39949
    },
    {
        "content": "def unflatten_mapping(mapping):\n    \"\"\"Unflatten a dict with dot-concatenated keys to a dict of dicts\n\n    Examples\n    --------\n    >>> x = {'a1.b1.c1': 1,\n    ...      'a1.b2': 2,\n    ...      'a2.b1': 3}\n    >>> unflatten_mapping(x)  # doctest: +SKIP\n    {'a1': {'b1': {'c1': 1},\n     'b2': 2},\n     'a2': {'b1': 3}}\n    \"\"\"\n    out = {}\n    for k, v in mapping.items():\n        keys = k.split('.')\n        o = out\n        for k2 in keys[:-1]:\n            o = o.setdefault(k2, {})\n        o[keys[-1]] = v\n    return out",
        "sha1": "83c5f8a65f0e92ee9af670ff03ffc9b6ca770983",
        "id": 154058
    },
    {
        "content": "def _date_to_string(d):\n    \"\"\"Make string of date in Nexus format\n    Args:\n        d (datetime): Date object\n    Returns:\n        String with date\n    \"\"\"\n\n    id = d.day\n    im = d.month\n    iy = d.year\n\n    if im < 10:\n        st = '0' + str(im)\n    else:\n        st = str(im)\n\n    st = st + r'/'\n\n    if id < 10:\n        st = st + '0' + str(id)\n    else:\n        st = st + str(id)\n\n    st = st + r'/' + str(iy)\n\n    return st",
        "sha1": "3deb73805f5123f7b7c1cb92bde1169f573f6fae",
        "id": 636048
    },
    {
        "content": "import math\n\n\ndef arctan_z_term(z, term_index):\n    \"\"\"\n    Generates each term in Gregory-Leibniz series for arctan(z)\n    \n    \"\"\"\n    term_power = (term_index*2+1)  # Each term is raised to this power, and also divided by this value\n    \n    return (math.pow(z, term_power)/term_power)*((-1)**(term_index))",
        "sha1": "44bb0a822e0bd4c64de703ee0d0549c17046dfd8",
        "id": 589875
    },
    {
        "content": "import uuid\n\n\ndef export_table(bigquery, cloud_storage_path,\n                 project_id, dataset_id, table_id,\n                 export_format=\"CSV\",\n                 num_retries=5,\n                 compression=\"NONE\"):\n    \"\"\"\n    Starts an export job\n\n    Args:\n        bigquery: initialized and authorized bigquery\n            google-api-client object.\n        cloud_storage_path: fully qualified\n            path to a Google Cloud Storage location.\n            e.g. gs://mybucket/myfolder/\n        export_format: format to export in;\n            \"CSV\", \"NEWLINE_DELIMITED_JSON\", or \"AVRO\".\n        compression: format to compress results with,\n            \"NONE\" (default) or \"GZIP\".\n\n    Returns: an extract job resource representing the\n        job, see https://cloud.google.com/bigquery/docs/reference/v2/jobs\n    \"\"\"\n    # Generate a unique job ID so retries\n    # don't accidentally duplicate export\n    job_data = {\n        'jobReference': {\n            'projectId': project_id,\n            'jobId': str(uuid.uuid4())\n        },\n        'configuration': {\n            'extract': {\n                'sourceTable': {\n                    'projectId': project_id,\n                    'datasetId': dataset_id,\n                    'tableId': table_id,\n                },\n                'destinationUris': [cloud_storage_path],\n                'destinationFormat': export_format,\n                'compression': compression\n            }\n        }\n    }\n    return bigquery.jobs().insert(\n        projectId=project_id,\n        body=job_data).execute(num_retries=num_retries)",
        "sha1": "02333ac59c1b28ed1ac75ad99d7c9d2ba173e363",
        "id": 293744
    },
    {
        "content": "def shift_onset(name, group, df_onset, visit_col='Visit_Date', onset_col='onsetdt'):\n    \"\"\"Calculate time since symptom onset in years by subtracting visit date from date of symptom onset\"\"\"\n    group[visit_col] = (group[visit_col] - df_onset.loc[name][onset_col]) / 365.4\n    return group",
        "sha1": "ce774d6d4571d441debf88ac7ae340429bcc32b6",
        "id": 136500
    },
    {
        "content": "def owner_organizations(user):\n    \"\"\"Return organizations the user is owner.\"\"\"\n    return user.owner_organizations.all()",
        "sha1": "77bf5529ef0555aaa1d1976506324464295241e8",
        "id": 482055
    },
    {
        "content": "def convert(img_size, coords):\n    \"\"\"Convert from xmin,xmax,ymin,ymax to xywh normalised format\n    Args:\n        img_size(array): array of (img_width, img_height)\n        coords(array): Array of class_id, xmin, xmax, ymin, ymax\n    Returns:\n        xywh(array): Array of normalised xywh coords\n    \"\"\"\n    img_width = img_size[0]\n    img_height = img_size[1]\n    #Calculate Bounding box dimensions from xml\n    class_id = str(coords[0])\n    x_centre = str(((coords[1]+coords[2])/2)/img_width)\n    y_centre = str(((coords[3]+coords[4])/2)/img_height)\n    height = str((coords[4]-coords[3])/img_height)\n    width = str((coords[2]-coords[1])/img_width)\n    \n    xywh = (class_id, x_centre, y_centre, width, height)\n    return xywh",
        "sha1": "1230c661e90801e65b96d63e56f2c0b71cca9c3f",
        "id": 229908
    },
    {
        "content": "from operator import le\n\n\ndef refinement_le(le=le):\n    \"\"\"\n    Construct an order based on refinement.\n\n    Parameters\n    ----------\n    le : func\n        A function representing the \"less than or equal\" operator.\n        Defaults to <=.\n\n    Returns\n    -------\n    r_le : func\n        Function implementing refinement ordering with the specified `le`.\n    \"\"\"\n    def r_le(alpha, beta):\n        \"\"\"\n        a <= b --> for all a in alpha, there exists a b in beta such that a <= b.\n        \"\"\"\n        for a in alpha:\n            if not any(le(a, b) for b in beta):\n                return False\n        return True\n\n    return r_le",
        "sha1": "ad8035588386720d7b82b6e51d5c1b08d7e18b05",
        "id": 220776
    },
    {
        "content": "def _format_args(args):\n\t\"\"\"Formats a series of arguments as an STSADM argument string\"\"\"\n\targstring = \"\"\n\tfor kw in args.keys():\n\t\tif type(args[kw]) is bool:\n\t\t\tif args[kw] == True:\n\t\t\t\targstring += \"-\" + kw\n\t\telse:\n\t\t\targstring += \"-\" + kw + \" \\\"\" + args[kw] + \"\\\"\"\n\t\targstring += \" \"\n\t\t\n\treturn argstring",
        "sha1": "f994d01787a9ed6ebde050aa1f50d4fb18d7014f",
        "id": 525371
    },
    {
        "content": "def split_fields(fields: str = '', delimiter: str = ';') -> dict:\n    \"\"\"Split str fields of Demisto arguments to SNOW request fields by the char ';'.\n\n    Args:\n        fields: fields in a string representation.\n        delimiter: the delimiter to use to separate the fields.\n    Returns:\n        dic_fields object for SNOW requests.\n    \"\"\"\n    dic_fields = {}\n\n    if fields:\n        if '=' not in fields:\n            raise Exception(\n                f\"The argument: {fields}.\\nmust contain a '=' to specify the keys and values. e.g: key=val.\")\n        arr_fields = fields.split(delimiter)\n        for f in arr_fields:\n            field = f.split('=', 1)  # a field might include a '=' sign in the value. thus, splitting only once.\n            if len(field) > 1:\n                dic_fields[field[0]] = field[1]\n\n    return dic_fields",
        "sha1": "e57cd3345db2a35d43031718f529d7439e0c4ea9",
        "id": 367685
    },
    {
        "content": "from typing import Tuple\n\n\ndef get_overlap(bb1: Tuple[int, int, int, int], bb2: Tuple[int, int, int, int]) -> float:\n    \"\"\"\n    Computes the overlap between two bounding boxes. \n    Returns the ratio of area between bb1 and the intersection of bb1 and bb2.\n    \"\"\"\n    intersection = [\n        max(bb1[0], bb2[0]),\n        max(bb1[1], bb2[1]),\n        min(bb1[2], bb2[2]),\n        min(bb1[3], bb2[3]),\n    ]\n\n    if intersection[0] > intersection[2] or \\\n        intersection[1] > intersection[3]:\n        return 0.0\n\n    area_bb1 = (bb1[2]-bb1[0])*(bb1[3]-bb1[1])\n    area_intersection = (intersection[2]-intersection[0])*(intersection[3]-intersection[1])\n\n    return area_intersection/area_bb1",
        "sha1": "d5cdd5bda13ee39c43eaabe593f03e6335729379",
        "id": 308149
    },
    {
        "content": "async def get_sensor(client, sensor_name):\n    \"\"\"Get last sensor value (in encoded form), or None if no value.\"\"\"\n    reply, informs = await client.request('sensor-value', sensor_name)\n    assert reply == [b'1']\n    status = informs[0].arguments[3]\n    value = informs[0].arguments[4]\n    if status in {b'nominal', b'warning', b'error'}:\n        return value\n    else:\n        return None",
        "sha1": "18e300422457b27d30be09fda703977ec1e98070",
        "id": 629042
    },
    {
        "content": "def __data_flow_name(name_prefix, src_id, dst_id, prio):\n    \"\"\"\n    Generate name for a data flow\n\n    Args:\n        name_prefix (str): name prefix\n        src_id (int): ID of the source port\n        dst_id (int): ID of the destination port\n        prio (int): priority of the flow\n\n    Returns:\n        Name of the flow (str)\n    \"\"\"\n    return \"{} {} -> {} Prio {}\".format(name_prefix, src_id, dst_id, prio)",
        "sha1": "e73d456b705914d122468af5af6499062f5e8672",
        "id": 471667
    },
    {
        "content": "def len_iter(iterator):\n    \"\"\"Count items in an iterator\"\"\"\n    return sum(1 for i in iterator)",
        "sha1": "1d828b150945cc4016cdcb067f65753a70d16656",
        "id": 5152
    },
    {
        "content": "def liste_donnes(points):\n    \"\"\"\n    Renvoie une liste contenant les points issu de cette liste sans leur\n    coordonn\u00e9e\n\n    Parameters\n    ----------\n    points : list\n        liste de points\n\n    Returns\n    -------\n    donnees : list\n        liste de points\n\n    \"\"\"\n    donnees = []\n    for point in points:\n        liste = [point[i] for i in range(len(points[0])-1)]\n        donnees.append(liste)\n        liste = []\n    return donnees",
        "sha1": "77913b04aca0af13cc1f567213873282cf68c742",
        "id": 376864
    },
    {
        "content": "def load_partial_heads(\n    jiant_model, weights_dict, allow_missing_head_weights=False, allow_missing_head_model=False\n):\n    \"\"\"Loads model weights and returns lists of missing head weights or missing heads (if any).\n\n    Args:\n        jiant_model (JiantModel): jiant model (encoder and task models are core components).\n        weights_dict (Dict): model weights.\n        allow_missing_head_weights (bool): If False, throw exception if there are missing keys.\n        allow_missing_head_model (bool): If False, throw exception if there are unexpected keys.\n\n    Returns:\n        Dict[str, List] containing lists of missing head weights or missing heads if any.\n\n    \"\"\"\n    mismatch = jiant_model.load_state_dict(weights_dict, strict=False)\n    result = {}\n    if mismatch.missing_keys:\n        assert allow_missing_head_weights\n        missing_head_weights = set()\n        for k in mismatch.missing_keys:\n            missing_head_weights.add(k.split(\".\")[1])\n        result[\"missing_head_weights\"] = list(missing_head_weights)\n    if mismatch.unexpected_keys:\n        assert allow_missing_head_model\n        missing_heads_model = set()\n        for k in mismatch.unexpected_keys:\n            missing_heads_model.add(k.split(\".\")[1])\n        result[\"missing_heads_model\"] = list(missing_heads_model)\n    return result",
        "sha1": "ee328244ffa0c0b7c3c74f84b76ba0c2389c9517",
        "id": 484231
    },
    {
        "content": "import fcntl\n\n\ndef is_valid_fd(fd):\n    \"\"\"Check whether the passed in fd is open\"\"\"\n    try:\n        fcntl.fcntl(fd, fcntl.F_GETFD)\n        return True\n    except:\n        return False",
        "sha1": "5760ea0e97d17956e8cc57b17dbb44410c5f3f1a",
        "id": 368019
    },
    {
        "content": "def reallength(value):\n  \"\"\"Returns the length of the value - useful for lists.\"\"\"\n  try:\n     value.count()\n  except:\n     return len(value)",
        "sha1": "0caae6350678dca0f07fc720ec80d694cf389185",
        "id": 154357
    },
    {
        "content": "def ConstTimeCompare(string_a, string_b):\n  \"\"\"Compare the the given strings in constant time.\"\"\"\n  if len(string_a) != len(string_b):\n    return False\n\n  equals = 0\n  for char_x, char_y in zip(string_a, string_b):\n    equals |= ord(char_x) ^ ord(char_y)\n\n  return equals == 0",
        "sha1": "74ccb72b25dec60ead4e18bce3a1414b6c185092",
        "id": 320758
    },
    {
        "content": "import pymysql.cursors\n\n\ndef mysql_conn(\n    host: str = \"localhost\",\n    port: int = 3306,\n    user: str = \"root\",\n    db_name: str = \"newbook\",\n):\n    \"\"\"Establish a PyMySQL connection object\n\n    Connect to a MySQL Database to be used with a pnguin RemoteFrame\n\n    Args:\n        host (str): A hostname\n        port (int): A port number\n        user (str): A DB user\n        db_name (str): The DB name being connected to\n\n    Returns:\n        pymysql.connections.Connection: Representation of a socket to a MySQL server\n\n    \"\"\"\n\n    return pymysql.connect(\n        host=host,\n        port=port,\n        db=db_name,\n        user=user,\n        charset=\"utf8mb4\",\n        cursorclass=pymysql.cursors.DictCursor,\n    )",
        "sha1": "651803e2e4bb0597f9eba0b698eb847a37c19275",
        "id": 598038
    },
    {
        "content": "import collections\n\n\ndef eval_step(test_batch, snlds_model, num_samples, temperature):\n  \"\"\"Runs evaluation of model on the test set and returns evaluation metrics.\n\n  Args:\n    test_batch: a batch of the test data.\n    snlds_model: tf.keras.Model, SNLDS model to be evaluated.\n    num_samples: int, number of samples per trajectories to use at eval time.\n    temperature: float, annealing temperature to use on the model.\n\n  Returns:\n    Dictionary of metrics, str -> list[tf.Tensor],\n      aggregates the result dictionaries returned by the model.\n  \"\"\"\n  test_values = collections.defaultdict(list)\n  for _ in range(10):\n    result_dict = snlds_model(\n        test_batch, temperature, num_samples=num_samples)\n    for k, v in result_dict.items():\n      test_values[k].append(v)\n\n  return test_values",
        "sha1": "2a7ec12f43925aecf266048c6eaa0b331641a4fe",
        "id": 29890
    },
    {
        "content": "from typing import Counter\nfrom functools import reduce\n\n\ndef create_maps(words, tags, min_word_freq=5, min_char_freq=1):\n    \"\"\"\n    Creates word, char, tag maps.\n\n    :param words: word sequences\n    :param tags: tag sequences\n    :param min_word_freq: words that occur fewer times than this threshold are binned as <unk>s\n    :param min_char_freq: characters that occur fewer times than this threshold are binned as <unk>s\n    :return: word, char, tag maps\n    \"\"\"\n    word_freq = Counter()\n    char_freq = Counter()\n    tag_map = set()\n    for w, t in zip(words, tags):\n        word_freq.update(w)\n        char_freq.update(list(reduce(lambda x, y: list(x) + [' '] + list(y), w)))\n        tag_map.update(t)\n\n    word_map = {k: v + 1 for v, k in enumerate([w for w in word_freq.keys() if word_freq[w] > min_word_freq])}\n    char_map = {k: v + 1 for v, k in enumerate([c for c in char_freq.keys() if char_freq[c] > min_char_freq])}\n    tag_map = {k: v + 1 for v, k in enumerate(tag_map)}\n\n    word_map['<pad>'] = 0\n    word_map['<end>'] = len(word_map)\n    word_map['<unk>'] = len(word_map)\n    char_map['<pad>'] = 0\n    char_map['<end>'] = len(char_map)\n    char_map['<unk>'] = len(char_map)\n    tag_map['<pad>'] = 0\n    tag_map['<start>'] = len(tag_map)\n    tag_map['<end>'] = len(tag_map)\n\n    return word_map, char_map, tag_map",
        "sha1": "f873ebc6447eb2024f3abac4aec0a0feccd08470",
        "id": 67097
    },
    {
        "content": "def payper(Rate, NumPeriods, PresentValue, FutureValue, Due):\n    \"\"\"\n    Periodic payment of loan or annuity\n\n    :param Rate:            Interest rate per period. Enter as a decimal fraction.\n    :param NumPeriods:      Number of payment periods in the life of the instrument.\n    :param PresentValue:    Present value of the instrument.\n    :param FutureValue:     (Optional) Future value or target value to be attained after NumPeriods periods.\n    :param Due:             (Optional) When payments are due: 0 = end of period (default), or 1 = beginning of period.\n    :return:\n\n\n    This example shows how to find the monthly payment for a three-year\n    loan of $9000 with an annual interest rate of 11.75%.\n\n    Example:\n        >>> Payment = payper(0.1175/12, 36, 9000, 0, 0)\n        297.85528322\n\n    \"\"\"\n\n    x = 1 / (1 + Rate)\n    q = (x ** (NumPeriods + 1) - x ) / (x - 1)\n    PM = -1 * (FutureValue - PresentValue) / q\n\n    # print \"PM = \", PM\n    return PM",
        "sha1": "f62313c62f175037507727b76c5d2870074e34ff",
        "id": 244960
    },
    {
        "content": "import requests\n\n\ndef get_policy_from_server(policy_url, application_name, dane_id):\n    \"\"\"Return JSON response from policy server.\"\"\"\n    params = {\"application_name\": application_name, \"device_name\": dane_id}\n    response = requests.get(policy_url, params=params)\n    return response.json()",
        "sha1": "f316bbfecf7efa74ffff9251a74964bb677423b1",
        "id": 305643
    },
    {
        "content": "def initialiser( moduleName ):\n    \"\"\"Compute the initialiser function name from module name\"\"\"\n    moduleName = moduleName.replace(\".\",\"_\")\n    parts = moduleName.split('_')[1:] # strip OpenGL prefix\n    return \"\".join([\n        parts[0].lower(),\n        'Init'\n    ] + [ x.title() for x in parts[2:]] + [parts[1]])",
        "sha1": "4294ad7fe14575e30d4153eaefeb38ea7b12df0c",
        "id": 460382
    },
    {
        "content": "def tz_dst2std(tzstr: str) -> str:\n    \"\"\"Return the standard time zone abbreviation for the given\n    timezone abbreviation. Needed, because we cannot use DST abbreviations\n    when setting the timezone via timedatectl on the tablet.\n\n    Using DST-to-STD mappings from:\n    https://en.wikipedia.org/wiki/List_of_tz_database_time_zones\n    except for GMT --> IST (Irish Std Time)\n    \"\"\"\n    mapping = {\n        'ACDT': 'ACST',\n        'ADT': 'AST',\n        'AEDT': 'AEST',\n        'AKDT': 'AKST',\n        'BST': 'GMT',\n        'CDT': 'CST',\n        'CEST': 'CET',\n        'EDT': 'EST',\n        'EEST': 'EET',\n        'HDT': 'HST',\n        'IDT': 'IST',\n        'MDT': 'MST',\n        'NDT': 'NST',\n        'NZDT': 'NZST',\n        'PDT': 'PST',\n        'WEST': 'WET'\n    }\n    if tzstr in mapping:\n        return mapping[tzstr]\n    else:\n        return tzstr",
        "sha1": "85f0a276b1b0430befad56b58103bc8f5249bb30",
        "id": 565150
    },
    {
        "content": "def tamper(payload, **kwargs):\n    \"\"\"\n    Replaces instances of UNION ALL SELECT with UNION SELECT counterpart\n\n    >>> tamper('-1 UNION ALL SELECT')\n    '-1 UNION SELECT'\n    \"\"\"\n\n    return payload.replace(\"UNION ALL SELECT\", \"UNION SELECT\") if payload else payload",
        "sha1": "312f6fb99f23011a635131f1f35fd1c763f6d080",
        "id": 208814
    },
    {
        "content": "from typing import Dict\nfrom typing import Any\nimport yaml\n\n\ndef load_config(config_path: str) -> Dict[str, Any]:\n    \"\"\"Load a YAML configuration file given its path.\"\"\"\n    with open(config_path, \"r\") as fh:\n        config_yaml = yaml.safe_load(fh)\n    return config_yaml",
        "sha1": "ce0788a5e9668df287c31cc557f618f17f020051",
        "id": 550989
    },
    {
        "content": "def get_node_components_from_abstraction_graph(graph):\n    \"\"\"Extract the set of node ids from each node label in the input graph.\n\n    Parameters\n    ----------\n    graph : Networkx undirected graphs\n        Abstract graph with set of node ids as labels.\n\n    Returns\n    -------\n    node_components : list[set(node ids)]\n        A list of sets of node ids.\n    \"\"\"\n    components = []\n    for u in graph.nodes():\n        components.append(set(graph.nodes[u]['label']))\n    return components",
        "sha1": "d7b62f223d460043e7aa90ce53bb47c6fb15a40e",
        "id": 640916
    },
    {
        "content": "import re\n\n\ndef package_from_requirement(requirement):\n    \"\"\"Convert pip requirement string to a package name.\"\"\"\n    return re.sub(r'-',\n                  r'_',\n                  re.sub(r'\\[.*?\\]|.*/([^@/]*?)(\\.git)?.*',\n                         r'\\1',\n                         requirement))",
        "sha1": "492b0d14edb84a995570b58da7d0567ae9b74a1f",
        "id": 167903
    },
    {
        "content": "def _Spaced(lines):\n  \"\"\"Adds a line of space between the passed in lines.\"\"\"\n  spaced_lines = []\n  for line in lines:\n    if spaced_lines:\n      spaced_lines.append(' ')\n    spaced_lines.append(line)\n  return spaced_lines",
        "sha1": "750929d11f7c106075dbe93695c7beecb94b1ba7",
        "id": 426367
    },
    {
        "content": "import gzip\n\n\ndef open_file_stream(filename):\n    \"\"\"Open specified filename and return a stream.  If the filename ends\n    with a '.gz' then read the file as a gzip file.  This is used in some\n    of the argparse definitions for this project.\"\"\"\n    if filename.endswith('.gz'):\n        return gzip.open(filename, 'rt')\n    else:\n        return open(filename, 'rt')",
        "sha1": "0ff569cc348598e8cb0915f6d4cd6222c661024b",
        "id": 421978
    },
    {
        "content": "def my_extract(value, delim=',', index=0):\n    \"\"\"Split a given string and return value at given index position.\"\"\"\n    return value.split(delim)[index]",
        "sha1": "43c179f3468775fbcd2be942fa02f4fe0897eb5f",
        "id": 624811
    },
    {
        "content": "def user_info_columns(table):\n    \"\"\"Returns the column expression for all required info retrieved by\n    a user lookup.\n\n    *table* is the users SQLAlchemy table object. Required to preserve type\n    information for the columns.\n\n    \"\"\"\n\n    return [\n        table.c.user_id,\n        table.c.system_id,\n        table.c.full_name,\n        table.c.email,\n        table.c.email_verified,\n        table.c.is_active,\n        table.c.last_login_try,\n        table.c.last_login_success,\n        table.c.failed_login_tries,\n        table.c.created_on,\n        table.c.last_updated,\n        table.c.user_role,\n        table.c.extra_info,\n        table.c.emailverify_sent_datetime,\n        table.c.emailforgotpass_sent_datetime,\n        table.c.emailchangepass_sent_datetime\n    ]",
        "sha1": "7b8207c430de32ec2b7e7aeb2661682f0aeecb9e",
        "id": 242605
    },
    {
        "content": "def get_val(KrigInfo, key, num=None):\n    \"\"\"Helper function to get values from KrigInfo dictionary.\n\n    Multiobjective Kriging values are not mapped to single values and\n    need to be extracted using the num parameter.\n\n    If running multiobjective Kriging, values are stored in a\n    list/dictionary under each key in the Kriging model dictionary,\n    where the index/key is the objective function number 'num'.\n\n    Args:\n        KrigInfo (dict): The Kriging model dictionary.\n        key (str): The variable key of desired value.\n        num (int, optional): The objective function number. Defaults\n            to None.\n\n    Returns:\n        Value extracted from dictionary.\n\n    Raises:\n        KeyError: If key does not exist in KrigInfo or if specified\n            key also requires a multiobjective function number and\n            num is not specified or nonexistent.\n        IndexError: If a num is specified that is outside the range of\n            the list for the desired key variable.\n    \"\"\"\n    try:\n        val = KrigInfo[key]\n    except KeyError:\n        msg = f\"'Key {key}' was not found in Kriging model dictionary.\"\n        raise KeyError(msg)\n\n    if num is not None:\n        try:\n            val = val[num]\n        except KeyError:\n            msg = (\n                f\"Objective function number '{num}' was not found for key \"\n                f\"'{key}' in Kriging model dictionary.\"\n            )\n            raise KeyError(msg)\n        except IndexError:\n            msg = (\n                f\"Objective function number '{num}' was not found for key \"\n                f\"'{key}' in Kriging model dictionary.\"\n            )\n            raise IndexError(msg)\n\n    # I'm not sure which one you're using!\n    if isinstance(val, dict):\n        keys = \", \".join([str(v) for v in val.keys()])\n        msg = f\"{key} has multiobjective function keys: {keys}. Specify 'num'.\"\n        raise ValueError(msg)\n\n    if isinstance(val, list):\n        keys = \", \".join([str(v) for v in range(len(val))])\n        msg = f\"{key} has multiobjective function keys: {keys}. Specify 'num'.\"\n        raise ValueError(msg)\n    return val",
        "sha1": "0278a0b7a0e3543093c588f45f4dc140ed9a3f3a",
        "id": 493460
    },
    {
        "content": "def find_indexes(s, ch='\\n'):\n    \"\"\"Finds all instances of given char and returns list of indexes \"\"\"\n    return [i for i, ltr in enumerate(s) if ltr == ch]",
        "sha1": "b94bfcf277de7e10f2adee0aabc441b20bc18467",
        "id": 434008
    },
    {
        "content": "import json\n\n\ndef robust_load_json_file(path):\n    \"\"\"\n    Load json file by given file path without breaking test\n    :param path: full file path like \"/home/onrack/src/test.json\"\n    :return: a dict includes exit_code and message, an example:\n        {\"exit_code\": 0, \"message\": {\"config_a\": 0, \"config_b\": 1}}\n        message is a json object if loaded json file successfully\n    \"\"\"\n    exit_status = {}\n    try:\n        with open(path) as data_file:\n            json_obj = json.load(data_file)\n    except IOError:\n        exit_status[\"exit_code\"] = -1\n        exit_status[\"message\"] = \"Can't find or unable to access {}\".format(path)\n    except ValueError:\n        exit_status[\"exit_code\"] = -1\n        exit_status[\"message\"] = \"Can't load {}, json format is required\".format(path)\n    else:\n        exit_status[\"exit_code\"] = 0\n        exit_status[\"message\"] = json_obj\n    return exit_status",
        "sha1": "3a049144a2b1b1d253570c3b855277e2bd66e082",
        "id": 427076
    },
    {
        "content": "def gross_count(spec, c1, c2):\n    \"\"\"Returns total number of counts in a spectrum between two channels\"\"\"\n\n    if c1 > c2:\n        raise ValueError(\"c1 must be less than c2\")\n    if c1 < 0:\n        raise ValueError(\"c1 must be positive number above 0\")\n    if c2 > max(spec.channels):\n        raise ValueError(\"c2 must be less than max number of channels\")\n\n    gc = sum(spec.counts[c1:c2])\n    return gc",
        "sha1": "154f0e45463d38bd8dfb6ab1f678ef2d1980410c",
        "id": 645081
    },
    {
        "content": "from functools import reduce\n\n\ndef get_residue_strings(resname_list, resid_list):\n    \"\"\"\n    Give a list of residue names and list of residue PDB sequence identifiers,\n    return string represnetation of the lists to put in SVG for use e.g\n    in hovertext on interactive SVG.\n\n    Parameters:\n       resname_list - list of 3-letter residue names\n       resid_list - list of residue PDB sequence numbers\n\n    Return value:\n       tuple (residue_names, residue_list) where\n       residue_names is string with all residue names space-separated\n       residue_list is string with all sequene numbers space-separarated\n    \"\"\"\n    # can have empty residue name and id lists for eg connectors\n    # that are between a terminus and an SSE node, and no coil region\n    # between terminus and that SSE\n    if len(resname_list) > 1:\n        residue_names = reduce(lambda a,b : a + ' ' + b, resname_list)\n        residue_ids = reduce(lambda a,b : str(a) + ' ' + str(b), resid_list)\n    elif len(resname_list) == 1:\n        residue_names = str(resname_list[0])\n        residue_ids = str(resid_list[0])\n    else:\n        residue_names = \"\"\n        residue_ids = \"\"\n    return (residue_names, residue_ids)",
        "sha1": "9eacc6c29287811fd02e807d2f0d0fbecb5a100a",
        "id": 299090
    },
    {
        "content": "def read_file_as_binary( fPath ):\n    \"\"\" Open the file in binary mode and return its contents \"\"\"\n    # URL , Pickle and Image: https://mail.python.org/pipermail/python-list/2005-April/313870.html\n    f = open( fPath , 'rb' )\n    f.seek(0)\n    rtnData = f.read()\n    f.close()\n    return rtnData",
        "sha1": "37c8b0f6b8cf9ccc58c1bfdea7fb135cdc3da8df",
        "id": 592457
    },
    {
        "content": "def boolNot(num):\n    \"\"\" return True if num is not 0 \"\"\"\n    return num != 0",
        "sha1": "445aee860bf58c174cae51faa48b0aa55c70f553",
        "id": 636315
    },
    {
        "content": "import re\n\n\ndef standardizeCapitalization(input_string, algorithm):\n    \"\"\"\n    Converts title case words (e.g., ' The ') to lowercase e.g., ' the '). Allows conversion algorithms for multiple\n    scenarios (e.g., author names vs titles) and languages via keyword arguments of 'algorithm' parameter.\n\n    Args:\n        input_string (str): the string to be converted.\n        algorithm: capitalization algorithm to be used\n\n    Keyword Args:\n        \"English title\" (algorithm):\n\n    Returns:\n        The converted string\n\n    Examples:\n        >>> from triplicator.bibTools import standardizeCapitalization\n        >>> standardizeCapitalization(\"Phantom Of The Opera\", \"en_title\")\n        'Phantom of the Opera'\n    \"\"\"\n\n    formatted_string = input_string\n\n    # convert title case to lowercase (DBpedia format)\n    if algorithm is \"en_title\":\n        formatted_string = re.sub(\" In \", \" in \", formatted_string)\n        formatted_string = re.sub(\" The \", \" the \", formatted_string)\n        formatted_string = re.sub(\" A \", \" a \", formatted_string)\n        formatted_string = re.sub(\" An \", \" an \", formatted_string)\n        formatted_string = re.sub(\" As \", \" as \", formatted_string)\n        formatted_string = re.sub(\" On \", \" on \", formatted_string)\n        formatted_string = re.sub(\" At \", \" at \", formatted_string)\n        formatted_string = re.sub(\" For \", \" for \", formatted_string)\n        formatted_string = re.sub(\" With \", \" with \", formatted_string)\n        formatted_string = re.sub(\" From \", \" from \", formatted_string)\n        formatted_string = re.sub(\" By \", \" by \", formatted_string)\n        formatted_string = re.sub(\" Of \", \" of \", formatted_string)\n        formatted_string = re.sub(\" Vs \", \" vs \", formatted_string)\n        formatted_string = re.sub(\" And \", \" and \", formatted_string)\n\n        formatted_string = re.sub(\" Be \", \" be \", formatted_string)\n        formatted_string = re.sub(\" Been \", \" been \", formatted_string)\n        formatted_string = re.sub(\" Not \", \" not \", formatted_string)\n        formatted_string = re.sub(\" Is \", \" is \", formatted_string)\n        formatted_string = re.sub(\" Isn\\'t \", \" isn\\'t \", formatted_string)\n        formatted_string = re.sub(\" Are \", \" are \", formatted_string)\n        formatted_string = re.sub(\" Aren\\'t \", \" aren\\'t \", formatted_string)\n        formatted_string = re.sub(\" Does \", \" does \", formatted_string)\n        formatted_string = re.sub(\" Doesn\\'t \", \" doesn\\'t \", formatted_string)\n        formatted_string = re.sub(\" Do \", \" do \", formatted_string)\n        formatted_string = re.sub(\" Don\\'t \", \" don\\'t \", formatted_string)\n        formatted_string = re.sub(\" Was \", \" was \", formatted_string)\n        formatted_string = re.sub(\" Wasn\\'t \", \" wasn\\'t \", formatted_string)\n        formatted_string = re.sub(\" Were \", \" were \", formatted_string)\n        formatted_string = re.sub(\" Weren\\'t \", \" weren\\'t \", formatted_string)\n        formatted_string = re.sub(\" Did \", \" did \", formatted_string)\n        formatted_string = re.sub(\" Didn\\'t \", \" didn\\'t \", formatted_string)\n        # This list is not exhaustive\n\n    else:\n        raise Exception ('Unknown algorithm parameter: \"' + algorithm + '\". Please enter a valid capitalization algorithm such as \"en_title\".')\n\n    return formatted_string",
        "sha1": "4a5d2c1ba937876f37d0e29989356bbcb5c8fe8e",
        "id": 454961
    },
    {
        "content": "from pathlib import Path\n\n\ndef read_file(filename):\n    \"\"\"Read a text file and return its contents.\"\"\"\n    project_home = Path(__file__).parent.resolve()\n    file_path = project_home / filename\n    return file_path.read_text(encoding=\"utf-8\")",
        "sha1": "a17f99cd438a182c25e466e0f5c36af262dca5eb",
        "id": 588863
    },
    {
        "content": "import yaml\n\n\ndef read_yaml_file(filepath):\n    \"\"\"Return contents of a yaml file.\n\n    Parameters\n    ----------\n    filepath : string\n        The full path to the yaml file.\n\n    Returns\n    -------\n    list of dictionaries\n        The contents of the yaml file where each dictionary corresponds\n        to a line in the file and the key of the dictionary is the name\n        of the column.\n    \"\"\"\n    with open(filepath, \"r\") as stream:\n        contents = yaml.safe_load(stream)\n    return contents",
        "sha1": "27111c678ea3640172bcaccff6688f60650a960d",
        "id": 667573
    },
    {
        "content": "import time\nimport struct\n\n\ndef writeICCdatetime(t=None):\n    \"\"\"`t` should be a gmtime tuple (as returned from\n    ``time.gmtime()``).  If not supplied, the current time will be used.\n    Return an ICC dateTimeNumber in a 12 byte string.\n    \"\"\"\n\n\n    if t is None:\n        t = time.gmtime()\n    return struct.pack(\">6H\", *t[:6])",
        "sha1": "aff169fcb3d89d3ec4e785c128e59a00f816e482",
        "id": 458975
    },
    {
        "content": "def _is_python_file(filename: str) -> bool:\n    \"\"\"Check if file is a Python file.\"\"\"\n    return filename.endswith(\".py\")",
        "sha1": "6e4e366514b04a57116b91f4edf357e86950e8d7",
        "id": 602008
    },
    {
        "content": "def is_positive(value):\n    \"\"\"\n    Confirms an integer is not a negative number\n    \"\"\"\n    return False if value < 0 else True",
        "sha1": "ad322cbc8e7887ed3b0e2cbef57a87dd96fb8f24",
        "id": 502574
    },
    {
        "content": "import string\n\n\nasync def make_reply_prefix(item_ind):\n    \"\"\"Makes the prefix for a question response. e.g. 3 -> 'c)'\n\n    Takes the number (up to 52) of the response.\n\n    NOTE: Above 52 is not supported (and if you have a quiz with 52 answers,\n    maybe you also want to rethink your quiz?).\n\n    \"\"\"\n\n    return \"{})\".format(string.ascii_letters[item_ind])",
        "sha1": "06c203846ece601d1c6e8268c250e365f4113fea",
        "id": 430458
    },
    {
        "content": "import hashlib\n\n\ndef compute(query: str) -> str:\n    # type (str) -> str\n    \"\"\"\n    Computes the query hash via SHA-256.\n\n    Args:\n        query: The query\n\n    Returns:\n        The query hash\n    \"\"\"\n    return hashlib.sha256(query.encode()).hexdigest()",
        "sha1": "6391092dfeb4a4c03d6f8dec2648dbe22f78a1d0",
        "id": 165346
    },
    {
        "content": "def findCommonIndices(index1, index2):\n    \"\"\"\n    Finds the indices common to both.\n\n    Parameters\n    ----------\n    index1: list/index\n    index2: list/index\n    \n    Returns\n    -------\n    sorted list\n    \"\"\"\n    indices = list(set(index1).intersection(index2))\n    indices.sort()\n    return(indices)",
        "sha1": "5a2861ec411143e5a10853f9726e55a62d689ed3",
        "id": 379335
    },
    {
        "content": "def get_res_string(res):\n    \"\"\"Converts resolution in bp to string (e.g. 10kb)\"\"\"\n    res_kb = res//1000\n    if res_kb < 1000:\n        return str(res_kb) + \"kb\"\n    else:\n        return str(int(res_kb/1000)) + \"mb\"",
        "sha1": "a2c68386a25dfd5ddde18ad843a8a86b87613eaf",
        "id": 544992
    },
    {
        "content": "def quantize_dequantize(quantization, tensor):\n    \"\"\"Simple helper to quantize and dequantize.\"\"\"\n    quantized_tensor, context = quantization.quantize(tensor)\n    dequantized_tensor = quantization.dequantize(quantized_tensor, context)\n    return quantized_tensor, dequantized_tensor, context",
        "sha1": "435e14a99b61c4cdb88dc7927280e0322e880ffc",
        "id": 629776
    },
    {
        "content": "def expand_qgrams(text, qsize, output):\n    \"\"\"Expands a text into a set of q-grams\n\n    :param text: Text\n    :type text: str\n    :param qsize: q-gram size\n    :type qsize: int\n    :param output: output\n    :type output: list\n\n    :returns: output\n    :rtype: list\n\n    Example:\n\n    >>> from microtc.textmodel import expand_qgrams\n    >>> output = list()\n    >>> expand_qgrams(\"Good morning.\", 3, output)\n    ['q:Goo', 'q:ood', 'q:od ', 'q:d m', 'q: mo', 'q:mor', 'q:orn', 'q:rni', 'q:nin', 'q:ing', 'q:ng.']\n    \"\"\"\n\n    _ = [\"\".join(a) for a in zip(*[text[i:] for i in range(qsize)])]\n    [output.append(\"q:\" + x) for x in _]\n    return output",
        "sha1": "9fea6d0726f31fb4e27de8249ed0153d273e65a6",
        "id": 502943
    },
    {
        "content": "def f_lin(x, a, b):\n    \"\"\"Fonction lin\u00e9aire : renvoie f(x) = a*x + b\n    \n    Parameters\n    ==========\n    x : float or ndarray\n    a : float\n        The line slope.\n    b : float\n        The origin ordinate.\n    \n    Returns\n    =======\n    f(x) = a*x + b : float or ndarray\n    \"\"\"\n    return a*x + b",
        "sha1": "a8921fad8f17784702fc1d67c80ba1eb5d80d7d2",
        "id": 87783
    },
    {
        "content": "import re\n\n\ndef parsefloat(string):\n    \"\"\" Robustly parse floats in files, such as:\n    '1.234(5)' = 1.234, note that it means 1.234+/-0.005\n    \"\"\"\n    string = float(re.sub(r'\\([^)]*\\)', '', string))\n    return float(string)",
        "sha1": "8a7093358aaec37fa198702753aa8bbe73823999",
        "id": 307859
    },
    {
        "content": "def convert_time(time):\n    \"\"\"convert time from hh:dd to minutes after 00:00\"\"\"\n    hours, minutes = time.split(\":\")\n    return int(hours) * 60 + int(minutes)",
        "sha1": "6aac13a8c35352e6b112a8554576baf62392449a",
        "id": 131858
    },
    {
        "content": "import torch\n\n\ndef prepare_sequence(seq, to_ix):\n    \"\"\"return indexes sequence of input word sequence\n\n    Args:\n        seq (list): word sequence\n        to_ix (dict): word to index map\n\n    Returns:\n        Tensor: indexes in Tensor format\n    \"\"\"\n    idxs = [to_ix[w] for w in seq]\n    return torch.tensor(idxs, dtype=torch.long)",
        "sha1": "da8276a4d54e15fa2f5cd9964abfa5483b9b5fd8",
        "id": 416348
    },
    {
        "content": "import torch\n\n\ndef get_valid_triplets_mask(labels):\n\t\"\"\"\n\tTo be valid, a triplet (a,p,n) has to satisfy:\n\t\t- a,p,n are distinct embeddings\n\t\t- a and p have the same label, while a and n have different label\n\t\"\"\"\n\tindices_equal = torch.eye(labels.size(0)).byte().cuda()\n\tindices_not_equal = ~indices_equal\n\ti_ne_j = indices_not_equal.unsqueeze(2)\n\ti_ne_k = indices_not_equal.unsqueeze(1)\n\tj_ne_k = indices_not_equal.unsqueeze(0)\n\tdistinct_indices = i_ne_j & i_ne_k & j_ne_k\n\n\tlabel_equal = torch.eq(labels.unsqueeze(1), labels.unsqueeze(0))\n\ti_eq_j = label_equal.unsqueeze(2)\n\ti_eq_k = label_equal.unsqueeze(1)\n\ti_ne_k = ~i_eq_k\n\tvalid_labels = i_eq_j & i_ne_k\n\n\tmask = distinct_indices & valid_labels\n\treturn mask",
        "sha1": "ae2cbf9667c864f8a3c51c5b90263dba4a6177e1",
        "id": 492437
    },
    {
        "content": "def hex_to_binary(hex_string: str) -> str:\n    \"\"\"Convert hexadecimal string to binary\n\n    Args:\n        hex_string (str): Hexadecimal string\n\n    Returns:\n        str: Binary string\n    \"\"\"\n    return f\"{int(hex_string, 16):0{len(hex_string * 4)}b}\"",
        "sha1": "3a442a394c2e0d08b7820eee721b2c6cfc007158",
        "id": 197735
    },
    {
        "content": "def normalize(val):\n    \"\"\" Normalize a string so that it can be used as an attribute\n    to a Python object \"\"\"\n    \n    if val.find('-') != -1:\n        val = val.replace('-','_')\n\n    return val",
        "sha1": "b299420edbff46751c2a5ab8fb4a17c13f6b71f4",
        "id": 346078
    },
    {
        "content": "import torch\n\n\ndef rotate_tensor(l: torch.Tensor, n: int = 1) -> torch.Tensor:\n    \"\"\"Roate tensor by n positions to the right\n\n    Args:\n        l (torch.Tensor): input tensor\n        n (int, optional): positions to rotate. Defaults to 1.\n\n    Returns:\n        torch.Tensor: rotated tensor\n    \"\"\"\n\n    return torch.cat((l[n:], l[:n]))",
        "sha1": "9cdaa7be718f0676ad85e05b01ee918459697c60",
        "id": 210
    },
    {
        "content": "def test_conflict_post(N):\n    \"\"\"A 1-SAT problem that requires N variables to all be true, and the first one to also be false\"\"\"\n    return [[-1]] + [[i+1] for i in range(N)]",
        "sha1": "73eb0aa03e28d65eb5278a29fae4e6a5e6b74774",
        "id": 160055
    },
    {
        "content": "import re\n\n\ndef clean(text):\n    \"\"\"Remove posting header, split by sentences and words, keep only letters\"\"\"\n    lines = re.split('[?!.:]\\s', re.sub('^.*Lines: \\d+', '', re.sub('\\n', ' ', text)))\n    return [re.sub('[^a-zA-Z]', ' ', line).lower().split() for line in lines]",
        "sha1": "bbef7189a97ddfa09b7a0bdd37b8147cbaeaa695",
        "id": 415212
    },
    {
        "content": "def expand_tensor(tensor, length):\n    \"\"\"\n    :param tensor: dim: N x M\n    :param length: l\n    :return: tensor of (N * l) x M with every row intercalated and extended l times\n    \"\"\"\n    rows, cols = tensor.size()\n    repeated = tensor.repeat(1, length)\n    return repeated.view(rows * length, cols)",
        "sha1": "5682f59bca4b93d6d0544c55afbe14de1f5e7876",
        "id": 256399
    },
    {
        "content": "def scale(x, min_value, max_value):\n    \"\"\"\n    scale x to values between 0 and 1\n    \"\"\"\n    domain = max_value - min_value\n    x = (x - min_value) / domain\n    return x",
        "sha1": "a2c1d6b95b5d621faaff533dafc8bc28c7a5e221",
        "id": 285803
    },
    {
        "content": "def should_retry_command(run_command_result):\n    \"\"\"Returns True when the command that produces the given output\n    should be retried.\n\n    Retry when:\n        - the error is a \"No route to host\" error;\n        - the error is a \"Connection refused\" error.\n\n    This method takes as input the output of the run_command() method:\n    (out, err, retcode).\n    \"\"\"\n    retry_error_messages = [\n        \"No route to host\",\n        \"Connection refused\",\n    ]\n    error = ''.join(run_command_result[1])\n    return any(\n        message in error for message in retry_error_messages)",
        "sha1": "ae16a056556eb51c3c2a536417d596a77c82ffec",
        "id": 92402
    },
    {
        "content": "def part2graph(parts):\n    \"\"\"Creates a graph (2d dict) that represents the partitioning\n\n       note: items in 'parts' should not be integers,\n             (usually strings or objects)\n    \"\"\"\n\n    # create graph\n    i = 0\n    vertices = {}\n    for part in parts:\n        for item in part:\n            vertices.setdefault(item, {})[i] = 1\n            vertices.setdefault(i, {})[item] = 1\n        i += 1\n    return vertices",
        "sha1": "387fd22b71c9badc96b889c4cda5ea8fe25d9ec9",
        "id": 140643
    },
    {
        "content": "import io\nimport base64\n\n\ndef _embed_img(display):\n    \"\"\"\n    Parameters\n    ----------\n    display: obj\n        A Nilearn plotting object to display\n\n    Returns\n    -------\n    embed : str\n        Binary image string\n    \"\"\"\n    if display is None:  # no image to display\n        return None\n\n    else:  # we were passed a matplotlib display\n        io_buffer = io.BytesIO()\n        display.frame_axes.figure.savefig(io_buffer, format='svg',\n                                          facecolor='white',\n                                          edgecolor='white')\n        display.close()\n\n        io_buffer.seek(0)\n        data = base64.b64encode(io_buffer.read())\n\n        return '{}'.format(data.decode())",
        "sha1": "1a5df607636e5f833db2ad87d123f4f62be23ba8",
        "id": 484596
    },
    {
        "content": "def iterate_cell(merged_cell):\n    \"\"\"Takes a cell and sum of neighbours and returns True/False if the cell is\n    alive/dead. \"\"\"\n    cell = bool(merged_cell & 16)\n    sum_of_neighbours = merged_cell - 16 if cell else merged_cell\n\n    if 2 <= sum_of_neighbours <= 3 and cell:\n        # if between 2 and 3 nbrs, keep alive\n        return True\n    elif not cell and sum_of_neighbours == 3:\n        # else if 3 nbrs, make alive\n        return True\n    elif sum_of_neighbours < 2:\n        # Else, if lonely, kill\n        return False\n    elif sum_of_neighbours > 3:\n        # Else, if overpopulated, kill\n        return False\n    else:\n        # No reason to keep alive\n        return False",
        "sha1": "28b03ce01fab7ba168cb7241152d717cda777017",
        "id": 296423
    },
    {
        "content": "def time2frame(time, hop_size=3072, win_len=4096) -> int:\n    \"\"\"\n    Takes a time position and outputs the best frame representing it.\n    The input must use the same unity of measure for ``time``, ``hop_size``,\n    and ``win_len`` (e.g. samples or seconds).  Indices start from 0.\n\n    Returns and int!\n    \"\"\"\n    return round((time - win_len / 2) / hop_size)",
        "sha1": "e25bdd8275865977985c69fb062369f967de5d27",
        "id": 238346
    },
    {
        "content": "def remap(value, old_min, old_max, new_min, new_max):\n    \"\"\"\n    Remaps the value to a new min and max value\n    Args:\n        value: value to remap\n        old_min: old min of range\n        old_max: old max of range\n        new_min: new min of range\n        new_max: new max of range\n\n    Returns:\n        The remapped value in the new range\n    \"\"\"\n    return new_min + (((value - old_min) / (old_max - old_min)) * (new_max - new_min))",
        "sha1": "847239ce37cad105fe381941a37448b00ffc3331",
        "id": 173143
    },
    {
        "content": "def overplot_lines(ax, linelabels, lineloc):\n    \"\"\"\n    Overplots emission lines on an already existing plot with axes\n    Input:\n        ax: matplolib axis\n        linelabels: list of latex formatted names for the lines\n        lineloc: list of wavelengths of the lines (most likely in Ang)\n    Output:\n        ax2: returns the new axis in case we want to overplot some more stuff on\n             top\n    \"\"\"\n    ax2 = ax.twiny()\n\n    for xc in lineloc:\n        ax.axvline(x=xc, color=\"0.8\", linestyle=\"--\")\n\n    ax.set_zorder(ax2.get_zorder() + 1)  # put ax in front of ax2\n    ax.patch.set_visible(False)  # hide the 'canvas'\n    ax2.patch.set_visible(True)\n    ax2.axes.get_xaxis().set_ticks(lineloc)\n    ax2.axes.xaxis.set_ticklabels(linelabels)\n    ax2.set_xlim(ax.get_xlim())\n    ax2.xaxis.set_label_position(\"top\")\n    ax2.set_xticklabels(ax2.xaxis.get_majorticklabels(), rotation=90)\n    return ax2",
        "sha1": "d82f24ad9366defafc11e2d3ac7739659dcf1a04",
        "id": 655583
    },
    {
        "content": "def identity(p):\n    \"\"\"Identity fnc, for flows that don't accept lambda (pickling etc).\n\n    Parameters\n    ----------\n    p : object\n        Input parameter.\n\n    Returns\n    -------\n    object\n        Same as `p`.\n\n    \"\"\"\n    return p",
        "sha1": "b4de580fea21ba9ae4f83bcace849503e9b49d54",
        "id": 480251
    },
    {
        "content": "from typing import Union\n\n\ndef all_lower(o: Union[str, tuple, list, set, dict]) -> Union[str, tuple, list, set, dict]:\n    \"\"\"\n    Lower the letter cases of all elements in ``obj``.\n\n    Will **NOT** modify the ``o`` itself.\n\n    Do the following corresponding to its type:\n\n    :class:`str`\n        > return the lower case of :class:`str`.\n    :class:`tuple`, :class:`list`, :class:`set`\n        > return the corresponding data structure\n        which every element with a :class:`str` content will be lowered.\n    :class:`dict`\n        > return lowered case of data which is the value of a pair.\n    (Not matching the above)\n        > return the original ``o``\n\n    :param o: object to lower the letter case\n    :returns: an object with the same shape of `obj` but the letter case of the string elements are lowered\n    \"\"\"\n    if isinstance(o, str):\n        return o.lower()\n\n    if isinstance(o, (tuple, list, set)):\n        org_type = type(o)\n        tmp = list(o)\n\n        for idx, oo in enumerate(o):\n            tmp[idx] = all_lower(oo)\n\n        return org_type(tmp)\n\n    if isinstance(o, dict):\n        tmp = o.copy()\n        for k, v in tmp.items():\n            tmp[k] = all_lower(v)\n\n        return tmp\n\n    return o",
        "sha1": "1970172333879ee6dfa3b6abffc5741f42023049",
        "id": 589229
    },
    {
        "content": "def remove_formatting(ingredients_series):\n    \"\"\"Remove text formatting and non alphabetic characters,\n    replace . with ,\"\"\"\n    return (\n        ingredients_series.str.replace(\"<strong>\", \"\", regex=False)\n        .str.replace(\"</strong>\", \"\", regex=False)\n        .str.replace(\" \\(.*?\\)\", \"\", regex=True)\n        .str.replace(\" \\<.*?\\>\", \"\", regex=True)\n        .str.replace(\" \\{.*?\\}\", \"\", regex=True)\n        .str.replace(\" \\[.*?\\]\", \"\", regex=True)\n        .str.replace(\"[^a-zA-Z\\s,:-]+\", \"\", regex=True)\n        .str.replace(\".\", \",\", regex=False)\n    )",
        "sha1": "b60d56385eadf77b5174af9809651728a46310ee",
        "id": 661178
    },
    {
        "content": "def get_maintainers_account_creation_date(pypi_profiles):\n    \"\"\"Retrieve dates that maintainers' PyPI accounts were created\"\"\"\n\n    dates = []\n    # Loop through beautiful soup-ified maintainer data to extract dates\n    for soup in pypi_profiles[\"maintainers_data\"]:\n        # Because 'time' elements will appear in multiple locations on\n        # a PyPI maintainer profile page, filter in only those html\n        # tags associated with author metadata\n        author_metadata_elements = soup.findAll(\n            \"div\", {\"class\": \"author-profile__metadiv\"}\n        )\n        for elem in author_metadata_elements:\n            # Extract any time-related elements and add to dates list\n            # if it exists\n            date = elem.find(\"time\")\n            if date:\n                # The [0] slice is because contents is a list and\n                # strip() is not a valid method for lists\n                dates.append(date.contents[0].strip())\n\n    return dates",
        "sha1": "06de38d26db877ff0c3ad8bcc197744cac31f3de",
        "id": 385410
    },
    {
        "content": "def is_code_line(line):\n    \"\"\"A code line is a non empty line that don't start with #\"\"\"\n    return line.strip()[:1] not in {'#', '%', ''}",
        "sha1": "558a37d8a30f20d75f5dcc9b72e82f5981e6f84b",
        "id": 291996
    },
    {
        "content": "from functools import reduce\n\n\ndef combine_play_stats(games):\n    \"\"\"\n    Combines a list of games into one big player sequence containing play\n    level statistics.\n\n    This can be used, for example, to get PlayPlayerStats objects corresponding\n    to statistics across an entire week, some number of weeks or an entire\n    season.\n\n    This function should be used in lieu of combine_game_stats when more\n    detailed statistics such as receiver targets, yards after the catch and\n    punt/FG blocks are needed.\n\n    N.B. Since this combines *all* play data, this function may take a while\n    to complete depending on the number of games passed in.\n    \"\"\"\n    if not games:\n        return []\n    return reduce(lambda p1, p2: p1 + p2,\n                  [g.drives.players() for g in games if g is not None])",
        "sha1": "fd9859edeb4f4c7eb9a6db9859ff523a17760f0d",
        "id": 591480
    },
    {
        "content": "def merge_sort(arr):\n    \"\"\"\n    merge sort\n    Time: O(nlog(n))\n    Space: O(n)\n    \"\"\"\n    if len(arr) <= 1:\n        return arr\n\n    mid = len(arr) // 2\n    left = merge_sort(arr[:mid])\n    right = merge_sort(arr[mid:])\n\n    # Merge the sorted lists into a new one\n    i = j = k = 0\n    while i < len(left) and j < len(right):\n        if left[i] < right[j]:\n            arr[k] = left[i]\n            i += 1\n        else:\n            arr[k] = right[j]\n            j += 1\n        k += 1\n\n    # Checking if any element was left\n    while i < len(left):\n        arr[k] = left[i]\n        i += 1\n        k += 1\n\n    while j < len(right):\n        arr[k] = right[j]\n        j += 1\n        k += 1\n    return arr",
        "sha1": "0ead74af71350d1f3383b53d532674a733fdb747",
        "id": 122827
    },
    {
        "content": "from typing import Union\nfrom typing import TextIO\nfrom typing import List\nimport json\n\n\ndef load_jsonl(fstream: Union[TextIO, str]) -> List[dict]:\n    \"\"\"\n    Parses a JSONL file into a list of objects.\n\n    Args:\n        fstream: a filename or `TextIO` handle.\n\n    Returns:\n        A list of objects parsed from the file\n    \"\"\"\n    if isinstance(fstream, str):\n        with open(fstream) as fstream_:\n            return load_jsonl(fstream_)\n\n    return [json.loads(line) for line in fstream]",
        "sha1": "9d40329564b71fe2a0209b849b05b6566457d5d2",
        "id": 237028
    },
    {
        "content": "def get_euler_solution(derivative, numsteps, upper, initial):\n    \"\"\"\n    Returns an explicit Euler's method solution to a given differential equation,\n    called `derivative`, of the form dy/dx = f(x, y)\n    Can operate on numpy vectors\n    Runs with `numsteps` iterations, from x = 0 to x = `upper`\n    Uses `initial` as the initial value for y\n    \"\"\"\n    inputs = [0]\n    outputs = [initial]\n    delta = float(upper) / numsteps\n    for i in range(1, numsteps):\n        inputs.append(inputs[i-1] + delta)\n        outputs.append(outputs[i-1] + delta * derivative(inputs[i-1], outputs[i-1]))\n    return inputs, outputs",
        "sha1": "e00c0411c854f31d71bc552f0361ead876a282f0",
        "id": 584542
    },
    {
        "content": "def owner_group(owner_id):\n    \"\"\" String ACL representation of owner permission.\n    \"\"\"\n    return 'owner:{}'.format(owner_id)",
        "sha1": "5dd8179e1f7321f97f2af76b56589016add50d76",
        "id": 527367
    },
    {
        "content": "import pathlib\n\n\ndef get_file(file: pathlib.Path) -> str:\n    \"\"\"Extract all lines from a file.\"\"\"\n    with open(file, \"r\") as f:\n        return f.read()",
        "sha1": "86f7a57b5b5394e82082a11f69a37514f39e0e71",
        "id": 600925
    },
    {
        "content": "def from_disk(session, schema, path, depth=0, extension=None):\n    \"\"\"Return a DataFrame object from files read.\n\n    Arguments:\n    session -- SparkSession object \n    schema  -- data types for each column\n    path    -- location where files are stored\n    \n    Keyword arguments:\n    depth     -- depth of directory tree underneth path to data files\n    extension -- file extension to read (json or parquet)\"\"\"\n\n    \n    depth = depth if depth > 0 else 1\n    wild_card_path = path + '/'.join(['*'for _ in range(depth)]) + '.' + extension\n    \n    df = None\n    if extension == 'json':\n        df = session.read.json(\n            path=wild_card_path, \n            schema=schema, \n            multiLine=True,\n            encoding='UTF-8',\n            mode='DROPMALFORMED')\n    elif extension == 'parquet':\n        df = session.read.parquet(wild_card_path)\n    else:\n        print(f'ERROR: {extension} files are not supported')\n        \n    return df",
        "sha1": "3e2607f2351aa31e57583ad4466b9c985dd28dd3",
        "id": 102756
    },
    {
        "content": "def mean_additive_bias(da_cmp, da_ref, over_dims):\n    \"\"\" \n        Returns the additive bias between comparison and reference datasets\n        Author: Dougie Squire\n        Date: 28/04/2018\n        \n        Parameters\n        ----------\n        da_cmp : xarray DataArray\n            Array containing data to be compared to reference dataset (usually forecasts)\n        da_ref : xarray DataArray\n            Array containing reference data (usually observations)\n        over_dims : str or sequence of str, optional\n            Dimensions over which to compute the mean additive bias\n            \n        Returns\n        -------\n        mean_additive_bias : xarray DataArray\n            Array containing the mean additive biases\n            \n        Examples\n        --------\n        >>> da_cmp = xr.DataArray(np.random.normal(size=(3,3)), \n        ...                       coords=[('x', np.arange(3)), ('y', np.arange(3))])\n        >>> da_ref = xr.DataArray(np.random.normal(size=(3,3)), \n        ...                       coords=[('x', np.arange(3)), ('y', np.arange(3))])\n        >>> doppyo.skill.mean_additive_bias(da_cmp, da_ref, over_dims='x')\n        <xarray.DataArray 'mean_additive_bias' (y: 3)>\n        array([0.328462, 0.172263, 0.402438])\n        Coordinates:\n          * y        (y) int64 0 1 2\n        \n        Notes\n        -----\n        See http://www.cawcr.gov.au/projects/verification/\n    \"\"\"\n    \n    if isinstance(over_dims, str):\n        over_dims = [over_dims]\n        \n    if over_dims == None:\n        over_dims = []\n\n    return (da_cmp - da_ref).mean(dim=over_dims, skipna=True) \\\n                            .rename('mean_additive_bias')",
        "sha1": "520bc098f909fc0dbb100bb2b5aaa9dc9df3eb2a",
        "id": 209161
    },
    {
        "content": "def ode(x,r,xc):    \n    \"\"\"\n    Compute the slope of a coordinate in an circle \n    Params:\n        x = the coordinate of desire slop \n        r = radius of the slope \n        xc = x coordinate of the center \n        \n    return: \n        slop of at the coordinate provided \n    \"\"\"\n    return -(r**2 - (x - xc)**2)**(-0.5)*(-1.0*x + 1.0*xc)\n    #%%",
        "sha1": "c9fa6dd67eb9ad3884a23e9a338fd24f34ead2e4",
        "id": 302977
    },
    {
        "content": "import math\n\n\ndef distance(x, y):\n    \"\"\"\n    Compute the distance between two songs.\n\n    Params:\n        - x: First song dict\n        - y: Second song dict\n    Returns: The cartesian distance between the two songs.\n    \"\"\"\n    return math.sqrt(\n        (x[\"tempo\"] - y[\"tempo\"])**2 +\n        (x[\"amplitude\"] - y[\"amplitude\"])**2 +\n        (x[\"frequency\"] - y[\"frequency\"])**2 +\n        (x[\"attack\"] - y[\"attack\"])**2\n    )",
        "sha1": "c441ad68de09d21de1c1308bad4d9188f1a08b51",
        "id": 382474
    },
    {
        "content": "def _get_all_row_labels(rangeA1Notation):\n    \"\"\"\n    This function returns a list which gives all the row labels present in the table\n\n    Args:\n        rangeA1Notation : Type-str\n                          Denotes the data range selected by user. \n                          Example : \"A1:C18\" , \"AA1:CC33\"\n    Returns:\n        List of all column labels where data is present.\n        Example : [1,2,3,4,5,...,17,18]\n    \"\"\"\n\n    starting_row = \"\"\n    ending_row = \"\"\n\n    starting_row_filled = False\n\n    for c in rangeA1Notation: \n        if c.isnumeric():\n            if not starting_row_filled :\n                starting_row += c\n            else :\n                ending_row += c\n        elif c == ':' :\n            starting_row_filled = True\n\n    list_of_row_labels = []\n\n    for label_number in range(int(starting_row) + 1 , int(ending_row) + 1):\n        list_of_row_labels.append(label_number) \n\n    return list_of_row_labels",
        "sha1": "423a5623c3ace0cecd39c5515c29d6cb29c5bcea",
        "id": 243712
    },
    {
        "content": "def compare(predicted_data, real_data):\n    \"\"\"Compare predicted image with real image.\n\n    Args:\n        predicted_data: numpy array, int32 - [height, width].\n            Array of the prediction.\n        real_data: numpy array, int32 - [height, width].\n            Array of the real.\n\n    Returns:\n        result: float32.\n            Similarity of the images.\n    \"\"\"\n    height, width = predicted_data.shape[:2]\n\n    match_pixels = (predicted_data == real_data).flatten()\n    result = match_pixels[match_pixels]\n    result = 100.0 * result.shape[0] / (height * width)\n    return result",
        "sha1": "1dfef1199253aec8fde28566b8f824bb5056390f",
        "id": 434274
    },
    {
        "content": "import time\n\n\ndef validate_counter(counter):\n    \"\"\"\n    Validates a counter ensuring it's in a sliding window.\n    Window is +/- 12 hours (43200 seconds)\n    \"\"\"\n    currentTime = int(time.time())\n    return (currentTime-43200) <= counter <= (currentTime+43200)",
        "sha1": "7f7866c3f1f038bbe87ffdce4f05bb385f72d030",
        "id": 82009
    },
    {
        "content": "import torch\n\n\ndef _batch_linspace(start, end, steps):\n    \"\"\"Linspace with a batch dimension.\n    start and end are 1D tensors, steps is a scalar.\n    Returns a 2D tensor dim(steps) x dim(start).\"\"\"\n    t = torch.linspace(0, 1, steps).unsqueeze(1)\n    return (1-t)*start.unsqueeze(0) + t*end.unsqueeze(0)",
        "sha1": "22070a06fb613a9edc3b716b48f7e4b1d5600ccd",
        "id": 296344
    },
    {
        "content": "def distance(point1, point2):\n    \"\"\" calc distance between two 3d points \"\"\"\n    return ((point1[0] - point2[0]) ** 2 +\n            (point1[1] - point2[1]) ** 2 +\n            (point1[2] - point2[2]) ** 2) ** 0.5",
        "sha1": "adb811013fec6f486e64f22622c893f3fc58523e",
        "id": 168970
    },
    {
        "content": "def from_string(key: str, ghs_dict: dict) -> int:\n    \"\"\"Get status value by key from dictionary.\"\"\"\n\n    for string_val, return_val in ghs_dict.items():\n        if key == string_val:\n            return return_val\n    return 0",
        "sha1": "0ada609f6324702ee8bafc1f0d0acaf94ef68c11",
        "id": 574804
    },
    {
        "content": "def expected_future_worth(x, percent_return=0.07, years=20, annual_payment=None):\n    \"\"\"\n    estimate the future value of a current investment given the percent rate of return,\n    number of years, and optional annual payment amount\n\n    :param x: the present value of your account \n    :param percent_return: the market historically delivers a typical 7 percent annually\n    :param years: the number of years over which to compound interest\n    :param annual_payment: optional constant annual contribution to account\n    :return type: float\n    \"\"\"\n\n    i = percent_return\n    n = years\n\n    f = x * (1 + i) ** n\n\n    if annual_payment is not None:\n        f += (annual_payment / i) * (((1 + i) ** n) - 1)\n    else:\n        annual_payment = 0\n\n    print('\\n'.join(['', 'With annual contribution of ${0:,.2f} and', \n                     '\\t{1:.2}% rate of return,',\n                     '\\texpected account value in {2} years: ${3:,.2f}',\n                     '']).format(annual_payment, i*100, n, f))\n\n    return round(f, 2)",
        "sha1": "1cea7263a3a890d401a9e1c736ffbc7755edcece",
        "id": 110598
    },
    {
        "content": "from email.generator import Generator\nfrom io import StringIO\n\n\ndef flatten_message(msg):\n    \"\"\"Returns the message flattened to a string, for use in writing to a file.  NOTE:\n    use this instead of message.as_string() to avoid mangling message.\n    \"\"\"\n    fp = StringIO()\n    g = Generator(fp, mangle_from_=False)\n    g.flatten(msg)\n    return fp.getvalue()",
        "sha1": "54bb30ea6f2d6931aeb9d2c16eff604d2d9ad78a",
        "id": 548792
    },
    {
        "content": "def create_float_data_func(attribute='text', fmt=\"%.5f\", invalid=\"#NA#\"):\n    \"\"\"\n        Creates a data function that can be used to render floats as formatted\n        strings, with detection of invalid values (e.g. None)\n    \"\"\"\n    def float_renderer(column, cell, model, itr, args=None):\n        nr = model.get_value(itr, column.get_col_attr(attribute))\n        try:\n            cell.set_property('text', fmt % nr)\n        except:\n            cell.set_property('text', invalid)\n    return float_renderer",
        "sha1": "4fe5ce943710eb8aef33d314055445db9592bbb3",
        "id": 210159
    },
    {
        "content": "import shlex\n\n\ndef split_string(s):\n    \"\"\"split string to list\"\"\"\n    if s is None:\n        return []\n    else:\n        return shlex.split(s)",
        "sha1": "e9cbd0c1928d16c673d70cc9aeffc0275de43f30",
        "id": 23892
    },
    {
        "content": "def parse_element_id(element):\n    \"\"\"Extract the id number from the link.\"\"\"\n    return int(element[\"href\"].split(\"/\")[2].split(\"-\")[1])",
        "sha1": "978bb2d88e98b6d9fad945b7f73a4c89b552c172",
        "id": 289679
    },
    {
        "content": "def _spd_getitem_ ( s ,  i ) :\n    \"\"\"Get item form the pair:\n    >>> p = ...\n    >>> p[0], p[1]\n    \"\"\"\n    if   0 == i : return s.first \n    elif 1 == i : return s.second\n    raise IndexError('Invalid index %s' % i )",
        "sha1": "a690040c60a63cd55a81f4d54aaec3cf60a9df3f",
        "id": 310037
    },
    {
        "content": "from typing import List\n\n\ndef _get_edge_indices(edges: int) -> List[int]:\n    \"\"\"\n    Finds which edge numbers are intersected given the bit representation\n    detailed in marching_cubes_data.EDGE_TABLE.\n\n    Args:\n        edges: an integer corresponding to the value at cube_index\n            from the EDGE_TABLE in marching_cubes_data.py\n\n    Returns:\n        edge_indices: A list of edge indices\n    \"\"\"\n    if edges == 0:\n        return []\n\n    edge_indices = []\n    for i in range(12):\n        if edges & (2**i):\n            edge_indices.append(i)\n    return edge_indices",
        "sha1": "946b9c02aa2c8266e145a5199f156632d9470497",
        "id": 152306
    },
    {
        "content": "import hashlib\n\n\ndef hash_document(document):\n    \"\"\"Returns hash of document\"\"\"\n    return hashlib.sha1(document).hexdigest()",
        "sha1": "5fa682f9d2e1e5bc2be6249c65c415ee20e868ce",
        "id": 83409
    },
    {
        "content": "import base64\nimport json\n\n\ndef decode_token(token):\n    \"\"\"\n    Decodes an \"opaque\" token string to a dictionary.\n    \"\"\"\n    json_string = base64.b64decode(token.encode('utf-8')).decode('utf-8')\n    return json.loads(json_string)",
        "sha1": "786008eac513abbdb36bebb196a2935b5c85e863",
        "id": 230356
    },
    {
        "content": "def get_transitions(sequence):\n\t\"\"\"\n\tExtracts a list of transitions from a sequence, returning a list of lists containing each transition.\n\n\tExample\n\t--------\n\t>>> sequence = [1,2,2,1,2,3,2,3,1]\n\t>>> ps.get_transitions(sequence)\n\t[[1, 2], [2, 1], [1, 2], [2, 3], [3, 2], [2, 3], [3, 1]]\n\n\t\"\"\"\n\n\ttransitions = []\n\tfor position in range(len(sequence) - 1):\n\t\tif sequence[position] != sequence[position + 1]:\n\t\t\ttransitions.append([sequence[position], sequence[position + 1]])\n\n\treturn transitions",
        "sha1": "25c2e7de0f4701517c1f41f466a3710a7f124c4d",
        "id": 25240
    },
    {
        "content": "def gnome_sort(arr):\n    \"\"\"\n    Gnome Sort also called Stupid sort is based on the concept of a Garden Gnome sorting his flower pots.\n    A garden gnome sorts the flower pots by the following method-\n\n    He looks at the flower pot next to him and the previous one;\n    if they are in the right order he steps one pot forward, otherwise he swaps them and steps one pot backwards.\n    If there is no previous pot (he is at the starting of the pot line), he steps forwards;\n    if there is no pot next to him (he is at the end of the pot line), he is done.\n\n    This is an in-place sort.\n\n    :param arr: the array of values to sort\n    :return: the sorted array, which is the same reference as arr\n    \"\"\"\n    index = 0\n    while index < len(arr):\n        if index == 0:\n            index = index + 1\n        elif arr[index] >= arr[index - 1]:\n            index = index + 1\n        else:\n            arr[index], arr[index - 1] = arr[index - 1], arr[index]\n            index = index - 1\n\n    return arr",
        "sha1": "7212415f0c133e03dfdeb6d9b30d34c0edcdaff6",
        "id": 94721
    },
    {
        "content": "def parse_env_urls(urls=None):    \n    \"\"\"Parses a list of urls\n    >>> parse_env_urls(urls='https://kibana.energy.svc.dbg.com | https://grafana.energy.svc.dbg.com')\n    ['https://kibana.energy.svc.dbg.com', 'https://grafana.energy.svc.dbg.com']\n    \"\"\"\n\n    urls_list = [] if urls == None else urls.split('|')\n    urls_list_stripped = list(map(str.strip, urls_list))\n\n    return urls_list_stripped",
        "sha1": "518a2ae449c037cc63697b492fcfbf4a35123bd6",
        "id": 93919
    },
    {
        "content": "def fdtfile2fieldstring(fdtfile):\n    \"\"\"Make a string of field definitions separated by % (percent sign)\n    reading from a fdtfile or just take the string and bring into shape:\n\n        e.g.  1,AA,20,A,NC,NN%1,F4,4,F,NU,DT=E(UNIXTIME)\n\n    :param fdtfile:  fdtfile or field string\n\n    :return: tuple (number of fields, field string)\n\n    \"\"\"\n    if ',' in fdtfile:      # fdt or file name\n        fields=fdtfile      # fdt field defn contains commas\n                            # file name has no comma\n        lines=[line.split(';')[0].rstrip() for line in fields.split('%')]\n    else:\n        f=open(fdtfile)\n        lines=[]\n        for line in f.readlines():\n            # drop CRLF, take string before ';', cut off trailing blanks\n            lines.append(line[:-1].split(';')[0].rstrip())\n        f.close()\n\n    fields='%'.join(lines)\n    numfields=len(lines)\n\n    return numfields,fields",
        "sha1": "dc292c7c0f7d5e8c6c14c5b108dfd28bad13a640",
        "id": 631195
    },
    {
        "content": "def connect_points(pts1, pts2):\n    \"\"\"\n    Connects each point in the first list with all points in the second.\n    If the first list has N points and the second has M, the result are 2 lists\n    with N*M points each, representing the connections.\n\n    Parameters:\n\n    * pts1 : list\n        List of (x, y) coordinates of the points.\n    * pts2 : list\n        List of (x, y) coordinates of the points.\n\n    Returns:\n\n    * results : lists of lists = [connect1, connect2]\n        2 lists with the connected points\n\n    \"\"\"\n    connect1 = []\n    append1 = connect1.append\n    connect2 = []\n    append2 = connect2.append\n    for p1 in pts1:\n        for p2 in pts2:\n            append1(p1)\n            append2(p2)\n    return [connect1, connect2]",
        "sha1": "df1e89fda7cee228db31bfc14a5a5c50459f3e6e",
        "id": 614356
    },
    {
        "content": "def first(seq, pred=None):\n    \"\"\"Return the first item in seq for which the predicate is true.\n    \n    If the predicate is None, return the first item regardless of value.\n\n    If no items satisfy the predicate, return None.\n    \"\"\"\n    if pred is None:\n        pred = lambda x: True\n    for item in seq:\n        if pred(item):\n            return item\n    return None",
        "sha1": "f545ab4deb8c6d8103dd46dc85e0afd1f2597c6e",
        "id": 678333
    },
    {
        "content": "import torch\n\n\ndef box_transform_inv(box, deltas):\n    \"\"\"\n    apply deltas to box to generate predicted boxes\n\n    Arguments:\n    box -- tensor of shape (N, 4), boxes, (c_x, c_y, w, h)\n    deltas -- tensor of shape (N, 4), deltas, (\u03c3(t_x), \u03c3(t_y), exp(t_w), exp(t_h))\n\n    Returns:\n    pred_box -- tensor of shape (N, 4), predicted boxes, (c_x, c_y, w, h)\n    \"\"\"\n\n    c_x = box[:, 0] + deltas[:, 0]\n    c_y = box[:, 1] + deltas[:, 1]\n    w = box[:, 2] * deltas[:, 2]\n    h = box[:, 3] * deltas[:, 3]\n\n    c_x = c_x.view(-1, 1)\n    c_y = c_y.view(-1, 1)\n    w = w.view(-1, 1)\n    h = h.view(-1, 1)\n\n    pred_box = torch.cat([c_x, c_y, w, h], dim=-1)\n    return pred_box",
        "sha1": "c2c0abe3ddd6ff68e32b5e194a22454883085fcc",
        "id": 59549
    },
    {
        "content": "def _findProteinClusters(protToPeps, pepToProts):\n    \"\"\"Find protein clusters in the specified protein to peptide mappings.\n\n    A protein cluster is a group of proteins that are somehow directly or\n    indirectly connected by shared peptides.\n\n    :param protToPeps: dict, for each protein (=key) contains a set of\n        associated peptides (=value). For Example {protein: {peptide, ...}, ...}\n    :param pepToProts: dict, for each peptide (=key) contains a set of parent\n        proteins (=value). For Example {peptide: {protein, ...}, ...}\n    :returns: a list of protein clusters, each cluster is a set of proteins\n    \"\"\"\n    clusters = list()\n    resolvingProteins = set(protToPeps)\n    while resolvingProteins:\n        protein = resolvingProteins.pop()\n        proteinCluster = set([protein])\n\n        peptides = set(protToPeps[protein])\n        parsedPeptides = set()\n\n        while len(peptides) != len(parsedPeptides):\n            for peptide in peptides:\n                proteinCluster.update(pepToProts[peptide])\n            parsedPeptides.update(peptides)\n\n            for protein in proteinCluster:\n                peptides.update(protToPeps[protein])\n        clusters.append(proteinCluster)\n        resolvingProteins = resolvingProteins.difference(proteinCluster)\n    return clusters",
        "sha1": "88675372cfddf93446c647b11fb80c992b0548c3",
        "id": 178826
    },
    {
        "content": "from typing import Union\n\n\ndef string_encode(value: Union[float, int, str]) -> bytes:\n    \"\"\"Encode a value.\n\n    :param value: The value to encode.\n    :return: The encoded value\n\n    \"\"\"\n    return str(value).encode(\"utf-8\")",
        "sha1": "ac450c39f7752a7ad607417f3d7701e1d825c42b",
        "id": 282550
    },
    {
        "content": "def GetRemoveLabelsListFromArgs(args):\n  \"\"\"Returns the remove labels list from the parsed args.\n\n  Args:\n    args: The parsed args.\n\n  Returns:\n    The remove labels list from the parsed args.\n  \"\"\"\n  return args.remove_labels",
        "sha1": "d8b2209f289180c500198be73f4ef9469cbd0c06",
        "id": 74698
    },
    {
        "content": "import torch\nfrom typing import Counter\n\n\ndef categories_to_block_matrix(category_per_sentence):\n    \"\"\"\n    Create a block matrix where each block entry is 1 and everything else is 0.\n    Imagine this but where each block has value 1.0: https://i.ytimg.com/vi/a60r50XvtVo/maxresdefault.jpg\n\n    Example input:\n        category_per_sentence = [0, 0, 1, 1, 1, 2]  # categories' id must be sorted like this\n    Example output for this input:\n        tensor([[1., 1., 0., 0., 0., 0.],\n                [1., 1., 0., 0., 0., 0.],\n                [0., 0., 1., 1., 1., 0.],\n                [0., 0., 1., 1., 1., 0.],\n                [0., 0., 1., 1., 1., 0.],\n                [0., 0., 0., 0., 0., 1.]])\n\n    :param category_per_sentence: the list of categories id of each example. If counting occurences, we get the length of each block.\n    :return: the corresponding block matrix, of shape [len(category_per_sentence), len(category_per_sentence)].\n    \"\"\"\n    tmp = list(sorted(category_per_sentence))\n    assert (tmp == category_per_sentence), \"Categories must be in sorted order (increasing category id)\"\n    category_per_sentence = tmp\n    size = len(category_per_sentence)\n    block_matrix = torch.zeros((size, size))\n\n    c = Counter(category_per_sentence)\n    block_sizes = [val for key, val in sorted(list(c.items()))]\n    a = 0\n    for size in block_sizes:\n        b = a + size\n        block_matrix[a:b, a:b] = 1.0\n        a = b\n\n    return block_matrix",
        "sha1": "fb22e34ada885650ff679aded13f7ef2831ac488",
        "id": 199936
    },
    {
        "content": "def intersection_list(*lists) -> list:\n    \"\"\"\n    Common elements in all given lists.\n    \"\"\"\n    l = list(set.intersection(*[set(lst) for lst in lists]))\n    l.sort()\n    return l",
        "sha1": "696713944f0a13c7d6575d9a5e71b88ae8e428c7",
        "id": 333812
    },
    {
        "content": "def banner(text: str, *, borderChar: str = '='):\n    \"\"\"Print 'text' as banner, optionally customise 'borderChar'.\"\"\"\n\n    border = borderChar * len(text)\n    return '\\n'.join([border, text, border])",
        "sha1": "76d27b762173e35a15e0e445eccea85cdef3b327",
        "id": 701922
    },
    {
        "content": "def find_center_by_center_of_image(data, verbose=False, **kwargs):\n    \"\"\"\n    Find image center simply from its dimensions.\n    \"\"\"\n    return (data.shape[1] // 2, data.shape[0] // 2)",
        "sha1": "681ead53548686293774dc416ae0c1b1bc2fb3a3",
        "id": 478287
    },
    {
        "content": "def _swift_bool(bstring):\n    \"\"\"\n    Convert a SWIFT-VOevent style boolean string ('true'/'false') to a bool.\n    \"\"\"\n    if bstring == 'true':\n        return True\n    elif bstring == 'false':\n        return False\n    else:\n        raise ValueError(\"This string does not appear to be a SWIFT VOEvent \"\n                          \"boolean: %s\" % bstring)",
        "sha1": "0b759846a858f625c1e1a0931261a5f66094ec82",
        "id": 380922
    },
    {
        "content": "def trloadchan(dbin, t0, t1, sta, chan):\n    \"\"\"Load time-series data for a given station, channel, and time-interval into memory\"\"\"\n\n    return dbin.loadchan(t0, t1, sta, chan)",
        "sha1": "63ec20feed91577d278f03c1f6ba2cc10a61afd3",
        "id": 585469
    },
    {
        "content": "def print_month(month: int) -> str:\n    \"\"\"Returns `month` in markdown format\"\"\"\n    return f\"### {month} \uc6d4\"",
        "sha1": "23d0d82d2532efc700965cce42e96ea749cb3465",
        "id": 69761
    },
    {
        "content": "def main(digits):\n    \"\"\"\n        Find the index of the first term in the Fibonacci sequence to contain the\n    given number of digits.\n\n    :param digits: Number of digits that the requested term should contains.\n    :rtype: int\n    \"\"\"\n    # The number of digits of the Fibonacci terms is increased according to the\n    # following sequences, where the values in the list are the number of terms\n    # with the specified number of digits (=index), starting from 1-digit -> 1th term.\n    initial = [0, 6, 5, 5]\n    # .................................after the first 4-digit term, the pattern\n    # below is repeated continuously\n    pattern = [4, 5, 5, 5, 4, 5, 5, 5, 5, 4, 5, 5, 5, 5]\n    length = 14  # ....................instead of: len(pattern)\n\n    if digits < 5:\n        return sum(initial[:digits]) + 1\n\n    digits -= 4\n    index = ((digits // length) * sum(pattern)) + sum(pattern[:(digits % length)])\n    return index + 16",
        "sha1": "274176089dd1accdd0b3b2356787fed100d4aab3",
        "id": 55125
    },
    {
        "content": "def get_spark_df(spark, data, schema):\n    \"\"\"\n    This function returns a spark data frame\n    :param spark: Spark session\n    :param data: data fields\n    :param schema: schema of the data fields\n    :return: spark data frame\n    \"\"\"\n    df = spark.createDataFrame(data=data, schema=schema)\n    return df",
        "sha1": "ee044f060f299b14dfa0242dc20f270a2223304d",
        "id": 388760
    },
    {
        "content": "def _parse_disallowed_patterns(disallowed_patterns):\n    \"\"\" Parse disallowed patterns from flake8 options.\n\n    Parameters\n    ----------\n    disallowed_patterns : str\n        Configuration that represents a pairing of filename pattern\n        and regular expression for disallowed import.\n        Multiple items should be separated by newlines.\n\n    Returns\n    -------\n    results : list of tuple(str, str)\n        The first item in the tuple is the pattern for matching files.\n        The second item in the tuple is the pattern for matching disallowed\n        imports.\n    \"\"\"\n    results = []\n    disallowed_patterns = disallowed_patterns.replace(\" \", \"\")\n    for rule in disallowed_patterns.splitlines():\n        if not rule:\n            continue\n        file_pattern, disallowed = rule.split(\":\")\n        results.append((file_pattern.strip(), disallowed.strip()))\n    return results",
        "sha1": "b5fec00857db6f0eff397c5f1cda51c9150e81e3",
        "id": 251671
    },
    {
        "content": "def str_input(prompt=\"\", max_length=0):\n    \"\"\"Uses `input(prompt)` to request a str value from the user, retrying if the user doesn't enter anything or\n    only enters whitespace.\n\n     @param str prompt: The prompt to display.\n     @param int max_length: The maximum length of the string. Defaults to no length limit.\n     @return str: The entered value.\n    \"\"\"\n\n    while True:\n        string = input(prompt)\n        if not len(string.strip()):\n            continue\n        if max_length != 0 and len(string) > max_length:\n            print(\"Your text is too long. It must not exceed a length of %i characters.\" % max_length)\n        return string",
        "sha1": "30a8579e82220b96e530d5b0142f4e7028b547f4",
        "id": 51469
    },
    {
        "content": "def intervals_frac_overlap(iv1, iv2):\n    \"\"\"\n    Given two intervals, calculates the\n    fraction of iv1 covered by iv2\n    \"\"\"\n    return iv1.overlap_size(iv2.begin, iv2.end) / iv1.length()",
        "sha1": "747eaacdf41cb4e9994973ed13e2cd79164cd717",
        "id": 58243
    },
    {
        "content": "def standardize(X):\n    \"\"\" Standardize the dataset X \"\"\"\n    return (X - X.mean(axis=0)) / X.std(axis=0)",
        "sha1": "9a625e887ebd472d142ac97dd58056f4dd0c6288",
        "id": 374889
    },
    {
        "content": "def s3_read_write_policy_in_json(s3_bucket_name):\n    \"\"\"\n    Define an IAM policy statement for reading and writing to S3 bucket.\n    :return: an IAM policy statement in json.\n    \"\"\"\n    return {\n        \"Version\": \"2012-10-17\",\n        \"Statement\": [\n            {\n                \"Effect\": \"Allow\",\n                \"Action\": [\n                    \"s3:Put*\",\n                    \"s3:Get*\"\n                ],\n                \"Resource\": [\n                    \"arn:aws:s3:::{}/*\".format(s3_bucket_name)\n                ]\n            }\n        ]\n    }",
        "sha1": "b872fd3c833e0384ed891ab3709826a9e9a823bf",
        "id": 693764
    },
    {
        "content": "def rescale(num, old_min, old_max, new_min, new_max):\n    \"\"\"\n    Rescale num from range [old_min, old_max] to range [new_min, new_max]\n    \"\"\"\n    old_range = old_max - old_min\n    new_range = new_max - new_min\n    new_val = new_min + (((num - old_min) * new_range)/old_range)\n\n    return new_val",
        "sha1": "f823f46267d3b666ae0921957c2c3a3ca8c0a393",
        "id": 19701
    },
    {
        "content": "def stripfalse(seq):\n    \"\"\"Returns a sequence with all false elements stripped out of seq.\n    \"\"\"\n    return [x for x in seq if x]",
        "sha1": "a04824ee1f07472b9735ab8375aa85d60210f711",
        "id": 471244
    },
    {
        "content": "def _get_header(soup):\n    \"\"\"Extract the column names from the \"list-row-headings\" class.\n\n    Parameters\n    ----------\n    soup: bs4.BeautifulSoup\n        The html web page.\n\n    Returns\n    -------\n    list\n        Each element is a string with a column name.\n    \"\"\"\n    column_heading_class = \"list-row-headings\"\n    columns = soup.find(class_=column_heading_class).get_text()\n    column_list = [\n        column_name for column_name in columns.split(\"\\n\") if column_name != \"\"\n    ]\n    return column_list",
        "sha1": "43161030c6365c7b9de115ea1373697de0074f11",
        "id": 646020
    },
    {
        "content": "from pathlib import Path\n\n\ndef find_programfiles_dir(child):\n    \"\"\"\n    Find a file in Program Files or Program Files (x86)\n    \"\"\"\n    pgfiles = Path(r'C:\\Program Files'), Path(r'C:\\Program Files (x86)')\n    candidates = (root / child for root in pgfiles)\n    return next(candidate for candidate in candidates if candidate.isdir())",
        "sha1": "d7173acfb1640fb49035a0c38c156994f9c1bf50",
        "id": 430462
    },
    {
        "content": "from typing import List\n\n\ndef chop(sequence: str, fragment_len: int) -> List[str]:\n    \"\"\"Chop a whole sequence into subsequences of length fragment_len.\n\n    The subsequences are not-overlapping and their length is exactly fragment_len.\n    so the ends of the sequences will usually be chopped off.\n\n    Returns\n    -------\n        list: a list of subsequences\n    \"\"\"\n    n_fragments = len(sequence) // fragment_len\n    if n_fragments == 0:\n        return [sequence]\n    else:\n        return [\n            sequence[i * fragment_len : i * fragment_len + fragment_len]\n            for i in range(n_fragments)\n        ]",
        "sha1": "47d0c6b68e47196697e6f4a8aef9e45d2cdeda0a",
        "id": 378932
    },
    {
        "content": "def get_min_mtu(ifc):\n    \"\"\"\n    Helper to discover the minimum MTU of interfaces linked with the\n    given interface.\n    \"\"\"\n    mtu = ifc.mtu\n    if not ifc.net:\n        return mtu\n    for i in ifc.net.netifs():\n        if i.mtu < mtu:\n            mtu = i.mtu\n    return mtu",
        "sha1": "b1142740760f59d310ce3aa5915e0df5b9d1e162",
        "id": 423011
    },
    {
        "content": "def conj(x):\n    \"\"\"Return conjugate of x.\"\"\"\n    return x.conjugate()",
        "sha1": "d710ca7536775d26176d6a759cb072532ff9428d",
        "id": 693768
    },
    {
        "content": "def get_block_row(matrix, block_indices, row_range):\n    \"\"\"\n    Extract a subset of rows from a matrix\n\n    Parameters:\n    -----------\n    matrix: 2-d numpy array\n        block matrix\n    block_indices: integer of list of integers\n        indices of extracted blocks, 0-indexed. If indices is a list, return\n        the concatenation of all blocks\n    row_range: list of intergers\n        in the form of [0, c_1, c_1 + c_2, ..., c_1 + c_2 + ... + c_N]\n        where c_i is the number of rows in the i-th block\n\n    Returns:\n    --------\n    a 2-d matrix\n    \"\"\"\n    assert matrix.ndim == 2, 'Expect to receive 2-d array input, got shape {}'.format(matrix.shape)\n    if isinstance(block_indices, int):\n        block_indices = [block_indices]\n    # if isinstance(block_indices, (list, np.ndarray, np.generic))\n    ids = []\n    for i in block_indices:\n        ids = ids + list(range(row_range[i], row_range[i+1]))\n    return matrix[ids, :].copy()",
        "sha1": "1f315404a50b26336ad4e73a0b1ca2de4f489712",
        "id": 221307
    },
    {
        "content": "def method_node(node):\n    \"\"\"Get the function AST of the class method.\"\"\"\n    return node.body[1].body[0]",
        "sha1": "db8492f49e48b42d9a05abd08a407b3cafb31f96",
        "id": 330298
    },
    {
        "content": "def get_nodes(G, key, value):\n    \"\"\"Return list of nodes that has attribute key = value\"\"\"\n    result_nodes = []\n    for node in G.nodes:\n        if G.node[node].get(key) == value:\n            result_nodes.append(node)\n    return result_nodes",
        "sha1": "5d41e77ff028a99fe752f9dca9fc60c81783ab58",
        "id": 193761
    },
    {
        "content": "def is_namedtuple(cls: type) -> bool:\n  \"\"\"Determines if a class is built from typing.NamedTuple.\"\"\"\n  return (\n      isinstance(cls, type) and issubclass(cls, tuple) and\n      hasattr(cls, '_fields') and hasattr(cls, '__annotations__'))",
        "sha1": "b3e6f0c6b6b531c4eceba202933bee7fe9895614",
        "id": 568269
    },
    {
        "content": "def compute_avg_over_multiple_runs(number_episodes, number_runs, y_all_reward, y_all_cum_reward, y_all_timesteps):\n    \"\"\"\n    Compute average of reward and timesteps over multiple runs (different dates)\n    \"\"\"\n    y_final_reward = []\n    y_final_cum_reward = []\n    y_final_timesteps = []\n    for array_index in range(0, number_episodes):\n        sum_r = 0\n        sum_cr = 0\n        sum_t = 0\n        count = 0\n        for date_index in range(0, number_runs):  # compute average\n            sum_r += y_all_reward[date_index][array_index]\n            sum_cr += y_all_cum_reward[date_index][array_index]\n            sum_t += y_all_timesteps[date_index][array_index]\n            count += 1\n        y_final_reward.append(sum_r / float(count))\n        y_final_cum_reward.append(sum_cr / float(count))\n        y_final_timesteps.append(sum_t / float(count))\n\n    return y_final_reward, y_final_cum_reward, y_final_timesteps",
        "sha1": "a991f1f71ada7852a6ed94d7764d8112c6015cd1",
        "id": 18213
    },
    {
        "content": "from pathlib import Path\nfrom typing import Optional\n\n\ndef init_vagrant(out_dir: Path = Path('.'), force=False) -> Optional[Path]:\n    \"\"\"Initializes out_dir directory with a templated Vagrantfile for mounting downloaded images\"\"\"\n    template = Path(__file__).parent.joinpath(Path('files/Vagrantfile'))\n    out = out_dir.joinpath(Path('Vagrantfile').name)\n    if out.exists() and not force:\n        return None\n    else:\n        out.write_text(template.read_text())\n        return out",
        "sha1": "ef8ce6a3b8e5b6cb6090cd3ad320f145d5f16f3e",
        "id": 320417
    },
    {
        "content": "import re\n\n\ndef ParseTimeCommandResult(command_result):\n  \"\"\"Parse command result and get time elapsed.\n\n  Args:\n     command_result: The result after executing a remote time command.\n\n  Returns:\n     Time taken for the command.\n  \"\"\"\n  time_data = re.findall(r'real\\s+(\\d+)m(\\d+.\\d+)', command_result)\n  time_in_seconds = 60 * float(time_data[0][0]) + float(time_data[0][1])\n  return time_in_seconds",
        "sha1": "fc92d4b996716ddb2253bf4eb75ed9860c43b2d7",
        "id": 4047
    },
    {
        "content": "def isfile_s3(bucket, key: str) -> bool:\n    \"\"\"Returns T/F whether the file exists.\"\"\"\n    objs = list(bucket.objects.filter(Prefix=key))\n    return len(objs) == 1 and objs[0].key == key",
        "sha1": "e5ded8c24764a0d8d0c9b84aa96ee16db688c8c1",
        "id": 670416
    },
    {
        "content": "from typing import List\nimport random\n\n\ndef roll_dice(dice_count: int, dice_type: int) -> List[int]:\n    \"\"\"\n    Get multiple and various dice roll result.\n    ex) `roll_dice(2, 6)` means 2D6 and return each result like [2, 5].\n    Arguments:\n        dice_count {int} -- [description]\n        dice_type {int} -- [description]\n    Returns:\n        List[int] -- All dice results\n    \"\"\"\n    results = []\n    for _ in range(dice_count):\n        results.append(random.randint(1, dice_type))\n    return results",
        "sha1": "9315a7368e76a9c3dcf847888b4162f562e0737f",
        "id": 487119
    },
    {
        "content": "import pickle\n\n\ndef pickle_to_bytes(redis_server, hkey, key):\n    \"\"\"Convert pickled matrix stored in entry under `key`\n    in Redis hash under `hkey` to bytes. \n\n    Args:\n        redis_server: Redis server containing hash under `hkey`.\n        hkey (str): Key under which Redis hash is stored. \n        key (str): Key under which the pickled matrix is stored. \n    \n    Returns:\n        key_bytes: Matrix converted to bytes. \n    \"\"\"\n    key_pickled = redis_server.hget(hkey, key)\n    key = pickle.loads(key_pickled)\n    key_bytes = key.tobytes()\n    return key_bytes",
        "sha1": "c430d4036bfcdc78644de4de13342a329c0de7fb",
        "id": 610422
    },
    {
        "content": "def get_event_id(trace):\n    \"\"\"Return event GCMT ID from obspy Trace.\n\n    Args:\n        trace (Trace): obspy Trace object\n\n    Returns:\n        str: GCMT ID\n\n    \"\"\"\n    evid = trace.stats.sac.kevnm\n    return evid",
        "sha1": "b8654e292644778bdad76c3c3f1b743c43c0fc83",
        "id": 187749
    },
    {
        "content": "def hash_array(array):\n    \"\"\"Gives the hashed result of a numpy array.\"\"\"\n    array.flags.writeable = False\n    return hash(array.data)",
        "sha1": "359c279e6c409eae6921bb8a9b480f507c629622",
        "id": 554117
    },
    {
        "content": "from typing import Counter\n\n\ndef small_class(raw_data, labels, threshold=20):\n    \"\"\"Removes samples and classes for classes that have less than\n    `threshold` number of samples.\"\"\"\n    counts = Counter(labels)\n    data, n_labels = [], []\n    for i, l in enumerate(labels):\n        if counts[l] >= threshold:\n            data.append(raw_data[i])\n            n_labels.append(l)\n    return data, n_labels",
        "sha1": "cf80bd67ccc3d69baf0b71f226c8b56ef5b80e7c",
        "id": 27818
    },
    {
        "content": "import csv\n\n\ndef _load_csv(filename):\n    \"\"\"Load famplex csv file as list of rows\n\n    Parameters\n    ----------\n    filename : str\n\n    Returns\n    -------\n    rows : list\n    \"\"\"\n    with open(filename) as f:\n        csvreader = csv.reader(f, delimiter=str(u','),\n                               lineterminator='\\r\\n',\n                               quoting=csv.QUOTE_MINIMAL,\n                               quotechar=str(u'\"'))\n        rows = [row for row in csvreader]\n    return rows",
        "sha1": "7d46a0c6269f73bc9f59160baff450bbd9e5d217",
        "id": 415748
    },
    {
        "content": "def get_result(response, ctxlen):\n    \"\"\"Process results from OpenAI API response.\n\n    :param response: dict\n        OpenAI API Response\n    :param ctxlen: int\n        Length of context (so we can slice them away and only keep the predictions)\n    :return:\n        continuation_logprobs: np.array\n            Log probabilities of continuation tokens\n        is_greedy: bool\n            whether argmax matches given continuation exactly\n    \"\"\"\n    is_greedy = True\n    logprobs = response[\"logprobs\"][\"token_logprobs\"]\n    continuation_logprobs = sum(logprobs[ctxlen:])\n\n    for i in range(ctxlen, len(response[\"logprobs\"][\"tokens\"])):\n        token = response[\"logprobs\"][\"tokens\"][i]\n        top_tokens = response[\"logprobs\"][\"top_logprobs\"][i]\n        top_token = max(top_tokens.keys(), key=lambda x: top_tokens[x])\n        if top_token != token:\n            is_greedy = False\n            break\n\n    return continuation_logprobs, is_greedy",
        "sha1": "2014c3f5d3c92220a56419372d47039ab472967f",
        "id": 100424
    },
    {
        "content": "def sub(x,y):\n    \"\"\"\n    Returns the difference x-y\n    \n    Parameter x: The value to subtract from\n    Precondition: x is a number\n    \n    Parameter y: The value to subtract\n    Precondition: y is a number\n    \"\"\"\n    return x-y",
        "sha1": "9c7d9fcef236dff3e5d4b9840c082cbeacc9c7e5",
        "id": 32129
    },
    {
        "content": "def _Contains(i, j, areas, lens, cls):\n    \"\"\"Return True if path i contains majority of vertices of path j.\n\n    Args:\n      i: index of supposed containing path\n      j: index of supposed contained path\n      areas: list of floats - areas of all the paths\n      lens: list of ints - lenths of each of the paths\n      cls: dict - maps pairs to result of _ClassifyPathPairs\n    Returns:\n      bool - True if path i contains at least 55% of j's vertices\n    \"\"\"\n\n    if i == j:\n        return False\n    (jinsidei, joni) = cls[(i, j)]\n    if jinsidei == 0 or joni == lens[j] or \\\n       float(jinsidei) / float(lens[j]) < 0.55:\n        return False\n    else:\n        (insidej, _) = cls[(j, i)]\n        if float(insidej) / float(lens[i]) > 0.55:\n            return areas[i] > areas[j]  # tie breaker\n        else:\n            return True",
        "sha1": "a88cb7a16b7b856cdcd5dc30991cc1c1efa4387b",
        "id": 52690
    },
    {
        "content": "def jaccard(list1,list2):\n    \"\"\"\n    Berechnet Jaccard-Koeffizienten\n    :param list1: tokenliste 1\n    :param list2: tokenliste 2\n    :return: Jaccard-Koeffizient\n    \"\"\"\n    s1 = set(list1)\n    s2 = set(list2)\n    return (len(s1.intersection(s2)) / len(s1.union(s2)))",
        "sha1": "1658575e685b49c561a1045bc6055d380cd5c1a5",
        "id": 508661
    },
    {
        "content": "def the_box(box, width=3, height=3):\n    \"\"\"Return all coordinates of the fields of the given box number.\n\n    Args:\n        box (int): The number of the box.\n        width (int): The width of the sudoku.\n        height (int): The height of the sudoku.\n\n    Returns:\n        list: The coordinates of the box with the given number.\n\n    Raises:\n        ValueError: If the box number is invalid.\n\n    Example::\n        >>> the_box(0, width=3, height=3)\n        [(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n    \"\"\"\n    if not 0 <= box < width * height:\n        raise ValueError(\n            \"box must be less equal 0 and less than %d\" % width * height)\n\n    x = (box % height) * width\n    y = (box // height) * height\n    return [(y + i, x + j) for i in range(height) for j in range(width)]",
        "sha1": "e099b13a55310488809149d00df9719998a99191",
        "id": 19134
    },
    {
        "content": "import time\n\n\ndef try_with_times(times, interval, sleep_first, logger, tag, func, *args, **kwargs):\n    \"\"\"\n    Try a function with certain chances and retry intervals\n    :param times: Amount of chances\n    :param interval: Interval between retries\n    :param logger: Logger to log information\n    :param tag: Tag of the task, used only by logger\n    :param func: Function to execute\n    :param sleep_first: If sleep should be called before the first try\n    :param args: Args for the function\n    :param kwargs: keyword args for the function\n    :return: Tuple, (If the try is success, return value of the function (None if the try failed))\n    \"\"\"\n    logger.info(\"Trying to %s with %d chances.\" % (tag, times))\n    # Sleep before the first try if required\n    if sleep_first:\n        time.sleep(interval)\n    while times > 0:\n        try:\n            # Try to run the function and return when no exceptions\n            res = func(*args, **kwargs)\n            logger.info(\"Operation %s succeeded.\" % tag)\n            return True, res\n        except:\n            times -= 1\n            # Log the info of the exception\n            logger.warning(\"Failed to %s, %d more chances.\" % (tag, times), exc_info=True)\n        if times > 0:\n            time.sleep(interval)\n\n    logger.error(\"Operation %s failed.\" % tag)\n    # Return when finally failed.\n    return False, None",
        "sha1": "11c12c0ac0a9068c7c200ad9efbdcc0ed3d711be",
        "id": 618129
    },
    {
        "content": "def get_h5_attributes(stack):\n    \"\"\"Get attributes from a stack.\"\"\"\n\n    element_size_um = axislabels = None\n\n    if 'element_size_um' in stack.attrs.keys():\n        element_size_um = stack.attrs['element_size_um']\n\n    if 'DIMENSION_LABELS' in stack.attrs.keys():\n        axislabels = stack.attrs['DIMENSION_LABELS']\n\n    return element_size_um, axislabels",
        "sha1": "4fca6c56424d523fd38cd76d0ffd68425ac2a7b1",
        "id": 614854
    },
    {
        "content": "def convert_RGB_to_BGR(color):\n    \"\"\"Converts a RGB color to a BGR color.\n\n    :param color: The RGB color.\n    :type color: Tuple[int, int, int]\n    :return: The corresponding BGR color.\n    :rtype: Tuple[int, int, int]\n    \"\"\"\n    return color[2], color[1], color[0]",
        "sha1": "fbab9f0b6e491023c82bf9a8cc20eb52f9ffc75f",
        "id": 304284
    },
    {
        "content": "def AddFieldToUpdateMask(field, patch_request):\n  \"\"\"Adds name of field to update mask.\"\"\"\n  update_mask = patch_request.updateMask\n  if not update_mask:\n    patch_request.updateMask = field\n  elif field not in update_mask:\n    patch_request.updateMask = update_mask + \",\" + field\n  return patch_request",
        "sha1": "f8533b38e3ecf7658aa06850869727ab3f34e7de",
        "id": 21038
    },
    {
        "content": "def checkSum(intList, intVal):\n    \"\"\"checks if the sum of elements in intList equals intVal\"\"\"\n    s = 0\n    for x in intList:\n        s += x\n    return (s == intVal)",
        "sha1": "7713c6d4f600394b0f65cd12356e249876cb2265",
        "id": 337991
    },
    {
        "content": "def read_inst_bytes(bv, il):\n  \"\"\" Get the opcode bytes for an instruction\n  Args:\n    bv (binja.BinaryView)\n    il (binja.LowLevelILInstruction)\n  Returns:\n    str\n  \"\"\"\n  inst_len = bv.get_instruction_length(il.address)\n  return bv.read(il.address, inst_len)",
        "sha1": "cc218790a39fdf2b79c55dd6c246009877e9eb01",
        "id": 433819
    },
    {
        "content": "from typing import List\n\n\ndef get_mprocess_names_type1_set_kraus_matrices() -> List[str]:\n    \"\"\"returns the list of valid MProcess names of `set Kraus matrices` of type1.\n\n    Returns\n    -------\n    List[str]\n        the list of valid MProcess names of `set Kraus matrices` of type1.\n    \"\"\"\n    names = [\n        \"xxparity-type1\",\n        \"zzparity-type1\",\n    ]\n    return names",
        "sha1": "2025610a32bcca94751c3a379bd94c7bf52e880c",
        "id": 552257
    },
    {
        "content": "import time\n\n\ndef selection_sort(array: list) -> tuple:\n    \"\"\"\n    Return sorted list, name of sorting method, number of comparisons\n    and execution time using Selection Sort method\n    \"\"\"\n    name = \"Selection sort\"\n    comparisons = 0\n    start = time.time_ns()\n    for i in range(len(array)):\n        min_idx = i\n        for j in range(i+1, len(array)):\n            comparisons += 1\n            if array[min_idx] > array[j]:\n                min_idx = j\n        array[i], array[min_idx] = array[min_idx], array[i]\n    waiting = time.time_ns() - start\n\n    return array, name, comparisons, waiting",
        "sha1": "26ab8db0aeaa6cf69c6aa47e75fb0164e8e3a21d",
        "id": 172143
    },
    {
        "content": "from colorama import Fore, Style\nfrom colorama import init\n\n\ndef show_supported(supported):\n    \"\"\"\n    Returns OK (in green) if supported evaluates to True, otherwise NOT OK (in red).\n    \"\"\"\n    try:\n\n        init()\n        startcolor = Fore.GREEN if supported else Fore.RED\n        stopcolor = Style.RESET_ALL\n    except:\n        startcolor = stopcolor = \"\"\n    output = \"OK\" if supported else \"NOT OK\"\n    return f\"{startcolor}{output}{stopcolor}\"",
        "sha1": "9cecc3a18880364609e03fe9be45c250a96139c9",
        "id": 569856
    },
    {
        "content": "from typing import Sequence\nfrom typing import List\n\n\ndef _quote(values: Sequence[str]) -> List[str]:\n    \"\"\"Return a sequence of strings surrounding each value in double quotes.\"\"\"\n    return list(map(lambda v: f\"\\\"{v}\\\"\", values))",
        "sha1": "75bf37aba3f106f496925c63236cdbb54a4b2589",
        "id": 454315
    },
    {
        "content": "from functools import reduce\n\n\ndef union_regions(regions):\n  \"\"\"Returns the union of sublime.Region instances (using their term 'cover')\"\"\"\n  return reduce((lambda maybe,region: region if maybe == None else maybe.cover(region)),regions,None)",
        "sha1": "1c22e5a6871f5fff3c6d05953f1c3893b03adb86",
        "id": 540270
    },
    {
        "content": "from unittest.mock import Mock\n\n\ndef constructorMock(name):\n    \"\"\"Create fake constructor that returns Mock object when invoked\"\"\"\n    instance = Mock()\n    instance._name_of_parent_class = name\n    constructor = Mock(return_value=instance)\n    return constructor",
        "sha1": "5a5acbb7592e41fdc9e2efdf46ca5dd2c29ec073",
        "id": 385820
    },
    {
        "content": "def is_vowel(char):\n    \"\"\"Check if it is vowel.\"\"\"\n    return char in ['a', 'e', 'i', 'o', 'u', 'A', 'E', 'I', 'O', 'U']",
        "sha1": "97f747d96e08cd15fe7a1b742b36f5ff6cc2f3fc",
        "id": 697932
    },
    {
        "content": "def hourangle (lst, ra) :\n    \"\"\" Calculate hourangle of specified ra, -12 to +12\n    args:\n        lst: local sidereal time, in hours\n        ra: ra of object, in degrees\n    returns:\n        hourangle, in hours, -12 to +12\n    \"\"\"\n    return (lst - ra / 15.0 + 12.0) % 24.0 - 12.0",
        "sha1": "63b3950f108440fc10c1220653aa2a63325d574a",
        "id": 523528
    },
    {
        "content": "def get_times_list(binout):\n    \"\"\"\n    Method to get a times list from binary\n    output.\n\n    Parameters\n    ----------\n        binout : fp.binarayfile class instance\n\n    Returns\n    -------\n        list of times\n    \"\"\"\n    return sorted([float(\"{0:15.6f}\".format(t)) for t in\n                   binout.recordarray[\"totim\"]])",
        "sha1": "bbf4e3635bdc54e1713f050021156f8e369d348c",
        "id": 218178
    },
    {
        "content": "def parse_map(input_list):\n    \"\"\"\n    Convert the input list consisting of rows of dots and hashes to an integer map - each list\n    member shows the indices of trees (hashes)\n\n    Args:\n        input_list: list of input strings\n\n    Returns:\n        A tuple of (map_width, map_height, parsed_map) where parsed_map is the integer map\n    \"\"\"\n    map_width = len(input_list[0])\n    map_height = len(input_list)\n    parsed_map = list(map(\n        lambda x: tuple(index for index, item in enumerate(x[1]) if item == \"#\"),\n        enumerate(input_list)\n    ))\n    return(map_width, map_height, parsed_map)",
        "sha1": "c3c464c1eb31e2529cd9f30ce5776c5e1efb81b6",
        "id": 473910
    },
    {
        "content": "import operator\n\n\ndef find_package_date(releases):\n    \"\"\" Find max date inside package \"\"\"\n    return max(\n        releases,\n        key=operator.itemgetter('date')\n        ).get('date')",
        "sha1": "c50685a46d957edf7011d50a42f6c80a876b3121",
        "id": 332176
    },
    {
        "content": "import re\n\n\ndef _get_rating_for_regex_dict(fname, regex_dictionary):\n    \"\"\"\n    iterate check if fname match to any item of regexlist,\n    returns raiting for the best hit\n\n    Parameters\n    ----------\n    fname : str\n        File name.\n    regex_dictionary : dict\n        Dictionary of (regex, rating).\n\n    Returns\n    -------\n    best_rating : int or None\n        Returns the found raiting. None if no matching regex pattern was found.\n    \"\"\"\n    best_rating = 0\n    for reg, rating in regex_dictionary.items():\n        if re.match(str(reg), fname) and rating > best_rating:\n            best_rating = rating\n    return best_rating",
        "sha1": "b5c6e051aba210b5ee5077f3828a2352ec1d014f",
        "id": 287987
    },
    {
        "content": "def zooms_string(z1, z2):\n  \"\"\"Prints 'zoom N' or 'zooms N-M'.\"\"\"\n  if z2 != z1:\n    return \"zooms {}-{}\".format(min(z1, z2), max(z1, z2))\n  else:\n    return \"zoom {}\".format(z1)",
        "sha1": "2e433472d721767cfc152b75a74f9976ba340f7a",
        "id": 698848
    },
    {
        "content": "def letterMatch(letter, alphalist, letterlist, hanglist):\n    \"\"\"Hangman letter test with logic for no retries of prior submitted letters.\n       Return True if match, False if no match, or None if invalid letter.\"\"\"\n\n    ch = letter[0] if (str == type(letter)) and (0 < len(letter)) else None\n\n    fmatch = None\n    if (None != ch) and (ch in alphalist):\n        fmatch = False\n        alphalist.remove(ch)\n        for k in range(0, len(letterlist)):\n            if ch == letterlist[k]:\n                fmatch = True\n                hanglist[k] = ch\n\n    return fmatch",
        "sha1": "7791eab74d157f5f4d30f20f1c83f13ec1b0c970",
        "id": 268263
    },
    {
        "content": "import math\n\n\ndef adj_mg(tm, mg_conc, seq):\n    \"\"\"Adjust melting temperature a duplexx based on sodium concentration.\n    Based on Owczarzy et al, Biochemistry(47), 2008\n\n    Args:\n      tm (float): melting temperature at [Mg] = 0 M.\n      mg_conc (float): divalent species concentration in M.\n      seq (str): input sequence.\n\n    Returns:\n      float: adjusted melting temperature.\n    \"\"\"\n\n    # Default\n    Tm3 = tm\n\n    # Parameters from paper\n    mg_a = 3.92e-5\n    mg_b = -9.11e-6\n    mg_c = 6.26e-5\n    mg_d = 1.42e-5\n    mg_e = -4.82e-4\n    mg_f = 5.25e-4\n    mg_g = 8.31e-5\n\n    # Adjust\n    fgc = (seq.count(\"G\") + seq.count(\"C\")) / float(len(seq))\n    if 0 < mg_conc:\n        mg_conc_log = math.log(mg_conc)\n        Tm3r = 1.0 / tm + mg_a + mg_b * mg_conc_log + fgc * (mg_c + mg_d * mg_conc_log)\n        Tm3r_factor = mg_e + mg_f * mg_conc_log + mg_g * (mg_conc_log) ** 2\n        Tm3r_factor *= 1.0 / (2 * (len(seq) - 1))\n        Tm3r += Tm3r_factor\n        Tm3 = 1.0 / Tm3r\n\n    return Tm3",
        "sha1": "e1529ffe00082f50a66848588b80cc6704d5e42e",
        "id": 207162
    },
    {
        "content": "def get_total_weight(eqns):\n    \"\"\"Gets the total weight of the equations\n\n    This functions sums the total weight of the equations. Then the normalized\n    weight for each of the equations would be their given weight over the\n    return value of this function.\n\n    :param eqns: An iterable of the equations.\n    :returns: The total weight of the equations.\n    :rtype: float\n    \"\"\"\n\n    return sum(\n        eqn.weight for eqn in eqns\n        )",
        "sha1": "41b4bf96f3e4e2d41444794f03a2246c81648569",
        "id": 216323
    },
    {
        "content": "import unittest\n\n\ndef get_TestCase_classes( test_module ):\n    \"\"\"\n    Searches the given module for classes that derive from unittest.TestCase,\n    and returns a map from the class name as a string to the class object.\n    \"\"\"\n    tcD = {}\n    for name in dir(test_module):\n        obj = getattr( test_module, name )\n        try:\n            if issubclass( obj, unittest.TestCase ):\n                tcD[name] = obj\n        except:\n            pass\n\n    return tcD",
        "sha1": "1ee873384804a3ba8871f2aeda8e5285f627a3e8",
        "id": 281800
    },
    {
        "content": "def get_file(file_name: str) -> list:\n    \"\"\"\n    Takes a file name (string), reads the file line by line and saves it to a list,\n    which it then returns.\n    \"\"\"\n    with open(file_name, \"r\", encoding=\"utf-8\") as my_file:\n        return my_file.read().split(\"\\n\")",
        "sha1": "d091a6961d05237a4b859428fd57f78c59b4834e",
        "id": 230577
    },
    {
        "content": "from typing import Set\n\n\ndef cromwell_execution_folder_reserved_files() -> Set[str]:\n    \"\"\"\n    Return filenames which are used by cromwell itself.\n    :return: A set of filenames\n    \"\"\"\n    reserved_files: Set[str] = set()\n    prefixes = [\"stdout\", \"script\", \"stderr\"]\n    suffixes = [\"\", \".submit\", \".check\", \".background\"]\n    for prefix in prefixes:\n        for suffix in suffixes:\n            reserved_files.add(prefix + suffix)\n    reserved_files.add(\"rc\")\n    return reserved_files",
        "sha1": "e3e8115e09f0c2cb3f03ae270de51ae6c16b68ae",
        "id": 172909
    },
    {
        "content": "def total_time(entry):\n    \"\"\"\n    textual representation of total work time\n    \"\"\"\n    total_min = entry.total_min\n    if total_min:\n        return \"%d:%02d\" % (total_min // 60, total_min % 60)",
        "sha1": "7c952d5365f5260f2bb0a9a88deeb78a3469fc04",
        "id": 359704
    },
    {
        "content": "def param_name(param_name, rig_type):\n    \"\"\" Get the actual parameter name, sans-rig-type.\n    \"\"\"\n    return param_name[len(rig_type) + 1:]",
        "sha1": "a68a3f78b1381c471c45a883ba9cf49e4a9869be",
        "id": 561231
    },
    {
        "content": "def flat_to_deep(cfg):\n    \"\"\"\n    Function converts a flat config to a nested config in dictionary format. This is\n    the inverse of `deep_to_flat()`.\n    \"\"\"\n    res_cfg = {}\n    # By iterating over the items of `cfg` we resolve one level of nesting.\n    for key, val in cfg.items():\n        if \".\" in key:\n            root, leaf = tuple(key.split(\".\", 1))  # Split off the first part\n            if root not in res_cfg:\n                res_cfg[root] = {}\n            res_cfg[root][leaf] = val\n        else:\n            res_cfg[key] = val\n\n    # Now we iterate again and call the function recursively to resolve deeper levels\n    # of nesting.\n    for key, val in res_cfg.items():\n        if isinstance(val, dict):\n            res_cfg[key] = flat_to_deep(val)\n\n    return res_cfg",
        "sha1": "050ee2ea4a3619f14f66c63dd6bfa7a6b6059829",
        "id": 268440
    },
    {
        "content": "def _websocket_mask_python(mask, data):\n    \"\"\"Websocket masking function.\n\n    `mask` is a `bytes` object of length 4; `data` is a `bytes` object\n    of any length.  Returns a `bytes` object of the same length as\n    `data` with the mask applied as specified in section 5.3 of RFC\n    6455.\n\n    This pure-python implementation may be replaced by an optimized\n    version when available.\n\n    \"\"\"\n    return bytes(b ^ mask[i % 4] for i, b in enumerate(data))",
        "sha1": "a0194d25aa2017d83fcc13e67791a5f735ee7fa5",
        "id": 601810
    },
    {
        "content": "def read_file_into_list(filepath):\n    \"\"\"Read file into a list of strings while dropping empty lines.\"\"\"\n    with open(filepath) as f:\n        lst = []\n        for line in f:\n            stripped_line = line.strip()\n            if stripped_line:\n                lst.append(stripped_line)\n    return lst",
        "sha1": "9d7518ea5e25801c2b15df42023ec2e3d2a4a7a2",
        "id": 157656
    },
    {
        "content": "def get_waitsignals_cases(order, working):\n    \"\"\"\n    Builds combinations for signals to be emitted and expected for working cases (i.e. blocker.signal_triggered == True)\n    and non-working cases, depending on the order.\n\n    Note:\n    The order (\"none\", \"simple\", \"strict\") becomes stricter from left to right.\n    Working cases of stricter cases also work in less stricter cases.\n    Non-working cases in less stricter cases also are non-working in stricter cases.\n    \"\"\"\n    if order == \"none\":\n        if working:\n            cases = get_waitsignals_cases(order=\"simple\", working=True)\n            cases.extend([\n                # allow even out-of-order signals\n                (('A1', 'A2'), ('A2', 'A1'), True),\n                (('A1', 'A2'), ('A2', 'Ax'), True),\n                (('A1', 'B1'), ('B1', 'A1'), True),\n                (('A1', 'B1'), ('B1', 'Ax'), True),\n                (('A1', 'B1', 'B1'), ('B1', 'A1', 'B1'), True),\n            ])\n            return cases\n        else:\n            return [\n                (('A2',), ('A1',), False),\n                (('A1',), ('B1',), False),\n                (('A1',), ('Bx',), False),\n                (('A1', 'A1'), ('A1', 'B1'), False),\n                (('A1', 'A1'), ('A1', 'Bx'), False),\n                (('A1', 'A1'), ('B1', 'A1'), False),\n                (('A1', 'B1'), ('A1', 'A1'), False),\n                (('A1', 'B1'), ('B1', 'B1'), False),\n                (('A1', 'B1', 'B1'), ('A1', 'A1', 'B1'), False),\n            ]\n    elif order == \"simple\":\n        if working:\n            cases = get_waitsignals_cases(order=\"strict\", working=True)\n            cases.extend([\n                # allow signals that occur in-between, before or after the expected signals\n                (('B1', 'A1', 'A1', 'B1', 'A1'), ('A1', 'B1'), True),\n                (('A1', 'A1', 'A1'), ('A1', 'A1'), True),\n                (('A1', 'A1', 'A1'), ('A1', 'Ax'), True),\n                (('A1', 'A2', 'A1'), ('A1', 'A1'), True),\n            ])\n            return cases\n        else:\n            cases = get_waitsignals_cases(order=\"none\", working=False)\n            cases.extend([\n                # don't allow out-of-order signals\n                (('A1', 'B1'), ('B1', 'A1'), False),\n                (('A1', 'B1'), ('B1', 'Ax'), False),\n                (('A1', 'B1', 'B1'), ('B1', 'A1', 'B1'), False),\n                (('A1', 'B1', 'B1'), ('B1', 'B1', 'A1'), False),\n            ])\n            return cases\n    elif order == \"strict\":\n        if working:\n            return [\n                # only allow exactly the same signals to be emitted that were also expected\n                (('A1',), ('A1',), True),\n                (('A1',), ('Ax',), True),\n                (('A1', 'A1'), ('A1', 'A1'), True),\n                (('A1', 'A1'), ('A1', 'Ax'), True),\n                (('A1', 'A1'), ('Ax', 'Ax'), True),\n                (('A1', 'A2'), ('A1', 'A2'), True),\n                (('A2', 'A1'), ('A2', 'A1'), True),\n                (('A1', 'B1'), ('A1', 'B1'), True),\n                (('A1', 'A1', 'B1'), ('A1', 'A1', 'B1'), True),\n                (('A1', 'A2', 'B1'), ('A1', 'A2', 'B1'), True),\n                (('A1', 'B1', 'A1'), ('A1', 'A1'), True),  # blocker doesn't know about signal B1 -> test passes\n                (('A1', 'B1', 'A1'), ('Ax', 'A1'), True),\n            ]\n        else:\n            cases = get_waitsignals_cases(order=\"simple\", working=False)\n            cases.extend([\n                # don't allow in-between signals\n                (('A1', 'A1', 'A2', 'B1'), ('A1', 'A2', 'B1'), False),\n            ])\n            return cases",
        "sha1": "0be0d56872f177ff340d41316dbd7d53717bb244",
        "id": 426252
    },
    {
        "content": "def length(x):\n    \"\"\"Calculate the length of a vector.\n\n    This function is equivalent to the `length` function in GLSL.\n    Args:\n        x (:class:`~taichi.Matrix`): The vector of which to calculate the length.\n\n    Returns:\n        The Euclidean norm of the vector.\n\n    Example::\n\n        >>> x = ti.Vector([1, 1, 1])\n        >>> length(x)\n        1.732051\n    \"\"\"\n    return x.norm()",
        "sha1": "bab7dfde88c3cb7d9dc4a2f697dfe2443e9acabd",
        "id": 38017
    },
    {
        "content": "import re\n\n\ndef convert_invalid_symbol(input_str):\n    \"\"\"Convert invalid symbols in string into '_' \"\"\"\n    invalid_symbol = r\"[/:\\s\\.]\"\n    new_str = re.sub(invalid_symbol, \"_\", input_str)\n    return new_str",
        "sha1": "007f7515ca7316c10ffae5029e0914c113e87b77",
        "id": 403171
    },
    {
        "content": "def raw2ML(cfg, typ, hyper, x=None, z=None, inverse=False):\n    \"\"\"\n    transform raw values to ML-friendly values\n    inverse == False: raw --> ML\n    inverse == True: ML --> raw\n    \"\"\"\n    def normalize(x, mu, std, inverse):\n        if inverse:  # ML --> raw\n            x = std * x + mu\n        else:  # raw --> ML\n            x = (x - mu) / std\n        return x\n\n    if x is not None:\n        if x.dim() != 3:  # [b,*,n_x]\n            raise ValueError(x.shape)\n        if typ == \"setup\":\n            x[:, :, 0] = normalize(x[:, :, 0], hyper[\"mu\"][\"N\"], hyper[\"std\"][\"N\"], inverse)\n            x[:, :, 1] = normalize(x[:, :, 1], hyper[\"mu\"][\"mass_agg\"], hyper[\"std\"][\"mass_agg\"], inverse)\n            x[:, :, 2] = normalize(x[:, :, 2], hyper[\"mu\"][\"gamma\"], hyper[\"std\"][\"gamma\"], inverse)\n            x[:, :, 3:3+4] = normalize(x[:, :, 3:3+4], hyper[\"mu\"][\"fraction_mat\"], hyper[\"std\"][\"fraction_mat\"], inverse)\n            for dim in range(3):  # sx,sy,sz\n                x[:, :, dim + 7] = normalize(x[:, :, dim + 7], hyper[\"mu\"][\"spin\"], hyper[\"std\"][\"spin\"], inverse)  # proj\n                x[:, :, dim + 10] = normalize(x[:, :, dim + 10], hyper[\"mu\"][\"spin\"], hyper[\"std\"][\"spin\"], inverse)  # targ\n            for dim in range(6):  # x,y,z,vx,vy,vz\n                x[:, :, dim + 13] = normalize(x[:, :, dim + 13], hyper[\"mu\"][\"pos_vel\"][dim], hyper[\"std\"][\"pos_vel\"][dim], inverse)  # proj\n                x[:, :, dim + 19] = normalize(x[:, :, dim + 19], hyper[\"mu\"][\"pos_vel\"][dim], hyper[\"std\"][\"pos_vel\"][dim], inverse)  # targ\n        elif typ == \"sph\":\n            raise NotImplementedError()\n\n    if z is not None:\n        if z.dim() != 3:  # [t,*,n_z]\n            raise ValueError(z.shape)\n        if typ == \"agg\":\n            z[:, :, 0] = normalize(z[:, :, 0], hyper[\"mu\"][\"mass_agg\"], hyper[\"std\"][\"mass_agg\"], inverse)\n            z[:, :, 3] = normalize(z[:, :, 3], hyper[\"mu\"][\"mass_agg\"], hyper[\"std\"][\"mass_agg\"], inverse)\n            z[:, :, 6] = normalize(z[:, :, 6], hyper[\"mu\"][\"mass_agg\"], hyper[\"std\"][\"mass_agg\"], inverse)\n            z[:, :, 1] = normalize(z[:, :, 1], hyper[\"mu\"][\"fraction_mat\"], hyper[\"std\"][\"fraction_mat\"], inverse)\n            z[:, :, 2] = normalize(z[:, :, 2], hyper[\"mu\"][\"fraction_mat\"], hyper[\"std\"][\"fraction_mat\"], inverse)\n            z[:, :, 4] = normalize(z[:, :, 4], hyper[\"mu\"][\"fraction_mat\"], hyper[\"std\"][\"fraction_mat\"], inverse)\n            z[:, :, 5] = normalize(z[:, :, 5], hyper[\"mu\"][\"fraction_mat\"], hyper[\"std\"][\"fraction_mat\"], inverse)\n            z[:, :, 7] = normalize(z[:, :, 7], hyper[\"mu\"][\"fraction_mat\"], hyper[\"std\"][\"fraction_mat\"], inverse)\n            z[:, :, 8] = normalize(z[:, :, 8], hyper[\"mu\"][\"fraction_mat\"], hyper[\"std\"][\"fraction_mat\"], inverse)\n            for dim in range(6):  # x,y,z,vx,vy,vz\n                z[:, :, dim + 9] = normalize(z[:, :, dim + 9], hyper[\"mu\"][\"pos_vel\"][dim], hyper[\"std\"][\"pos_vel\"][dim], inverse)  # largest\n                z[:, :, dim + 15] = normalize(z[:, :, dim + 15], hyper[\"mu\"][\"pos_vel\"][dim], hyper[\"std\"][\"pos_vel\"][dim], inverse)  # 2nd largest\n                z[:, :, dim + 21] = normalize(z[:, :, dim + 21], hyper[\"mu\"][\"pos_vel\"][dim], hyper[\"std\"][\"pos_vel\"][dim], inverse)  # rest\n        else:\n            raise NotImplementedError()\n        \n    return x, z",
        "sha1": "f65ccd01f9cb1946cf7fef9fd1905b8d9724e737",
        "id": 433879
    },
    {
        "content": "import ast\n\n\ndef has_return_type(function_node: ast.FunctionDef, tree: ast.AST) -> bool:\n    \"\"\"\n    Returns true if the function has a return type annotation (function_node.returns).\n    \"\"\"\n    return function_node.returns is not None",
        "sha1": "ed3caed934aa4c16cf6c05368c6b00d5d15f634d",
        "id": 449517
    },
    {
        "content": "def get_ds(dc, name):\n    \"\"\"\n    Pick a datastore by its name.\n    \"\"\"\n    for ds in dc.datastore:\n        try:\n            if ds.name == name:\n                return ds\n        except:  # Ignore datastores that have issues\n            pass\n    raise Exception(\"Failed to find %s on datacenter %s\" % (name, dc.name))",
        "sha1": "964a4c997e46d2d655659962479baab53cbb765d",
        "id": 44739
    },
    {
        "content": "def descendants(cls: type) -> list:\n\t\"\"\"\n\tReturn a list of all descendant classes of a class\n\t\n\tArguments:\n\t\tcls (type): Class from which to identify descendants\n\tReturns:\n\t\tsubclasses (list): List of all descendant classes\n\t\"\"\"\n\n\tsubclasses = cls.__subclasses__()\n\tfor subclass in subclasses:\n\t\tsubclasses.extend(descendants(subclass))\n\n\treturn(subclasses)",
        "sha1": "08c1e1e32e1b41e8af61ed281a395a56437b09d6",
        "id": 351599
    },
    {
        "content": "def _sanitize_key(column_name: str) -> str:\n    \"\"\"\n    Sanitize a column_name read from a CSV file, by removing leading and trailing spaces\n    and quotes.\n\n    :param column_name: the column_name read from a CSV file\n    :return: sanitized column name\n    \"\"\"\n    return column_name.strip('\" ')",
        "sha1": "a5b7ebeca802a1f8ae90af1c1f9f1628089d627b",
        "id": 230241
    },
    {
        "content": "import contextlib\nimport socket\n\n\ndef tcp_port_connectable(hostname, port):\n    \"\"\"\n    Return true if we can connect to a TCP port\n    \"\"\"\n    try:\n        with contextlib.closing(socket.socket(socket.AF_INET,\n                                              socket.SOCK_STREAM)) as sk:\n            sk.settimeout(5)\n            sk.connect((hostname, port))\n            return True\n    except socket.error:\n        return False",
        "sha1": "7a6c3f5baddfe1726e5f6c1c364c21c3cf94bbf9",
        "id": 685273
    },
    {
        "content": "import json\n\n\ndef _is_json(str_to_check: str) -> bool:\n    \"\"\"\n    Checks if str is JSON\n\n    :param str_to_check: sting to check\n\n    :return: true if str is JSON, otherwise raises ValueError or TypeError\n\n    :raises TypeError: if str_to_check is not str type\n    :raises ValueError: if str_to_check is not valid JSON\n    \"\"\"\n    json.loads(str_to_check)\n    return True",
        "sha1": "3bdb6387d8c4e076c2a45de2ac6638de5b249b05",
        "id": 62550
    },
    {
        "content": "def dependants_to_dependencies(graph):\n    \"\"\"Inverts a dependant's graph to yield a dependency graph.\n\n    Notes\n    -----\n    The graph must be directed and acyclic.\n\n    Parameters\n    ----------\n    graph: dict(str, list(str))\n        The graph to invert. Each key in the dictionary represents a node in the graph, and each\n        string in the value list represents a node which depends on the node defined by the key.\n\n    Returns\n    -------\n    dict(str, list(str))\n        The inverted graph. Each key in the dictionary represents a node in the graph, and each\n        string in the value list represents a node which the node defined by the key depends on.\n    \"\"\"\n\n    dependencies = {}\n\n    for node in graph:\n\n        if node not in dependencies:\n            dependencies[node] = []\n\n        for dependant in graph[node]:\n\n            if dependant not in dependencies:\n                dependencies[dependant] = []\n\n            if node not in dependencies[dependant]:\n                dependencies[dependant].append(node)\n\n    return dependencies",
        "sha1": "f810b5b6459aff23213868a56a4ec071f0830a13",
        "id": 493537
    },
    {
        "content": "import torch\n\n\ndef sample_diag_gauss(mean, std, generator=None, is_radial=False):\n    \"\"\"Get a sample from a multivariate Gaussian distribution with diagonal\n    covariance matrix.\n\n    Samples are produced using the reparametrization trick.\n\n    Note: \n        If ``is_radial`` is set to ``True``, samples will instead by obtained\n        from a radial BNN distribution instead of a multivariate Gaussian.\n        For details refer to docstring of :func:`decode_and_sample_diag_gauss`.\n\n    Args:\n        mean: A list of tensors. See return value of method\n            :func:`extract_mean_std`.\n        std: A list of tensors with the same shapes as `mean`. See return value\n            of method :func:`extract_mean_std`.\n        generator (torch.Generator, optional): A generator can be passed to\n            obtain more control over the reproducibility of the random sampling\n            process.\n        is_radial (bool, optional): If ``True``, the weights will be sampled\n            according to a radial distribution, and not a Gaussian one.\n\n    Returns:\n        A list of tensors, where each is a sample from the diagonal Gaussian\n        distributions defined by entries of `mean` and `std`.\n    \"\"\"\n    sample = []\n    device = mean[0].device if len(mean) > 0 else None\n    for i, m in enumerate(mean):\n        eps = torch.normal(torch.zeros_like(m), 1., generator=generator)\n\n        if not is_radial:\n            sample.append(m + eps * std[i])\n        else:\n            r = torch.normal(torch.tensor(0.).to(device), 1., \\\n                             generator=generator)\n            sample.append(m +  std[i] * r * eps / torch.norm(eps, p=2))\n\n    return sample",
        "sha1": "c0f0c7232ba9ef6b6800510642fb603442eef334",
        "id": 582203
    },
    {
        "content": "def parse_condition(condition):\n    \"\"\"\n    Parses a condition for cond_labels\n\n    >>> parse_condition(\"review.approval    > 3\")\n    ['review.approval', '3']\n    \"\"\"\n    elems = condition.split(\">\")\n    if len(elems) != 2:\n        raise ValueError(\"Unable to parse \")\n    return [e.strip() for e in elems]",
        "sha1": "cf6a5f7dc2be6144ac9e72d2f2fc3dbcd496b045",
        "id": 243395
    },
    {
        "content": "import re\n\n\ndef remove_extra_whitespace(line):\n    \"\"\" Removes all line breaking characters.\n    Collapses consecutive spaces into just one space \"\"\"\n\n    line = line.lstrip()\n    line = re.sub('[\\n\\r\\f\\v\\t]', ' ', line)\n    line = re.sub('  +', ' ', line )\n\n    return line",
        "sha1": "31c8d83aeb5fbcdac9c2d7be820849294debb9d2",
        "id": 299232
    },
    {
        "content": "def combine(list1, list2):\n    \"\"\"\n    Write a function that combines two lists by alternatingly taking elements.\n    For example: given the two lists [a, b, c] and [1, 2, 3], the function\n    should return [a, 1, b, 2, c, 3].\n    \"\"\"\n    output = []\n    for i in range(max(len(list1), len(list2))):\n        output.append(list1[i])\n        output.append(list2[i])\n    return output",
        "sha1": "59e42aa72862b26f220de9f97fff4ef5d504bd20",
        "id": 257217
    },
    {
        "content": "def strip_action(action: str) -> str:\n    \"\"\"\n    removes whitespace and changes all characters to lower case\n    :param action: the name of the action taken on a position\n    :return: the input string minus the above mentioned\n    \"\"\"\n\n    action = action.replace(\" \", \"\")\n    action = action.casefold()\n\n    return action",
        "sha1": "ef85ba082043f5f25cb3f33c73504de48e3cf530",
        "id": 23853
    },
    {
        "content": "import numbers\nimport itertools\n\n\ndef bindown(array, factor, mode='avg'):\n    \"\"\"Bin (resample) an array.\n\n    Parameters\n    ----------\n    array : numpy.ndarray\n        array of values\n    factor : int or sequence of int\n        binning factor.  If an integer, broadcast to each axis of array,\n        else unique factors may be used for each axis.\n    mode : str, {'avg', 'sum'}\n        sum or avg, how to adjust the output signal\n\n    Returns\n    -------\n    numpy.ndarray\n        ndarray binned by given number of samples\n\n    Notes\n    -----\n    For each axis of array, shape must be an integer multiple of factor.\n\n    array may be ND, a scalar factor will broadcast to all dimensions.\n\n    To bin an image cube e.g. of shape (3, m, n),  use bindown(img, [1, factor, factor])\n\n    Raises\n    ------\n    ValueError\n        invalid mode\n\n    \"\"\"\n    if isinstance(factor, numbers.Number):\n        factor = tuple([factor] * array.ndim)\n\n    # these two lines look very complicated\n    # we want to take an array of shape (m, n) and a binning factor of say, 2\n    # and reshape the array to (m/2, 2, n/2, 2)\n    # these lines do that, for an arbitrary number of dimensions\n    output_shape = tuple(s//n for s, n in zip(array.shape, factor))\n    output_shape = tuple(itertools.chain(*zip(output_shape, factor)))\n    intermediate_view = array.reshape(output_shape)\n    # reduction axes produces (1, 3) for 2D, or (1, 3, 5) for 3D, etc.\n    reduction_axes = tuple(range(1, 2*array.ndim, 2))\n\n    if mode.lower() in ('avg', 'average', 'mean'):\n        output_data = intermediate_view.mean(axis=reduction_axes)\n    elif mode.lower() == 'sum':\n        output_data = intermediate_view.sum(axis=reduction_axes)\n    else:\n        raise ValueError('mode must be average or sum.')\n\n    return output_data",
        "sha1": "c2442c7734610c109aaa16c9c4e0cc2c550f83ff",
        "id": 398744
    },
    {
        "content": "def all_(obj):\n    \"\"\"Return True if all items in obj are true or if obj is empty.\"\"\"\n    return all(obj)",
        "sha1": "8ae3691d2faa911d197a81ee50c67963836bcdf3",
        "id": 507131
    },
    {
        "content": "from typing import Union\nfrom typing import List\nfrom typing import Tuple\nfrom typing import Iterable\n\n\ndef flatten(lst: Union[List, Tuple]) -> List:\n    \"\"\"Recursive function that flatten a list (of potential `Iterable` objects).\"\"\"\n    if isinstance(lst, Iterable) and not isinstance(lst, (str, bytes)):\n        return [a for i in lst for a in flatten(i)]\n    else:\n        return [lst]",
        "sha1": "f007f78f8b8b1e3bef9ce70550efdbfb8362539c",
        "id": 630901
    },
    {
        "content": "def _in_directories(filename, dirs):\n  \"\"\"Tests whether `filename` is anywhere in any of the given dirs.\"\"\"\n  for dirname in dirs:\n    if (filename.startswith(dirname)\n        and (len(filename) == len(dirname) or filename[len(dirname)] == '/')):\n      return True\n  return False",
        "sha1": "fc93e4bef45a4364446c0daa3e5969f143fbacc4",
        "id": 692039
    },
    {
        "content": "def _isIp(value):\n    \"\"\"\n    Check if the input string represents a valid IP address.\n\n    A valid IP can be one of the two forms:\n\n    1. A string that contains three '.' which separate the string into\n    four segments, each segment is an integer.\n\n    2. A string be either \"INVALID_IP(XXXl)\" or \"AUTO/NONE(XXXl)\",\n    where XXX is a long value.\n\n    Return a tuple, first element in the tuple is a boolean tells the validation result, while\n    the second element contains the error message if there is one.\n    \"\"\"\n    addrArray = value.split(\".\")\n    if not len(addrArray) == 4:\n        if value.startswith(\"INVALID_IP\") or value.startswith(\"AUTO/NONE\"):\n            tail = value.split(\"(\")\n            if len(tail) == 2:\n                longStrParts = tail[1].split(\"l\")\n                if len(longStrParts) == 2:\n                    try:\n                        int(longStrParts[0])\n                        return True, None\n                    except ValueError:\n                        return False, \"Invalid ip string: '{}'\".format(value)\n        return False, \"Invalid ip string: '{}'\".format(value)\n    else:\n        for segments in addrArray:\n            try:\n                segmentVal = int(segments)\n            except ValueError:\n                return (\n                    False,\n                    \"Ip segment is not a number: '{}' in ip string: '{}'\".format(\n                        segments, value\n                    ),\n                )\n            if not 0 <= segmentVal <= 255:\n                return (\n                    False,\n                    \"Ip segment is out of range 0-255: '{}' in ip string: '{}'\".format(\n                        segments, value\n                    ),\n                )\n        return True, None",
        "sha1": "261e39cf1e4c41838011513226003db1edb06cfc",
        "id": 422654
    },
    {
        "content": "def convert_dataset_name(store_name):\n    \"\"\"Parse {dataset}-{split} dataset name format.\"\"\"\n    dataset, *split = store_name.rsplit('-')\n    if split:\n        split = split[0]\n    else:\n        split = None\n    return dataset, split",
        "sha1": "351a96c3a79582e03e485e9ddbe77343e58440f4",
        "id": 382701
    },
    {
        "content": "def getPkgNameVer(pkgs):\n    \"\"\"\n    Takes a string list describing pinned packages e.g. numpy==1.15.0\n    Returns the package names and versions as separate string lists numpy and 1.15.0\n    \"\"\"\n    pkgNames = [m[0] for m in [k.split('==') for k in pkgs]]\n    pkgVers = [m[1] for m in [k.split('==') for k in pkgs]]\n    return pkgNames, pkgVers",
        "sha1": "d60a4942016f4586e7fb22a575bd3eaffe55d1b6",
        "id": 163216
    },
    {
        "content": "def indent_iterable(elems, num=2):\n    \"\"\"Indent an iterable.\"\"\"\n    return [\" \" * num + l for l in elems]",
        "sha1": "c79c336c2664f5ac588deb34f61bbd350c1b3367",
        "id": 348839
    },
    {
        "content": "def template_filters(repo, template_name):\n    \"\"\"Return True if the repo is created using the template\"\"\"\n    if repo.template != template_name:\n        return False\n    return True",
        "sha1": "df33b04d63be7d66ede4ebb3747085e2427f8e43",
        "id": 332659
    },
    {
        "content": "def is_shape(obj, ndim=None):\n    \"\"\"Check if an object is a shape, i.e., a n-dimension (n positive) tuple with positive values.\n\n    Parameters\n    ----------\n    obj : tuple\n        The object to be checked if it is a shape.\n    ndim : int, optional\n        The expected number of dimensions. Default value is None.\n\n    Returns\n    -------\n    bool\n        True if argument is a shape, False otherwise.\n\n    \"\"\"\n    if not isinstance(obj, tuple):\n        return False\n\n    if len(obj) == 0:\n        return False\n\n    if ndim is not None and len(obj) != ndim:\n        return False\n\n    for n in obj:\n        if n <= 0:\n            return False\n\n    return True",
        "sha1": "80632f871cc5ffce72d83a122b44fb92550c1587",
        "id": 200698
    },
    {
        "content": "def int_shape(x):\n    \"\"\"Shape of tensor as a list\"\"\"\n    return list(map(int, x.get_shape()))",
        "sha1": "cd6f8db6307d6afcbfc87880097546bce7a4c630",
        "id": 410719
    },
    {
        "content": "def make_safe_star_id(star_id):\n    \"\"\"  Make a star id that is safe to include in a URL.\n    :param star_id: star id that may contain spaces or plus signs [string].\n    :return: star_id safe to include in a URL [string].\n    \"\"\"\n    return star_id.replace(\"+\", \"%2B\").replace(\" \", \"+\")",
        "sha1": "904c6919b5c52ccd472f08955057d254e58b5ea0",
        "id": 213155
    },
    {
        "content": "def list_encoder_factory(type_callable):\n    \"\"\"\n    Creates a function encoder that iterates on the elements of a list to apply the specified type_callable format.\n    :param type_callable:  type to apply to data\n    :return:  function that applies type_callable to a supplied list of data\n    \"\"\"\n\n    def inner(data):\n        return [type_callable(x) for x in data]\n\n    return inner",
        "sha1": "6a892956d94e88e24ad738de6a19e2394aa34842",
        "id": 21470
    },
    {
        "content": "import math\n\n\ndef approximate_parameters(T):\n    \"\"\"\n    Creates L and k parameters from papers, based on how many iterations need to be\n    performed, and how much memory should be used.\n    \"\"\"\n    log_memory = math.log(10000000, 2)\n    log_T = math.log(T, 2)\n    L = 1\n    if (log_T - log_memory > 0):\n        L = math.ceil(pow(2, log_memory - 20))\n    k = round(math.log(T / (10 * L), 2))\n    return (L, k)",
        "sha1": "f26f7b4584b1c41744a9b5b8fd71fd9a46766960",
        "id": 203679
    },
    {
        "content": "def pair_sorter(aln):\n    \"\"\"Get the alignment name and attributes for sorting.\"\"\"\n    return (\n        aln.name,\n        not aln.first_in_pair,\n        aln.unmapped,\n        aln.supplementary_alignment,\n        aln.secondary_alignment)",
        "sha1": "217eac7c89a12f68f4c9fe324c4feb6c2a955d58",
        "id": 708724
    },
    {
        "content": "def median(freqs):\n\t\"\"\"Median from list of numbers and their frequencies\"\"\"\n\n\tsamples = 0\n\n\tfor (number, freq) in freqs:\n\t\tsamples += freq\n\n\tsamples_median = samples / 2.0\n\n\tcurrent_samples = 0\n\tfor (number, freq) in freqs:\n\t\tcurrent_samples += freq\n\t\tif current_samples >= samples_median:\n\t\t\treturn number\n\n\treturn 0",
        "sha1": "c72b913d8b647e54263996a148e30b169cea9fe3",
        "id": 164494
    },
    {
        "content": "def is_grpc_service_dir(files):\n    \"\"\"Returns true iff the directory hosts a gRPC service.\"\"\"\n    return \".grpc_service\" in files",
        "sha1": "3f32edbda28361e41a0f565ce9b9e22a660cb807",
        "id": 169951
    },
    {
        "content": "def ping(request, s=None):\n    \"\"\" Test integration.\n\n    Returns\n    -------\n    str\n        String 'pong'\n\n    \"\"\"\n    return 'pong'",
        "sha1": "cfbaaf3fc4eeaabe6fc8302d7dcc63d233696e46",
        "id": 627260
    },
    {
        "content": "def interpolate_tempfilt_loop(tempfilt, zgrid, zi, output):\n    \"\"\"    \n    Linear interpolate an Eazy \"tempfilt\" grid at z=zi.  \n    \n    `tempfilt` is [NFILT, NTEMP, NZ] integrated flux matrix\n    `zgrid` is [NZ] redshift grid\n    `output` is empty [NFILT, NTEMP] grid to speed up execution\n    \"\"\"\n    sh = tempfilt.shape\n    NF, NT, NZ = sh[0], sh[1], sh[2]\n    #output = np.zeros((NF, NT))\n    for iz in range(NZ-1):\n        dz = zgrid[iz+1]-zgrid[iz]\n        fint = 1 - (zi-zgrid[iz])/dz\n        if (fint > 0) & (fint <= 1):\n            fint2 = 1 - (zgrid[iz+1]-zi)/dz\n            # print iz, zgrid[iz], fint, fint2\n            for ifilt in range(NF):\n                for itemp in range(NT):\n                    #print ifilt, itemp\n                    output[ifilt, itemp] = tempfilt[ifilt, itemp, iz]*fint + tempfilt[ifilt, itemp, iz+1]*fint2\n            break              \n    \n    return output",
        "sha1": "07c288d02bde0763f1e74577f34a51d03b2ea796",
        "id": 92316
    },
    {
        "content": "def getnightinfo(sdb, obsdate):\n    \"\"\"Get the NightInfo_Id for an observing date\n\n    Parameters\n    ----------\n    sdb: ~mysql.mysql\n       A connection to the sdb database\n\n    obsdate: str\n       Observing date in YYYYMMDD format\n\n    \"\"\"\n\n    return sdb.select('NightInfo_Id', 'NightInfo', 'Date=%s' % obsdate)[0][0]",
        "sha1": "91d0ed44e511abdeaddb4c0cfa59dbfc6e18cef6",
        "id": 420877
    },
    {
        "content": "def get_high_cardinality_features(X, threshold=50):\n    \"\"\"Get features with more unique values than the specified threshold.\"\"\"\n    high_cardinality_features = []\n    for c in X.columns:\n        if X[c].nunique() > threshold:\n            high_cardinality_features.append(c)\n    return high_cardinality_features",
        "sha1": "4ab222c12a68c2ce259b9de41998d649d80b30ef",
        "id": 19370
    },
    {
        "content": "def get_user_doc(db, name):\n    \"\"\"Get the document for the account given by name (email or username).\n    Raise ValueError if no such account.\"\"\"\n    # 'name' is the email address for the account\n    if '@' in name:\n        viewname = 'user/email'\n    # else 'name' is the username for the account\n    else:\n        viewname = 'user/username'\n    result = list(db.view(viewname, include_docs=True)[name])\n    if len(result) != 1:\n        raise ValueError(\"no such user account '{0}'\".format(name))\n    return result[0].doc",
        "sha1": "8420522019fe69509c700f7c28259b3a5c1e1f8a",
        "id": 411519
    },
    {
        "content": "def filter_X_dilutions(df, concentration):\n    \"\"\"Select only one dilution ('high', 'low', or some number).\"\"\"\n    assert concentration in ['high','low'] or type(concentration) is int\n    df = df.sort_index(level=['CID','Dilution']) \n    df = df.fillna(999) # Pandas doesn't select correctly on NaNs\n    if concentration == 'low':\n        df = df.groupby(level=['CID']).first()\n    elif concentration == 'high':\n        df = df.groupby(level=['CID']).last()\n    else:\n        df = df.loc[[x for x in df.index if x[1]==concentration]]\n        df = df.groupby(level=['CID']).last()\n    df = df.replace(999,float('NaN')) # Undo the fillna line above. \n    return df",
        "sha1": "b886c87c1c5b96e6efc951ef197d3a0fb13707c1",
        "id": 4930
    },
    {
        "content": "def create_precalc_ye_filename(config,psm_key,prior_kind):\n    \"\"\"\n    Create the filename to use for the precalculated Ye file from the provided\n    configuration.  Uses the prior, psm, and proxy configuration to determine a\n    unique name for the current experiment.\n\n    Parameters\n    ----------\n    config: Config\n        Instance of an LMR_config.Config object.\n    psm_key: str\n        Indicates the psm type with which Ye values were calculated.\n    prior_kind: str\n        Indicates whether Ye values were calculated using \n        anomalies ('anom') of full field ('full') as prior data\n\n    Returns\n    -------\n    str:\n        Filename string based on current configuration\n    \"\"\"\n\n    proxy_database = config.proxies.use_from[0]\n\n    # Generate PSM calibration string\n    calib_str_ext = ''\n    if config.core.anom_reference_period:\n        calib_str_ext = calib_str_ext+'_ref{}-{}'.format(str(config.core.anom_reference_period[0]),\n                                                         str(config.core.anom_reference_period[1]))\n    if config.psm.calib_period:\n        calib_str_ext = calib_str_ext+'_cal{}-{}'.format(str(config.psm.calib_period[0]),\n                                                         str(config.psm.calib_period[1]))\n    \n    if psm_key == 'linear':\n        calib_avgPeriod = config.psm.linear.avgPeriod\n        calib_str = config.psm.linear.datatag_calib+calib_str_ext\n        state_vars_for_ye = config.psm.linear.psm_required_variables\n        \n    elif psm_key == 'linear_TorP':\n        calib_avgPeriod = config.psm.linear_TorP.avgPeriod\n        calib_str = 'T:{}-PR:{}'.format(config.psm.linear_TorP.datatag_calib_T,\n                                        config.psm.linear_TorP.datatag_calib_P)\n        calib_str = calib_str+calib_str_ext\n        state_vars_for_ye = config.psm.linear_TorP.psm_required_variables\n\n    elif psm_key == 'bilinear':\n        calib_avgPeriod = config.psm.bilinear.avgPeriod\n        calib_str = 'T:{}-PR:{}'.format(config.psm.bilinear.datatag_calib_T,\n                                        config.psm.bilinear.datatag_calib_P)\n        calib_str = calib_str+calib_str_ext\n        state_vars_for_ye = config.psm.bilinear.psm_required_variables\n        \n    elif psm_key == 'h_interp':\n        calib_avgPeriod = None\n        calib_str = ''\n        state_vars_for_ye = config.psm.h_interp.psm_required_variables\n\n    elif  psm_key == 'bayesreg_uk37':\n        calib_avgPeriod = ''.join([str(config.prior.avgInterval['multiyear'][0]),'yrs'])\n        calib_str = ''\n        state_vars_for_ye = config.psm.bayesreg_uk37.psm_required_variables\n        \n    else:\n        raise ValueError('Unrecognized PSM key.')\n\n    \n    if calib_avgPeriod:\n        psm_str = psm_key +'_'+ calib_avgPeriod + '-' + calib_str\n    else:\n        psm_str = psm_key + '-' + calib_str\n\n    proxy_str = str(proxy_database)\n    if proxy_str == 'LMRdb':\n        proxy_str = proxy_str + str(config.proxies.LMRdb.dbversion)\n    elif proxy_str == 'NCDCdtda':\n        proxy_str = proxy_str + str(config.proxies.NCDCdtda.dbversion)\n        \n    # Generate appropriate prior string\n    prior_str = '-'.join([config.prior.prior_source] +\n                         sorted(state_vars_for_ye) + [prior_kind])\n\n    return '{}_{}_{}.npz'.format(prior_str, psm_str, proxy_str)",
        "sha1": "0f13c90eb9c74894f6be37ea9f978d5a49a4ed18",
        "id": 423613
    },
    {
        "content": "def check_overlapping(relation, formatted_relation):\n    \"\"\"\n    Check if there is no overlapping between the two expanded arguments of a relation.\n    If there is an overlap, we drop the expansion for the relation.\n    \"\"\"\n    arg1_b, arg1_e = formatted_relation[\"arg1_begin_char\"], formatted_relation[\"arg1_end_char\"]\n    arg2_b, arg2_e = formatted_relation[\"arg2_begin_char\"], formatted_relation[\"arg2_end_char\"]\n\n    overlap = False or (arg1_b < arg2_e and arg1_e >= arg2_b) or (arg2_b < arg1_e and arg2_e >= arg1_b)\n\n    if overlap:\n        arg1_i, arg2_i = relation[\"arg1_index\"], relation[\"arg2_index\"]\n        relation[\"arg1_begin_token\"], relation[\"arg1_end_token\"] = arg1_i, arg1_i\n        relation[\"arg2_begin_token\"], relation[\"arg2_end_token\"] = arg2_i, arg2_i\n        return relation\n    else:\n        return formatted_relation",
        "sha1": "9a605e7a31ada23d8a002f7c2fb481832cfd6870",
        "id": 546971
    },
    {
        "content": "def AdaptDate(date_obj):\n  \"\"\"Adapts a datetime.date object into its daynumber since Common Era.\"\"\"\n  return date_obj.toordinal()",
        "sha1": "0cd8bac08613170411f0d07799261a556bcf3f7f",
        "id": 177109
    },
    {
        "content": "def dict_of_samples(samples):\n    \"\"\"\n    Gives a dictionary containing {'Sample_name' : [all samples]}\n    Args:\n        samples (list): The output from make_samples. Each entry in the list is a tuple containing (parameter_dict, species_dict)\n\n    Returns:\n        A dictionary containing {'Sample_name' : [all samples]}\n\n    \"\"\"\n\n    samples_dict = {}\n\n    for s in samples:\n        # s = (param_dict, species_dict)\n        for name in {**s[0], **s[1]}:\n\n            try:\n                values = samples_dict[name]\n            except KeyError:\n                values = samples_dict[name] = []\n\n            sample_to_add = {**s[0], **s[1]}[name]\n            values.append(sample_to_add)\n\n    return samples_dict",
        "sha1": "a7dc89b1aa8567c62fe0a92067455b9b79d1839c",
        "id": 141036
    },
    {
        "content": "def netperf_commands(target_ips):\n    \"\"\"Generate latency or throughput commands for netperf\n\n    Args:\n        target_ips (list(str)): List of ips to use netperf to\n        mode (str): Generate latency or throughput commands\n\n    Returns:\n        list(str): List of netperf commands\n    \"\"\"\n    lat_commands = []\n    tp_commands = []\n    for ip in target_ips:\n        lat_commands.append(\n            [\n                \"netperf\",\n                \"-H\",\n                ip,\n                \"-t\",\n                \"TCP_RR\",\n                \"--\",\n                \"-O\",\n                \"min_latency,mean_latency,max_latency,stddev_latency,transaction_rate,p50_latency,p90_latency,p99_latency\",\n            ]\n        )\n\n        tp_commands.append([\"netperf\", \"-H\", ip, \"-t\", \"TCP_STREAM\"])\n\n    return lat_commands, tp_commands",
        "sha1": "53879db0c29f2db3c259b774e997a39bcd41efd3",
        "id": 429227
    },
    {
        "content": "def read_maze(maze_file):\n    \"\"\" (file open for reading) -> list of list of str\n\n    Return the contents of maze_file in a list of list of str,\n    where each character is a separate entry in the list.\n    \"\"\"\n\n    res = []\n    for line in maze_file:\n        maze_row = [ch for ch in line.strip()]\n        res.append(maze_row)\n\n    return res",
        "sha1": "2084ac891012932774d46d507f550e8070e3cc47",
        "id": 42814
    },
    {
        "content": "def get_risk_score(track, other):\n    \"\"\"\n    Get \"risk\" score between a track an another track. The \"risk\" evaluates\n    how likely the two tracks are to be the same, lower being better, with\n    0 being a perfect match. This is a super handcrafted score, with little\n    to no tuning, will probably not catch all possible cases.\n\n    The scoring rules are:\n      * if N artists over M are missing, then risk += N/M\n      * if either the track title or the other track title are empty; score +=1\n      * if the titles of the two tracks do not match, add to\n        score += (index of disagreement between title and other) / (length of track title)\n\n    Parameters\n    ----------\n    track : bes.track.Track\n        Reference track.\n    other : bes.track.Track\n        Track to compare to reference and evaluate risk for.\n\n    Returns\n    -------\n    score : float\n        Risk score, positive real number. 0 means perfect match.\n    missing_artists : list of str\n        Missing artists.\n    title_deviation : str\n        If the title of the two tracks deviate after N elements, this returns\n        the title of the \"other\" track after this deviation. So for instance\n        let's say we have:\n          1. track.title = \"teknology (original mix)\"\n          2. other.title = \"teknology (gods of technology remix)\"\n        Then title_deviation will be \"gods of technology remix)\".\n\n    \"\"\"\n    score = 0\n\n    # do the artists match?\n    expected_artists = set([artist.casefold() for artist in track.artists])\n    matched_artists = set([artist.casefold() for artist in other.artists])\n    # union should be the same as intersection\n    missing_artists = expected_artists - matched_artists\n    if len(missing_artists) and len(expected_artists) > 1:\n        # check if name is not stored as single artist like Oden & Fatzo\n        expected_artist = ' & '.join(track.artists).casefold().strip()\n        matched_artist = other.artists[0].casefold().strip()\n        if expected_artist != matched_artist:\n            score += len(missing_artists) / len(expected_artists)\n        else:\n            missing_artists = set()\n    else:\n        score += len(missing_artists) / len(expected_artists)\n\n    # does the track name match?\n    expected_title = track.title.casefold().strip()\n    matched_title = other.title.casefold().strip()\n    i = 0\n    if len(expected_title) == 0 or len(matched_title) == 0:\n        # something problematic happened upstream, bump risk by 1\n        score += 1\n    elif expected_title != matched_title:\n        indices = [i for i in range(min(len(expected_title), len(matched_title))) if matched_title[i] != expected_title[i]]\n        index = len(expected_title) if not len(indices) else indices[0]\n        score += (index + 1) / len(expected_title)\n\n    return score, missing_artists, matched_title[len(expected_title) - i:]",
        "sha1": "4216a872c1c67aeb72002f9ce0dc03bc879e98dc",
        "id": 630621
    },
    {
        "content": "def verificar_qtd_de_dados(x, y):\n    \"\"\" Verifica se a quantidade de amostras de entrada \u00e9 igual a quantidade de amostras de sa\u00edda.\"\"\"\n    if len(x) != len(y):\n        print(\"N\u00e3o foi poss\u00edvel treinar a rede.\")\n        print(\"Quantidade de dados de entrada diferente da quantidade de dados de saida.\")\n        print(\"Entrada: {0} amostras\".format(len(x)))\n        print(\"Saida: {0} amostras\".format(len(y)))\n        return False\n    return True",
        "sha1": "be5fd0f8bfb883d9fb8f345e3658e8f44ba1cc62",
        "id": 364458
    },
    {
        "content": "import tempfile\n\n\ndef _tmpfile(content):\n    \"\"\"Returns the path to a temp file with the given contents.\"\"\"\n    tmp = tempfile.mkstemp()[1]\n    with open(tmp, 'w') as f:\n        f.write(content)\n    return tmp",
        "sha1": "2f6eb60a434293574af2dd77d69410d5dfea0faa",
        "id": 569976
    },
    {
        "content": "def import_cytoband(file, target, delimiter='\\t'):\n    \"\"\"\n    Returns the genomic region from the cytoband composite file\n    for a given target\n    \"\"\"\n    bp_chr = list()\n    bp_coords = list()\n    with open(file, 'r') as fh:\n        for line in fh:\n            fields = line.rstrip().split()\n            if len(fields) != 6:\n                continue\n            (chrom, start, end, _, _, key) = fields\n            start = int(start)\n            end = int(end)\n\n            # handle ambiguous regions\n            if key.find(target) != -1:\n                bp_chr.append(chrom)\n                bp_coords.append(start)\n                bp_coords.append(end)\n    bp_coords = sorted(bp_coords)\n    chrom = bp_chr[0]\n    start = bp_coords[0]\n    end = bp_coords[-1]\n    region = {\"chrom\" : chrom, \"start\": start, \"end\": end}\n    return region\n    #return f'{chrom}:{start}-{end}'",
        "sha1": "200a56dbe6f3a1c92d8a78ecfec7f7a319bfb1f6",
        "id": 278701
    },
    {
        "content": "def pairs_do_overlap(algns1, algns2, allowed_offset=5):\n    \"\"\"\n    Forward read:                             Reverse read:\n    ----------------------->      <------------------------\n             algns1                        algns2\n    5----------3_5----------3     3----------5_3----------5\n    algn1_chim5   algn1_chim3     algn2_chim3   algn2_chim5\n    chim_left     chim_right      chim_left     chim_right\n\n    Two pairs of alignments overlap if:\n    1) algn1_chim5 and algn2_chim3 originate from the same region (chim_left),\n    2) algn1_chim3 and algn2_chim5 originate from the same region (chim_right).\n    or:\n    3) pos3 of algn1_chim5 is close to pos3 of algn2_chim3,\n    4) pos5 of algn1_chim3 is close to pos5 of algn2_chim5.\n\n    Return: 1 of the pairs of alignments are overlaps,\n            0 if they are not.\n    \"\"\"\n\n    # Some assignments to simplify the code\n    algn1_chim5 = algns1[0]\n    algn1_chim3 = algns1[1]\n    algn2_chim5 = algns2[0]\n    algn2_chim3 = algns2[1]\n\n    # We assume that successful alignment cannot be an overlap with unmapped or multi-mapped region\n    mapped_algn1_chim5 = (algn1_chim5['is_mapped'] and algn1_chim5['is_unique'])\n    mapped_algn1_chim3 = (algn1_chim3['is_mapped'] and algn1_chim3['is_unique'])\n    mapped_algn2_chim5 = (algn2_chim5['is_mapped'] and algn2_chim5['is_unique'])\n    mapped_algn2_chim3 = (algn2_chim3['is_mapped'] and algn2_chim3['is_unique'])\n\n    if not mapped_algn1_chim5 and not mapped_algn2_chim3:\n        chim_left_overlap = True\n    elif not mapped_algn1_chim5 and mapped_algn2_chim3:\n        chim_left_overlap = False\n    elif mapped_algn1_chim5 and not mapped_algn2_chim3:\n        chim_left_overlap = False\n    else:\n        chim_left_overlap = True\n        chim_left_overlap &= (algn1_chim5['chrom'] == algn2_chim3['chrom'])\n        chim_left_overlap &= (algn1_chim5['strand'] != algn2_chim3['strand'])\n\n    if not mapped_algn1_chim3 and not mapped_algn2_chim5:\n        chim_right_overlap = True\n    elif not mapped_algn1_chim3 and mapped_algn2_chim5:\n        chim_right_overlap = False\n    elif mapped_algn1_chim3 and not mapped_algn2_chim5:\n        chim_right_overlap = False\n    else:\n        chim_right_overlap = True\n        chim_right_overlap &= (algn1_chim3['chrom'] == algn2_chim5['chrom'])\n        chim_right_overlap &= (algn1_chim3['strand'] != algn2_chim5['strand'])\n\n    same_junction = True\n    same_junction &= (abs(algn1_chim5['pos3'] - algn2_chim3['pos5']) <= allowed_offset)\n    same_junction &= (abs(algn1_chim3['pos5'] - algn2_chim5['pos3']) <= allowed_offset)\n\n    if chim_left_overlap & chim_right_overlap & same_junction:\n        return 1\n    else:\n        return 0",
        "sha1": "618777f0dc30db4fbcd9da4b86fd62b64705a663",
        "id": 267839
    },
    {
        "content": "def page_not_found(e):\n    \"\"\" Return error 404 \"\"\"\n    return 'Page not found from script-api'",
        "sha1": "7fd7614fe4d38cfeb27a9e2d0f8dea31380339d3",
        "id": 283276
    },
    {
        "content": "import platform\nimport click\n\n\ndef strip_style_win32(styled_output: str) -> str:\n    \"\"\"Strip text style on Windows.\n\n    `click.style` produces ANSI sequences, however they were not supported\n    by PowerShell until recently and colored output is created differently.\n    \"\"\"\n    if platform.system() == \"Windows\":\n        return click.unstyle(styled_output)\n    return styled_output",
        "sha1": "581ab6062efd99764aa15ac3b4574e2c165ec642",
        "id": 82493
    },
    {
        "content": "def istype(obj, check):\n    \"\"\"like isinstance(obj, check), but strict\n\n    This won't catch subclasses.\n    \"\"\"\n    if isinstance(check, tuple):\n        for cls in check:\n            if type(obj) is cls:\n                return True\n        return False\n    else:\n        return type(obj) is check",
        "sha1": "9729106f715c1eb71abd0c80937063e2617d4ab8",
        "id": 449997
    },
    {
        "content": "import math\n\n\ndef distance_vehicle(waypoint, vehicle_position):\n    \"\"\"\n    Calculate distance between waypoint and vehicle position\n    \"\"\"\n    dx = waypoint[0] - vehicle_position[0]\n    dy = waypoint[1] - vehicle_position[1]\n    dz = waypoint[2] - vehicle_position[2]\n\n    return math.sqrt(dx * dx + dy * dy + dz * dz)",
        "sha1": "b8f9ef777996b45335511834e775afa22d085859",
        "id": 227780
    },
    {
        "content": "def favorite_animal(users_animal):\n    \"\"\"Display a message to the user that changes based on their favorite animal.\"\"\"\n    return f'Wow, {users_animal} is my favorite animal, too!'",
        "sha1": "f4ba5a418e6462b296b08257c945a5724eb59a80",
        "id": 530004
    },
    {
        "content": "def remove_test_deps(deps):\n    \"\"\"If running with pytest coverage enabled, these deps will show up. We\n    don't want run-env-dependent tests, so we just pop them out.\n    \"\"\"\n    for test_dep in [\"pytest_cov\", \"coverage\"]:\n        try:\n            deps.remove(test_dep)\n        except ValueError:\n            continue\n    return deps",
        "sha1": "dc18caffca45d75398f7fc278c34762fd21289a7",
        "id": 485085
    },
    {
        "content": "import json\nfrom pathlib import Path\n\n\ndef load_settings(filepath):\n    \"\"\"\n    Load settings from json file.\n\n    Parameters\n    ----------\n    filepath: pathlib.Path\n        Where to find redcode_settings.json\n\n    Returns\n    -------\n    dict\n    \"\"\"\n    try:\n        sett = json.loads(filepath.read_text())\n    except FileNotFoundError:\n        sett = {\"loadfrom\": str(Path.home())}\n        filepath.write_text(json.dumps(sett, indent=2))\n    return sett",
        "sha1": "e9bc1dd516e9bea5128931bc62224b9b17332e8b",
        "id": 349790
    },
    {
        "content": "import traceback\n\n\ndef _get_stack_info(frame):\n    \"\"\"\n    Get the stack information.\n\n    Args:\n        frame(frame): the frame requiring information.\n\n    Returns:\n        str, the string of the stack information.\n    \"\"\"\n    sinfo = None\n    stack_prefix = 'Stack (most recent call last):\\n'\n    sinfo = stack_prefix + \"\".join(traceback.format_stack(frame))\n    return sinfo",
        "sha1": "dfd50284dcbac11333b1c74e52ebfb7b3dfb6f23",
        "id": 284935
    },
    {
        "content": "from pathlib import Path\n\n\ndef test_directory(request):\n    \"\"\"Gets root directory of tests\"\"\"\n    return Path(request.config.rootdir) / \"tests\"",
        "sha1": "bd812fbcbc84eee350819b55ad4c590337bddeb2",
        "id": 567871
    },
    {
        "content": "def calculate_overshoot(cigar):\n    \"\"\" The calculate overshoot function calculates the number of basepairs that have not been mapped but are part of\n    the read.\n\n    :param cigar: a list containing tuples representing the cigar string.\n    :return overshoot: an integer indicating the number of basepairs in the read before it is mapped.\n    \"\"\"\n    overshoot = 0\n    for element in cigar:\n        if element[0]:\n            overshoot += element[1]\n        else:\n            break\n\n    return overshoot",
        "sha1": "687ce44bef05376339fbe7892b1c07b84981710f",
        "id": 79018
    },
    {
        "content": "def get_users(metadata):\n    \"\"\"\n    Pull users, handles hidden user errors\n    Parameters:\n        metadata: sheet of metadata from mwclient\n    Returns:\n        the list of users\n    \"\"\"\n    users = []\n    for rev in metadata:\n        try:\n            users.append(rev[\"user\"])\n        except (KeyError):\n            users.append(None)\n    return users",
        "sha1": "48dbae6a63019b0e4c2236a97e147102fe4d8758",
        "id": 2354
    },
    {
        "content": "import yaml\n\n\ndef load_yaml(fileuri):\n    \"\"\"Load yaml file into a dict\"\"\"\n    fd = open(fileuri, 'r')\n    try:\n        return yaml.safe_load(fd)\n    except Exception as e:\n        raise e\n    finally:\n        fd.close()",
        "sha1": "8f749af4b46d3e8255c6a6ca4af5c3797bbb4bed",
        "id": 660760
    },
    {
        "content": "import torch\n\n\ndef torch_pca_svd(X, center=True):\n    \"\"\"\n    perform PCA over X\n    - X is of size (num_points, vec_size)\n\n    NOTE: unlike PCA_eig, the number of vectors/values returned is: min(num_points, vec_size)\n    \"\"\"\n    n, _ = X.shape\n    # center points along axes\n    if center:\n        X = X - X.mean(dim=0)\n    # perform singular value decomposition\n    u, s, v = torch.svd(X)\n    # sort components by decreasing variance\n    # these are already sorted?\n    components = v.T\n    explained_variance = torch.mul(s, s) / (n-1)\n    return components, explained_variance",
        "sha1": "7c23ee8a8790f020ae00356d75b58e409d466754",
        "id": 261752
    },
    {
        "content": "def clone_leaf_type(src_api, src_type_id, dest_project, dest_api=None):\n    \"\"\" Clone leaf type.\n\n    This can be used to clone leaf types from one project to another or from one\n    host to another.\n\n    Example for different host:\n\n    .. code-block:: python\n\n        src_api = tator.get_api(host, token)\n        dest_api = tator.get_api(other_host, other_token)\n        src_type_id = 1\n        dest_project = 1\n        response = tator.util.clone_leaf_type(src_api, src_type_id, dest_project, dest_api)\n        print(response.message)\n\n    Example for same host:\n\n    .. code-block:: python\n\n        api = tator.get_api(host, token)\n        src_type_id = 1\n        dest_project = 1\n        response = tator.util.clone_leaf_type(src_api, src_type_id, dest_project)\n        print(response.message)\n\n    :param src_api: :class:`tator.TatorApi` object corresponding to source host or only\n        host if this is a clone on same host.\n    :param src_type_id: Unique integer identifying leaf type to clone.\n    :param dest_project: Unique integer identifying destination project.\n    :param dest_api: :class:`tator.TatorApi` object corresponding to destination host.\n    :returns: Response from leaf type creation request.\n    \"\"\"\n    type_obj = src_api.get_leaf_type(src_type_id)\n    spec = {'name': type_obj.name,\n            'description': type_obj.description,\n            'attribute_types': type_obj.attribute_types}\n    if dest_api is None:\n        dest_api = src_api\n    return dest_api.create_leaf_type(dest_project, leaf_type_spec=spec)",
        "sha1": "03fa35cbfe9ff70190cb7f173b616a4a9e360382",
        "id": 452889
    },
    {
        "content": "from pathlib import Path\n\n\ndef file_exists(file_path: Path):\n    \"\"\"Returns True if file exists, False otherwise.\"\"\"\n\n    return file_path.is_file()",
        "sha1": "4bf5bfdd0eb6d2bf715821f5ba18f1a5580ed2bd",
        "id": 282639
    },
    {
        "content": "def get_file_offset(vfp: int) -> int:\n    \"\"\"Convert a block compressed virtual file pointer to a file offset.\"\"\"\n    address_mask = 0xFFFFFFFFFFFF\n    return vfp >> 16 & address_mask",
        "sha1": "fbbca6bcf9fdcc95867d59a63ac6dbab877723f2",
        "id": 71878
    },
    {
        "content": "def bbox2location(bbox):\n\t\"\"\"\n\tFrom bbox [x,y,width,height] to location [top,right,bot,left]\n\t\n\tArgs:\n\t\tbbox (list): bounding box [x,y,width,height]\n\tReturns:\n\t\t**location** (list) - coordinate [top,right,bot,left]\n\t\"\"\"\n\tlocation = [bbox[1],bbox[0]+bbox[2],bbox[1]+bbox[3],bbox[0]]\n\n\treturn location",
        "sha1": "cca690d72c7887d96eea03810b439c06878dba4f",
        "id": 375147
    },
    {
        "content": "def color_text(string, color):\n\t\"\"\"\n\tcolor output in polybar format\n\t\"\"\"\n\n\tcolor_begin = '%{F' + color +  '}'\n\tcolor_end = '%{F-}'\n\treturn color_begin + string + color_end",
        "sha1": "60b984d068cbf4f368b23ee91566938e328958c0",
        "id": 387257
    },
    {
        "content": "def possible_negation_suffix(text: str) -> bool:\n    \"\"\"\n    Checks if the texts contains a possible negation suffix\n\n    :param text: string containing a token\n\n    :return: True if the texts ends with a possible negation suffix, False if not\n    \"\"\"\n    suffixes = (\"less\",)\n    # length is mentioned so it doesn't consider \"less\" as containing the suffix\n    return text.endswith(suffixes) and len(text) >= 5",
        "sha1": "0cb8c2f81d29e6b836c2b4a2da2198613bb894cd",
        "id": 33275
    },
    {
        "content": "from typing import Dict\n\n\ndef truecase(word: str, case_counter: Dict[str, int]):\n    \"\"\"\n    Truecase a word using a Truecase dictionary\n\n    :param word: a word\n    :param case_counter: A counter; a dictionary of words/tokens and their relative frequency counts\n    :return: the truecased word\n\n    >>> case_counts ={\"caesar\": 1, \"Caesar\": 99}\n    >>> truecase('CAESAR', case_counts)\n    'Caesar'\n\n    \"\"\"\n    lcount = case_counter.get(word.lower(), 0)\n    ucount = case_counter.get(word.upper(), 0)\n    tcount = case_counter.get(word.title(), 0)\n    if lcount == 0 and ucount == 0 and tcount == 0:\n        return word  #: we don't have enough information to change the case\n    if tcount > ucount and tcount > lcount:\n        return word.title()\n    if lcount > tcount and lcount > ucount:\n        return word.lower()\n    if ucount > tcount and ucount > lcount:\n        return word.upper()\n    return word",
        "sha1": "afc7d5de89ffb5c84ab43b129a88a3c43ce4b98a",
        "id": 373773
    },
    {
        "content": "def i2s(i):\n    \"\"\" Convert individual to string \"\"\"\n    return \"[%0.3f,%0.3f]\" % (i[0], i[1])",
        "sha1": "fd8f24e34c44bc82e5e7f9400d2a54b4bd642e46",
        "id": 465920
    },
    {
        "content": "def _compute_mod(score: int) -> int:\n    \"\"\"Compute a mod given an ability score\n\n    Args:\n        score (int): Ability score\n    Returns:\n        (int) Modifier for that score\n    \"\"\"\n    return score // 2 - 5",
        "sha1": "c427d5ac3a438bfb210066ebe759fb1dac1a677b",
        "id": 367386
    },
    {
        "content": "def remove_comma(in_str):\n    \"\"\" Remove comma from given string\n    \"\"\"\n    return str(in_str).replace(\",\", \" \").replace(\"  \", \" \")",
        "sha1": "c6058d6afb3acaa8cf5e98e0900bd75e921fbaed",
        "id": 306654
    },
    {
        "content": "def findTop(row, treshold):\n    \"\"\"\n    Find if the given row values is above the percentile treshold\n    \"\"\"\n    if row[\"Max score\"] >= treshold:\n        return 1\n    else:\n        return 0",
        "sha1": "63bf4ad8ff4b647581118a1a759d8f50b1b0a9cb",
        "id": 231304
    },
    {
        "content": "def ParseExecution(args):\n  \"\"\"Get and validate execution from the args.\"\"\"\n  return args.CONCEPTS.execution.Parse()",
        "sha1": "44f8f978ad558b05e9b959141563701ac22b6e9f",
        "id": 553602
    },
    {
        "content": "def dowt2di(Do, WT):\n    \"\"\"Calculate pipe inner diameter from outer diameter and wall thickness.\n    \"\"\"\n    return Do - 2 * WT",
        "sha1": "7b3846d93cbc43e7b6f724f4ceda1d05a02c0470",
        "id": 421612
    },
    {
        "content": "import socket\n\n\ndef is_ip(s):\n    \"\"\"Returns whether or not a given string is an IPV4 address.\"\"\"\n    try:\n        return bool(socket.inet_aton(s))\n    except socket.error:\n        return False",
        "sha1": "aa4d9f29afe68cea1efe3ab3100907e1b45b241d",
        "id": 587384
    },
    {
        "content": "import csv\n\n\ndef read_scores(csv_file):\n    \"\"\"\n    Reads the labels and scores from the CSV file.\n\n    :param csv_file: the CSV file to read\n    :type csv_file: str\n    :return: tuple of the dictionary with label -> score relations and the top label\n    :rtype: tuple\n    \"\"\"\n    label = \"?\"\n    prob = 0.0\n    label_scores = dict()\n    with open(csv_file, \"r\") as cf:\n        reader = csv.DictReader(cf)\n        for row in reader:\n            if ('probability' in row) and ('label' in row):\n                label_scores[row['label']] = float(row['probability'])\n                if float(row['probability']) > prob:\n                    prob = float(row['probability'])\n                    label = row['label']\n    return label_scores, label",
        "sha1": "19bd09b5188f408102970a38f38ac734074b7d91",
        "id": 425914
    },
    {
        "content": "def sum_of_squares(nn):\n    \"\"\"\n    return sum of the squares of the integers [1,nn]\n    \"\"\"\n    return sum ([X**2 for X in range(1, nn + 1)])",
        "sha1": "0aafa7e33d57e92397b92a68bcb5686f60ebbd0d",
        "id": 224283
    },
    {
        "content": "import math\n\n\ndef get_total_page(total, per):\n    \"\"\"\n    Get the page count.\n    :param total: total count\n    :param per: count per page\n    :return: page count int\n    \"\"\"\n    return int(math.ceil(1.0 * total / per))",
        "sha1": "0307fe68dc47f804b39f55ba7d8374ef5dca7468",
        "id": 363704
    },
    {
        "content": "def valid(bo, pos, num):\n    \"\"\"\n    Returns if the attempted move is valid\n    :param bo: 2d list of ints\n    :param pos: (row, col)\n    :param num: int\n    :return: bool\n    \"\"\"\n\n    # Check row\n    for i in range(0, len(bo)):\n        if bo[pos[0]][i] == num and pos[1] != i:\n            return False\n\n    # Check Col\n    for i in range(0, len(bo)):\n        if bo[i][pos[1]] == num and pos[1] != i:\n            return False\n\n    # Check box\n\n    box_x = pos[1]//3\n    box_y = pos[0]//3\n\n    for i in range(box_y*3, box_y*3 + 3):\n        for j in range(box_x*3, box_x*3 + 3):\n            if bo[i][j] == num and (i,j) != pos:\n                return False\n\n    return True",
        "sha1": "3a7efec63644b0835019edcfd7c5fdcfe95dc5f3",
        "id": 502149
    },
    {
        "content": "import re\n\n\ndef version_tuple(name):\n    \"\"\"Given 'Houdini 13.0.376', returns the tuple (13, 0, 376).\"\"\"\n    return tuple([int(d) for d in re.findall('[0-9]{1,4}', name)])",
        "sha1": "4dba212e72f8b60a09f6c8ccf329e025c000c84f",
        "id": 286857
    },
    {
        "content": "def format_nav_item(url: str, title: str) -> str:\n    \"\"\"Format a single entry for the navigation bar (navbar).\"\"\"\n    return '<li class=\"nav-item\"><a class=\"nav-link\" href=\"{}\">{}</a></li>'.format(url, title)",
        "sha1": "d9c3e6a424bee4846a7f62822476dbeb6a909d99",
        "id": 74335
    },
    {
        "content": "import re\n\n\ndef convert_RL_name(name):\n    \"\"\"Switch R-L or L-R presceding, following or in-between underscores\n\n    Args:\n        name(str): String to operate on\n    Returns:\n        str: Switched name\n\n    Example:\n        _L -> _R /  _L0_ -> _R0_ / L_ > R_ ...\n    \"\"\"\n\n    # If the input is just L or R return the converted\n    if name == \"L\":\n        return \"R\"\n    elif name == \"R\":\n        return \"L\"\n\n    # Creates pattern\n    re_str = \"_[RL][0-9]+_|^[RL][0-9]+_|_[RL][0-9]+$|_[RL]_|^[RL]_|_[RL]$\"\n    re_pattern = re.compile(re_str)\n\n    # Search for matches\n    re_match = re.search(re_pattern, name)\n\n    # Returns unchanged name if nothing matches\n    if not re_match:\n        return name\n\n    instance = re_match.group(0)\n    if instance.find(\"R\") != -1:\n        rep = instance.replace(\"R\", \"L\")\n    else:\n        rep = instance.replace(\"L\", \"R\")\n\n    name = re.sub(re_pattern, rep, name)\n    return name",
        "sha1": "1010e3049afffd0ff1a745d8a92dd78fa8d0adcc",
        "id": 612913
    },
    {
        "content": "from typing import List\n\n\ndef has_empty_location(board: List[List[str]]) -> bool:\n    \"\"\"\n        Check if there are any empty placeholder.\n        (Note: top left corner is always empty and hence must be ignored.)\n\n        Parameters\n        ----------\n        board: List[List[str]]\n            2D array containing all the game detail, including column header, row header and placed buildings.\n    \"\"\"\n    count = 0\n\n    for row in board:\n        for col in row:\n            if col == ' ':\n                # * Use to early break if there are more than 2 empty placeholder.\n                if count > 1:\n                    return True\n                count += 1\n\n    return count > 1",
        "sha1": "3cab540ed2634f9d741fca2cae4365203a783c31",
        "id": 155971
    },
    {
        "content": "import json\n\n\ndef decode_response(res):\n    \"\"\"Parse a WebSuccess or WebError response.\"\"\"\n    decoded_dict = json.loads(res.data.decode('utf-8'))\n    return (decoded_dict['status'], decoded_dict['message'],\n            decoded_dict['data'])",
        "sha1": "e0e8b74ce31d6db6d77f91730b2a921562911afe",
        "id": 45406
    },
    {
        "content": "def init_empty_bounds(weights):\n    \"\"\"\n    Initialize list of empty bounds for a neural network\n    :param weights: weights in network (from which we can decide number of inputs and neurons)\n    :return: empty bounds list\n    \"\"\"\n    # Add input bounds\n    w, b = weights[0]\n    bounds = [[None] * w.shape[0]]\n\n    # Add neuron bounds (including output neuron)\n    for w, b in weights:\n        bounds += [[None] * w.shape[1]]\n\n    return bounds",
        "sha1": "041f6ad1e4315fe89117348fb88fe93d2d7c9106",
        "id": 505203
    },
    {
        "content": "def convert_size(free, total, mode):\n    \"\"\"\n    Takes free and total size and coverts them based on conversion mode\n\n    free - returns free\n    total - returns total\n    used - returns difference between total and free\n    pfree - returns free as percentage of total\n    pused - returns used as percentage of total\n\n    :param mode: one of SIZE_CONVERSION_MODES\n    \"\"\"\n    if total == 0:\n        return 0  # even if free is not 0, it is better to alert authorities\n    value = None\n    if mode == 'free':\n        value = free\n    elif mode == 'total':\n        value = total\n    elif mode == 'used':\n        value = total - free\n    elif mode == 'pfree':\n        value = (free / total) * 100\n    elif mode == 'pused':\n        used = (total - free)\n        value = (used / total) * 100\n    return value",
        "sha1": "62ae759359758d511792412ee04b6b9a581a4fb7",
        "id": 102934
    },
    {
        "content": "def nohighlight(nick):\n    \"\"\"add a ZWNJ to nick to prevent highlight\"\"\"\n    return nick[0] + \"\\u200c\" + nick[1:]",
        "sha1": "1b8d0cafc5df4a442daafdece59af1675ab1de33",
        "id": 705607
    },
    {
        "content": "def byte_metrics(metrics: list) -> list:\n    \"\"\"Filter all metrics to only include those related to bytes\n\n    Args:\n        metrics (list): List of all metrics found on nodes\n\n    Returns:\n        list: List of filtered metrics\n    \"\"\"\n    metrics = [metric for metric in metrics if \"Bytes\" in metric]\n    return list(set(metrics))",
        "sha1": "f3b1ebe24008584d68cf80c0e492d9f199eee979",
        "id": 420779
    },
    {
        "content": "def handle_path_darwin(path):\n    \"\"\"find the proper path if using darwin based OS\"\"\"\n    if path.endswith(\"/\"):\n        return \"{0}simc\".format(path)\n    return \"{0}/simc\".format(path)",
        "sha1": "2d5141c0b13c3a7a812ee960e177b2eba458f332",
        "id": 375466
    },
    {
        "content": "from pathlib import Path\nfrom typing import List\n\n\ndef read_csv_to_list(path: Path) -> List[str]:\n    \"\"\"Takes in a csv file path and splits the strings\n    and puts them into a list\n\n    Args:\n        path (Path): CSV file path\n\n    Returns:\n        List[str]: List of strings from the file\n    \"\"\"\n    list_of_strings = []\n    with open(path) as file:\n        for line in file:\n            line = line.strip()\n            list_of_strings.extend([match for match in line.split(\",\") if match != \"\"])\n\n    return list_of_strings",
        "sha1": "1bb6a4ff1dca94451970df96275d40f52672f4cf",
        "id": 385731
    },
    {
        "content": "import torch\nimport math\n\n\ndef affine_transformation_rotation2d(angle_radian: float) -> torch.Tensor:\n    \"\"\"\n    Defines a 2D rotation transform\n    Args:\n        angle_radian: the rotation angle in radian\n\n    Returns:\n        a 3x3 transformation matrix\n    \"\"\"\n\n    rotation = torch.tensor([\n        [math.cos(angle_radian), math.sin(angle_radian), 0],\n        [-math.sin(angle_radian), math.cos(angle_radian), 0],\n        [0, 0, 1]\n    ], dtype=torch.float32)\n    return rotation",
        "sha1": "527f73fe0ee0a4c6d680bf69c444b568b002ec20",
        "id": 560209
    },
    {
        "content": "import torch\n\n\ndef collate_embedding(batch):\n    \"\"\"\n    unzip and merge list of torch.Tensor into single Tensor by stacking them\n    \"\"\"\n\n    batch, indices = zip(*batch)\n\n    return torch.stack(batch), indices",
        "sha1": "ab0ab2f39e38a3b91ac1b32d8d0c44d3bf55a497",
        "id": 555024
    },
    {
        "content": "def flip(str_array: list[str]) -> list[str]:\n    \"\"\"\n    Flip a list of strings on a top-left to bottom-right diagonal\n\n    :param str_array: array as a list of strings\n    :return: array flipped\n    \"\"\"\n    return list(reversed(str_array.copy()))",
        "sha1": "572ca4337da74493f9f3e489e2875ea7f356f16f",
        "id": 658548
    },
    {
        "content": "def add_single_letter_words(words):\n    \"\"\"Add single letter words to the dictionary.\n    \"\"\"\n    words['']=None\n    words['i']=None\n    words['a']=None\n    return words",
        "sha1": "0c2eacaf97dddbe0d8106bf38d6b6a7e86f31fa5",
        "id": 96443
    },
    {
        "content": "import re\n\n\ndef remove_punctuation(text, exceptions=None):\n    \"\"\"\n    Return a string with punctuation removed.\n\n    Parameters:\n        text (str): The text to remove punctuation from.\n        exceptions (list): List of symbols to keep in the given text.\n\n    Return:\n        str: The input text without the punctuation.\n    \"\"\"\n\n    all_but = [\n        r'\\w',\n        r'\\s'\n    ]\n\n    if exceptions is not None:\n        all_but.extend(exceptions)\n\n    pattern = '[^{}]'.format(''.join(all_but))\n\n    return re.sub(pattern, '', text)",
        "sha1": "0e8d9ebbf7345a1da489197e5f768e3e480d990c",
        "id": 182930
    },
    {
        "content": "def find_min_pos(input_list):\n  \"\"\"Function to find index with minimum value.\"\"\"\n  length = len(input_list)\n  if length == 0:\n    return -1\n\n  curr_min = input_list[0]\n  curr_min_index = 0\n\n  for j in range(1, length):\n    if curr_min > input_list[j]:\n      curr_min = input_list[j]\n      curr_min_index = j\n\n  return curr_min_index",
        "sha1": "d2d72b0fdf6ada026f7675f37ff7223384ec076f",
        "id": 671353
    },
    {
        "content": "def get_imag(input, input_type=\"linear\", channels_axis=1):\n    \"\"\"Returns the imaginary components of the complex-valued input.\n\n    Arguments\n    ---------\n    input : torch.Tensor\n        Input tensor.\n    input_type : str,\n        (convolution, linear) (default \"linear\")\n    channels_axis : int.\n        Default 1.\n    \"\"\"\n\n    if input_type == \"linear\":\n        nb_hidden = input.size()[-1]\n        if input.dim() == 2:\n            return input.narrow(\n                1, nb_hidden // 2, nb_hidden // 2\n            )  # input[:, :nb_hidden / 2]\n        elif input.dim() == 3:\n            return input.narrow(\n                2, nb_hidden // 2, nb_hidden // 2\n            )  # input[:, :, :nb_hidden / 2]\n    else:\n        nb_featmaps = input.size(channels_axis)\n        return input.narrow(channels_axis, nb_featmaps // 2, nb_featmaps // 2)",
        "sha1": "168f2a89dd76cd0bfaa41eae1f8b4138832ecffc",
        "id": 620322
    },
    {
        "content": "import pytz\n\n\ndef all_timezones() -> list:\n    \"\"\"return a list contains all timezone names\"\"\"\n    return pytz.all_timezones",
        "sha1": "bdb10587f24489c4b8b4db230ec598351ea0aaea",
        "id": 484819
    },
    {
        "content": "def get_score(words_from_dialog, subs):\n    \"\"\"\n    :param subs: A list containing the substitles we try to match to this dialog line. sub.txt has to be BOW.\n    :param words_from_dialog:  A dialog line (i.e. a character's dialog block from the screenplay) in BOW representation.\n    :return: the matching's score.\n    \"\"\"\n    # will run untill match cannot improve anymore\n    try:\n        words_from_subtitles = frozenset.union(*(sub.txt for sub in subs))\n    except TypeError:\n        # no subtitles\n        words_from_subtitles = frozenset()\n\n    # calculate BOW intersection and union\n    intersection = len(words_from_dialog & words_from_subtitles)\n    union = len(words_from_dialog | words_from_subtitles)\n\n    return intersection / union",
        "sha1": "bc259f3d84d975f177ff0ee7c2cb46d09ea2c397",
        "id": 585821
    },
    {
        "content": "def encode_label(label):\n    \"\"\"\n    Encode category label to one-hot vector\n    @label: Category label, 0 or 1\n    @return: One-hot vector\n    \"\"\"\n    return [1, 0] if int(label)==0 else [0,1]",
        "sha1": "a75b4fa5b7e9753b08a4f59e522b12b6a11637f8",
        "id": 221045
    },
    {
        "content": "def freq_in_kmskpc(vo,ro):\n    \"\"\"\n    NAME:\n\n       freq_in_kmskpc\n\n    PURPOSE:\n\n       convert a frequency to km/s/kpc\n\n    INPUT:\n\n       vo - velocity unit in km/s\n\n       ro - length unit in kpc\n\n    OUTPUT:\n\n       conversion from units where vo=1. at ro=1.\n\n    HISTORY:\n\n       2013-09-01 - Written - Bovy (IAS)\n\n    \"\"\"\n    return vo/ro",
        "sha1": "64f772917dfd78e1029e07d7987f9999bd171e39",
        "id": 602964
    },
    {
        "content": "def bio_to_spans(predictions, label_dict):\n  \"\"\" Convert BIO-based predictions to a set of arguments.\n      Arguments:\n        predictions: A single integer array, already truncated to the original sequence lengths.\n        label_dict: Label dictionary.\n      Returns:\n        A sequence of labeled arguments: [ (\"ARG_LABEL\", span_start, span_end), ... ], ordered by their positions.\n  \"\"\"\n  args = []\n  tags = [label_dict.idx2str[p] for p in predictions]\n  for (i, tag) in enumerate(tags):\n    if tag == 'O':\n      continue\n    label = tag[2:]\n    # Append new span.\n    if tag[0] == 'B' or len(args) == 0 or label != tags[i-1][2:]:\n      args.append([label, i, -1])\n    # Close current span.\n    if i == len(predictions) - 1 or tags[i+1][0] == 'B' or label != tags[i+1][2:]:\n      args[-1][2] = i\n  return args",
        "sha1": "d392ccb2ab8301f6837bf124b1428ed0a4385e0f",
        "id": 272009
    },
    {
        "content": "from typing import List\n\n\ndef card_average(hand: List[int]) -> float:\n    \"\"\"\n\n    :param hand: list - cards in hand.\n    :return:  float - average value of the cards in the hand.\n    \"\"\"\n    return sum(hand) / len(hand)",
        "sha1": "49aa3f8d4b82606250d491985c9aa9ff192703ef",
        "id": 613471
    },
    {
        "content": "def add_to_dict(d, key, base):\n    \"\"\"Function to add key to dictionary, either add base or start with base\"\"\"\n\n    if key in d:\n        d[key] += base\n    else:\n        d[key] = base\n\n    return d",
        "sha1": "a13cbd99956bc551999cf30a39e87fb5dde90ece",
        "id": 644016
    },
    {
        "content": "def ScalarUsub(x):\n    \"\"\"Implement `scalar_usub`.\"\"\"\n    return -x",
        "sha1": "6889aa8dbcbb5cbd92a7547f2388fc2247b1b632",
        "id": 149259
    },
    {
        "content": "def count_number_of_entries(row, feature, ref_counts):\n    \"\"\"Count the number entries for given building based on\n    building reference number.\n\n    row : pandas.Series\n        EPC dataset row.\n\n    feature: str\n        Feature by which to count building entries.\n        e.g. \"BUILDING_REFERNCE_NUMBER\" or \"BUILDING_ID\"\n\n    ref_counts : pandas.Series\n        Value counts for building reference number.\n\n    Return\n    ---------\n    counts : int\n        How many entries are there for given building.\"\"\"\n\n    building_ref = row[feature]\n    try:\n        counts = ref_counts[building_ref]\n    except KeyError:\n        return building_ref\n\n    return counts",
        "sha1": "2658281beeb51cea8ca1bd3484a8ecd089763d55",
        "id": 36319
    },
    {
        "content": "def classNameValidator (className):\n    \"\"\"Checks if className is a valid class name.\n\n    Format of string: DEPT NUM\n\n    Allows for department names of 2-4 characters. Examples:\n        - EE 101\n        - FIN 310\n        - CSCI 160\n    Allows for lab and honors classes. Examples:\n        - BIOL 150L\n        - CSCI 493HON\n\n    .. warning::\n        Accounts for course MATH 98 but does NOT account for any other\n        remedial courses (class numbers <100).\n\n    :param str className: The name of the class to be validated\n    :except AttributeError: className must be string\n    :return: The name of class if valid, else None\n    :rtype: str or None\n    \"\"\"\n    try:\n        className = className.upper()\n    except AttributeError:\n        return None\n    if className == \"MATH 98\":\n        return className\n\n    # min example: \"EE 101\"  --> len == 6\n    # max example: \"CSCI 493HON\" --> len == 11\n    elif len (className) in range (6, 12):\n        departmentClassNumberSplit = className.split ()\n        if len (departmentClassNumberSplit) == 2:\n            department = departmentClassNumberSplit[0]\n            classNumber = departmentClassNumberSplit[1]\n            #\n            if (\n                # department could be 2 to 4 chars\n                # examples: EE, FIN, CSCI\n                len (department) in range (2, 5) and\n                department.isalpha () and\n                len (classNumber) in range (3, 7) and\n                classNumber[:3].isnumeric ()\n            ):\n                # account for honor and lab courses\n                if (\n                    len (classNumber) == 3 or\n                    classNumber[3:] == \"HON\" or\n                    classNumber[3:] == \"L\"\n                ):\n                    return className",
        "sha1": "ca160c9ae7ab2f0986fa4ade5eef8f966efa2e58",
        "id": 544256
    },
    {
        "content": "def _get_centering_constraint_from_dmatrix(design_matrix):\n    \"\"\" Computes the centering constraint from the given design matrix.\n\n    We want to ensure that if ``b`` is the array of parameters, our\n    model is centered, ie ``np.mean(np.dot(design_matrix, b))`` is zero.\n    We can rewrite this as ``np.dot(c, b)`` being zero with ``c`` a 1-row\n    constraint matrix containing the mean of each column of ``design_matrix``.\n\n    :param design_matrix: The 2-d array design matrix.\n    :return: A 2-d array (1 x ncols(design_matrix)) defining the\n     centering constraint.\n    \"\"\"\n    return design_matrix.mean(axis=0).reshape((1, design_matrix.shape[1]))",
        "sha1": "47aff43f5e6658309e7c11ed2328b279a44e2243",
        "id": 36649
    },
    {
        "content": "def decompress(tree, data):\n    \"\"\"Decompress datas\n\n    Args:\n        tree (Tree): the tree used to compress\n        data (str): the compressed datas\n\n    Returns:\n        The decompressed datas.\n    \"\"\"\n    content = \"\"\n    current_node = tree.root\n    # Convert data to bits\n    bits = ''.join(bin(c)[2:].rjust(8, '0') for c in data)\n    # First 6 bytes = size of the uncompressed data\n    content_size = int(bits[:48], 2)\n    # Iter until we get all uncompress characters\n    i = 48\n    current_size = 0\n    while current_size < content_size:\n        if bits[i] == \"0\":\n            current_node = current_node.left\n        else:\n            current_node = current_node.right\n        if current_node.isLeaf():\n            current_size += 1\n            content += current_node.letter\n            current_node = tree.root\n        i += 1\n\n    return content",
        "sha1": "56d9f6d5c4e37bc8b7e4e63868bfcc2151e7e201",
        "id": 566172
    },
    {
        "content": "def get_zscores(returns):\n    \"\"\"\n    Returns the Z-scores of the input returns.\n\n    Parameters\n    ----------\n    returns : Series or DataFrame, required\n        Series or DataFrame of returns\n\n    Returns\n    -------\n    Series or DataFrame\n    \"\"\"\n    # Ignore 0 returns in calculating z score\n    nonzero_returns = returns.where(returns != 0)\n    z_scores = (nonzero_returns - nonzero_returns.mean())/nonzero_returns.std()\n    return z_scores",
        "sha1": "aa258a23b75bd67c0e9cf147d78473238990fafd",
        "id": 59630
    },
    {
        "content": "import re\n\n\ndef header(line):\n    \"\"\"\n    add '>' if the symbol is missing at the end of line\n    \"\"\"\n    line=line.strip()\n    if re.match(\"##.*<.*\", line) and line[-1:] != '>':\n        line=line+'>'\n    return line",
        "sha1": "b06488ff95d154836d622f55ebb5b26661d1fceb",
        "id": 696957
    },
    {
        "content": "def trunc_time(time, freq):\n    \"\"\" \n    Truncates values in provided time array to provided frequency. E.g. 2018-01-15T12:00 with \n    freq = 'M' becomes 2018-01-01. \n    \"\"\"\n    \n    return time.astype('<M8[' + freq + ']')",
        "sha1": "826ade5852da8e757bb0392654ac1c295364020b",
        "id": 631673
    },
    {
        "content": "def hello() -> str:\n\t\"\"\"\n\tUsed to test that the library is installed successfully.\n\n\tReturns\n\t----------\n\tThe string 'world'.\n\t\"\"\"\n\treturn \"world\"",
        "sha1": "266d7c0b53f47dc1a36b048cd4359ec00957a2dd",
        "id": 509540
    },
    {
        "content": "def encode_schedule(schedule):\n  \"\"\"Encodes a schedule tuple into a string.\n\n  Args:\n    schedule: A tuple containing (interpolation, steps, pmfs), where\n      interpolation is a string specifying the interpolation strategy, steps\n      is an int array_like of shape [N] specifying the global steps, and pmfs is\n      an array_like of shape [N, M] where pmf[i] is the sampling distribution\n      at global step steps[i]. N is the number of schedule requirements to\n      interpolate and M is the size of the probability space.\n\n  Returns:\n    The string encoding of the schedule tuple.\n  \"\"\"\n  interpolation, steps, pmfs = schedule\n  return interpolation + ' ' + ' '.join(\n      '@' + str(s) + ' ' + ' '.join(map(str, p)) for s, p in zip(steps, pmfs))",
        "sha1": "d660bc6826dcef2bbc5fbfca0eafaf2d72ab061f",
        "id": 694922
    },
    {
        "content": "import math\n\n\ndef convert_to_degrees(radian):\n    \"\"\"\n    Convert an angle in radians to the equivalent angle in degrees.\n    \"\"\"\n    return radian * (180 / math.pi)",
        "sha1": "93b00eaef4cbdefa070c5be3d6db31cb9c8e439e",
        "id": 190078
    },
    {
        "content": "def fetch_courses(soups):\n    \"\"\"Fetches each course inside a given page.\"\"\"\n    courses = []\n    for soup in soups:\n        course = soup.find_all('div', class_='item-frame')\n        courses.append(course)\n\n    return courses",
        "sha1": "1b728f7b8a42343ced3e84e3fce069a9dc7a1739",
        "id": 690472
    },
    {
        "content": "import inspect\n\n\ndef depth_filter(path, parent, children):\n  \"\"\"Depth filter.\n\n  This is intended to filter out \"non-public\" objects. The general idea is that\n  in the root directory we define an __init__.py that imports all the modules\n  that should be exposed publicly. For each of those modules, there is also an\n  associated __init__.py that imports everything that should be exposed publicly\n  at that level. We don't have anything nested deeper than that in the public\n  API (e.g. tfma.rootimport.subimport.object). If this changes then this\n  filter will need to be updated.\n\n  As such, we can filter on depth: we show objects at depth 2\n  (e.g. tfma.evaluators.MetricsAndPlotsEvaluator), but not modules at depth 2\n  (e.g. tfma.evaluators.counter_util).\n\n  We also do not descend into modules beyond depth 2 - so we descend into\n  tfma, and tfma.evaluators (and so on), but no further.\n\n  Args:\n    path: Path to parent\n    parent: Parent\n    children: List of children\n\n  Returns:\n    Filtered list of children.\n  \"\"\"\n  del parent\n\n  if len(path) == 1:\n    return children\n\n  # At depth 2 and beyond don't descend into child modules.\n  filtered_children = []\n  for pair in children:\n    _, child = pair\n    if inspect.ismodule(child):\n      continue\n    filtered_children.append(pair)\n  return filtered_children",
        "sha1": "2d406bf2afe7e7caea820dcab1081562784b4fdc",
        "id": 450109
    },
    {
        "content": "def config_parser_to_dict(config):\n    \"\"\"\n    Translates the items in the config parser object to dict.\n\n    :param config: RawConfigParser\n\n    :return: Dictionary with the contents\n    :rtype: dict\n    \"\"\"\n    contents = {}\n    for section in config.sections():\n        contents.update({section: {item[0]: item[1] for item in config.items(section)}})\n\n    return contents",
        "sha1": "b093af16a4b775b1b0eb8e7d8f2ff603f2257827",
        "id": 606514
    },
    {
        "content": "import six\n\n\ndef pretty_unicode(string):\n    \"\"\"\n    Make sure string is unicode, try to decode with utf8, or unicode escaped string if failed.\n    \"\"\"\n    if isinstance(string, six.text_type):\n        return string\n    try:\n        return string.decode(\"utf8\")\n    except UnicodeDecodeError:\n        return string.decode('Latin-1').encode('unicode_escape').decode(\"utf8\")",
        "sha1": "3857d63817bd99acc2cf5d0e4106055152d66bc1",
        "id": 647974
    },
    {
        "content": "def split_data(seq, train_ratio, val_ratio):\n    \"\"\"\n    Split data into train/val partitions\n    \"\"\"\n    train_num = int(len(seq) * train_ratio)\n    val_num = int(len(seq) * val_ratio)\n\n    train_seq = seq[:train_num]\n    val_seq = seq[train_num:train_num + val_num]\n    test_seq = seq[train_num + val_num:]\n    return train_seq, val_seq, test_seq",
        "sha1": "b898d80d855926a670144aef8e42d182cac31aa3",
        "id": 445447
    },
    {
        "content": "from typing import Dict\nfrom typing import List\nfrom typing import Tuple\n\n\ndef count_bags(\n    rules: Dict[str, List[Tuple[str, int]]], bag_name: str, multiplier: int\n) -> int:\n    \"\"\"\n    Count the number of bags necessarily contained in `multipler` bags of\n    type `bag_name` according to the `rules`.\n\n    Note that this includes the outer bags themselves!\n    \"\"\"\n    return multiplier * (\n        1 + sum(count_bags(rules, name, mult) for name, mult in rules[bag_name])\n    )",
        "sha1": "fd4c8d95e14f7cdf70517e3469adaf21ef4f2f08",
        "id": 103645
    },
    {
        "content": "def safe_add(x, y):\n    \"\"\"\n    Adds two values which are either numeric types or None.\n\n    - If both values are numeric, the result is the sum of those values.\n    - If only one numeric value is provided, that value is returned.\n    - If both values are None, then None is returned.\n    \"\"\"\n    if x is not None and y is not None:\n        return x + y\n    elif x is not None:\n        return x\n    elif y is not None:\n        return y\n    else:\n        return None",
        "sha1": "26965be2b64c438f2662b2b06059f660f94bf255",
        "id": 604001
    },
    {
        "content": "def preprocess(X, y, info=None):\n    \"\"\"Prepare the data for the learning\"\"\"\n    # No preprocessing.\n    return X, y, info",
        "sha1": "c7d51d064084c5b8e235991f8046f22cf217bd95",
        "id": 138198
    },
    {
        "content": "def formatList(list):\n    \"\"\"Format a list to an enumeration.\n\n    e.g.: [a,b,c,d] -> a, b, c and d\n    \"\"\"\n    if len(list) == 0:\n        return \"no one\"\n    elif len(list) == 1:\n        return list[0]\n    else:\n        s = \"\"\n        for e in list[:len(list) - 2]:\n            s = s + str(e) + \", \"\n        s = s + str(list[len(list) - 2]) + \" and \" + str(list[len(list) - 1])\n        return s",
        "sha1": "33c4e2bbdc99365565160d0d17cc35630723dcd0",
        "id": 506703
    },
    {
        "content": "def _include_chooser_msg_wildcard_docs(f):\n    \"\"\"\n    Combines the basic Chooser options (wildard, message) docsstring\n    with the wrapped function's doc string.\n    \"\"\"\n    _doc = \"\"\":param wildcard: Sets the wildcard, which can contain multiple file types, for \n                     example: \"BMP files (.bmp)|.bmp|GIF files (.gif)|.gif\"\n    :param message:  Sets the message that will be displayed on the dialog.\n    \"\"\"\n    f.__doc__ = (f.__doc__ or '') + _doc\n    return f",
        "sha1": "cd824d05049c28a6b31aeba60e63739d4971554f",
        "id": 54708
    },
    {
        "content": "def _split_divisible(num, num_ways, divisible_by=8):\n  \"\"\"Evenly splits num, num_ways so each piece is a multiple of divisible_by.\"\"\"\n  assert num % divisible_by == 0\n  assert num / num_ways >= divisible_by\n  # Note: want to round down, we adjust each split to match the total.\n  base = num // num_ways // divisible_by * divisible_by\n  result = []\n  accumulated = 0\n  for i in range(num_ways):\n    r = base\n    while accumulated + r < num * (i + 1) / num_ways:\n      r += divisible_by\n    result.append(r)\n    accumulated += r\n  assert accumulated == num\n  return result",
        "sha1": "bfa7dce06054325e50ecca644ad085a3eec1e0b3",
        "id": 629933
    },
    {
        "content": "def _parse_quad_str(s):\n    \"\"\"Parse a string of the form xxx.x.xx.xxx to a 4-element tuple of integers\"\"\"\n    return tuple(int(q) for q in s.split('.'))",
        "sha1": "79dcb2bffad831a7f60fd471ea5f4b747e693d1e",
        "id": 682676
    },
    {
        "content": "def agestr2years(age_str: str) -> int:\n    \"\"\"Convert an Age String into a int where the age unit is\n    in years. Expected formats are: nnnD, nnnW, nnnM, nnnY.\n\n    Notes\n    -----\n    The return value may not yield precise results as the following\n    assumptions are made: there are 365 days in a year, there are 52\n    weeks in a year, and there are 12 months in a year.\n\n    Parameters\n    ----------\n    age_str : str\n        A DICOM Age String value.\n\n    Returns\n    -------\n    int\n        The number of years as an int.\n\n    Raises\n    ------\n    ValueError\n        A ValueError is raised if the age_str is not a valid\n        Age String.\n    \"\"\"\n    if not age_str or len(age_str) != 4:\n        raise ValueError(\n            f\"Expected the age string to be in the 'nnn[DWMY]' format. Obtained: {age_str}\"\n        )\n    age_unit = age_str[-1].upper()\n    if age_unit not in \"DWMY\":\n        raise ValueError(\n            f\"Expected the age string unit to be one of 'D', 'W', 'M', 'Y'. Obtained: {age_unit}\"\n        )\n\n    age_value = age_str[:3]\n    if age_unit == \"D\":\n        return int(age_value) // 365\n    elif age_unit == \"W\":\n        return int(age_value) // 52\n    elif age_unit == \"M\":\n        return int(age_value) // 12\n    else:\n        return int(age_value)",
        "sha1": "76a3b579aed95de608e529e2a8009f897763ecaa",
        "id": 671998
    },
    {
        "content": "def gauss_sum(number):\n    \"\"\" Computes the gaussian sum for an input number. \"\"\"\n    return (number * (number + 1)) / 2",
        "sha1": "53cb6e7695c266a1f00f8a780d8b320340feb370",
        "id": 70077
    },
    {
        "content": "def getClientIP(request):\n    \"\"\"\n    Pull the requested client IP address from the X-Forwarded-For request\n    header. If there is more than one IP address in the value, it will return\n    the first one.\n\n    For more info, see: 'def access_route' in\n    https://github.com/mitsuhiko/werkzeug/blob/master/werkzeug/wrappers.py\n\n    :param request:\n    :return str: The client IP address, or none if neither the X-Forwarded-For\n       header, nor REMOTE_ADDR are present in the environment.\n    \"\"\"\n    if request.access_route > 0:\n        ip = request.access_route[0]\n    else:\n        ip = None\n    return ip",
        "sha1": "4ed0d2693e805aab782a304dd0ab9a62bc1f91d4",
        "id": 60242
    },
    {
        "content": "def is_number(something):\n    \"\"\"\n    Check if `something` is a number.\n\n    Parameters\n    ----------\n    something : anything\n        Something to be checked.\n\n    Returns\n    -------\n    boolean\n        True if `something` is a number. False otherwise.\n\n    \"\"\"\n    try:\n        float(something)\n        return True\n    except ValueError:\n        return False",
        "sha1": "562ea49825eafb641914a540cdaae9c03f1d8658",
        "id": 142433
    },
    {
        "content": "import io\n\n\ndef hex_to_bin(s):\n    \"\"\"Converts a string of hex digits to bytes\"\"\"\n    stream = io.StringIO(s)\n    data = b\"\"\n    while True:\n        char_hex = stream.read(2)\n        if not char_hex:\n            break\n        char_int = int(char_hex, 16)\n        data += char_int.to_bytes(1, byteorder=\"big\")\n    return data",
        "sha1": "c20aa837546e9f61bb233ea6a53748a8f7a68780",
        "id": 140290
    },
    {
        "content": "import collections\nimport json\n\n\ndef seq_length_to_json(df):\n    \"\"\"Convert sequence length distribution to JSON object.\n\n    Args:\n        df: DataFrame containing a subset of sequence length.\n    \n    Returns:\n        String: A JSON-formatted string for visualization of sequence length distribution.\n\n    \"\"\"\n    json_data = {}\n    for column in list(df):\n        json_values = []\n        distribution = collections.Counter(df.loc[:, column].dropna().tolist())\n        for key, value in distribution.items():\n            json_values.append({\"x\": int(key), \"y\": value})\n        json_data[column] = json_values\n    return json.dumps(json_data)",
        "sha1": "7e81df92f412869013b79067ad976c01cda345d3",
        "id": 649648
    },
    {
        "content": "def qualitative_pcs_value(pcs_value):\n    \"\"\"Convert PCS value into qualitative description\"\"\"\n    if pcs_value < 369:\n        output = \"Very good\"\n    elif pcs_value < 1088:\n        output = \"Good\"\n    elif pcs_value < 1700:\n        output = \"Standard\"\n    elif pcs_value < 4622:\n        output = \"Bad\"\n    elif pcs_value < 7696:\n        output = \"Very Bad\"\n    else:\n        output = \"Hazardous\"\n    return output",
        "sha1": "00fac8fa8b7eb3ac5a3d1d2ec6ed86c0b4809e6c",
        "id": 552220
    },
    {
        "content": "def set_paused(animations: list[str], paused: bool) -> dict:\n    \"\"\"Sets the paused state of a set of animations.\n\n    Parameters\n    ----------\n    animations: list[str]\n            Animations to set the pause state of.\n    paused: bool\n            Paused state to set to.\n    \"\"\"\n    return {\n        \"method\": \"Animation.setPaused\",\n        \"params\": {\"animations\": animations, \"paused\": paused},\n    }",
        "sha1": "1313250dcc3c6c7d4068a7036738bbde26d2f8ea",
        "id": 566456
    },
    {
        "content": "def apply_opacity(im, opacity):\n    \"\"\" Apply opacity to an image. \"\"\"\n    if opacity == 255:\n        return im\n\n    if im.mode == 'RGB':\n        im.putalpha(opacity)\n        return im\n    elif im.mode in ('RGBA', 'LA'):\n        alpha_index = len(im.mode) - 1\n        a = im.split()[alpha_index]\n        opacity_scale = opacity / 255\n        a = a.point(lambda i: i * opacity_scale)\n        im.putalpha(a)\n        return im\n    else:\n        raise NotImplementedError()",
        "sha1": "020187f4a38286e0f0bb98f4c75fe7b10cbd0af8",
        "id": 106679
    },
    {
        "content": "import base64\n\n\ndef invoke_lambda_and_get_duration(lambda_client, payload, function_name):\n    \"\"\"\n    Invokes Lambda and return the duration.\n    :param lambda_client: Lambda client.\n    :param payload: payload to send.\n    :param function_name: function name.\n    :return: duration.\n    \"\"\"\n    response = lambda_client.invoke(\n        FunctionName=function_name,\n        InvocationType='RequestResponse',\n        LogType='Tail',\n        Payload=payload,\n    )\n\n    # Extract duration from Lambda log\n    lambda_log = base64.b64decode(response['LogResult']).decode('utf-8')\n    report_data = \\\n        [line for line in lambda_log.split('\\n')\n         if line.startswith('REPORT')\n        ][0]\n    duration = \\\n        [col for col in report_data.split('\\t')\n         if col.startswith('Duration')\n         ][0]\n    duration = float(duration.split()[1])\n    return duration",
        "sha1": "c8d2b77e4f7bc338efcdfd21db4f7297a625b05c",
        "id": 8433
    },
    {
        "content": "import string\n\n\ndef remove_punctuation(words):\n    \"\"\"\n    remove the punctuation from words\n    \"\"\"\n    if isinstance(words, str):\n        words = [words]\n\n    # translation table to map punctuations to None\n    punctuation_table = str.maketrans(dict.fromkeys(string.punctuation))\n\n    return [\n        word.translate(punctuation_table)\n        for word in words\n    ]",
        "sha1": "7bb0477cdeaaf2d8b025ab96891c58f00df459f7",
        "id": 203474
    },
    {
        "content": "def _spark_calc_values_chunk(points):\n    \"\"\"\n    Compute some basic information about the chunk points values\n\n    The returned information are :\n    * count : the number of points in chunk\n    * max : the maximum value in chunk\n    * min : the minimum value in chunk\n    * sum : the sum of the values in chunk\n    * sqr_sum : the sum of the square values in chunk (used for variance calculation)\n\n\n    :param points: list of data values for each point in the chunk\n    :type points: numpy.array\n\n    :return: a dict composed of the basic information computed\n    :rtype: dict\n    \"\"\"\n\n    try:\n        nb_points = len(points)\n    except TypeError:\n        return None\n\n    if nb_points > 0:\n        sum_chunk_value = sum(points)\n        square_sum_chunk_value = sum([x * x for x in points])\n        max_chunk_value = max(points)\n        min_chunk_value = min(points)\n    else:\n        # Empty chunk, skip it\n        return None\n\n    return {\n        \"count\": nb_points,\n        \"max\": max_chunk_value,\n        \"min\": min_chunk_value,\n        \"sum\": sum_chunk_value,\n        \"sqr_sum\": square_sum_chunk_value,\n    }",
        "sha1": "04dd208038bf1df0172b821943337533f9759291",
        "id": 75114
    },
    {
        "content": "def getSum(a, b):\n    \"\"\"\n    a XOR b is the sum\n    a AND b is the carry\n    If there is a nonzero carry, add the carry to the sum and shift the carry to the left. Repeat until the carry is zero.\n    \"\"\"\n    sum = a ^ b\n    carry = a & b\n    \n    while carry:\n        sum ^= carry\n        carry <<= 1\n        \n    return sum",
        "sha1": "d1e55d91ae0e98c7783c05a9d75ac90338b0d8f9",
        "id": 202904
    },
    {
        "content": "def get_bot_scores_id_from_parts(problem_id, username, botname):\n    \"\"\"\n    :return: e.g. 'crizcraig#goodbot-on-deepdrive#unprotected_left'\n    \"\"\"\n\n    # Forward slashes not allowed on firestore\n    # https://stackoverflow.com/a/54918283/134077\n    problem_id = problem_id.replace('/', '#')\n\n    ret = f'{username}#{botname}-on-{problem_id}'\n    return ret",
        "sha1": "0105a54719fe1530add814d211b37fa08ebbe893",
        "id": 123929
    },
    {
        "content": "def array_push(space, w_arr, args_w):\n    \"\"\" Push one or more elements onto the end of array \"\"\"\n    if len(args_w) < 1:\n        space.ec.warn(\"array_push(): at least 2 parameters are required\"\n                      \", 1 given\")\n        return space.w_Null\n    for w_arg in args_w:\n        w_arr.appenditem_inplace(space, w_arg)\n    return space.wrap(w_arr.arraylen())",
        "sha1": "f6f4bfccc4f495222b018a75cdacbc459ad369c1",
        "id": 399445
    },
    {
        "content": "import re\n\n\ndef clean_sentence(sentence):\n    \"\"\"\n    Remove unreadable Unicode characters and non-alpha characters from the text.\n    \n    :param sentence: Sentence to clean.\n    \n    :return text: Cleaned sentence.\n    \"\"\"\n    text = re.sub(r'[^a-zA-Z\\']', ' ', sentence)\n\n    # remove Unicode characters\n    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n\n    return text",
        "sha1": "3b8ae2ff0bb1f80e940dcc73c24c795f5e3498b7",
        "id": 399064
    },
    {
        "content": "def remove_missing_keys(a, b):\n    \"\"\"Remove keys from a that don't appear in b. Expects both maps to be flat\"\"\"\n    n = dict()\n    for k in b.keys():\n        if k in a:\n            n[k] = a[k]\n    return n",
        "sha1": "f803bd3a370ebb19f3cf65e553ca28b11d955353",
        "id": 501047
    },
    {
        "content": "def polygon_area(points):  \n    \"\"\"Return the area of the polygon whose vertices are given by the\n    sequence points.\n    \"\"\"\n    area = 0\n    q = points[-1]\n    for p in points:  \n        area += p[0] * q[1] - p[1] * q[0]\n        q = p\n    return area / 2",
        "sha1": "aaa85b3e3b687413d32bcef3546722773bbb1c7e",
        "id": 382619
    },
    {
        "content": "def cards_to_string(cards):\n    \"\"\"\n    convert list of Card objects to string for db deck field\n    :param cards:\n    :return:\n    \"\"\"\n    return ','.join(map(lambda x: str(x.as_number()), cards))",
        "sha1": "fd18e1c672801bb0a30354eff3007cc0c5a9813b",
        "id": 322108
    },
    {
        "content": "import random\n\n\ndef form_batches(batch_size, idx):\n    \"\"\"Shuffles idx list into minibatches each of size batch_size\"\"\"\n    idxs = [i for i in idx]\n    random.shuffle(idxs)\n    return [idxs[i:(i+batch_size)] for i in range(0,len(idxs),batch_size)]",
        "sha1": "2afdc93202f553b29a8f4c68dcf6e226816d9de0",
        "id": 41145
    },
    {
        "content": "import textwrap\n\n\ndef _wrap_text(text):\n    \"\"\"\n    Wrap text at given width using textwrap module. Indent should consist of\n    spaces. Its length is deducted from wrap width to ensure exact wrapping.\n    \"\"\"\n    wrap_max = 80\n    indent = '    '\n    wrap_width = wrap_max - len(indent)\n    return textwrap.fill(text, width=wrap_width, initial_indent=indent,\n                               subsequent_indent=indent, break_long_words=False,\n                               break_on_hyphens=False)",
        "sha1": "1c8ad1401e229bec60eb7b82acecd72811689fc2",
        "id": 661071
    },
    {
        "content": "def color_component_int_to_float(rgb_component: int) -> float:\n    \"\"\"\n    Converts a color component from integer to float\n\n    :param rgb_component: a color component given in int range [0, 255]\n    :return:  a color component in float range of [0,1]\n    \"\"\"\n    return rgb_component / 255.0",
        "sha1": "edd1390e2c3a2462d906091b3ac58c1d74a2891f",
        "id": 348016
    },
    {
        "content": "import bs4\n\n\ndef is_processing_instruction(obj):  # pragma: no cover\n    \"\"\"Is processing instruction.\"\"\"\n\n    return isinstance(obj, bs4.ProcessingInstruction)",
        "sha1": "aa00ef00b621787c17e66d8e98d9a1d2c1d0b89a",
        "id": 522056
    },
    {
        "content": "def read_pid(pid_file):\n    \"\"\"\n    Read PID from given PID file.\n\n    :param str pidfile: Name of the PID file to read from.\n    :return: PID from given PID file.\n    :rtype: int\n    \"\"\"\n    with open(pid_file, 'r') as pidfd:\n        return int(pidfd.readline().strip())",
        "sha1": "61df745a73483bd9a9dd16b722ca53d559f07539",
        "id": 26312
    },
    {
        "content": "def find_interaction_involving(block, current_node, prev_node):\n    \"\"\"\n    Given a list of `interactions` in vermouth format, find an\n    interaction from bonds, constraints, or virtual-sites that\n    involves the `current_node` and `prev_node`. Return if this\n    interaction is a virtual-site and the corresponding parameter.\n\n    Parameters\n    -----------\n    block:  :class:`vermouth.molecule.Block`\n         vermouth block\n    current_node:   int\n         node index\n    prev_node:      int\n         node index\n\n    Returns:\n    ---------\n    bool\n      is the interaction a virtual-site\n    vermouth.Interaction\n      interaction definition\n    str\n      interaction type\n    \"\"\"\n    interactions = block.interactions\n    for inter_type in [\"bonds\", \"constraints\", \"virtual_sitesn\",\n                       \"virtual_sites2\", \"virtual_sites3\", \"virtual_sites4\"]:\n        inters = interactions.get(inter_type, [])\n        for interaction in inters:\n            if current_node in interaction.atoms:\n                if prev_node in interaction.atoms and inter_type in [\"bonds\", \"constraints\"]:\n                    return False, interaction, inter_type\n                elif prev_node in interaction.atoms and inter_type.split(\"_\")[0] == \"virtual\":\n                    return True, interaction, inter_type\n    else:\n        msg = '''Cannot build template for residue {}. No constraint, bond, virtual-site\n                 linking atom {} and atom {}.'''\n        raise IOError(msg.format(block.nodes[0][\"resname\"], prev_node, current_node))",
        "sha1": "59b514b1266fb333cbb5f96a6fe7f05dc7e8b97c",
        "id": 188049
    },
    {
        "content": "def distance(u, v, ancestor):\n    \"\"\"\n    Computes distance of the path from u to v using the lca node as:\n        d(u,v) = d(root, u) + d(root, v) - 2 * d(root, lca)\n    :param u:\n    :param v:\n    :param ancestor:\n    :return:\n    \"\"\"\n    dru = u.depth\n    drv = v.depth\n    drlca = ancestor.depth\n\n    return dru + drv - 2 * drlca",
        "sha1": "9767ebf508bf39d100c164f3e25fed018ba6b344",
        "id": 265502
    },
    {
        "content": "def getPoints(f1,a,b,n):\n    \"\"\"returns a list of n points on the graph of f1.  The interval [a,b] is\n       divided into n equal intervals for the x coordinates of the points\"\"\"\n    points = []\n    dt = (b-a)/float(n)\n    count = 0\n    while count <=n:\n        x = a+count*dt\n        points.append((x, f1(x)))\n        count = count+1\n    \n    return points",
        "sha1": "03556e0b2caaa220e38560a8b8137d37cb3eeb04",
        "id": 523779
    },
    {
        "content": "def listify(o):\n    \"\"\"Ensure an object is a list by wrapping if necessary\"\"\"\n    if isinstance(o, list):\n        return o\n    return [o]",
        "sha1": "7b1015dfd84b007be3597401ff6035bcd0b8a181",
        "id": 589287
    },
    {
        "content": "import sqlite3\n\n\ndef db_open(db_filepath):\n    \"\"\"\n    Returns a db connection to the given database (creates one if not found)\n    \"\"\"\n    db_conn = sqlite3.connect(db_filepath)\n    return db_conn",
        "sha1": "d6f5a73f27adda7ca029ca28b106420f9e81b079",
        "id": 234418
    },
    {
        "content": "def bitstr_to_int(a):\n    \"\"\" Convert binary string to int \"\"\"\n    return int(a, 2)",
        "sha1": "1188a2ff24b1bf6c70db9a458d39f08f44f006e8",
        "id": 30567
    },
    {
        "content": "def get_layer_id_for_vit(name, num_layers):\n    \"\"\"\n    Assign a parameter with its layer id\n    Following BEiT: https://github.com/microsoft/unilm/blob/master/beit/optim_factory.py#L33\n    \"\"\"\n    if name in ['encoder.cls_token', 'encoder.encoder_pos_embedding']:\n        return 0\n    if name.startswith('encoder.stem'):\n        return 0\n    if name.startswith('encoder.encoder.blocks'):\n        return int(name.split('.')[3]) + 1\n    return num_layers",
        "sha1": "ac5d0a366a85326b8c2430dd62bad15589b593ee",
        "id": 283864
    },
    {
        "content": "def deserialize(serial):\n    \"\"\"\n    Deserialize or convert endianness.\n    A lot of data in the block is stored in a serialized manner,\n    specifically, the bytes are stored backwards. This function\n    corrects that. Note that it reverses characters in pairs because\n    one byte has 2 hex values.\n    \"\"\"\n    value = ''\n    for i in range(0, len(serial), 2):\n        value = ''.join([serial[i:i+2], value])\n    return value",
        "sha1": "ec0b5e3a6b63fe77c031d49a3d041d71fb273ad2",
        "id": 530142
    },
    {
        "content": "def _test_results(res_json, url):\n    \"\"\"\n    tests that the api returns successfully with one or more results,\n    returns the first result for convenience\n    \"\"\"\n    assert res_json.get('results'), f\"{url} no results\"\n    assert len(res_json['results']), f\"{url} no results\"\n    return res_json['results'][0]",
        "sha1": "0a577b004a87898f09d4022a15ee77e846e164b9",
        "id": 150291
    },
    {
        "content": "import torch\n\n\ndef tensor_prepend_zero(x):\n    \"\"\"Prepending tensor with zeros.\n\n    Args:\n        x (Tensor): tensor to be extended\n    Returns:\n        the prepended tensor. Its shape is (x.shape[0]+1, x.shape[1:])\n    \"\"\"\n    return torch.cat((torch.zeros(1, *x.shape[1:], dtype=x.dtype), x))",
        "sha1": "5da6c31ce67df5db47382d4dadaf7202ea2afbca",
        "id": 525916
    },
    {
        "content": "def user_in_role(user, roles):\n    \"\"\"\n    True if user has one of specified roles\n    \"\"\"\n    if user:\n        result = False\n        for role in roles:\n            result |= (role in user.roles)\n        return result\n    return False",
        "sha1": "18f1d151d5bd8b6daa7831947239be1849e24e78",
        "id": 224530
    },
    {
        "content": "def loadGraph(name):\n    \"\"\"Load a graph in the DIMACS ascii format from\n       the file \"name\" and return it as a list of sets\"\"\"\n\n    V = 0\n    E = 0\n    G = []\n\n    f = open(name, \"r\")\n    lines = f.readlines()\n    for l in lines:\n        s = l.split()\n        if len(s) < 1:\n            continue\n        if s[0] == \"c\":\n            continue\n        elif s[0] == \"p\":\n            V = int(s[2]) + 1\n            E = int(s[3])\n            G = [set() for x in range(V)]\n        elif s[0] == \"e\":\n            (x, y) = (int(s[1]), int(s[2]))\n            G[x].add(y)\n            G[y].add(x)\n\n    f.close()\n    return G",
        "sha1": "cda9f1a5f7861f10579f29cdc95baaaee5845b18",
        "id": 430661
    },
    {
        "content": "from typing import Callable\nfrom typing import Any\nimport inspect\n\n\ndef get_func_args(func: Callable[[Any], Any]):\n    \"\"\"Returns a list of arguments for the function\"\"\"\n\n    sig = inspect.signature(func)\n\n    return [\n        arg_name\n        for arg_name, param in sig.parameters.items()\n        if param.kind == inspect.Parameter.POSITIONAL_OR_KEYWORD\n    ]",
        "sha1": "8b7f3f06b5667b9fa0498ce013f8294bd24dc186",
        "id": 106256
    },
    {
        "content": "import string\nimport random\n\n\ndef gen_random_string(length_of_str):\n    \"\"\"generate a string containing random characters of length length_of_str\n    \"\"\"\n    source_set = string.ascii_letters+string.digits+string.punctuation\n    result=[]\n    for x in range(length_of_str):\n        result.append(random.choice(source_set))\n    return ''.join(result).encode('us-ascii')",
        "sha1": "49eec867d6ef890a71d020720d7d359e57dfb069",
        "id": 560480
    },
    {
        "content": "import itertools\n\n\ndef expand_value(value):\n    \"\"\"Expands the given configuration value. All lists in the value are\n    flattened, and a cross-product of the lists are returned.\n    \"\"\"\n    if not isinstance(value, list) and not isinstance(value, dict):\n        return value\n    elif isinstance(value, list):\n        # Treat 1-length arrays as scalars.\n        if len(value) == 1:\n            return expand_value(value[0])\n        # Return a flattened list of expanded subitems.\n        expanded_list = []\n        for v in value:\n            v = expand_value(v)\n            if isinstance(v, list):\n                expanded_list.extend(v)\n            else:\n                expanded_list.append(v)\n        return expanded_list\n    # We have a dict.\n    keys = value.keys()\n    vals = []\n    for v in value.values():\n        v = expand_value(v)\n        vals.append(v if isinstance(v, list) else [v])\n    zipped_vals = list(zip(keys, v) for v in itertools.product(*vals))\n    return [dict(z) for z in zipped_vals]",
        "sha1": "7d5f6d1456a1d3a5d6983340bbeb0090155f1896",
        "id": 455614
    },
    {
        "content": "def is_number(s):\n    \"\"\"Determine if the string `s` is a number.\n\n    Examples:\n        >>> is_number('5')\n        True\n        >>> is_number('5.5')\n        True\n        >>> is_number('foo')\n        False\n    \"\"\"\n    try:\n        float(s)\n        return True\n    except ValueError:\n        return False",
        "sha1": "9bf81c428050a464f20ffbc20bfdb753f1febeef",
        "id": 215992
    },
    {
        "content": "from typing import Tuple\nfrom typing import Optional\n\n\ndef split_package_build_string(build_string: str) -> Tuple[str, Optional[int]]:\n    \"\"\"Helper method for parsing build_string.\n\n    Returns:\n        Tuple (build_string, build_number)\n    \"\"\"\n\n    if '' == build_string:\n        return '', None\n\n    if build_string.isdigit():\n        return '', int(build_string)\n\n    _pos = build_string.rindex('_') if '_' in build_string else -1\n    if _pos >= 1:\n        # Build number will be the last part - check if it's an integer\n        # Updated logic given https://github.com/CycloneDX/cyclonedx-python-lib/issues/65\n        build_number = build_string[_pos + 1:]\n        if build_number.isdigit():\n            return build_string, int(build_number)\n\n    return build_string, None",
        "sha1": "7718c51d45cd77bd03c7d2c62efa0631ce1450d0",
        "id": 436860
    },
    {
        "content": "from typing import Literal\n\n\ndef sh_bool(boolean: bool) -> Literal[\"yes\", \"no\"]:\n    \"\"\"\n    Formats a boolean to be passed to a bash script environment (eg run_job.sh)\n\n    :param boolean: A boolean flag (True or False)\n    :type boolean: bool\n    :return: 'yes' or 'no'\n    :rtype: str\n    \"\"\"\n    if boolean:\n        return \"yes\"\n\n    return \"no\"",
        "sha1": "a9ab0629d99ce5dfd9dfa2bf734d6b682dcffa91",
        "id": 622089
    },
    {
        "content": "import getpass\n\n\ndef get_login() :\n    \"\"\"Returns login name\n    \"\"\"\n    #return os.getlogin()\n    return getpass.getuser()",
        "sha1": "b53c7229c935f6c850d3365cd9f694b6a51b81cb",
        "id": 465743
    },
    {
        "content": "def format_includes(includes):\n    \"\"\"\n    Format includes for the api query (to {'include' : <foo>,<bar>,<bat>})\n\n    :param includes: str or list: can be None, related resources to include\n    :return: dict: the formatted includes\n    \"\"\"\n    result = None\n    if isinstance(includes, str):\n        result = includes\n    elif isinstance(includes, list):\n        result = ','.join(includes)\n    return {'include': result} if result is not None else {}",
        "sha1": "9f15ac9b767b6612794bec7b14427b6f90d4a734",
        "id": 39067
    },
    {
        "content": "import pathlib\nimport zipfile\n\n\ndef _extract_epub(epub_file_path: pathlib.Path, directory_to_extract_to: pathlib.Path) -> list[pathlib.Path]:\n    \"\"\"\n    Extract an epub file.\n    :return: paths to the files containing the actual articles.\n    \"\"\"\n    with zipfile.ZipFile(epub_file_path, \"r\") as epub_file:\n        epub_file.extractall(directory_to_extract_to)\n    return list(pathlib.Path(directory_to_extract_to / \"OEBPS\").glob(\"article_*.xhtml\"))",
        "sha1": "5f1f37c9513b33ce5f76a99b82f93063802229e9",
        "id": 230543
    },
    {
        "content": "def parse_card(card: str) -> tuple: \n    \"\"\"Separates the card into value and suit.\n\n    Args:\n        card (str): String representing a poker card, in the format ValueSuit, like '9D' (9 of Diamonds).\n\n    Returns:\n        tuple: Returns a tuple of the card, like (Value, Suit). Ex: '9D' -> ('9', 'D').\n    \"\"\"\n    if len(card) == 3:\n        #If we receive a card whose len is 3, this is 10 + S(suit), so we replace 10 for T to make things easier.\n        return 'T', card[2]\n    else:\n        return card[0], card[1]",
        "sha1": "de9051906327dfcf01a3b2076acbed216ce43ced",
        "id": 700937
    },
    {
        "content": "def has_dind_support() -> bool:\n    \"\"\"\n    Return whether this repo image supports Docker-in-Docker.\n    \"\"\"\n    return True",
        "sha1": "be7e9acc14603ec238e3f6dc4fc9199e208bc12e",
        "id": 481788
    },
    {
        "content": "def is_article_link(wikilink):\n    \"\"\"Return True is wikilink is an article link.\n\n    Parameters\n    ----------\n    wikilink : str\n        Wikilink to be tested\n\n    Returns\n    -------\n    result : bool\n        True is wikilink is an article link\n\n    Examples\n    --------\n    >>> is_article_link('[[Danmark]]')\n    True\n    >>> is_article_link('[[Kategori:Danmark]]')\n    False\n\n    \"\"\"\n    if wikilink.startswith('[[') and len(wikilink) > 4:\n        wikilink = wikilink[2:]\n    if not (wikilink.startswith('Diskussion:')\n            or wikilink.startswith('Fil:')\n            or wikilink.startswith('File:')\n            or wikilink.startswith('Kategori:')\n            or wikilink.startswith('Kategoridiskussion:')\n            or wikilink.startswith('Wikipedia:')\n            or wikilink.startswith('Wikipedia-diskussion:')\n            or wikilink.startswith(u'Hj\u00e6lp:')\n            or wikilink.startswith(u'Hj\u00e6lp-diskussion')\n            or wikilink.startswith('Bruger:')\n            or wikilink.startswith('Brugerdiskussion:')):\n        return True\n    return False",
        "sha1": "c862c3d9655ac60d97cb4a7b0e102fa7bc3ee9e8",
        "id": 639540
    },
    {
        "content": "import io\nimport traceback\n\n\ndef format_exception(exc):\n    \"\"\"\n    Format and return the specified exception information as a string.\n\n    This default implementation just uses\n    traceback.print_exception()\n    \"\"\"\n    ei = (type(exc), exc, exc.__traceback__)\n    sio = io.StringIO()\n    tb = ei[2]\n    traceback.print_exception(ei[0], ei[1], tb, None, sio)\n    s = sio.getvalue()\n    sio.close()\n    if s[-1:] == \"\\n\":\n        s = s[:-1]\n    return s",
        "sha1": "0a27486a331c98fb0721683b063582c37420ef12",
        "id": 435696
    },
    {
        "content": "def nest(l, depth=1, reps=1):\n    \"\"\"create a nested list of depth 'depth' and with 'reps' repititions\"\"\"\n    if depth == 0:\n        return(None)\n    elif depth == 1:\n        return(l)\n    else:\n        return([nest(l, depth-1, reps)] * reps)",
        "sha1": "fefa3997dbbf3d1c36ce856256d6dae013183358",
        "id": 295386
    },
    {
        "content": "def tensor2np(x):\n    \"\"\"Convert torch.Tensor to np.ndarray\n    Args:\n        x (torch.Tensor)\n    Returns:\n        np.ndarray\n    \"\"\"\n    if x is None:\n        return x \n    return x.cpu().detach().numpy()",
        "sha1": "41b841331259a55827e00349e1b971107c11928b",
        "id": 535097
    },
    {
        "content": "def list_access_keys(iam_client, user):\n    \"\"\" List IAM access keys for a user \"\"\"\n    return iam_client.list_access_keys(UserName=user)",
        "sha1": "3285d29daf9292d4e61dad65f24e00f4eb60f5fa",
        "id": 510517
    },
    {
        "content": "import codecs\n\n\ndef read_file_content(path):\n    \"\"\"\n    Read file from given path\n    :param path: file path\n    :return: file content\n    \"\"\"\n    f = codecs.open(path, 'r', 'utf8')\n    content = f.read()\n    f.close()\n    return content",
        "sha1": "a4d7930397da9c73e4a36a348c73a85662012058",
        "id": 569814
    },
    {
        "content": "def compare_schemas(current: dict, old: dict) -> bool:\n    \"\"\"Returns true if schemas are functionally the same\"\"\"\n    return current == old",
        "sha1": "6016a891b48479ef07111aacd2b68cee8be68f16",
        "id": 655925
    },
    {
        "content": "from typing import Any\n\n\ndef order_spec_dict(spec_dict: dict[str, Any]) -> dict[str, Any]:\n    \"\"\"Returns spec_dict with keys in a specific order. Requires Python 3.6+.\"\"\"\n    if 'exclude' in spec_dict:\n        spec_dict['exclude'] = order_spec_dict(spec_dict['exclude'])\n    ordered_spec_dict: dict[str, Any] = {}\n    for key in ['dataset_labels', 'taxa', 'exclude', 'max_count', 'prioritize']:\n        if key in spec_dict:\n            ordered_spec_dict[key] = spec_dict[key]\n    return ordered_spec_dict",
        "sha1": "be2b6220cfe6460b970e510882255f0a7d9662df",
        "id": 135952
    },
    {
        "content": "def has_field(val, name):\n    \"\"\"Check whether @p val (gdb.Value) has a field named @p name\"\"\"\n    try:\n        val[name]\n        return True\n    except Exception:\n        return False",
        "sha1": "a802c563432dea3a24c7bfa8669dd5333f493a60",
        "id": 267552
    },
    {
        "content": "def decodeList(line):\n    \"\"\" Decode list of items from user data file\n\n    @param line: line containing list of items, encoded with encodeList\n    @rtype: list of unicode strings\n    @return: list of items in encoded in line\n    \"\"\"\n    items = []\n    for item in line.split('\\t'):\n        item = item.strip()\n        if not item:\n            continue\n        items.append(item)\n    return items",
        "sha1": "c42711822ad7cfd6390fab9a3d78990d2f50671e",
        "id": 421665
    },
    {
        "content": "def probabilidades(simX):\n    \"\"\"\n    Computa las probabilidades de los elementos del alfabeto\n    de s\u00edmbolos de la serie X. \n    \n    Par\u00e1metros\n    ----------\n    simX : Serie simb\u00f3lica X\n    \n    Regresa\n    ----------\n    Probabilidades del diccionario de s\u00edmbolos\n    \"\"\"\n    \n    # Inicializaci\u00f3n\n    p = {}\n    n = simX.size\n\n    for xi in simX:\n        if xi in p:\n            p[xi] += 1.0 / n\n        else:\n            p[xi] = 1.0 / n\n    \n    return p",
        "sha1": "89ce861ae372ccec2b4b56bcdcc24a22db1f86b9",
        "id": 179869
    },
    {
        "content": "def getcellidxs(event):\n    \"\"\"\n    From a mouse event on the DataGridView,\n    returns the row and column indices of \n    the cell as a 2 tuple of integers.\n    \"\"\"\n\n    # this is redundant with DataGridForm.getcell\n    #    class methods were not handling unpacking\n    #        of tuple with cell in it\n    #    trying a separate function\n    # print event.RowIndex, event.ColumnIndex\n    # print(\"----?--\")\n    # print(event.RowIndex, event.ColumnIndex)\n    # print(\"=====>=\")\n    return event.RowIndex, event.ColumnIndex",
        "sha1": "829114304ffae916d99b82faf6078170477748a2",
        "id": 245277
    },
    {
        "content": "def comp(returns):\n    \"\"\"Calculates total compounded returns\"\"\"\n    return returns.add(1).prod() - 1",
        "sha1": "a7f393f33f43a9ec3492aff31ebdaa29739a0ecb",
        "id": 260264
    },
    {
        "content": "def dic_to_string(dic={}):\n    \"\"\"\n    transform dic to a command line string.\n\n    input\n    dic - commands as a dict\n\n    output\n    string contains all commands\n\n    example:\n    command_dic = {\n        \"-a\": \"b\",\n        \"--c\": \"d\"\n    }\n\n    dic_to_string(command_dic)\n    -> \"-a b --c d \" (notice the space at the end of the string)\n    \"\"\"\n    opt_string = \"\"\n    for key, value in dic.items():\n        opt_string += key + \" \" + str(value) + \" \"\n    return opt_string",
        "sha1": "4f97de1ee4895388445dbb93d16a4c758e26fe9f",
        "id": 416498
    },
    {
        "content": "def unpack_vis(vis_vec, shape):\n    \"\"\"Unpack visibility vector into order for unpolarised data.\n\n    Parameters\n    ----------\n    vis_vec : np.ndarray[:]\n        Packed visibility data.\n    shape : tuple\n        Shape of the data (baseline, time, freq).\n\n    Returns\n    -------\n    vis_vec : np.ndarray[baseline, time, freq]\n        Unpolarised visibility data\n    \"\"\"\n    vecr = vis_vec.reshape((shape[-1], 2) + shape[:-1])\n    return (vecr[:, 0] + 1.0J * vecr[:, 1]).transpose(1, 2, 0)",
        "sha1": "2ad1de8ed8e5b5bcbd6a3de921597239d13cd20d",
        "id": 88153
    },
    {
        "content": "def metric_delta_g(entity, schedule):\n    \"\"\"\n    Compute the factor \u2206g for the current schedule and the reference schedule.\n\n    Parameters\n    ----------\n    entity : ElectricalEntity\n        The Entity to calculate the delta g metric for.\n    schedule : str\n       Referenced Schedule\n\n       - 'default' : Normal schedule\n       - 'ref' : Reference schedule\n\n    Returns\n    -------\n    float :\n        Factor \u2206g.\n\n    Notes\n    -----\n    - Implementation as given in the lecture \"Elektrizitaetswirtschaft\"\n      by Prof. Dr.-Ing. Christian Rehtanz from TU Dortmund, Germany.\n    \"\"\"\n    p_el_min_dsm = min(entity.p_el_schedule)\n    p_el_max_dsm = max(entity.p_el_schedule)\n    p_el_min_ref = min(entity.schedules[schedule][\"p_el\"])\n    p_el_max_ref = max(entity.schedules[schedule][\"p_el\"])\n    g = 1.0 - (abs(p_el_max_dsm - p_el_min_dsm) / abs(p_el_max_ref - p_el_min_ref))\n    return g",
        "sha1": "a8ba59085e062d745f457d768b3fa504cf1f01bf",
        "id": 84984
    },
    {
        "content": "import requests\nimport math\n\n\ndef get_search_page_numbers(url, token):\n    \"\"\"\n    Get the pagination information for the request.\n    Parameters\n    ----------\n    url : str\n        The OSF url\n    token : str\n        OSF authorization token\n    Returns\n    -------\n    A dictionary of page numbers\n    \"\"\"\n    headers = {\"Authorization\": \"Bearer {}\".format(token)}\n    pagination_info = requests.get(url, headers=headers).json()['links']\n\n    next_page = pagination_info['next']\n    previous_page = pagination_info['prev']\n\n    if next_page:\n        next_page = next_page.partition('page=')[2]\n\n    if previous_page:\n        previous_page = previous_page.partition('page=')[2]\n        if previous_page == '':\n            previous_page = '1'\n\n    total_pages = math.ceil(pagination_info['meta']['total']/pagination_info['meta']['per_page'])\n\n    pages = {\n        \"first_page\": '1',\n        \"previous_page\": previous_page,\n        \"next_page\": next_page,\n        \"last_page\": str(total_pages),\n        \"total_pages\": str(total_pages),\n        \"per_page\": pagination_info['meta']['per_page']}\n\n    return pages",
        "sha1": "f868f3edf00141825950b9aee2f2ea2ab74672ca",
        "id": 129658
    },
    {
        "content": "import re\n\n\ndef process_gpu_clblas(stdout):\n    \"\"\" Process the stdout of the SYCLBLAS papre benchmarks.\n        stdout is a string containing multiple lines.\n    Example output:\n    t_copy, 0.000111871\n    t_axpy, 0.000119333\n    t_add, 0.00017468\n    \"\"\"\n    headers = []\n    values = {}\n    lines = stdout.splitlines(True)\n    for l in lines:\n        # Field Delimiter\n        fd = \"\\s*,\\s*\"\n        p = re.compile(\"(?P<name>t_[A-Za-z]+)\" + fd + \"(?P<first>[0-9]+\\.[0-9]+e?[0-9]*)\")\n        m = p.match(l)\n        if m:\n            values[m.group(\"name\")] = m.group(\"first\");\n    return values",
        "sha1": "5f31b5ebb9747cdc7e1865e6c37ec371c53e4e65",
        "id": 577791
    },
    {
        "content": "def add_source_init(module):\n    \"\"\"Looks for a function named ``source_init`` in ``module``;\n    if found, returns its content as a string.\n    \"\"\"\n    if hasattr(module, \"source_init\"):\n        return getattr(module, \"source_init\")()\n    return \"\"",
        "sha1": "deec406ec3ff1c91557514de6eebad0090977665",
        "id": 50083
    },
    {
        "content": "from pathlib import Path\nimport json\n\n\ndef load_json(path: Path) -> dict:\n    \"\"\"\n    Loads json file into dict.\n    \"\"\"\n    with open(str(path), \"r\") as f:\n        return json.load(f)",
        "sha1": "2c4b43ae644b1d9071fcce9dd7719f6b28fc07e3",
        "id": 427742
    },
    {
        "content": "def spin(program_list, amount):\n    \"\"\"Rotate program list by amount from the end of the list\n    e.g. abcde amount 3 gives cdeab.\"\"\"\n    return program_list[-amount:] + \\\n           program_list[:len(program_list) - amount]",
        "sha1": "90b536b8530f933e335e756f2ea1ff7297d5b41e",
        "id": 385628
    },
    {
        "content": "def get_chains(struct):\n    \"\"\"Returns all chains of a Bio.PDB.Structure.Structure.\"\"\"\n    return struct.child_list[0].child_list",
        "sha1": "f6816247dfe2ebc5fe02ae73ae25db4f4ca648dc",
        "id": 617969
    },
    {
        "content": "import torch\n\n\ndef RobustL2Loss(output, log_std, target):\n    \"\"\"\n    Robust L2 loss using a gaussian prior. Allows for estimation\n    of an aleatoric uncertainty.\n    \"\"\"\n    # NOTE can we scale log_std by something sensible to improve the OOD behaviour?\n    loss = 0.5 * torch.pow(output - target, 2.0) * torch.exp(-2.0 * log_std) + log_std\n    return torch.mean(loss)",
        "sha1": "d38b166ba8f521a7e61826897193d0563c226536",
        "id": 461996
    },
    {
        "content": "def canSignOpenBorders(fromTeam, toTeam):\n\t\"\"\"\n\tReturns True if the two CyTeams can sign an Open Borders agreement.\n\t\"\"\"\n\tif fromTeam.isOpenBorders(toTeam.getID()) or toTeam.isOpenBorders(fromTeam.getID()):\n\t\treturn False\n\treturn fromTeam.isOpenBordersTrading() or toTeam.isOpenBordersTrading()",
        "sha1": "76104e521b698d5fddebd9da7b682fa4ec8278ff",
        "id": 260371
    },
    {
        "content": "def get_namespace_by_name(namespace_name: str, client) -> str:\n    \"\"\" Returns namespace id of a namespace with name <namespace_name> \"\"\"\n    namespaces = client.list_namespaces()\n    for n in namespaces.get(\"Namespaces\"):\n        if n.get(\"Name\") == namespace_name:\n            return n.get(\"Id\")\n    raise RuntimeError(f\"Namespace {namespace_name} not found\")",
        "sha1": "5303f44b4157928a96eb07cb59dc1f6c76116c21",
        "id": 603297
    },
    {
        "content": "import re\n\n\ndef parse_waiting_time_data(waiting_time_data):\n    \"\"\"Parse waiting time string to minutes\"\"\"\n    numbers = re.findall(r'\\d+', waiting_time_data)\n\n    if len(numbers) == 0:\n        waiting_time = 0\n    elif \"min\" in waiting_time_data:\n        waiting_time = int(numbers[0])\n    elif \"hour\" in waiting_time_data:\n        waiting_time = int(numbers[0]) * 60\n    else:\n        waiting_time = int(numbers[0]) * 60 + int(numbers[1])\n\n    return waiting_time",
        "sha1": "27852b6626960b863d786278a2f666246622630f",
        "id": 452905
    },
    {
        "content": "import json\n\n\ndef GetErrorMessage(stdout):\n  \"\"\"Extract a message field from JSON output if present.\"\"\"\n  try:\n    return json.loads(stdout)['message']\n  except (ValueError, KeyError):\n    return stdout",
        "sha1": "54656974a891e7c7eefc03ae8fb8b200907b123c",
        "id": 620810
    },
    {
        "content": "def group_edges(node_prefix, df, group_ids, individual_id):\n    \"\"\"\n    Return the edge list from a grouping dataframe, either a Link Plus fuzzy matching or a dataframe, where people are\n    connected by appearing in the same family or case.\n\n    :param node_prefix: prefix for the nodes in the edge list.\n    :type node_prefix: str.\n\n    :param df: dataframe.\n    :type df: Pandas.Dataframe.\n\n    :param group_ids: grouping column names in grouping csv.\n    :type group_ids: [str].\n\n    :param individual_id: individual id column name in grouping csv.\n    :type individual_id: str.\n    \"\"\"\n    groups = df[group_ids+[individual_id]].dropna().drop_duplicates().set_index(group_ids)\n    edges = groups.merge(groups, left_index=True, right_index=True)\n    return [tuple(map(lambda v: (node_prefix, v), e)) for e in edges.values]",
        "sha1": "5f890e6c5d35cc97d8f13c15377e1c98019d91d3",
        "id": 143334
    },
    {
        "content": "def _get_a0(nshell, ncomponent):\n    \"\"\"Return the first mass number in the given shell\n    :param nshell: shell (0=s, 1=p, 2=sd, ...)\n    :param ncomponent: 1 -> neutrons, 2 -> protons & neutrons\n    \"\"\"\n    return int((nshell+2) * (nshell+1) * nshell/3 * ncomponent)",
        "sha1": "7228410c51c66e4fb88d649c6a82ed623242fea2",
        "id": 528579
    },
    {
        "content": "def vec2mat_index(N, I):\n    \"\"\"\n    Convert a vector index to a matrix index pair that is compatible with the\n    vector to matrix rearrangement done by the vec2mat function.\n    \"\"\"\n    j = int(I / N)\n    i = I - N * j\n    return i, j",
        "sha1": "c994f55f84168d84f33433fa491f4081f4e0d964",
        "id": 543796
    },
    {
        "content": "def find_selected_options(option_prefix: str, redcap_record:dict) -> list:\n    \"\"\"\n    Find all choosen options within *redcap_record* where option begins with\n    provided *option_prefix*.\n\n    Note: Values of options not choosen are empty strings.\n    \"\"\"\n    return [\n        value\n        for key, value\n        in redcap_record.items()\n        if key.startswith(option_prefix) and value\n    ]",
        "sha1": "6526fd00e52a18fa1b23bedb3d694d1ee6156f1c",
        "id": 570595
    },
    {
        "content": "def to_bytes_literal(seq):\n    \"\"\"Prints a byte sequence as a Python bytes literal that only uses hex encoding.\"\"\"\n    return 'b\"' + \"\".join(\"\\\\x{:02x}\".format(v) for v in seq) + '\"'",
        "sha1": "dadbc38fd86daf2b6dd618eee6c792bc2044d09c",
        "id": 696724
    },
    {
        "content": "def get_filename(metallicity=0.0, dust=0.0, age=1.0):\n    \"\"\"Generates the standard filename we use to access the files\n\n    Parameters:\n    metallicity (float): log(Z) in solar units (so 0.0 is solar metallicity)\n    dust (float): dust parameter\n    age (float): current age of stellar population in Gyr\n\n    Returns:\n    filename (str): Name of the file\n    \"\"\"\n    filename = f'Z{metallicity}_d{dust}_t{age}.df'\n    return filename",
        "sha1": "9042e9e0c703a5948f805092b87380af3e951825",
        "id": 357696
    },
    {
        "content": "def size_of(rect):\n    \"\"\"Return size of list|tuple `rect` (top, left, bottom, right) as tuple (width, height)\"\"\"\n    return (rect[3] - rect[1], rect[2] - rect[0])",
        "sha1": "07f50d974e74efca3b7985822fe3b3c84cdc2538",
        "id": 679177
    },
    {
        "content": "def atom_text(elem,pos):\n  \"\"\" convert elem,pos to text representation \"\"\"\n  assert len(elem) == len(pos)\n  lines = []\n  for iatom in range(len(elem)):\n      mypos = pos[iatom]\n      line = '%5s  %10.6f  %10.6f  %10.6f' % (elem[iatom],mypos[0],mypos[1],mypos[2])\n      lines.append(line)\n  atext = ';\\n'.join(lines)\n  return atext",
        "sha1": "77e6139a4958cbd52e89d0fa682c03c3332ed2b3",
        "id": 159905
    },
    {
        "content": "def calculate_input_vector_length(user_count, computer_count, auth_type_count, logon_type_count):\n    \"\"\"\n    Return model input vector length with user, computer, auth_type, logon_type one-hot encoded.\n    \"\"\"\n    return 3 + 2 * user_count + 2 * computer_count + auth_type_count + logon_type_count",
        "sha1": "12c4256d6c97c7d2406e36a07ee08254406a4331",
        "id": 108773
    },
    {
        "content": "def subtract_leak(cell, baseline_range, test_range, V_channel=1, I_channel=0):\n    \"\"\"Subtract leak conductance from the I channel of a Cell-like np.ndarray.\n\n    Calculates leak conductance based on Rm, which is extracted from test\n    pulse. Assumes test pulse is the same in each sweep.\n\n    Arguments\n    ---------\n    cell: Cell-like\n        [channels, time, sweeps] array.\n    baseline_range: slice\n        baseline time slice in timesteps\n    test_range: slice\n        test pulse time slice in timesteps\n    V_channel: int\n    I_channel: int\n\n    Returns\n    -------\n    Leak-subtracted array.\n\n    \"\"\"\n    Vtest_step = (\n        cell[V_channel, baseline_range, :].mean(axis=0)\n        - cell[V_channel, test_range, :].mean(axis=0)\n    ).mean()\n    Itest_step = (\n        cell[I_channel, baseline_range, :].mean(axis=0)\n        - cell[I_channel, test_range, :].mean(axis=0)\n    ).mean()\n\n    Rm = Vtest_step / Itest_step\n\n    I_leak = (\n        cell[V_channel, :, :] - cell[V_channel, baseline_range, :].mean()\n    ) / Rm\n\n    leak_subtracted = cell.copy()\n    leak_subtracted[I_channel, :, :] -= I_leak\n\n    return leak_subtracted",
        "sha1": "4c72eef13f0e9d7db173f5ef0a43ea2e9cc7613d",
        "id": 584653
    },
    {
        "content": "def convert_branch(branch):\n    \"\"\"\n    Convert release branch to MediaSDK and Media-driver branches\n\n    :param branch: Branch name\n    :type branch: String\n\n    :return: MediaSDK branch, Media-driver branch\n    :rtype: tuple\n    \"\"\"\n\n    if branch == 'mss2018_r2':\n        return branch, 'master'\n\n    if 'sdk' in branch:\n        sdk_branch = branch\n        driver_branch = branch.replace('sdk', '')\n    else:\n        sdk_branch = branch.replace('media', 'mediasdk')\n        driver_branch = branch\n\n    return sdk_branch, driver_branch",
        "sha1": "570de20aa5e15bb797ceb0b09cae1b212fe44303",
        "id": 590031
    },
    {
        "content": "from typing import Dict\n\n\ndef create_identity_parameters(identity: str) -> Dict[str, str]:\n    \"\"\"Creates request parameters containing identity\"\"\"\n    return {\"identity\": identity}",
        "sha1": "38c038071b10bb14bb501d9d5c38f8cd51f3204c",
        "id": 577146
    },
    {
        "content": "def is_blank_line(line):\n    \"\"\" Returns True if line is blank. \"\"\"\n    return not line.set_feature and not line.annotations and not line.comment",
        "sha1": "00c0466954bcb8b835fe07b8556cfb629f2db963",
        "id": 124131
    },
    {
        "content": "def basename(p):\n    \"\"\"Returns the final component of a pathname\"\"\"\n    i = p.rfind('/') + 1\n    return p[i:]",
        "sha1": "31ea0c8d5fb2be5acb64d0d5d4f16026a4fa999d",
        "id": 386872
    },
    {
        "content": "def pad(data):\n    \"\"\"Pads a string so the length is a multiple of 4\"\"\"\n\n    data += '=' * (len(data) % 4)\n    return data",
        "sha1": "bee70544893e280667539d3f3ca88b7e9ad3cfe5",
        "id": 464927
    },
    {
        "content": "def _deurnlidvid(lidvid: str) -> tuple[str, str]:\n    \"\"\"De-URN a LID VID.\n\n    Given a PDS ``lidvid`` as a Uniform Resource Name such as ``urn:nasa:pds:whatever::1.0``,\n    transform it to a double of ``whatever`` and ``1.0``.\n    \"\"\"\n    lid, vid = lidvid.split(\"::\")\n    return lid.split(\":\")[-1], vid",
        "sha1": "ab7c703e89553070b13af1510da85a2c7ef28715",
        "id": 420640
    },
    {
        "content": "def table_map_read(filename: str) -> dict:\n    \"\"\"\n    Reads dynamo table map file\n    :param file: table map file from dynamo\n    :return: dict of form {idx : '/path/to/tomogram'}\n    \"\"\"\n    table_map = open(filename, 'r')\n    lines = table_map.readlines()\n\n    out_dict = {}\n    for line in lines:\n        idx, path = line.strip().split()\n        out_dict[int(idx)] = path\n\n    table_map.close()\n    return out_dict",
        "sha1": "e7ffd0147801feb06ad986cb1f7331152b9c1ec0",
        "id": 213838
    },
    {
        "content": "def get_user_rating_max(ratings,n=20):\n    \"\"\"Return the keys of users with at most ratings\"\"\"\n    return [key for key,value in ratings.iteritems() if len(value)<=n]",
        "sha1": "9b4ba9c0ee6e11d1d5ec14ac41b1b93f029d6db4",
        "id": 45764
    },
    {
        "content": "import re\n\n\ndef has_problem_form(word):\n    \"\"\"\n    has_problem_form()\n\n    Purpose: Checks if the word has problem form.\n\n    @param word. A string\n    @return      the matched object if it has problem form, otheriwse None.\n\n    >>> has_problem_form('prognosis') is not None\n    True\n    >>> has_problem_form('diagnosis') is not None\n    True\n    >>> has_problem_form('diagnostic') is not None\n    True\n    >>> has_problem_form('arachnophobic') is not None\n    True\n    >>> has_problem_form('test') is not None\n    False\n    >>> has_problem_form('ice') is not None\n    False\n    \"\"\"\n    regex = r\".*(ic|is)$\"\n    return re.search(regex, word)",
        "sha1": "ac5a0da900feb75272a8f998d1a819a005c15fdf",
        "id": 394551
    },
    {
        "content": "def inc(n):\n    \"\"\"inc[rement].  Return n + 1\"\"\"\n    return n + 1",
        "sha1": "a10d60e00247cfe65f1cdb65e627745f780fbb23",
        "id": 507638
    },
    {
        "content": "from typing import List\nfrom typing import Dict\nfrom typing import Any\n\n\ndef _transform_dto_list_to_list_of_dicts(dto_list) -> List[Dict[str, Any]]:\n    \"\"\"\n    Given a list of DTO objects, this function returns a list of dicts, that can be passed to jsonify function.\n    \"\"\"\n    return [vars(dto_obj) for dto_obj in dto_list]",
        "sha1": "41d4d587aa78cf1e3879c22c2d95e28f9e4b0507",
        "id": 683823
    },
    {
        "content": "def exceptions_equal(exception1, exception2):\n    \"\"\"Returns True if the exceptions have the same type and message\"\"\"\n    # pylint: disable=unidiomatic-typecheck\n    return type(exception1) == type(exception2) and str(exception1) == str(exception2)",
        "sha1": "f677191d49e37cb11743743eb099d0594a66a781",
        "id": 308455
    },
    {
        "content": "import string\nimport random\n\n\ndef gen_string(min_length: int = 8, max_length: int = 12) -> str:\n  \"\"\"Generate a random string with letter, numbers, and symbols\n  Args:\n    min_length: minimum length of string\n    max_length: maximum length of string\n  Returns:\n    str Random length string with random characters\n  \"\"\"\n  allchar = string.ascii_letters + string.punctuation + string.digits\n  return \"\".join(\n      random.choice(allchar)\n      for _ in range(random.randint(min_length, max_length)))",
        "sha1": "9b4c2be544a19669e21ef023aeddab04eadc4f3b",
        "id": 510049
    },
    {
        "content": "def get_terminal_subgraph(g):\n    \"\"\"\n    Return the subgraph induced by terminal nodes in g\n    :param g:\n    :return:\n    \"\"\"\n    terminals = {node for node, d in g.nodes(data=True) if 'nt' not in d}\n    return g.subgraph(terminals).copy()",
        "sha1": "dfef760209436efdea0b219dd6d452f153b8d5a7",
        "id": 579177
    },
    {
        "content": "from io import StringIO\nimport csv\n\n\ndef parse_csv(data):\n    \"\"\"Parse a CSV string into an array of dictionaries\"\"\"\n    buf = StringIO(data)\n    result = [r for r in csv.DictReader(buf)]\n    buf.close()\n    return result",
        "sha1": "479d92fcf3de7214e08f4bb04626ec75461a3160",
        "id": 285135
    },
    {
        "content": "def isPalindrome(string): \n    \"\"\"\n    Checks if the given string is a palindrome.\n\n    Params\n    ======\n    string: str\n    \n    Returns\n    =======\n    result: bool\n    \"\"\"\n    # Length of string\n    length = len(string)\n    \n    # Loop through every character in the string \n    for cx, c in enumerate(string): \n\n        # Get the last index in the string\n        cx_end = length - cx - 1\n\n        # Only check if the characters are equal if cx is strictly less than cx_end\n        # If it is an even-numbered string, cx being strictly less than cx_end ends only after the midpoint\n        # If it is an odd-numbered string, the loop ends when cx == cx_end, which doesn't need to be checked\n        if cx >= cx_end:\n            break\n\n        # Check c with the index at the end of the string\n        if c != string[cx_end]:\n            return False \n\n    return True",
        "sha1": "64d3cfdaa6fc087a6ddd0117058301028fba426f",
        "id": 181072
    },
    {
        "content": "def is_pythagorean_triplet(a, b, c):\n    \"\"\"\n    A Pythagorean triplet is a set of three natural numbers a < b < c for which:\n    a^2 + b^2 = c^2\n    Example:    3,4,5\n                3^2 + 4^2 = 5^2\n                9 + 16 = 25\n    :param int a: first number\n    :param int b: second number\n    :param int c: third number\n    :return bool: returns True if a,b, and c are triplets\n    \"\"\"\n\n    try:\n        if a < b < c:\n            if (a**2 + b**2) == c**2:\n                return True\n            else:\n                return False\n        else:\n            return False\n    except TypeError:\n        raise TypeError(\"Input must be positive integers\")",
        "sha1": "4589cb489425ad89650287da530a722d70b9e36e",
        "id": 571979
    },
    {
        "content": "def step_learning_rate(base_lr, epoch, step_epoch, multiplier=0.1):\n    \"\"\"Sets the learning rate to the base LR decayed by 10 every step epochs\"\"\"\n    lr = base_lr * (multiplier ** (epoch // step_epoch))\n    return lr",
        "sha1": "27c012e53421c91838f63e6319f8d4f03d0ff76c",
        "id": 668051
    },
    {
        "content": "import re\n\n\ndef is_format_message(msg):\n    \"\"\"Is this message a _FORMAT string? These are often treated specially.\"\"\"\n    return re.match(r\"^[A-Z_]+_FORMAT$\", msg.msgid)",
        "sha1": "245f8e5ed1d443d9cb723852202c5ea28d195c53",
        "id": 421584
    },
    {
        "content": "def time_derivative(\n        coords, velocity_field, timestep, initial_velocity=None):\n    \"\"\"Computes the time derivative (du/dt).\n\n    Args:\n        coords (torch.Tensor) : coordinates of shape [N, D]\n        velocity_field (neuralff.Field) : velocity field function (aka u)\n        timestep (float) : delta t of the simulation\n        initial_velocity (torch.Tensor) : if provided, will use this instead of sampling the initial velocity.\n                                          This is useful for preconditioning the velocity field.\n\n    Returns:\n        (torch.Tensor, torch.Tensor) : \n            - time derivative of shape [N, D]\n            - velocity of shape [N, D]\n    \"\"\"\n    u = velocity_field.sample(coords)\n    if initial_velocity is None:\n        dudt = (u - u.detach()) / timestep\n    else:\n        dudt = (u - initial_velocity) / timestep\n    return dudt, u",
        "sha1": "1300115a8aaaa1dba5749c4200bce87c3b98a650",
        "id": 264144
    },
    {
        "content": "def get_row_nnz(mat, row):\n    \"\"\"Return the number of nonzeros in row.\n    \"\"\"\n    return mat.indptr[row+1] - mat.indptr[row]",
        "sha1": "a588e245763eb1cd237c9d2f71efacd7b27387c8",
        "id": 676884
    },
    {
        "content": "def extract_rules(lines):\n    \"\"\" Extracts rules from file. Rules are structured in nested dictionaries.\n    The outer dictionary has the color as key with a dictionary as value that contains the bag colors and amounts.\n    The inner dictionary has the bag color as key and the amount as value.\n\n    :param lines: Lines from input file\n    :return: dictionary\n    \"\"\"\n    rules = {}\n    # iterate over every line\n    for line in lines:\n        rule = {}\n        # get key and value\n        key, value = line.split('contain ')\n        key = key.split(' bags')[0]\n        # if bag doesn't contain other bags make value None\n        if value == 'no other bags.':\n            rules[key] = None\n        else:\n            # add bags to value\n            values = value.split(', ')\n            for v in values:\n                v = v.split(' ')\n                rule['{} {}'.format(v[1], v[2])] = int(v[0])\n\n            rules[key] = rule\n\n    return rules",
        "sha1": "5e57f8df21eae3c3778bd2be3dba6d4aedb8dc7b",
        "id": 513043
    },
    {
        "content": "def subfolders_in(whole_path):\n    \"\"\"\n    Returns all subfolders in a path, in order\n\n    >>> subfolders_in('/')\n    ['/']\n\n    >>> subfolders_in('/this/is/a/path')\n    ['/this', '/this/is', '/this/is/a', '/this/is/a/path']\n\n    >>> subfolders_in('this/is/a/path')\n    ['this', 'this/is', 'this/is/a', 'this/is/a/path']\n    \"\"\"\n    path_fragments = whole_path.lstrip('/').split('/')\n    if whole_path.startswith('/'):\n        path_fragments[0] = '/' + path_fragments[0]\n    path = path_fragments[0]\n    subfolders = [path]\n    for fragment in path_fragments[1:]:\n        path += '/' + fragment\n        subfolders.append(path)\n    return subfolders",
        "sha1": "a7389811a8acacea87abd55ba47892203e0b95e5",
        "id": 7112
    },
    {
        "content": "def _get_or_default(mylist, i, default=None):\n    \"\"\"return list item number, or default if don't exist\"\"\"\n    if i >= len(mylist):\n        return default\n    else :\n        return mylist[i]",
        "sha1": "758bd84c0aa257762d979b546096feee45f5f125",
        "id": 212377
    },
    {
        "content": "def __predict(X_test, model, X_scaler):\n    \"\"\"\n    Do prediction for provided fetures\n    Arguments:\n        X_test: the test data [n_samples, n_features]\n        model: the classification predictive model\n        X_scaler: the standard scaler used to scale train features\n    Return:\n        predicted labels as array of shape = [n_samples, n_classes] with probabilities\n        of each class\n    \"\"\"\n    X_test = X_scaler.transform(X_test)\n    labels = model.predict_proba(X_test)\n    return labels",
        "sha1": "7e4c3819d261e7bee0512362a1022a9ef25b4c74",
        "id": 662322
    },
    {
        "content": "from typing import Optional\n\n\ndef _Truncate(value: Optional[str], max_length: int) -> Optional[str]:\n  \"\"\"Truncate a string if it exceeds the maximum length.\"\"\"\n  if value and len(value) > max_length:\n    return value[:max_length - 3] + '...'\n  return value",
        "sha1": "c523a7ef12f441a57528a3bd7e3b32ac3793c06c",
        "id": 229023
    },
    {
        "content": "def extract_comparator_expr(comparative_step):\n    \"\"\"Extract comparator and numeric expression\n     of a comparative QDMR step\n\n Parameters\n ----------\n comparative_step : str\n     string of the QDMR comparative step\n\n Returns\n -------\n str\n     returns string representation of the comparator expression\n \"\"\"\n    comparator = None\n    if 'at least' in comparative_step:\n        comparator = '>='\n    elif 'at most' in comparative_step:\n        comparator = '=<'\n    elif ('more' in comparative_step) or \\\n            ('higher' in comparative_step) or ('larger' in comparative_step):\n        comparator = '>'\n    elif ('less' in comparative_step) or \\\n            ('smaller' in comparative_step) or ('lower' in comparative_step):\n        comparator = '<'\n    elif ('not ' in comparative_step) and (('same as' in comparative_step) or \\\n                                           ('equal' in comparative_step) or ('is' in comparative_step) or \\\n                                           ('was' in comparative_step) or ('are' in comparative_step)):\n        comparator = '!='\n    elif ('not ' not in comparative_step) and (('same as' in comparative_step) or \\\n                                               ('equal' in comparative_step) or ('is' in comparative_step) or \\\n                                               ('was' in comparative_step) or ('are' in comparative_step)) and \\\n            ('any' not in comparative_step):\n        comparator = '='\n    elif ('contain' in comparative_step):\n        comparator = 'CONTAINS'\n    else:\n        comparator = 'FILTER'\n    return comparator",
        "sha1": "4eefa45ccc15b6f241fe216e5b58e0947765af0c",
        "id": 71242
    },
    {
        "content": "def all_keys(input):\n    \"\"\"\n    Recursive function. Get every keyname in every descendant of a dictionary.\n    Iterates down on list and dict structures to search for more dicts with\n    keys.\n    \"\"\"\n    values = []\n    if isinstance(input, dict):\n        values = list(input.keys())\n        for key in list(input.keys()):\n            values = values + all_keys(input[key])\n    if isinstance(input, list):\n        for value in input:\n            valleys = all_keys(value)\n            for val in valleys:\n                values.append(val)\n    return values",
        "sha1": "06afdf72ca7f7a101fc294a0c36f9310cf70bf6d",
        "id": 401232
    },
    {
        "content": "def sanitize_strings(txt):\n    \"\"\"\n    Removes newlines from a piece of text\n    :param txt: a string.\n    :return: a string without new lines.\n    \"\"\"\n    if not txt:\n        return \"\"\n    if len(txt) > 0:\n        return \"\".join(txt.splitlines())\n    else:\n        return \"\"",
        "sha1": "946c615ca058229f6f9ea3d46c74287067c43dbf",
        "id": 432179
    },
    {
        "content": "def normalized_device_coordinates_to_image(image):\n    \"\"\"Map normalized value from [-1, 1] -> [0, 255].\n    \"\"\"\n    return (image + 1.0) * 127.5",
        "sha1": "39f244b32c18807f7eb87f529ebccf14adc895ad",
        "id": 397787
    },
    {
        "content": "def bytes_8_to_int(b: bytes) -> int:\n    \"\"\"Deserialize an integer from the corresponding big endian bytes.\"\"\"\n    assert len(b) == 8\n    return int.from_bytes(b, byteorder='big')",
        "sha1": "edaf9ab7d07288d8f895027339bf06a990f90b52",
        "id": 453059
    },
    {
        "content": "import six\n\n\ndef safe_filename(filename, extension=None):\n    \"\"\"\n    Returns a filename with FAT32-, NTFS- and HFS+-illegal characters removed.\n\n    Unicode or bytestring datatype of filename is preserved.\n\n    >>> safe_filename(u'spam*?: \ud800\udf43\ud800\udf40\ud800\udf30\ud800\udf3c-&.txt')\n    u'spam \ud800\udf43\ud800\udf40\ud800\udf30\ud800\udf3c-&.txt'\n    \"\"\"\n    filename = filename if isinstance(filename, six.text_type) else filename.decode('utf8')\n    if extension is not None:\n        filename = \"{}.{}\".format(filename, extension)\n    unsafe_chars = ':*?\"<>|/\\\\\\r\\n'\n    for c in unsafe_chars:\n        filename = filename.replace(c, '')\n    return filename",
        "sha1": "d3571ea8d272d1081fc132c204b1a8c5544c2cd2",
        "id": 115202
    },
    {
        "content": "def generate_potential_box_dimensions(settings,feature_to_input_x,feature_to_input_y):\n    \"\"\"\n        Generate potential boxes height & width for each point aka anchor boxes given the \n        ratio between feature map to input scaling for x and y\n        Assumption 1: Settings will have the following attributes\n            AspectRatioW_div_W: A list of float values representing the aspect ratios of\n                the anchor boxes at each location on the feature map\n            Scales: A list of float values representing the scale of the anchor boxes\n                at each location on the feature map.\n    \"\"\"\n    box_width_height = []\n    for scale in settings[\"Scales\"]:\n        for aspect_ratio_w_div_h in settings[\"AspectRatioW_div_W\"]:\n            width = round(feature_to_input_x*scale*aspect_ratio_w_div_h)\n            height = round(feature_to_input_y*scale/aspect_ratio_w_div_h)\n            box_width_height.append({\"Width\":width,\"Height\":height})\n    return box_width_height",
        "sha1": "1713f5de5e7030d2f52aeb9b5742c48069365141",
        "id": 147773
    },
    {
        "content": "def FindFieldDef(field_name, config):\n  \"\"\"Find the specified field, or return None.\"\"\"\n  if not field_name:\n    return None\n  field_name_lower = field_name.lower()\n  for fd in config.field_defs:\n    if fd.field_name.lower() == field_name_lower:\n      return fd\n\n  return None",
        "sha1": "16f5fbd5ffb64a79723716652f3571c0245d1568",
        "id": 585153
    },
    {
        "content": "def overlap(sma, low_1, high_1, low_2, high_2, threshold=1):\n    \"\"\" Tests for non_overlap between array sets 1 and 2\n\n    Args:\n        sma (arr): Test parameters, preferably a numpy arange object\n        low_1 (scipy.interpolate.interp1d): interp object for the lower bounds of set 1\n        high_1 (scipy.interpolate.interp1d): interp object for the higher bounds of set 1\n        low_2 (scipy.interpolate.interp1d): interp object for the lower bounds of set 2\n        high_2 (scipy.interpolate.interp1d): interp object for the higher bounds of set 2\n        threshold (int): How many consecutive times you see non-overlap between the two profiles.\n\n    Returns:\n        The index where the sufficient overlap is broken.\n    \"\"\"\n    count, i = 0, 0\n    for i in sma:\n        overlapping = not ((high_2(i) < low_1(i)) or (low_2(i) > high_1(i)))\n        if overlapping:\n            count = 0\n        if count == threshold:\n            break\n        count += 1\n\n    return i",
        "sha1": "80dff641d9170c5c7c4001db1841b3b1a3862d73",
        "id": 432799
    },
    {
        "content": "def commafy(s):\n    \"\"\"\n    Returns a copy of s, with commas every 3 digits.\n\n    Example:\n        commafy('5341267') = '5,341,267'\n\n    Parameter s: string representing an integer\n    Precondition: s a string with only digits, not starting with 0\n    \"\"\"\n    # You can't always check everything with an assert\n    # However, check what you can\n    assert type(s) == str, repr(s) + ' is not a string'\n\n    # Work on small data   (BASE CASE)\n    if len(s) <= 3:\n        return s\n\n    # Break up into halves (RECURSIVE CASE)\n    left = commafy(s[0:-3])\n    right = s[-3:]\n\n    # Combine the answer\n    return left + ',' + right",
        "sha1": "cc42631d39ace7165f9620547eca706615af3214",
        "id": 159501
    },
    {
        "content": "import re\n\n\ndef _single_line_filter(value: str) -> str:\n    \"\"\"Reduce a string to a single line with no insignificant padding.\"\"\"\n    regex = re.compile(r\"\\s{2,}\")\n    return regex.sub(\" \", (value or \"\").replace(\"\\n\", \" \")).strip()",
        "sha1": "62c44f2f02aae4eb404225a705e8e1df70d98496",
        "id": 589298
    },
    {
        "content": "def get_sample_count(profileDict):\n    \"\"\"\n    Gets the number of samples taken from a dictionary representing data from an\n    Arm MAP file\n\n    Args:\n        profileDict (dict): Dictionary from which to obtain the count of samples\n\n    Returns:\n        The number of samples taken (non-negative integer)\n    \"\"\"\n    assert isinstance(profileDict, dict)\n\n    return profileDict[\"samples\"][\"count\"]",
        "sha1": "26d31dd6fae3e0c4adf567706387712397e7fd28",
        "id": 111838
    },
    {
        "content": "def chunks(l, k):\n    \"\"\"\n    Take a list, l, and create k sublists.\n    \"\"\"\n    n = len(l)\n    return [l[i * (n // k) + min(i, n % k):(i+1) * (n // k) + min(i+1, n % k)] for i in range(k)]",
        "sha1": "7cf0c39941ed8f358c576046154af6b3ee54b70a",
        "id": 4566
    },
    {
        "content": "def extract_name_email (txt):\n    \"\"\" \n        Extracts the name and email from RFC-2822 encoded email address.\n        For eg. \"Jeff Jeff <jeff.jeff@gmail.com>\" returns (\"Jeff Jeff\", \"jeff.jeff@gmail.com\")\n    \"\"\"\n    if \"<\" in txt and \">\" in txt:\n        name, email = txt.split(\"<\")\n        return name[:-1], email[:-1]\n    elif \"<\" not in txt and \">\" not in txt:\n        return \"\", txt\n    else:\n        return None",
        "sha1": "5226ab6c7fd9cfd3047e6a512f076f1e22a05f8c",
        "id": 663705
    },
    {
        "content": "def FindLast(element, xpath):\n  \"\"\"Returns the last element returned by element.findall(xpath).\n\n  Args:\n    element: Element to search under.\n    xpath: xpath query to search with.\n\n  Returns:\n    Last found element, or None if none were found.\n  \"\"\"\n  return (element.findall(xpath) or [None])[-1]",
        "sha1": "9e5021d96094ff27c6629760c25352e588484240",
        "id": 234813
    },
    {
        "content": "import pathlib\n\n\ndef get_test_directory() -> pathlib.Path:\n    \"\"\"Get the directory of this test file\"\"\"\n    return pathlib.Path(__file__).parent",
        "sha1": "ae8086f1fb3c18e67a659b239cef6c0c69922da4",
        "id": 286260
    },
    {
        "content": "import click\n\n\ndef option_input_file(required: bool = False):\n    \"\"\"Get parameter options for input file.\"\"\"\n    return click.option(\n        \"--input-file\",\n        \"--if\",\n        \"input_file\",\n        metavar=\"string\",\n        required=required,\n        help=\"name of file to read from\",\n        type=click.File(\"r\"),\n    )",
        "sha1": "dfa96bf6c5a0cf7921014029900f7c3339735a78",
        "id": 507445
    },
    {
        "content": "def get_km_user_image_upload_path(km_user, imagename):\n    \"\"\"\n    Get the path to upload the kmuser image to.\n\n    Args:\n        km_user:\n            The km_user whose image is being uploaded.\n        imagename (str):\n            The original name of the image being uploaded.\n\n    Returns:\n        str:\n            The original image filename prefixed with\n            `users/<user_id>/{file}`.\n    \"\"\"\n    return \"know-me/users/{id}/images/{file}\".format(\n        file=imagename, id=km_user.id\n    )",
        "sha1": "01b350177f9d605b508debc5c391964dffcaf5e1",
        "id": 644382
    },
    {
        "content": "import socket\n\n\ndef get_available_port(default_port):\n    \"\"\"Find an available port to bind to.\"\"\"\n    port = default_port\n    with socket.socket() as s:\n        while True:\n            try:\n                s.bind((\"127.0.0.1\", port))\n            except OSError:\n                port += 1\n            else:\n                break\n\n    return port",
        "sha1": "1813b75270653524e64c8a6331dfa85f05d1f71c",
        "id": 90093
    },
    {
        "content": "def _format_collider_string(colliders):\n    \"\"\" Write the string for the bath gas collider and their efficiencies\n        for the Lindemann and Troe functional expressions:\n\n        :param colliders: the {collider: efficiency} dct\n        :type colliders: dct {str: float}\n        :return: collider_str: Chemkin string with colliders and efficiencies\n        :rtype: str\n    \"\"\"\n    collider_str = '    '  # name_buffer\n    collider_str += ''.join(\n        ('{0:s}/{1:4.3f}/   '.format(collider, efficiency)\n         for collider, efficiency in colliders.items()))\n    collider_str += '\\n'\n\n    return collider_str",
        "sha1": "4e4aa8ae46dfcf05f00222b4cf5c95384977d651",
        "id": 48788
    },
    {
        "content": "import hashlib\n\n\ndef get_file_md5(fpath, blocksize=2**20):\n    \"\"\"Get a file's MD5 checksum.\n\n    Code from stackoverflow.com/a/1131255\n\n    Parameters\n    ----------\n    fpath : string\n        Path to file\n\n    blocksize : int\n        Read file in chunks of this many bytes\n\n    Returns\n    -------\n    md5sum : string\n        32-characters representing hex MD5 checksum\n\n    \"\"\"\n    md5 = hashlib.md5()\n    with open(fpath, 'rb') as f:\n        while True:\n            buf = f.read(blocksize)\n            if not buf:\n                break\n            md5.update(buf)\n    return md5.hexdigest()",
        "sha1": "1df1e6c31ab30f025b27dc7d8a4b89146858bdf6",
        "id": 637412
    },
    {
        "content": "def get_unique(l):\n    \"\"\" Get unique values from list\n        Placed outside the class beacuse `list` conflicts our internal\n        method with the same name.\n    \"\"\"\n    return list(set(l))",
        "sha1": "6451ad4e6f8e38f7d16ef582c2dcb97dc2f6eaeb",
        "id": 583201
    },
    {
        "content": "def limit(min_, num, max_):\n\t\"\"\"limit a number within a specific range\"\"\"\n\treturn max(min(num,max_),min_)",
        "sha1": "7228f1b45bfea9ed978f46b356f9d883a8897b50",
        "id": 488398
    },
    {
        "content": "def source_for_file(filename):\n    \"\"\"Return the source filename for `filename`.\n\n    Given a file name being traced, return the best guess as to the source\n    file to attribute it to.\n\n    \"\"\"\n\n    return filename",
        "sha1": "ca4ddfca0cc8ac0bfbb920a7166c8b005a49baea",
        "id": 245162
    },
    {
        "content": "def recommend_auto_balance(message: str) -> str:\n    \"\"\"Expands a message with recommendation to :mod:`torchpipe.balance`.\"\"\"\n    return f\"\"\"{message}\n\nIf your model is still under development, its optimal balance would change\nfrequently. In this case, we highly recommend 'fairscale.nn.pipe.balance' for\nnaive automatic balancing:\n\n  from fairscale.nn import Pipe\n  from fairscale.nn.pipe.balance import balance_by_time\n\n  partitions = torch.cuda.device_count()\n  sample = torch.empty(...)\n  balance = balance_by_time(partitions, model, sample)\n\n  model = Pipe(model, balance, ...)\n\"\"\"",
        "sha1": "784a58403896fc7e722ceb961dfc0fc731a55761",
        "id": 527755
    },
    {
        "content": "def distinct(inlist):\n    \"\"\"\n    returns a list of distinct values\n    (no duplicated values)\n    \"\"\"\n    outlist = []\n    for elem in inlist:\n        if not elem in outlist:\n            outlist.append(elem)\n    return outlist",
        "sha1": "23878d211b8c813d8c9f24309cb684c9361a8bde",
        "id": 343350
    },
    {
        "content": "def split_task_parameters(line):\n    \"\"\" Split a string of comma separated words.\"\"\"\n    if line is None:\n        result = []\n    else:\n        result = [parameter.strip() for parameter in line.split(\",\")]\n    return result",
        "sha1": "edbd778c496464ba2e86d8efb00f4a6c78da8fe4",
        "id": 544278
    },
    {
        "content": "def whole_inference(imgs, model):\n    \"\"\"Inference with full image.\"\"\"\n\n    preds = model(imgs)\n    return preds",
        "sha1": "19c7667d527c0ae346bd418c7ad238a76bbfd945",
        "id": 611544
    },
    {
        "content": "import inspect\n\n\ndef collect_existing_subclasses(module, base_class):\n    \"\"\"Collect a set of class names in module that are subclasses of base_class\"\"\"\n    class_tuples = inspect.getmembers(module, inspect.isclass)\n    return set(\n        [\n            class_tuple[0]\n            for class_tuple in class_tuples\n            if issubclass(class_tuple[1], base_class)\n        ]\n    )",
        "sha1": "85054dbfbcd53e1c87c226cd123b28807db52e6b",
        "id": 473552
    },
    {
        "content": "def in_segregation(x0, R, n, N=None):\n    \"\"\"\n    return the actual indium concentration\n    in th nth layer\n\n    Params\n    ------\n    x0 : float\n        the indium concentration between 0 and 1\n    R : float\n        the segregation coefficient\n    n : int\n        the current layer\n    N : int\n        number of layers in the well\n    \"\"\"\n    if N:\n        return x0*(1-R**N)*R**(n-N)\n    return x0*(1-R**n)",
        "sha1": "b321557ea7e64cec5c26b4c99c60147dc283e685",
        "id": 542316
    },
    {
        "content": "def partition(list_, begin, end, pivotIndex):\n    \"\"\"\n    Partition a sublist around a pivot and return the final pivot index.\n    \n    list_ -- A list of items that can be compared using the less than or \n             equal (<=) operator.\n    begin -- The index of the first of the items we care about.\n    end -- The index just past the last of the items we care about.\n    pivotIndex -- the (initial) index of the pivot\n    \n    Partition the sublist list_[begin:end] into three parts:\n    - left,  which contains all items less or equal to the pivot,\n    - pivot, the pivot item itself, and\n    - right, which contains all items greater than or equal to the pivot\n    \"\"\"\n    pivot = list_[pivotIndex]\n    # move pivot to the end\n    list_[pivotIndex], list_[end - 1] = list_[end - 1], list_[pivotIndex]\n    # storeIndex will point to the first element that is greater than or\n    # equal to the pivot (initially points to the leftmost element)\n    storeIndex = begin\n    for i in range(begin, end - 1):\n        if list_[i] < pivot:\n            list_[i], list_[storeIndex] = list_[storeIndex], list_[i]\n            storeIndex += 1\n    # now everything before storeIndex is less than the pivot, and storeIndex\n    # points to the first item that is greater than or equal to the pivot;\n    # move pivot back to where storeIndex points\n    list_[storeIndex], list_[end - 1] = list_[end - 1], list_[storeIndex]\n    return storeIndex",
        "sha1": "794f3b741a877e65b17567df0baf2a6e00be53e7",
        "id": 464165
    },
    {
        "content": "def fc_params(in_features: int, out_features: int, bias: bool = True):\n    \"\"\"\n    Return the number of parameters in a linear layer.\n    Args:\n        in_features: Size of input vector.\n        out_features: Size of output vector.\n        bias: If true count bias too.\n    Returns:\n        The number of parameters.\n    \"\"\"\n    m = out_features + 1 if bias else out_features\n    return in_features * m",
        "sha1": "0138ae52f101975aba2fd6f38452471a09a2c8e1",
        "id": 33234
    },
    {
        "content": "def _extract_unit_style(stdout):\n    \"\"\"Parse LAMMPS stdout to extract unit style. \"\"\"\n    lines = stdout.split(\"\\n\")\n    line_nbr = ['Unit style' in item for item in lines].index(True)\n    unit_style = lines[line_nbr].split()[-1]\n    return unit_style",
        "sha1": "0b13649addc7e781c46e772dcdfabb4111caadbb",
        "id": 325769
    },
    {
        "content": "import hashlib\n\n\ndef _sha1_for_file(filename):\n    \"\"\"Return sha1 for contents of filename.\"\"\"\n    with open(filename, \"rb\") as fileobj:\n        contents = fileobj.read()\n        return hashlib.sha1(contents).hexdigest()",
        "sha1": "f0a0b67da6400abc063c7ca02c1d830d8101b9b6",
        "id": 389571
    },
    {
        "content": "def default_wd_filter(x):\n    \"\"\"default weight decay filter.\"\"\"\n    parameter_name = x.name\n    if parameter_name.endswith('.bias'):\n        # all bias not using weight decay\n        return False\n    if parameter_name.endswith('.gamma'):\n        # bn weight bias not using weight decay, be carefully for now x not include BN\n        return False\n    if parameter_name.endswith('.beta'):\n        # bn weight bias not using weight decay, be carefully for now x not include BN\n        return False\n\n    return True",
        "sha1": "defd970e4bcb589f48011dfb23a4af10fc9c0b50",
        "id": 637590
    },
    {
        "content": "import math\n\n\ndef count_digits(n):\n    \"\"\"\n    Count digits in integer\n    \"\"\"\n    if n > 0:\n        digits = int(math.log10(n)) + 1\n    elif n == 0:\n        digits = 1\n    else:\n        digits = int(math.log10(-n)) + 1\n    return digits",
        "sha1": "03e5f041e096f2137153418f99349f3e8e844d41",
        "id": 26589
    },
    {
        "content": "def find_scan_info(filename,  position = '__P', scan = '__S', date = '____'):\n    \"\"\"\n    Find laser position and scan number by looking at the file name\n    \"\"\"\n    try:\n        file = filename.split(position, 2)\n\n        file = file[1].split(scan, 2)\n        laser_position = file[0]\n\n        file = file[1].split(date, 2)\n        scan_number = file[0]\n    except IndexError:\n        laser_position = -1\n        scan_number = -1\n\n    return laser_position, scan_number",
        "sha1": "f98afb440407ef7eac8ceda8e15327b5f5d32b35",
        "id": 6218
    },
    {
        "content": "def get_single_key_value_pair(d):\n    \"\"\"\n    Get the key and value of a length one dictionary.\n\n    Parameters\n    ----------\n    d : dict\n        Single element dictionary to split into key and value.\n\n    Returns\n    -------\n    tuple\n        of length 2, containing the key and value\n\n    Examples\n    --------\n\n    >>> d = dict(key='value')\n    >>> get_single_key_value_pair(d)\n    ('key', 'value')\n\n    \"\"\"\n    assert isinstance(d, dict), f'{d}'\n    assert len(d) == 1, f'{d}'\n    return list(d.items())[0]",
        "sha1": "c4fb7a05aa74a69ebb55a867e23298748a919738",
        "id": 696983
    },
    {
        "content": "def connected_comp_edge_handler(graph, edge_weight='weight'):\n    \"\"\"\n    Modified version of contract_edges used to handle edges in strongly_connected_comp_splitter. Modified by JC.\n    Args:\n        graph (NetworkX MultiDiGraph): strongly connected component graph\n        edge_weight (str): edge attribute to use as weight\n    Returns:\n        list of tuples: [(edge node 1, edge node 2, edge attr dict)]\n    \"\"\"\n    packaged_edges = []\n    for e in graph.edges(data=True):\n        if 'name' in e[2]:\n            street_name = e[2]['name']\n        elif 'junction' in e[2]:\n            street_name = 'junction_' + e[2]['junction']\n        else:\n            street_name = 'unlabeled'\n        attr = {\n                'start_node': e[0],\n                'end_node': e[1],\n                'distance': e[2][edge_weight],\n                'path': [e[0], e[1]],\n                'turn_length' : e[2]['turn_length'],\n                'length': e[2][edge_weight],\n                'name': street_name,\n                'required': 1,\n                }\n\n        edge = (e[0], e[1], attr)\n        packaged_edges += [edge]\n    return packaged_edges",
        "sha1": "cdf8b139d90e1eec222a6c8896aec8eba0f5ce0b",
        "id": 507331
    },
    {
        "content": "def restore_entities(funql, mention_map):\n  \"\"\"Restore entities in funql.\"\"\"\n  for mention_mark, identifier in mention_map.items():\n    funql = funql.replace(mention_mark, \"%s\" % identifier)\n  return funql",
        "sha1": "7d24484e710e7bff65768037b208ac6e012a480e",
        "id": 456871
    },
    {
        "content": "def nids_to_hosts(nidlist):\n    \"\"\"Convert a list of integer nids to a list of hostnames\"\"\"\n    return [\"nid%06d\" % nid for nid in nidlist]",
        "sha1": "315901dac1f400776ab021f0cbb3fe5ec6bd565c",
        "id": 308866
    },
    {
        "content": "import re\n\n\ndef normalize_name(name):\n    \"\"\"\n    Converts camel-case style names into underscore seperated words. Example::\n\n        >>> normalize_name('oneTwoThree')\n        'one_two_three'\n        >>> normalize_name('FourFiveSix')\n        'four_five_six'\n\n    \"\"\"\n    new = re.sub('(((?<=[a-z])[A-Z])|([A-Z](?![A-Z]|$)))', '_\\\\1', name)\n    return new.lower().strip('_')",
        "sha1": "c02f24f3daa87fac5d5842825de052069ff38ba5",
        "id": 303395
    },
    {
        "content": "from typing import List\n\n\ndef filter_list_of_dicts(l : List[dict], keys :list) -> List[dict]:\n    \"\"\"Filter dict keys in list of dictionaries.\n    Args:\n        l       : list of dicts\n        filter  : name of keys to retain\n    Returns:\n        list : with filtered dicts\n    \"\"\"\n    return [{k:v for k,v in x.items() if k in keys} for x in l]",
        "sha1": "c99b7efd91114a0f69a97378f0041e07259ddcf3",
        "id": 174993
    },
    {
        "content": "def extract_version(package_name):\n    \"\"\"\n    Extracts the version from the package_name.\n\n    The version is defined as one of the following:\n\n    -3245s\n    -ab434\n    -1.1-343s\n    -2.3-4\n    -134-minimal-24\n\n    but not:\n\n    -ab-13\n    -ab-ab\n    -m14-m16\n\n    The leading \"-\" is discarded.\n\n    Example:\n\n    >>> extract_version(\"jinja-2.5\")\n    '2.5'\n\n    \"\"\"\n    def numeric(c):\n        if c in [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"]:\n            return True\n        return False\n\n    first_dash = package_name.find(\"-\")\n    last_dash = package_name.rfind(\"-\")\n    if first_dash == last_dash:\n        return package_name[first_dash+1:]\n    while not numeric(package_name[first_dash + 1]):\n        package_name = package_name[first_dash+1:]\n        first_dash = package_name.find(\"-\")\n        last_dash = package_name.rfind(\"-\")\n        if first_dash == last_dash:\n            return package_name[first_dash+1:]\n    return package_name[first_dash + 1:]",
        "sha1": "d0f5686f9b9e81654a6c9a8004c6c549fb49ac62",
        "id": 264427
    },
    {
        "content": "def load_alist(path):\n    \"\"\"Read `alist`-file [MacKay]_ and return nested list describing the\n    parity-check matrix of a code.\n\n    Many code examples can be found in [UniKL]_.\n\n    Input\n    -----\n    path:str\n        Path to file to be loaded.\n\n    Output\n    ------\n    alist: list\n        A nested list containing the imported alist data.\n    \"\"\"\n\n    alist = []\n    with open(path, \"r\") as reader: # pylint: disable=unspecified-encoding\n        # read list line by line (different length)\n        for line in reader:\n            l = []\n            # append all entries\n            for word in line.split():\n                l.append(int(word))\n            if l: # ignore empty lines\n                alist.append(l)\n\n    return alist",
        "sha1": "08579269ae33eb00f6c41b511f201531eb34f9c7",
        "id": 657681
    },
    {
        "content": "def vec(x):\n    \"\"\"Vectorizes a matrix by stacking the columns.\n    \n    NumPy arrays use row major ordering, while MATLAB uses column major\n    ordering. Therefore in NumPy `reshape((-1, 1))` stacks the rows instead of\n    columns.\n    \n    This function is just a shorthand for `reshape((-1, 1), order='F')`.\n\n    Args:\n        x: An ndarray to be vectorized.\n    \"\"\"\n    return x.reshape((-1, 1), order='F')",
        "sha1": "6c89ad4e2147c86c756523e42b45e9ddcf4ec09a",
        "id": 433482
    },
    {
        "content": "def invert_dictionary_with_array(dictionary: dict):\n    \"\"\"\n    Inverts a dictionary with arrays as values\n    e.g. {key: [value1, value2]} -> {value1: key, value2: key}\n    \"\"\"\n    inv_map = {}\n    for key, array in dictionary.items():\n        for value in array:\n            inv_map[value] = key\n    return inv_map",
        "sha1": "f37384c27f2c8a495cef19b3593e33d73c7e64dd",
        "id": 385874
    },
    {
        "content": "import base64\n\n\ndef encode(file):\n    \"\"\"\n    A base64 encoder for Images\n\n    :param file: Path of the Image which should be encoded. !IMPORTANT! file ending e.g.: eecevit.jpg\n    :return: the encoded Image as base64 String\n    \"\"\"\n    with open(file, \"rb\") as image_file:\n        encoded_string = base64.b64encode(image_file.read())\n        return encoded_string",
        "sha1": "5e76d861ca255f3be91d1337754477269e4ca1e2",
        "id": 507975
    },
    {
        "content": "def add_metadata(identifier, label, data):\n    \"\"\"\n    Used to add identifier and label to data tuple.\n    Label is always cast to str and uppercased.\n\n    data:\n        Read data.\n    identifier:\n        The unique identifier used to identify data sets\n    label:\n        Label  \n    \"\"\"\n    label = str(label).upper()\n    return [identifier, label] + data",
        "sha1": "3f526ab903d307a7a17e4b6657f95b20b0a1bede",
        "id": 148853
    },
    {
        "content": "def get_fmriprep_outlier_volumes_from_confounds(confounds_df):\n    \"\"\"extract which volume numbers are outliers from the fmriprep confounds df.\n\n    Returns:\n        bad_volumes: list\n\n    eg [34, 35, 100, 150]\n    \"\"\"\n\n    # get the motion columns\n    motion = confounds_df.filter(regex='motion')\n\n    # find any rows with values above 0\n    return_df = motion[(motion > 0).any(1)]\n\n    # return the index (row names) of this df\n    return list(return_df.index)",
        "sha1": "b62d833ec2b7f000584354ca6470863acb33682c",
        "id": 18777
    },
    {
        "content": "import math\n\n\ndef deg2rad(deg=1):\n    \"\"\"convert degrees to radians.\"\"\"\n    return math.pi * float(deg)/180.0",
        "sha1": "f8c3c7b681f4dc30f09001ea1be0d75b36497336",
        "id": 54426
    },
    {
        "content": "def gradient_of_rmse(y_hat, y, Xn):\n    \"\"\" \n    Returns the gradient of the Root Mean Square error with respect to the \n    parameters of the linear model that generated the prediction `y_hat'. \n    Hence, y_hat should have been generated by a linear process of the form\n    Xn.T.dot(theta)\n\n    Args:\n        \n       y_hat (np.array of shape N,): The predictions of the linear model\n       y (np.array of shape N,): The \"ground-truth\" values.\n\n    Returns:\n        The RMSE between y_hat and y\n    \"\"\"\n        \n    N = y.shape[0]\n    assert N > 0, ('At least one sample is required in order to compute the '\n                  'RMSE loss')\n   \n    losses = y - y_hat\n    gradient = - 2 * Xn.T.dot(losses) / N\n\n    return gradient",
        "sha1": "73a46197f90cf1b9c0a90a8ce2d2eae006c6d002",
        "id": 708016
    },
    {
        "content": "import json\n\n\ndef get_cache_key(args, kwargs):\n    \"\"\"\n    Serialize arguments to get a cache key. It will be used to store function\n    results.\n\n    Returns:\n        str: generated key\n    \"\"\"\n    if len(args) == 0 and len(kwargs) == 0:\n        return \"\"\n    elif len(args) == 0:\n        return json.dumps(kwargs)\n    elif len(kwargs.keys()) == 0:\n        return json.dumps(args)\n    else:\n        return json.dumps([args, kwargs])",
        "sha1": "ee162657352929fe9b295490f23fec76d7a9dec8",
        "id": 409443
    },
    {
        "content": "def get_player_input(player_char, char_list):\n    \"\"\"Get a players move until it is a valid move on the board with no piece currently there.\"\"\"\n    while True:\n        #Get user input\n        player_move = int(input(player_char + \": Where would you like to place your piece (1-9): \"))\n        #Move is on board\n        if player_move > 0 and player_move < 10:\n            #Move is an empty spot\n            if char_list[player_move - 1] == '_':\n                return player_move\n            else:\n                print(\"That spot has already been chosen.  Try again.\")\n        else:\n            print(\"That is not a spot on the board.  Try again.\")",
        "sha1": "2e2d1ac0b8e4fe6a2467a378dfb9e1821525cddb",
        "id": 15319
    },
    {
        "content": "def get_one(it):\n    \"\"\"return first item from iterator\n    >>> get_one([1,2,3])\n    1\n    >>> get_one(set([1,2,3])) in set([1,2,3])\n    True\n    \"\"\"\n    for i in it:\n        return i",
        "sha1": "31f7f51149f24af6da85f2d2ce2bed40fa5ed0e3",
        "id": 154720
    },
    {
        "content": "import json\n\n\ndef load_palettes(json_file):\n    \"\"\" \n    Load dictionary with palettes from json file \n    \"\"\"\n    with open (json_file,\"r\") as docs:\n        list_of_colours = json.load(docs)\n    return list_of_colours",
        "sha1": "9cb9e2310ae95a70d9148998c42b559b43c9e16d",
        "id": 409055
    },
    {
        "content": "import csv\nfrom pathlib import Path\n\n\ndef csv_to_kvstore(fname):\n    \"\"\" Parse CSV file into key-value store where keys are \n    always filepaths, and values are always floats \"\"\"\n    with open(fname, mode=\"r\") as f:\n        reader = csv.reader(f)\n        next(reader, None)  # Skip one row of headers\n        return {Path(row[0]): float(row[1]) for row in reader}",
        "sha1": "b60f63f5dc1ce9472122f235e4a0f49937239b93",
        "id": 217778
    },
    {
        "content": "def machine_lookup(session, hostname, public_ip = True):\n    \"\"\"Lookup the IP addresses for a given AWS instance name.\n\n        Note: If not address could be located an error message is printed\n\n    If there are multiple machines with the same hostname, to select a specific\n    one, prepend the hostname with \"#.\" where '#' is the zero based index.\n        Example: 0.auth.integration.boss\n\n    Retrieved instances are sorted by InstanceId.\n\n    Args:\n        session (Session) : Active Boto3 session\n        hostname (string) : Hostname of the EC2 instance\n        public_ip (bool) : Whether or not to return the public IP or private IP\n\n    Returns:\n        (string|None) : IP address or None if one could not be located.\n    \"\"\"\n\n    try:\n        idx, target = hostname.split('.', 1)\n        idx = int(idx) # if it is not a valid number, then it is a hostname\n        hostname = target\n    except:\n        idx = 0\n\n    client = session.client('ec2')\n    response = client.describe_instances(Filters=[{\"Name\":\"tag:Name\", \"Values\":[hostname]},\n                                                  {\"Name\":\"instance-state-name\", \"Values\":[\"running\"]}])\n\n    item = response['Reservations']\n    if len(item) == 0:\n        print(\"Could not find IP address for '{}'\".format(hostname))\n        return None\n    else:\n        item.sort(key = lambda i: i['Instances'][0][\"InstanceId\"])\n\n        if len(item) <= idx:\n            print(\"Could not find IP address for '{}' index '{}'\".format(hostname, idx))\n            return None\n        else:\n            item = item[idx]['Instances'][0]\n            if 'PublicIpAddress' in item and public_ip:\n                return item['PublicIpAddress']\n            elif 'PrivateIpAddress' in item and not public_ip:\n                return item['PrivateIpAddress']\n            else:\n                print(\"Could not find IP address for '{}'\".format(hostname))\n                return None",
        "sha1": "51f272809e53c490b5e4a888b79dc10f9d52cbd7",
        "id": 315026
    },
    {
        "content": "def replace_placeholders(str, placeHolder, replacement):\n    \"\"\"\n    Replace all the occurences of a substring in a string.\n\n    Parameters\n    ----------\n        str (str): The string to be modified.\n        placeHolder (str): The substring to replace.\n        replacement (str): The replacement for the substring.\n\n    Returns\n    -------\n        str (str): The new string with the substrings replaced.\n    \"\"\"\n    return str.replace(placeHolder, replacement)",
        "sha1": "2a1e2903ca925c1e695b3103272aee2b1ded824c",
        "id": 311266
    },
    {
        "content": "def chunk_file(fname, *, limit=5000, start_num=0):\n    \"\"\"\n    This function is inexorably tied to extras.fetch_eddb.jq_post_process.\n    Importantly jq doesn't wrap in list brackets or end in ','\n\n    Take any file with fname. Chunk that file out with limit lines per file.\n    Each chunk will be written to a file with name of form fname_001, fname_002, ...\n\n    Args:\n        fname: The filename to open and chunk based on limit.\n\n    Kwargs:\n        limit: This many lines of input file will be written to each file.\n        start_num: This is the starting number for chunks, appended to end of fname.\n\n    Returns: The last number used to generate chunk.\n    \"\"\"\n    lines = ['[\\n']\n    with open(fname, 'r', encoding='utf8') as fin:\n        for ind, line in enumerate(fin):\n            if ind and (ind % limit) == 0:\n                with open(f'{fname}_{start_num:03}', 'w', encoding='utf8') as fout:\n                    last_line = lines[-1][:-2] + '\\n'\n                    lines = lines[:-1] + [last_line, ']']\n                    fout.writelines(lines)\n                    lines = ['[\\n']\n                    start_num += 1\n\n            lines += [line.rstrip() + ',\\n']\n\n    if lines:\n        with open(f'{fname}_{start_num:03}', 'w') as fout:\n            last_line = lines[-1][:-2] + '\\n'\n            lines = lines[:-1] + [last_line, ']']\n            fout.writelines(lines)\n            lines.clear()\n\n    return start_num",
        "sha1": "dd39a73660c3eadcb7d928bfd675e22def0ada5c",
        "id": 627719
    },
    {
        "content": "def union_overlapping(intervals):\n  \"\"\"Union any overlapping intervals in the given set.\"\"\"\n  disjoint_intervals = []\n\n  for interval in intervals:\n    if disjoint_intervals and disjoint_intervals[-1].overlaps(interval):\n      disjoint_intervals[-1] = disjoint_intervals[-1].union(interval)\n    else:\n      disjoint_intervals.append(interval)\n\n  return disjoint_intervals",
        "sha1": "946e095d219282c4091cf037a78a7d238cc566a8",
        "id": 65077
    },
    {
        "content": "def dedupe_with_order(dupes):\n    \"\"\"Given a list, return it without duplicates and order preserved.\"\"\"\n\n    seen = set()\n    deduped = []\n    for c in dupes:\n        if c not in seen:\n            seen.add(c) \n            deduped.append(c)\n    return deduped",
        "sha1": "93326dddba608a9ae645064f246d0557047bb1be",
        "id": 56544
    },
    {
        "content": "from bs4 import BeautifulSoup\n\n\ndef lyrics_from_letrasmus(html):\n    \"\"\"Extracts lyrics from the html of a webpage of 'letras.mus.br'.\"\"\"\n\n    lyrics = ''\n    soup = BeautifulSoup(html, 'html.parser')\n    verses = soup.select('.cnt-letra article p')\n\n    for para in verses:\n        lyrics += para.get_text('\\n') + '\\n\\n'\n\n    return lyrics",
        "sha1": "24c074bb0e66e6e20c6177c70a037e67a0088736",
        "id": 644795
    },
    {
        "content": "def is_str(val):\n    \"\"\"Check if value is string\"\"\"\n    return isinstance(val, str)",
        "sha1": "c41d57c785dfebc3adc8645915b26e35c0a4897a",
        "id": 548027
    },
    {
        "content": "def extract_patches(features, size, stride):\n    \"\"\"\n    Arguments:\n        features: a float tensor with shape [c, h, w].\n        size: an integer, size of the patch.\n        stride: an integer.\n    Returns:\n        a float tensor with shape [N, c * size * size],\n        where N = n * m, n = 1 + floor((h - size)/stride),\n        and m = 1 + floor((w - size)/stride).\n    \"\"\"\n    c, h, w = features.size()\n    patches = features.unfold(1, size, stride).unfold(2, size, stride)\n    # it has shape [c, n, m, size, size]\n\n    # get the number of patches\n    n, m = patches.size()[1:3]\n    N = n * m\n\n    patches = patches.permute(1, 2, 0, 3, 4).contiguous()\n    patches = patches.view(N, c * size * size)\n    return patches",
        "sha1": "903a7c550d1e044b087e81b2024786f416f4527d",
        "id": 498545
    },
    {
        "content": "def postprocess_int(field, value, **options):\n    \"\"\"Convert a string to an integer.\"\"\"\n    try:\n        return int(value)\n    except (ValueError, TypeError):\n        return value",
        "sha1": "4a6776b714c2599719475f86638529c8daf0fa43",
        "id": 288428
    },
    {
        "content": "def norm(x, ord=None):\n    \"\"\"\n    Returns the vector norm of *x*, if that is defined.\n\n    The norm returned depends on the *ord* parameter, as in SciPy.\n\n    :param: ord : {1, 2, inf, 'fro', None}\n        Order of the norm.\n        *inf* means NumPy's :class:`inf` object.\n        *'fro'* means the string 'fro', and denotes the Frobenius norm.\n        If *None* and self is a Matrix instance, then assumes 'fro'.\n    \"\"\"\n    return x.norm(ord)",
        "sha1": "631ce244870a9acfb8cd25e7f9f0bfdffe1e2765",
        "id": 497203
    },
    {
        "content": "def get_index(leaves, indxes):\n  \"\"\"\n  Produces names of branches with indexes in them.\n  \"\"\"\n  return [ leaf + \"[%d]\" % index for leaf in leaves for index in indxes ]",
        "sha1": "40a88bf4ef3d4f0e978734831ad1058fd7fca79e",
        "id": 400811
    },
    {
        "content": "def for_experiment(experiment):\n    \"\"\"Return the Recruiter instance for the specified Experiment.\n\n    This provides a seam for testing.\n    \"\"\"\n    return experiment.recruiter",
        "sha1": "3f04d06ee11c5f63e1c29c4cb847920a47320349",
        "id": 364405
    },
    {
        "content": "def strip_comments(s):\n    \"\"\"Strips the comments from a multi-line string.\n    \n    >>> strip_comments('hello ;comment\\\\nworld')\n    'hello \\\\nworld'\n    \"\"\"\n    COMMENT_CHAR = ';'\n    lines = []\n    for line in s.split('\\n'):\n        if COMMENT_CHAR in line:\n            lines.append(line[:line.index(COMMENT_CHAR)])\n        else:\n            lines.append(line)\n    return '\\n'.join(lines)",
        "sha1": "06674a5e84c9b8e397009d4fac93c76a1b4e07d7",
        "id": 673337
    },
    {
        "content": "def create_price_sqm(data):\n    \"\"\"Create price per square meter feature.\"\"\"\n    data['Prezzo_per_m2'] = data['Prezzo'] / data['Superficie']\n    return data",
        "sha1": "6a50084f69f233374ffa4512de2653f37f749467",
        "id": 38293
    },
    {
        "content": "def pad_width_for_tiling(image_shape, tile_shape):\n    \"\"\"\n    >>> pad_width_for_tiling((1, 3, 3, 4), (1, 3, 2, 2))\n    ((0, 0), (0, 0), (1, 1), (1, 1))\n    >>> pad_width_for_tiling((1, 3, 3, 4), (1, 3, 4, 4))\n    ((0, 0), (0, 0), (2, 3), (2, 2))\n    >>> pad_width_for_tiling((1, 3, 3, 5), (1, 3, 4, 4))\n    ((0, 0), (0, 0), (2, 3), (2, 3))\n    >>> pad_width_for_tiling((1, 3, 5, 5), (1, 3, 4, 4))\n    ((0, 0), (0, 0), (2, 3), (2, 3))\n    \"\"\"\n    # if image_shape[2] <= tile_shape[2]:\n    #     rows_pad = (0, (tile_shape[2] - image_shape[2]))\n    # else:\n    #     rows_pad = (tile_shape[2] // 2, tile_shape[2] // 2 + (tile_shape[2] // 2 - image_shape[2]) % (tile_shape[2] // 2))\n    #\n    # if image_shape[3] <= tile_shape[3]:\n    #     cols_pad = (0, (tile_shape[3] - image_shape[3]))\n    # else:\n    #     cols_pad = (tile_shape[3] // 2, tile_shape[3] // 2 + (tile_shape[3] // 2 - image_shape[3]) % (tile_shape[3] // 2))\n    rows_pad = (tile_shape[2] // 2, tile_shape[2] // 2 + (tile_shape[2] // 2 - image_shape[2]) % (tile_shape[2] // 2))\n    cols_pad = (tile_shape[3] // 2, tile_shape[3] // 2 + (tile_shape[3] // 2 - image_shape[3]) % (tile_shape[3] // 2))\n\n    return (\n        (0, 0),\n        (0, 0),\n        rows_pad,\n        cols_pad\n    )",
        "sha1": "b573e657c354025abf3b855bf46eef947d7f168d",
        "id": 217527
    },
    {
        "content": "import re\n\n\ndef camel_case(string, upper_first=False):\n    \"\"\"\n    Transforms a string from snake_case to camelCase.\n\n    Example:\n        >>> camel_case('my_http_method')\n        'myHttpMethod'\n\n    Pass upper_first as True to also uppercase the first letter of the\n    given string.\n\n    Example:\n        >>> camel_case('my_http_method', upper_first=True)\n        'MyHttpMethod'\n    \"\"\"\n    if upper_first:\n        regex = r'(^[a-z])|_([0-9a-z])'\n        group = lambda m: m.group(2) if m.group(2) else m.group(1)\n\n    else:\n        regex = r'(?!^)_([0-9a-z])'\n        group = lambda m: m.group(1)\n\n    return re.sub(regex, lambda match: group(match).upper(), string)",
        "sha1": "6bd671d88538f4f02215274a730f356d234b8dd9",
        "id": 196170
    },
    {
        "content": "from typing import List\nfrom typing import Dict\n\n\ndef dictify(data, keys: List, val: Dict) -> Dict:\n    \"\"\"Turns a flat :class:`NodeTree` dictionary into a nested dictionary.\n\n    Helper function to generate nested dictionary from list of keys and value. \n    Calls itself recursively.\n    \n    Arguments:\n        data (dict): A dictionary to add value to with keys.\n        keys (list): A list of keys to traverse along tree and place value.\n        val (dict): A value for innermost layer of nested dict.\n\n    \"\"\"\n    key = keys[0]\n    key = int(key) if key.isdecimal() else key.lower()\n    if len(keys) == 1:\n        data[key] = val\n    else:\n        if key in data.keys():\n            data[key] = dictify(data[key], keys[1:], val)\n        else:\n            data[key] = dictify({}, keys[1:], val)\n    return data",
        "sha1": "e7c0111f67b7755a6e28d6264d8f7b300e94273c",
        "id": 31690
    },
    {
        "content": "def read_vcf(vcf_file):\n    \"\"\"\n    Read a vcf file to a dict of lists.\n\n    :param str vcf_file: Path to a vcf file.\n    :return: dict of lists of vcf records\n    :rtype: dict\n    \"\"\"\n    vcf_dict = []\n    with open(vcf_file, 'r') as invcf:\n        for line in invcf:\n            if line.startswith('#'):\n                continue\n            line = line.strip().split()\n            vcf_dict.append((line[0], line[1], line[3], line[4]))\n    return vcf_dict",
        "sha1": "cbb9b976204588aefc83302d95a8b8302aeb17e8",
        "id": 426667
    },
    {
        "content": "def _get_numeric_names(n_electrodes):\n    \"\"\"Create numeric electrode names: 1-n\"\"\"\n    return [str(i) for i in range(1, n_electrodes + 1)]",
        "sha1": "73c88c28cf7790c8a0b391d60c39ebf52e6243fa",
        "id": 565042
    },
    {
        "content": "def as_si(x, ndp):\n    \"\"\" Convert humber to latex-style x10 scientific notation string\"\"\"\n    s = '{x:0.{ndp:d}e}'.format(x=x, ndp=ndp)\n    m, e = s.split('e')\n    return r'{m:s}\\times 10^{{{e:d}}}'.format(m=m, e=int(e))",
        "sha1": "9caabb7102b953655f74771b0ea6f1f5d8eaac43",
        "id": 586583
    },
    {
        "content": "def convert_pyr_coeffs_to_pyr(pyr_coeffs):\n    \"\"\"this function takes a 'new pyramid' and returns the coefficients as a list\n\n    this is to enable backwards compatibility\n\n    Parameters\n    ----------\n    pyr_coeffs : `dict`\n        The `pyr_coeffs` attribute of a `pyramid`.\n\n    Returns\n    -------\n    coeffs : `list`\n        list of `np.array`, which contains the pyramid coefficients in each band, in order from\n        bottom of the pyramid to top (going through the orientations in order)\n    highpass : `np.array` or None\n        either the residual highpass from the pyramid or, if that doesn't exist, None\n    lowpass : `np.array` or None\n        either the residual lowpass from the pyramid or, if that doesn't exist, None\n\n    \"\"\"\n    highpass = pyr_coeffs.pop('residual_highpass', None)\n    lowpass = pyr_coeffs.pop('residual_lowpass', None)\n    coeffs = [i[1] for i in sorted(pyr_coeffs.items(), key=lambda x: x[0])]\n    return coeffs, highpass, lowpass",
        "sha1": "6b7ab3c4a6a85d05b7628cd7d31c46a62e549afc",
        "id": 46135
    },
    {
        "content": "def symmetrize(edges):\n  \"\"\"Symmetrizes the adjacency.\"\"\"\n  inv_edges = {(d, s) for s, d in edges}\n  return edges.union(inv_edges)",
        "sha1": "d22652dcd29b562f916f155f5d8709fa27e09a7b",
        "id": 503524
    },
    {
        "content": "def skip_comments(lines):\n    \"\"\"Filter out lines starting with a #.\"\"\"\n    return (line for line in lines if not line.startswith('#'))",
        "sha1": "c91426a14adcd8dcc8aa8564523d3c9650fd77d4",
        "id": 666943
    },
    {
        "content": "def decay(rho, beta, progress):\n    \"\"\"\n    Decay function from Rosokha & Younge (2019).\n    :param rho: trajectory shape, in [0,1]\n    :param beta: value to decay\n    :param progress: relative progress in interval [0,T]\n    :return: decayed value\n    \"\"\"\n\n    assert rho >= 0 and rho <= 1, 'Rho must be in [0,1]'\n    assert progress >= 0 and progress <= 1, 't/T must be in [0,1]'\n\n    # No decay\n    if rho == 0:\n        return beta\n\n    # Slow onset decay\n    elif rho > 0 and rho < 0.5:\n        return beta * (1 - (progress) ** (1 / (2 * rho)))\n\n    # Linear decay\n    elif rho == 0.5:\n        return beta * (1 - progress)\n\n    # Fast onset decay\n    elif rho > 0.5 and rho < 1:\n        return beta * (1 - progress) ** (1 / (2 * (1 - rho)))\n\n    # Immediate decay\n    elif rho == 1:\n        return 0",
        "sha1": "46cfe44f8d20238d6131e05b74a241bd413b2f51",
        "id": 392486
    },
    {
        "content": "from datetime import datetime\n\n\ndef format_unix(unix_ts, date_format):\n    \"\"\"Format unix timestamp as a string timestamp in date_format.\n\n    Args:\n        unix_ts: unix timestamp\n        date_format: datetime time stamp format\n\n    Returns:\n        string: formatted timestamp in `date_format`\n\n    \"\"\"\n    return datetime.fromtimestamp(unix_ts).strftime(date_format)",
        "sha1": "67ce12819803e63f0585937e34efd5ee508b5baa",
        "id": 612497
    },
    {
        "content": "def _determine_educ_type(model_name):\n    \"\"\"Determine whether an education model is school, preschool or nursery.\n\n    Args:\n        model_name (str): name of the education model, e.g. educ_school_0.\n\n    \"\"\"\n    name_parts = model_name.split(\"_\")\n    msg = f\"The name of your education model {model_name} does not \"\n    assert len(name_parts) == 3, msg + \"consist of three parts.\"\n    assert name_parts[0] == \"educ\", (\n        msg + f\"have educ as first part but {name_parts[0]}.\"\n    )\n    assert name_parts[1] in [\"nursery\", \"preschool\", \"school\"], (\n        msg + \"belong to ['nursery', 'preschool', 'school'].\"\n    )\n    assert name_parts[2].isdigit(), (\n        msg + f\"have a number as last part but {name_parts[2]}\"\n    )\n    return name_parts[1]",
        "sha1": "63a908edd3cf452d1d937c9f33768ca163608536",
        "id": 297689
    },
    {
        "content": "def _labels_to_state(scan_label, compscan_label):\n    \"\"\"Use scan and compscan labels to derive basic state of antenna.\"\"\"\n    if not scan_label or scan_label == 'slew':\n        return 'slew'\n    if scan_label == 'cal':\n        return 'track'\n    return 'track' if compscan_label == 'track' else 'scan'",
        "sha1": "a5a6429fa7c7d108fff469b0efe0848e4eb02fcb",
        "id": 693902
    },
    {
        "content": "def getfrom(v):\n    \"\"\"\n    pass through function for using the\n    filter_for decorator directly\n    \"\"\"\n    return v",
        "sha1": "7a01fecbac63bca67fef10bfb39f8641e0cacda7",
        "id": 7131
    },
    {
        "content": "def rename_level(level):\n    \"\"\"\n    Rename the level of theory so it can be used for folder names.\n\n    Args:\n        level (str): The level of theory to be renamed.\n\n    Returns:\n        str: The renamed level of theory\n    \"\"\"\n    level = level.replace('/', '_')\n    level = level.replace('*', 's')\n    level = level.replace('+', 'p')\n    level = level.replace('(', 'b')\n    level = level.replace(')', 'b')\n    level = level.replace(')', 'b')\n    level = level.replace(', ', 'c')\n    return level",
        "sha1": "bd45c15754b48227981a85a9adebe3e3ef2d9fd3",
        "id": 440718
    },
    {
        "content": "def generate_csd_link(refcode: str) -> str:\n    \"\"\"Take a refocde string and make a link to WebCSD\"\"\"\n    return '<a href=\"https://www.ccdc.cam.ac.uk/structures/Search?Ccdcid={}&DatabaseToSearch=Published\">{}</a>'.format(\n        refcode, refcode\n    )",
        "sha1": "18d36329efd3ba799d4220708450323b3762739e",
        "id": 57537
    },
    {
        "content": "def BAI(b3, b4, nodata=-9999):\n    \"\"\"Computes the burned area index.\n    \"\"\"\n    x = 1 / ( pow(b4-0.06,2) + pow(b3-0.1,2) )\n    x[(b4==nodata) | (b3==nodata)] = nodata\n    return(x)",
        "sha1": "d30b81608bf49c4513201c43c68bb9044ee53f7d",
        "id": 223742
    },
    {
        "content": "def top_of_stack(l):\n    \"\"\"Returns the element on top of the stack.\"\"\"\n    if not l:\n        return None\n    return l[-1]",
        "sha1": "0b070d7703a2077005d646b92d132d96afaecdf8",
        "id": 582635
    },
    {
        "content": "from typing import Counter\n\n\ndef create_vocabulary(text):\n    \"\"\"Create vocabulary dictionaries.\n\n    Args:\n        text(str): raw text.\n\n    Returns:\n        char2id(dict): character to id mapping.\n        id2char(dict): id to character mapping.\n    \"\"\"\n    char2id = {'<PAD>': 0}\n    id2char = {0: '<PAD>'}\n    freq = Counter(text)\n    for char, _ in freq.most_common():\n        id = len(char2id)\n        char2id[char] = id\n        id2char[id] = char\n\n    return char2id, id2char",
        "sha1": "8f1755750bf4440a7e0048881915652188ba9c6b",
        "id": 328689
    },
    {
        "content": "def extract_parameters_data(params_path):\n    \"\"\"\n    path: <string> path to read the file\n    returns: a list of data. Each entry corresponds to a line in the file to read\n    \"\"\"\n    l = []\n    with open(params_path, 'r') as f:\n        content = f.readlines()\n    for line in content:\n        print(line)\n        stripped = line.strip()\n        print(stripped)\n        l.append(stripped)\n    return l",
        "sha1": "90101dc331ae78323998ae54379ef21bfd7f44bc",
        "id": 425336
    },
    {
        "content": "def kalkulasi_kecepatan_akhir(\n    kecepatan_awal: float, percepatan: float, waktu: float\n) -> float:\n    \"\"\"\n    Menghitung kecepatan akhir dari suatu pergerakan\n    dengan percepatan yang berbeda\n    >>> kalkulasi_kecepatan_akhir(10, 2.4, 5)\n    22.0\n    >>> kalkulasi_kecepatan_akhir(10, 7.2, 1)\n    17.2\n    \"\"\"\n    # jika waktu 0 diisi dengan 1 detik\n    return kecepatan_awal + percepatan * waktu",
        "sha1": "1db3150cab6991de8063bbf546f475ce982db2bb",
        "id": 80329
    },
    {
        "content": "from typing import Dict\nimport pathlib\n\n\ndef get_asset_root() -> Dict[str, str]:\n    \"\"\"\n    :return: the different assets paths to get sql and csv directories\n    :rtype: dictionnary\n    \"\"\"\n    dir = pathlib.Path(__file__).parent\n    pql_root = str(pathlib.PurePath(dir, 'ext_files/pkl'))\n    csv_root = str(pathlib.PurePath(dir, 'ext_files/csv'))\n    txt_root = str(pathlib.PurePath(dir, 'ext_files/txt'))\n    dict = {}\n    dict[\"pql_root\"] = pql_root\n    dict[\"csv_root\"] = csv_root\n    dict[\"txt_root\"] = txt_root\n    return dict",
        "sha1": "628e1abeec467146623b232aefb792d7e9fc605c",
        "id": 157123
    },
    {
        "content": "def norm_float(string):\n    \"\"\"Normalize a float value.\"\"\"\n\n    if string.lower().endswith(('e-', 'e+', 'e')):\n        string += '0'\n    return float(string)",
        "sha1": "6cf02599ab9aa4c503d9463cfd11fe1a19a9428e",
        "id": 450268
    },
    {
        "content": "def _get_required_fn(fn, root_path):\n    \"\"\"\n    Definition of the MD5 file requires, that all paths will be absolute\n    for the package directory, not for the filesystem.\n\n    This function converts filesystem-absolute paths to package-absolute paths.\n\n    Args:\n        fn (str): Local/absolute path to the file.\n        root_path (str): Local/absolute path to the package directory.\n\n    Returns:\n        str: Package-absolute path to the file.\n\n    Raises:\n        ValueError: When `fn` is absolute and `root_path` relative or \\\n                    conversely.\n    \"\"\"\n    if not fn.startswith(root_path):\n        raise ValueError(\"Both paths have to be absolute or local!\")\n\n    replacer = \"/\" if root_path.endswith(\"/\") else \"\"\n\n    return fn.replace(root_path, replacer, 1)",
        "sha1": "49dfcc102e80ec9bd432252d7fbe70dbf8097da0",
        "id": 348846
    },
    {
        "content": "import re\n\n\ndef is_fraction(s):\n    \"\"\"\n    Determine if the input string appears to represent a fraction.\n    This does not include mixed numbers such as 1 1/3\n\n    :param s: A string value to check if it is formatted as a fraction.\n    \"\"\"\n    return bool(re.match(r'^-?\\d+/\\d+$', s))",
        "sha1": "10334c66120ad94204dba175b8659c2165031167",
        "id": 441671
    },
    {
        "content": "def numberdict(dct):\n    \"\"\" convert all numeric values in a dict to their appropriate type \"\"\"\n    o = {}\n    for k, v in dct.items():\n        if v.isdigit():\n            v = int(v)\n        else:\n            try:\n                v = float(v)\n            except ValueError:\n                v = v\n        o.update({k: v})\n    return o",
        "sha1": "e30352f8bda2157877c92d5cfedc5fcbb0142040",
        "id": 152389
    },
    {
        "content": "def user_to_color(user) -> int:\n    \"\"\"Gets the color for a given user.\n\n    Example:\n\n        `discord.Embed(color=user_to_color(ctx.author))`\n\n    :param user: Any user.\n    :type user: discord.User\n    :return: Any color.\n    :rtype: int\n    \"\"\"\n    return int(int(user.discriminator) / 9999 * 0xffffff)",
        "sha1": "32a2a7bc6936ecec2f9e8b7385a1726590b2a06d",
        "id": 219619
    },
    {
        "content": "def get_int_df(adata, lr, use_label, sig_interactions, title):\n    \"\"\"Retrieves the relevant interaction count matrix.\"\"\"\n    no_title = type(title) == type(None)\n    if type(lr) == type(None):  # No LR inputted, so just use all\n        int_df = (\n            adata.uns[f\"lr_cci_{use_label}\"]\n            if sig_interactions\n            else adata.uns[f\"lr_cci_raw_{use_label}\"]\n        )\n        title = \"Cell-Cell LR Interactions\" if no_title else title\n    else:\n        int_df = (\n            adata.uns[f\"per_lr_cci_{use_label}\"][lr]\n            if sig_interactions\n            else adata.uns[f\"per_lr_cci_raw_{use_label}\"][lr]\n        )\n        title = f\"Cell-Cell {lr} interactions\" if no_title else title\n\n    return int_df, title",
        "sha1": "957d6d400189d4804406dfdbf20aa7a196f04e67",
        "id": 169845
    },
    {
        "content": "import torch\n\n\ndef rotation_from_axis(axis):\n    \"\"\"\n    Expects Bx1x3 axis Tensor\n    The axis is x, y, z with the angle being norm(axis).\n    Returns a Bx3x3 rotation matix tensor\n    \"\"\"\n    device = axis.device\n\n    # Trick for batch_size broadcasting\n    theta = torch.norm(axis, p=2, dim=2).view(-1, 1, 1)\n    axis = torch.nn.functional.normalize(axis)\n\n    ctheta = torch.cos(theta)\n    stheta = torch.sin(theta)\n\n    batch_size = axis.size()[0]\n    identities = torch.eye(3).expand(batch_size, -1, -1).to(device)\n\n    axis_cross = torch.matmul(axis.transpose(1, 2), axis)\n\n    x = axis[:, :, 0]\n    y = axis[:, :, 1]\n    z = axis[:, :, 2]\n    zero = torch.zeros(axis.size()[:-1]).to(x.device)\n    axis_orth = torch.cat((zero, -z, y,\n                           z, zero, -x,\n                           -y, x, zero,), dim=1).view(-1, 3, 3)\n\n    rotations = (ctheta * identities +\n                 stheta * axis_orth +\n                 (1 - ctheta) * axis_cross)\n    return rotations",
        "sha1": "566a71ff883afb7952221429e6749d3550566898",
        "id": 641054
    },
    {
        "content": "def int_to_bits(int_str, qubit_count):\n    \"\"\"\n    Convert a number (possibly in string form) to a readable bit format.\n    For example, the result '11', which means both qubits were measured as 1, is returned by the API as \"3\".\n    This converts that output to the readable version.\n\n    Args:\n        int_str: A string or integer in base 10 that represents the measurement outcome.\n\n    Returns: A string of 0's and 1's\n    \"\"\"\n    # convert to an integer, then generate the binary string\n    # remove the \"0b\" prefix from the binary string\n    # then pad (using zfill) with 0's\n    return str(bin(int(int_str)))[2:].zfill(qubit_count)",
        "sha1": "1a2d25281c2ec9ff4e885daeba0ff76448bfbf0a",
        "id": 287691
    },
    {
        "content": "import itertools\n\n\ndef _get_common_prefix(list_of_strings):\n    \"\"\"\n    Returns the longest common prefix among the set of strings.\n    Helpful for finding a VBIS tag prefix.\n\n    Args:\n        list_of_strings (list of str): list of strings\n    Returns:\n        pfx (str): longest common prefix\n    \"\"\"\n    # https://stackoverflow.com/questions/6718196/determine-prefix-from-a-set-of-similar-strings\n    def all_same(x):\n        return all(x[0] == y for y in x)\n\n    char_tuples = zip(*list_of_strings)\n    prefix_tuples = itertools.takewhile(all_same, char_tuples)\n    return \"\".join(x[0] for x in prefix_tuples).strip(\"-\")",
        "sha1": "5269b7ef4674c9e244de7eef8364691f23aacb31",
        "id": 188805
    },
    {
        "content": "def validate_page_name(pagename):\n    \"\"\"Utility for validating wiki page name.\n\n    :param pagename: wiki page name to validate\n    \"\"\"\n    return pagename and \\\n           all(part not in ('', '.', '..') for part in pagename.split('/'))",
        "sha1": "902fe69cdaaca60e6ff3b21e312a38665137ca73",
        "id": 544131
    },
    {
        "content": "def _get_template_module(name, req, **context):\n    \"\"\"Get template module for a request email notification template.\n\n    :param name: template name\n    :param req: :class:`Request` instance\n    :param context: data passed to the template\n    \"\"\"\n    context['req'] = req\n    return req.definition.get_notification_template(name, **context)",
        "sha1": "5cfff3a9f76259b4033e1f448842f40e18291c3c",
        "id": 67215
    },
    {
        "content": "import collections\n\n\ndef _format_for_bq(rows):\n  \"\"\"Formats BigQueryRow entities and metadata for the BigQuery API.\n\n  Args:\n    rows: List[BigQueryRow], a list of rows to format for the BigQuery API.\n\n  Returns:\n    A Dictionary keyed by model type with rows for a given table.\n  \"\"\"\n  tables = collections.defaultdict(list)\n  for row in rows:\n    entity_dict = row.to_json_dict()\n    tables[row.model_type].append(\n        (entity_dict['ndb_key'], entity_dict['timestamp'],\n         entity_dict['actor'], entity_dict['method'], entity_dict['summary'],\n         entity_dict['entity']))\n  return tables",
        "sha1": "665de48eba56106ca430f27c12035dd6e23320bb",
        "id": 173021
    },
    {
        "content": "def dig2phys(signal, dmin, dmax, pmin, pmax):\n    \"\"\"\n    converts digital edf values to physical values\n\n    Parameters\n    ----------\n    signal : np.ndarray or int\n        A numpy array with int values (digital values) or an int.\n    dmin : int\n        digital minimum value of the edf file (eg -2048).\n    dmax : int\n        digital maximum value of the edf file (eg 2048).\n    pmin : float\n        physical maximum value of the edf file (eg -200.0).\n    pmax : float\n        physical maximum value of the edf file (eg 200.0).\n\n    Returns\n    -------\n    physical : np.ndarray or float\n        converted physical values\n\n    \"\"\"\n    m = (pmax-pmin) / (dmax-dmin)\n    b = pmax / m - dmax\n    physical = m * (signal + b)\n    return physical",
        "sha1": "6da0f0ba11a98abcfa1817569c6e7112351c4ff5",
        "id": 420178
    },
    {
        "content": "import re\n\n\ndef GuessDataType(value, column_id=None):\n  \"\"\"Guess the DSPL data type of a string value.\n\n  Defaults to 'string' if value is not an obvious integer, float, or date.\n  Also, does not try to detect boolean values, which are unlikely to be used\n  in DSPL tables.\n\n  Args:\n    value: A string to be evaluated.\n    column_id: (Optional) Name of the column containing data; used to resolve\n               ambiguities, e.g. between years and integers.\n\n  Returns:\n    One of {'date', 'integer', 'float', 'string'}\n  \"\"\"\n  stripped_value = value.strip().replace('\"', '')\n\n  if re.search('^-?[0-9]+$', stripped_value):\n    if column_id == 'year':\n      return 'date'\n    else:\n      return 'integer'\n  elif re.search('^-?[0-9]+\\.[0-9]+$', stripped_value):\n    return 'float'\n  elif re.search('^[0-9]+(/|-)[0-9]+((/|-)[0-9]+){0,1}$', stripped_value):\n    return 'date'\n  else:\n    return 'string'",
        "sha1": "cbfc17280664f9b6499b4d883a3ed084b4f934f5",
        "id": 60837
    },
    {
        "content": "def coerceValuesForColumn(column, values):\n  \"\"\"\n  Coerces the values to the type appropriate for the column\n  :param Column column:\n  :return: type appropriate for column\n  :raises: ValueError\n  \"\"\"\n  data_class = column.getDataClass()\n  values = data_class.cons(values)\n  values.name = column.getName(is_global_name=False)\n  return values",
        "sha1": "18559208a13fd77383798a38aac5fbbd2dd9d51c",
        "id": 587077
    },
    {
        "content": "def find_irreducible_prefix(brackets):\n  \"\"\"Find minimal prefix of string which is balanced (equal Ls and Rs).\n\n  Args:\n    brackets: A string containing an equal number of (and only) 'L's and 'R's.\n\n  Returns:\n    A two-element tuple.\n\n    The first element is the minimal \"irreducible prefix\" of\n    `brackets`, which is the shortest non-empty prefix of `brackets` which\n    contains an equal number of 'L's and 'R's.\n\n    The second element is the rest of the string.\n\n  Raises:\n    ValueError: No irreducible prefix could be found, or `brackets` was empty.\n  \"\"\"\n\n  depth = 0\n  for i, bracket in enumerate(brackets):\n    depth += 1 if bracket == 'L' else -1\n    if i > 0 and depth == 0:\n      return brackets[:i + 1], brackets[i + 1:]\n\n  raise ValueError('unbalanced or empty: %s' % ''.join(brackets))",
        "sha1": "fa8bd7998550de63f9fea9523424b198bfc96a45",
        "id": 652575
    },
    {
        "content": "def get_activating_points(trace, rcon_alpha):\n    \"\"\"\n    Returns a list of time instances where the trace is activated by RCon \u237a\n    :param trace: One trace (a list of states in time)\n    :param rcon_alpha: A proposition indicating the \u237a of the RCon\n    :return: a list of time instances where the trace is activated\n    \"\"\"\n\n    activation_times = []\n\n    if rcon_alpha == \"t_start\":\n        activation_times = [float(\"-inf\")]      # -\u221e is used as a special symbol for t_start\n    elif rcon_alpha == \"t_end\":\n        activation_times = [float(\"inf\")]       # +\u221e is used as a special symbol for t_end\n    else:\n        activation_times = [i for i in range(len(trace)) if rcon_alpha in trace[i]]\n\n    return activation_times",
        "sha1": "ab6136e4cb1223f94c233d87f46fa3f67e94bdc8",
        "id": 615569
    },
    {
        "content": "def mb_bl_ind(tr1, tr2):\n    \"\"\"Returns the baseline index for given track indices.\n\n    By convention, tr1 < tr2. Otherwise, a warning is printed,\n    and same baseline returned.\n    \"\"\"\n    if tr1 == tr2:\n        print(\"ERROR: no baseline between same tracks\")\n        return None\n    if tr1 > tr2:\n        print(\"WARNING: tr1 exepcted < than tr2\")\n    mx = max(tr1, tr2)\n    bl = mx*(mx-1)/2 + min(tr1, tr2)\n    return bl.astype(int)",
        "sha1": "7d1bc958ca9928f54e51935510d62c45f7fc927f",
        "id": 14037
    },
    {
        "content": "def work_grid(grid, fig):\n    \"\"\"Take a two dimensional grid, add subplots to a figure for each cell and do layer work.\n\n    Parameters:\n    -----------\n    grid: a two dimensional grid of layers\n    fig: matplotlib figure to draw on\n\n    Returns:\n    --------\n    axes: a two dimensional list of matplotlib axes\n    \"\"\"\n    nrows = len(grid)\n    ncols = len(grid[0])\n    axes = [[None for _ in range(ncols)] for _ in range(nrows)]\n    for row in range(nrows):\n        for col in range(ncols):\n            axes[row][col] = fig.add_subplot(nrows, ncols, ncols * row + col + 1)\n            grid[row][col].work(ax=axes[row][col])\n    return axes",
        "sha1": "f1cf6dc59e75f7d50c9aff8651c12ba6e9f5bee5",
        "id": 671842
    },
    {
        "content": "def mod_to_dict(o):\n    \"\"\"Introspects a model to convert it to a regular Python dict.\"\"\"\n    keys = o.__table__.columns.keys()\n    return {k: getattr(o, k) for k in keys}",
        "sha1": "d45da3a0ecc2020196a73d59d9c346a2084063e2",
        "id": 351790
    },
    {
        "content": "import ast\n\n\ndef normalize_compare(node):\n    \"\"\"Rewrites a compare expression to a `and` expression\n    1 < 2 < 3 > 0\n    1 < 2 and 2 < 3 and 3 > 0\"\"\"\n    and_values = []\n    left = node.left\n    for (op, val) in zip(node.ops, node.comparators):\n        comp = ast.Compare(ops=[op],\n                           left=left,\n                           comparators=[val],\n                           lineno=node.lineno,\n                           col_offset=node.col_offset)\n        and_values.append(comp)\n        left = val\n    return ast.BoolOp(op=ast.And(),\n                      values=and_values,\n                      lineno=node.lineno,\n                      col_offset=node.col_offset)",
        "sha1": "2cc41e9f85012b8fcaf3365ea003c23136c83ebc",
        "id": 250009
    },
    {
        "content": "def tokenize_scifact_claims(tokenizer, claims, max_seq_length):\n    \"\"\"\n    Parameters\n    ----------\n    tokenizer: transformers.PreTrainedTokenizerFast\n        The HuggingFace tokenizer corresponding to the desired model.\n\n    claims: list[dict]\n        The SciFact 2dataset split extracted as a list of dictionaries.\n\n    max_seq_length: int\n        The maximum sequence length to pad/truncate encodings to.\n\n    Returns\n    -------\n    encodings: transformers.tokenization_utils_base.BatchEncoding\n        The tokenized inputs in the HuggingFace tokenizer output format.\n    \"\"\"\n    texts = [c[\"claim\"] for c in claims]\n    encodings = tokenizer(\n        texts,\n        is_split_into_words=False,\n        padding=\"max_length\",\n        truncation=True,\n        max_length=max_seq_length,\n    )\n    return encodings",
        "sha1": "e57e7b1a254aed532b1ed2af91b6c60923337da0",
        "id": 333920
    },
    {
        "content": "import textwrap\nimport pprint\n\n\ndef pprint_msg(dic, prefix='  '):\n    \"\"\"\n    Give logger.info a string for neatly printing a dictionary.\n\n    Usage:\n    logger.info(pprint_msg(arbitrary_object))\n    \"\"\"\n    return \"\\n\" + textwrap.indent(pprint.pformat(dic), prefix=prefix)",
        "sha1": "2896a7389c27dc3a6dcae0bcd56bbaf74493b51f",
        "id": 108548
    },
    {
        "content": "from typing import Optional\nimport re\n\n\ndef parse_banner_timestamp(s: str) -> Optional[float]:\n    \"\"\"Get timestamp of rotation ad banner from an embedded script.\n    The timestamp is rounded to seconds.\n    \"\"\"\n    found = re.search(r\"&times=(\\d+)\", s)\n    if not found:\n        return None\n    timestamp_ns = float(found.group(1))\n    return round(timestamp_ns / 1000, 0)",
        "sha1": "a59cd8c50e9ee2bf02b77f89043750009ef8a1b3",
        "id": 393645
    },
    {
        "content": "def convert_to_time(time):\n    \"\"\"Print the training time in days: hours: minutes: seconds format.\"\"\"\n    days = time // (24 * 60 * 60)\n    time %= (24 * 60 * 60)\n    hrs = time // (60 * 60)\n    time %= (60 * 60)\n    mins = time // 60\n    time %= 60\n    secs = time\n    msg = f\"Training completed in {days:.0f} days: {hrs:.0f} hours: {mins:.0f} minutes: {secs:.0f} seconds\"\n    print(msg)\n    return msg",
        "sha1": "14d30bdd11b166fcf3f179ad00fd9aef2e250896",
        "id": 322942
    },
    {
        "content": "def ShouldUseRuntimeBuilders(service, strategy, needs_dockerfile):\n  \"\"\"Returns whether we whould use runtime builders for this application build.\n\n  If there is no image that needs to be built (service.RequiresImage() ==\n  False), runtime builders are irrelevant, so they do not need to be built.\n\n  If there is an image that needs to be built, whether to use runtime builders\n  is determined by the RuntimeBuilderStrategy, based on the service runtime and\n  whether the service being deployed has a Dockerfile already made, or whether\n  it needs one built.\n\n  Args:\n    service: ServiceYamlInfo, The parsed service config.\n    strategy: runtime_builders.RuntimeBuilderStrategy, the strategy for\n      determining whether a runtime should use runtime builders.\n    needs_dockerfile: bool, whether the Dockerfile in the source directory is\n      absent.\n\n  Returns:\n    bool, whether to use the runtime builders.\n\n  Raises:\n    ValueError: if an unrecognized runtime_builder_strategy is given\n  \"\"\"\n  return (service.RequiresImage() and\n          strategy.ShouldUseRuntimeBuilders(service.runtime, needs_dockerfile))",
        "sha1": "38f28d493e4d041973456bf261230eba0c1c58cb",
        "id": 506202
    },
    {
        "content": "def yes_no(prompt=''):\n    \"\"\"Prompts user with yes/no question.\n    Args:\n        prompt (str) opt: Question text. Defaults to ''.\n    Returns:\n        bool: Yes=True, No=False\n    \"\"\"\n    while True:\n        if prompt:\n            print(prompt)\n        answer = input('>>> ').lower()\n        if answer in ['y', 'yes', 'ok', '']:\n            return True\n        if answer in ['n', 'no']:\n            return False\n        print('Please, answer \\'yes\\' or \\'no\\'.')",
        "sha1": "6431cae54a878e9cf151c6378501918c2708be09",
        "id": 410722
    },
    {
        "content": "from typing import List\nfrom typing import Union\nfrom typing import Tuple\n\n\ndef find_empty(b : List[List[int]]) -> Union[Tuple[int, int], None]:\n    \"\"\"Finds the empty cell in sudoku\n\n    Parameters:\n        b(List[List[int]]) :  List of List that represents a sudoku board\n\n    Returns:\n        (i, j)(Tuple(i, j)) : index of empty cell\n\n    \"\"\"\n    for i in range(len(b)):\n        for j in range(len(b[0])):\n            if b[i][j] == 0:\n                return (i, j)  # row, col\n\n    return None",
        "sha1": "3ff4b86d1f3a1d754381318008e0a777a355fee1",
        "id": 288163
    },
    {
        "content": "def hex_to_bgr(hex_digits):\n    \"\"\"\n    Convert a hexadecimal color value to a 3-tuple of integers\n    \"\"\"\n    return tuple(int(s, 16) for s in (\n        hex_digits[:2], hex_digits[2:4], hex_digits[4:]))",
        "sha1": "15014092ac54e5c0bb9377b9b86e5f9ced026908",
        "id": 359720
    },
    {
        "content": "def to_dependencies_of(g):\n    \"\"\"\n    Compute the dependencies of each path var.\n\n    :param d: an adjacency list of dependency => [depends on, ...]\n    :return: an adjacency list of the given data structure such that\n        the k => [depends on, ...]. The vertices in the values are\n        presorted to ensure reproducible results\n    \"\"\"\n    deps = {}\n\n    for k, vertices in g.items():\n        for v in vertices:\n            if v not in deps:\n                deps[v] = set()\n            deps[v].add(k)\n\n    # I do this for deterministic ordering.\n    return {k: sorted(v) for k, v in deps.items()}",
        "sha1": "d4b5243acd7cd93f0753eccbfc9b0d5ee444328a",
        "id": 408177
    },
    {
        "content": "def center_crop(data, shape):\n    \"\"\"\n    Apply a center crop to the input real image or batch of real images.\n\n    Args:\n        data (torch.Tensor): The input tensor to be center cropped. It should\n            have at least 2 dimensions and the cropping is applied along the\n            last two dimensions.\n        shape (int, int): The output shape. The shape should be smaller than\n            the corresponding dimensions of data.\n\n    Returns:\n        torch.Tensor: The center cropped image.\n    \"\"\"\n    assert 0 < shape[0] <= data.shape[-2]\n    assert 0 < shape[1] <= data.shape[-1]\n\n    w_from = (data.shape[-2] - shape[0]) // 2\n    h_from = (data.shape[-1] - shape[1]) // 2\n    w_to = w_from + shape[0]\n    h_to = h_from + shape[1]\n\n    return data[..., w_from:w_to, h_from:h_to]",
        "sha1": "c1f220e555333d38a83271d5e9254c4b1178d3c1",
        "id": 672205
    },
    {
        "content": "def generate400response(error: str) -> dict:\n    \"\"\" A function that generates a '400-Bad Request' message and returns it as a dict \"\"\"\n    return {\n        \"status\": 400,\n        \"message\": \"Bad Request\",\n        \"error\": error\n    }",
        "sha1": "96915fc8a6ef5a092102f875560b230688c053c1",
        "id": 588932
    },
    {
        "content": "def workflow_marker(marker_name: str):\n    \"\"\"Create a decorator to mark a function for use within a workflow.\"\"\"\n\n    def _marker(func):\n        func.__workflow_marker__ = marker_name\n        return func\n\n    return _marker",
        "sha1": "30757f12718af8c99633a585e861f7cf8c23fb51",
        "id": 123016
    },
    {
        "content": "def _status_is_error(status_code: int) -> bool:\n    \"\"\"Determine if the given status code is an error.\"\"\"\n    return status_code >= 400",
        "sha1": "bb215d465512f40321637109b1f38a1d3188899c",
        "id": 378162
    },
    {
        "content": "def getTfidfFeat(words, dictionary, tfidf_model):\n    \"\"\" Generate tf-idf feature in VW format.\"\"\"\n    # bow feats\n    bow = dictionary.doc2bow(words)\n    # tf-idf feats\n    tfidf = tfidf_model[bow]\n    feat = \"\"\n    if len(tfidf) > 0:\n        for f in tfidf:\n            feat += \"%s:%s \" % (f[0], f[1])\n        feat = feat[:-1]\n    return feat",
        "sha1": "8fadf15e9a658eb7edfa179bbe1341a7c1879dc7",
        "id": 106147
    },
    {
        "content": "def _str_date(time):\n    \"\"\"\n    Transfer date to str.\n\n    Args:\n        time(datetime.datetime): time input.\n    \"\"\"\n    return str(time.date()) if time else None",
        "sha1": "4fc6951f0527b6f08544e06adba8ff172a73d2cf",
        "id": 250249
    },
    {
        "content": "def trim_txt(txt, limit=10000):\n    \"\"\"Trim a str if it is over n characters.\n\n    Args:\n        txt (:obj:`str`):\n            String to trim.\n        limit (:obj:`int`, optional):\n            Number of characters to trim txt to.\n\n            Defaults to: 10000.\n\n    Returns:\n        :obj:`str`\n\n    \"\"\"\n    trim_line = \"\\n... trimmed over {limit} characters\".format(limit=limit)\n    txt = txt[:limit] + trim_line if len(txt) > limit else txt\n    return txt",
        "sha1": "197dd9da07d8a5a42a33246431f50b6bae25b6bf",
        "id": 60873
    },
    {
        "content": "def connect_name(group_name):\n    \"\"\"\n    Returns string of root connect name, i.e. cms, osg, atlas, spt, etc.\n    :param group_name: unix string name of group\n    :return: string of connect name\n    \"\"\"\n    connect_name = '.'.join(group_name.split('.')[:2])\n    return connect_name",
        "sha1": "8cdf10e4996ed314ed1563acda0d09b44f2d233e",
        "id": 511145
    },
    {
        "content": "def sort_trades_by_time(trades):\n    \"\"\"\n    Sorts list of trades by time (earliest first).\n    \"\"\"\n    return sorted(trades, key=lambda trade: trade.timestamp)",
        "sha1": "5a8f8415457d803cd5c461812e8e59f1328ba326",
        "id": 111187
    },
    {
        "content": "def _verify_classifiers(classifiers, valid_classifiers):\n    \"\"\"Check classifiers against a set of known classifiers\"\"\"\n    invalid = classifiers - valid_classifiers\n    return [\"Unrecognised classifier: {!r}\".format(c)\n            for c in sorted(invalid)]",
        "sha1": "64259f25b769361ddafcb83e63845de0d052c88c",
        "id": 45580
    },
    {
        "content": "import math\n\n\ndef calculate_ideal_amount_of_iterations(n, k=1):\n    \"\"\"\n    Calculates the ideal amount of Grover's iterations required for n qubits\n    (assuming k solutions). See:\n    https://en.wikipedia.org/wiki/Grover%27s_algorithm#Extension_to_space_with_multiple_targets\n    \n    If there are no solutions, no iterations are needed (but actually those iterations wouldn't\n    matter as they would just scramble the states)\n\n    Args\n    --------------------\n    n: amount of qubits to be diffused\n    k: amount of solutions\n    \"\"\"\n    # return closest integer to the ideal amount of calculations\n    return round((math.pi/4)*( math.sqrt(2**n/k) )) if k>0 else 0",
        "sha1": "a94c7923cc0307bde29cdf21428e957b68912c2c",
        "id": 539649
    },
    {
        "content": "def parse_user_params(default_params=None, user_params=None):\n    \"\"\"\n    Parse a comma-separated list of key=value parameters, and populate a\n    dictionary with the values.\n\n    Enter: default_params: a dictionary with the default values.  Only\n                           parameters listed in the dictionary are set.\n                           Other parameters and value-less parameters are\n                           returned separately.  Values within the default\n                           dictionary are kept the same type as the\n                           default.\n           user_params: a comma-separated key=value list of parameters.\n    Exit:  params: a dictionary with the user parameters combined with the\n                   default parameters.\n           other_params: a dictionary of parameters that were not present\n                         in the default dictionary.\n           items: a list of keys without values.\n    \"\"\"\n    params = default_params.copy() if default_params else {}\n    other_params = {}\n    items = []\n    if user_params:\n        for param in user_params.split(','):\n            try:\n                key, value = param.split('=', 1)\n                if key not in params:\n                    params[key] = value\n                    continue\n                if isinstance(params[key], int):\n                    params[key] = int(value)\n                elif isinstance(params[key], float):\n                    params[key] = float(value)\n                else:\n                    params[key] = value\n            except Exception:\n                if param.strip():\n                    items.append(param.strip())\n    return params, other_params, items",
        "sha1": "fca04c9d0021f47749af91e1f92ede57252edc42",
        "id": 119955
    },
    {
        "content": "def trapmf(x, a, b, c, d):\n    \"\"\"\n    Trapezoidal membership function generator.\n    Parameters\n    ========\n    x : single element array like\n    abcd : 1d array, length 4\n        Four-element vector.  Ensure a <= b <= c <= d.\n    Returns\n    ========\n    y : 1d array\n        Trapezoidal membership function.\n    \"\"\"\n\n    assert (\n        a <= b and b <= c and c <= d\n    ), \"abcd requires the four elements \\\n                                          a <= b <= c <= d.\"\n    # Compute y1\n    if x > a and x < b:\n        y = (x - a) / (b - a)\n    elif x >= b and x <= c:\n        y = 1.0\n    elif x > c and x < d:\n        y = (d - x) / (d - c)\n    else:\n        y = 0.0\n\n    return y",
        "sha1": "6a7a8a50cf83194d072055054d71db4b0398bb2d",
        "id": 675810
    },
    {
        "content": "import torch\n\n\ndef compute_pitch_regularization_weight(segments, N, decay_size=25, max_w=0.5):\n    \"\"\"Compute pitch regularization weight given note segments\n\n    Args:\n        segments (list): list of note (start, end) indices\n        N (int): number of frames\n        decay_size (int): size of the decay window\n        max_w (float): maximum weight\n\n    Returns:\n        Tensor: weights of shape (N,)\n    \"\"\"\n    w = torch.zeros(N)\n\n    for s, e in segments:\n        L = e - s\n        w[s:e] = max_w\n        if L > decay_size * 2:\n            w[s : s + decay_size] *= torch.arange(decay_size) / decay_size\n            w[e - decay_size : e] *= torch.arange(decay_size - 1, -1, -1) / decay_size\n\n    return w",
        "sha1": "c262d0f98f582b4f3a3a9b5ff9ba32a9c2ee120b",
        "id": 363743
    },
    {
        "content": "import hashlib\n\n\ndef hashing_function(filename):\n    \"\"\" Takes in complete file path as input and returns MD5 hash of contents \"\"\"\n\n    md5_hash = hashlib.md5()\n\n    with open(filename, \"rb\") as f:\n        content = f.read()\n        md5_hash.update(content)\n\n    return md5_hash.hexdigest()",
        "sha1": "a057853298894bee89a737a5b6aabc95f0e70480",
        "id": 460355
    },
    {
        "content": "import re\n\n\ndef syntax_tweaks(lines):\n    \"\"\"\n    Applies custom tweaks to the Markdown syntax.\n\n    Namely, converts the Unicode bullet character (\u2022) to a standard\n    list-item marker (*) so that the CommomMark parser recognizes it\n    as such \u2014 which it regrettably doesn't.\n    \"\"\"\n    tweaked = [re.sub(r'^([ \\t]*)\u2022', r'\\1*', line) for line in lines]\n    return tweaked",
        "sha1": "ad3d8512e50bbff6dd29f3764263b499b7f2565f",
        "id": 197979
    },
    {
        "content": "from typing import List\nfrom typing import Dict\nfrom typing import Any\nfrom typing import Optional\n\n\ndef find_element_in_list(\n    element_list: List[Dict[str, Any]], **fields\n) -> Optional[Dict[str, Any]]:\n    \"\"\"\n    Find element with some fields in list of elements\n\n    >>> element_list = [dict(n=\"a\", v=2), dict(n=\"b\", v=3)]\n    >>> find_element_in_list(element_list, n=\"a\")\n    ... dict(n=\"a\", v=2)\n    >>> find_element_in_list(element_list, n=\"b\", v=3)\n    ... dict(n=\"b\", v=3)\n    >>> find_element_in_list(element_list, v=42)\n    ... None\n\n    Args:\n        element_list (List[Dict[str, Any]]): list of element with differents fields\n\n    Returns:\n        Dict[str, Any]: return element. Otherwise, return None.\n    \"\"\"\n    for element in element_list:\n        if element.items() >= fields.items():\n            return element\n    return None",
        "sha1": "655bbd1e343cf967d57d20fabaa97f5fb7700c49",
        "id": 641071
    },
    {
        "content": "def parse_cand_pfam_ids(pfam_string):\n    \"\"\"\n    Get flat list of pfam_ids from candidate csv field 'pfam_ids'\n    :param pfam_string: pfam_ids string separated by '|' (between each protein) and ';' (between each pfam_id)\n    :return: flat list of pfam_ids\n    \"\"\"\n    return [pfam_id for pfam_ids in pfam_string.split('|') for pfam_id in pfam_ids.split(';')]",
        "sha1": "f9b46517a787afeb803f43d118f41387cba7a2cc",
        "id": 190812
    },
    {
        "content": "def transform_keys(transform, d):\n    \"\"\"\n    Transforms the keys in a dict.\n\n    :param transform: If method, calls with key and value, returns new key\n                      If dict, maps keys to key values for new key\n                      If list, only returns dict with specified keys\n                      Else returns original dict\n    :param d: dictionary on which we're transforming keys, or list of dicts\n\n    :return: Dictionary with transformed keys\n\n    >>> transform_keys(lambda k, v: k.replace('o', '0'), dict(one=1, two=2))\n    {'tw0': 2, '0ne': 1}\n    >>> transform_keys({'one': 1}, dict(one=1, two=2))\n    {1: 1, 'two': 2}\n    >>> transform_keys(['one'], dict(one=1, two=2))\n    {'one': 1}\n    >>> transform_keys(None, dict(one=1, two=2))\n    {'two': 2, 'one': 1}\n    \"\"\"\n    if isinstance(d, list):\n        return [transform_keys(transform, i) for i in d]\n\n    if callable(transform):\n        return {transform(k, v): v for k, v in d.iteritems()}\n    elif isinstance(transform, dict):\n        return {transform.get(k, k): v for k, v in d.iteritems()}\n    elif isinstance(transform, list):\n        return {k: v for k, v in d.iteritems() if k in transform}\n    else:\n        return d",
        "sha1": "6ff91be287545ef280e68d03beaf1ea26f0f2e10",
        "id": 561265
    },
    {
        "content": "def get_issue_cover_metadata(issue_soup):\n    \"\"\"\n    Gets all the metadata from the cover section of the issue page.\n    \"\"\"\n    d = dict()\n    # get cover section\n    cover_soup = issue_soup.find(\"div\", {\"class\": \"cover\"})\n\n    # cover credits: editing, script, pencils, inks, colors, letters, characters, etc...\n    cover_credits = list(\n        zip(\n            [\n                x.contents[0]\n                for x in cover_soup.find_all(\"span\", {\"class\": \"credit_label\"})\n            ],\n            [\n                x.contents[0]\n                for x in cover_soup.find_all(\"span\", {\"class\": \"credit_value\"})\n            ],\n        )\n    )\n\n    d.update({\"cover_{}\".format(x.lower()): y for x, y in cover_credits})\n    d.pop(\"cover_reprints\", None)\n    d.pop(\"cover_awards\", None)\n    return d",
        "sha1": "099b3245652e46bc993190afa2a858738b829f79",
        "id": 193772
    },
    {
        "content": "def full_to_type_name(full_resource_name):\n    \"\"\"Creates a type/name format from full resource name.\"\"\"\n\n    return '/'.join(full_resource_name.split('/')[-2:])",
        "sha1": "9bcb3985be683060e0b4028183c9da750f4c619c",
        "id": 685108
    },
    {
        "content": "def get_block(page, start, end):\n    \"\"\" Returns a block as described by start and end markers.\n        If start marker not in page - returns none\n        If end marker is not in page - returns all after the start marker\n    \"\"\"\n    if page is None: return None\n    if start in page:\n        return page.split(start,1)[1].split(end,1)[0]",
        "sha1": "6b8419877fb6db1bb9bfe886bfd82acdb44015cb",
        "id": 496176
    },
    {
        "content": "def scale_zscore(zscore, mean, std):\n    \"\"\"Calculates the predicited insertion efficiency from the Z-score.\"\"\"\n    zscaled = zscore * std + mean\n    return zscaled",
        "sha1": "928872adcc4a460d6485dffeb24033cccdb2f6fd",
        "id": 318748
    },
    {
        "content": "def first(iterable, default=None):\n    \"\"\"\n    Returns the first item in the given iterable or `default` if empty, meaningful mostly with 'for' expressions.\n    \"\"\"\n    for i in iterable:\n        return i\n    return default",
        "sha1": "9d1ede6cf9bba99270021b3e3bc28cad79748a13",
        "id": 426387
    },
    {
        "content": "import string\nimport random\n\n\ndef create_random_string(\n    possible_chars: str = string.ascii_lowercase, seq_length: int = 8\n):\n    \"\"\"\n    Helper function for generation of random strings.\n\n    Inspired by: https://stackoverflow.com/questions/2257441/random-string-generation-with-upper-case-letters-and-digits-in-python\n\n    :param seq_length:\n    :param possible_chars:\n    :return:\n    \"\"\"\n\n    return \"\".join(\n        random.choice(possible_chars) for _ in range(random.randint(1, seq_length))\n    )",
        "sha1": "8325799fc20847461e15806756b7118edb7d453f",
        "id": 223892
    },
    {
        "content": "def bingpg(bingpg_maker):\n    \"\"\" return an initialized bingpg instance. \"\"\"\n    return bingpg_maker()",
        "sha1": "090587b6fcad1bdb881eb12e76740f5274c59819",
        "id": 487157
    },
    {
        "content": "from functools import reduce\n\n\ndef intersect(collection_of_sets):\n    \"\"\"\n    Given an collection of sets, returns the intersection of those sets.\n\n    Parameters\n    ----------\n    collection_of_sets: collections.abc.Collection[set]\n        A collection of sets.\n\n    Returns\n    -------\n    set\n        An intersection of all sets in `collection_of_sets`. Will have the same\n        type as the item initially taken from `collection_of_sets`.\n    \"\"\"\n    collection_of_sets = list(collection_of_sets)\n    first = collection_of_sets.pop()\n    out = reduce(set.intersection, collection_of_sets, set(first))\n    return type(first)(out)",
        "sha1": "3656a555719a80862dcf46a41e3d7c8d82423cc3",
        "id": 522188
    },
    {
        "content": "def is_admin(user):\n    \"\"\"Returns True if the user is an admin.\"\"\"\n    return user.is_staff or user.is_superuser",
        "sha1": "a94442dc835652ba378de2ac40e2d8711253cabb",
        "id": 85140
    },
    {
        "content": "def outOfGamutClipping(I):\n    \"\"\" Clips out-of-gamut pixels. \"\"\"\n    I[I > 1] = 1  # any pixel is higher than 1, clip it to 1\n    I[I < 0] = 0  # any pixel is below 0, clip it to 0\n    return I",
        "sha1": "ecec10f9cb10d1ee2fdcba5a3d804151e49bc7d3",
        "id": 462666
    },
    {
        "content": "def statement_present_verb(stmt_type):\n    \"\"\"Return the present verb form of a statement type.\n\n    Parameters\n    ----------\n    stmt_type : str\n        The lower case string form of a statement type, for instance,\n        'phosphorylation'.\n\n    Returns\n    -------\n    str\n        The present verb form of a statement type, for instance,\n        'phosphorylates'.\n    \"\"\"\n    override = {\n        'complex': 'binds',\n        'regulateamount': 'regulates the amount of',\n        'increaseamount': 'increases the amount of',\n        'decreaseamount': 'decreases the amount of',\n        'gef': 'acts as a GEF for',\n        'gap': 'acts as a GAP for',\n        'inhibition': 'inhibits',\n        'gtpactivation': 'activates when bound to GTP',\n        'regulateactivity': 'regulates the activity of',\n        'activeform': 'has active form',\n        'conversion': 'converts',\n        'influence': 'influences',\n        'modification': 'modifies',\n        'addmodification': 'adds a modification to',\n        'removemodification': 'removes a modification of',\n        'selfmodification': 'modifies itself',\n        'event': 'happens'\n    }\n    return override.get(stmt_type) if stmt_type in override else \\\n        stmt_type[:-3] + 'es'",
        "sha1": "d73671457092d5843bd927a167df847e6a79feec",
        "id": 378590
    },
    {
        "content": "from pathlib import Path\n\n\ndef getfilepath(filename: str) -> Path:\n    \"\"\"\n    Searches for the filepath in the current directory and parent directory\n    If an empty string or nothing is passed as the parameter, sets the current working directory as path\n    Args:\n        filename: An str object that represents a file or a folder path\n    Return:\n        Path to the file object\n    \"\"\"\n    filepath = None\n    if not filename:\n        filepath = Path(__file__).parent\n    else:\n        filepath = Path(filename)\n        if not filepath.exists():\n            check_in_parent = Path(__file__).parent / filename\n            if not check_in_parent.exists():\n                raise FileNotFoundError\n            else:\n                filepath = check_in_parent\n    return filepath",
        "sha1": "ef4efe7b333ca25c676e3ffc7e83d6fb59f32f91",
        "id": 246803
    },
    {
        "content": "def getUserIdentification(request, username=None):\n    \"\"\" Return user name or IP or '<unknown>' indicator.\n\n    @param request: the request object\n    @param username: (optional) user name\n    @rtype: string\n    @return: user name or IP or unknown indicator\n    \"\"\"\n    _ = request.getText\n\n    if username is None:\n        username = request.user.name\n\n    return username or (request.cfg.show_hosts and request.remote_addr) or _(\"<unknown>\")",
        "sha1": "3e278ac59ce0bdb318347647bfba0b1ccfa57c11",
        "id": 332320
    },
    {
        "content": "def items(mapping):\n    \"\"\"Get an items generator from a mapping\"\"\"\n    return mapping.items()",
        "sha1": "12ae369af8ef27cd9277bc4534012eeef86e1ccf",
        "id": 213091
    },
    {
        "content": "import random\n\n\ndef generate_lucky_numbers(how_many: int) -> list:\n    \"\"\"Returns a list of (random) 'lucky' numbers.\"\"\"\n    lucky_numbers = []\n    for _ in range(how_many):\n        lucky_numbers.append(random.randint(0, 99))\n    return lucky_numbers",
        "sha1": "29a72ccadfc64d766c1eaec1e81137d1bed6de9a",
        "id": 382636
    },
    {
        "content": "def alpha_synapse_params(lpu, weight = 1):\n    \"\"\"\n    Generate AlphaSynapse params.\n    \"\"\"\n    s = 0.04\n    k = 1000\n    if lpu == 'BU' or lpu == 'bu':\n        return {'conductance': True,\n                'ad': 0.16*1000,\n                'ar': 1.1*100,\n                'gmax': weight * 0.01 * s,\n                'reverse': -0.065 * k}\n    elif lpu == 'EB':\n        return {'conductance': True,\n                'ad': 0.16*1000,\n                'ar': 1.1*100,\n                'gmax': 0.01,\n                'reverse': 0.065* k}\n    elif lpu == 'FB':\n        return {'conductance': True,\n                'ad': 0.16*1000,\n                'ar': 1.1*100,\n                'gmax': 0.01,\n                'reverse': 0.065* k}\n    elif lpu == 'PB':\n        return {'conductance': True,\n                'ad': 0.19*1000,\n                'ar': 1.1*100,\n                'gmax': weight * 0.002,\n                'reverse': 0.065* k}\n    else:\n        raise ValueError('unrecognized LPU name')",
        "sha1": "b52826a6b16edd51bf87e8f2a910a8eee09a0b19",
        "id": 558869
    },
    {
        "content": "def pairwise(things):\n    \"\"\"\n    Return a list of pairs of adjacent elements from things.\n\n    The last element of this list is the pair (things[-1], None).\n\n    >>> list(pairwise(['a', 'b', 'c']))\n    [('a', 'b'), ('b', 'c'), ('c', None)]\n\n    >>> list(pairwise([]))\n    []\n    \"\"\"\n    return zip(things, things[1:] + [None])",
        "sha1": "2ae51e1a991df9ebf4408c9c01f23007f03b56f3",
        "id": 156155
    },
    {
        "content": "from typing import Dict\n\n\ndef index_post() -> Dict[str, str]:\n    \"\"\"Exemplar POST endpoint for the API service. Simply returns a welcome message.\"\"\"\n    return {\"message\": \"Welcome to the Hippo diagnosis service!\"}",
        "sha1": "295d5e8346cc44c0d74acdb8fad78c622be9a390",
        "id": 607217
    },
    {
        "content": "def _small_body(close, low, open, high):\n    \"\"\"\n    do we have a small body in relation to the wicks\n    :param close:\n    :param low:\n    :param open:\n    :param high:\n    :return:\n      0 if no\n      1 if yes (wicks are longer than body)\n\n    \"\"\"\n    size = abs(close - open)\n\n    if close > open:\n        top_wick = high - close\n        bottom_wick = open - low\n\n    else:\n        top_wick = high - open\n        bottom_wick = close - low\n\n    wick_size = top_wick + bottom_wick\n\n    if wick_size > size:\n        return 1\n    else:\n        return 0",
        "sha1": "c466fd7ebba16f62cc957d3ec43143d8d4a13500",
        "id": 322418
    },
    {
        "content": "def is_factor(number, checker):\n    \"\"\"Check if a number is a factor of another\"\"\"\n    if number%checker == 0:\n        return True\n    return False",
        "sha1": "dffab44bce7f803e491e85bef7290e9119ff59a2",
        "id": 417681
    },
    {
        "content": "def get_intersection_line_chart_horizontal(data_objects, data_object_nodes, line_pos, only_at_data_points):\n    \"\"\"\n    Calculates the intersection of objects from a line chart with an horizontal line.\n\n    :param data_objects: The data objects on the chart.\n    :type data_objects: dict[str, DataObject]\n    :param data_object_nodes: The nodes of the data object nodes in the chart.\n    :type data_object_nodes: dict[str, PolyLineNode]\n    :param line_pos: The necessary position of the node of the line according to the orientation.\n    :type line_pos: float\n    :param only_at_data_points: Only return intersection that are lying on data points on a item (if true).\n    :type: bool\n    :return: All nodes that were intersect through the line and the positions according to the line orientation of the intersections.\n    :rtype: dict[str, list[tuple[float, float]]]\n    \"\"\"\n    def line_part(y):\n        \"\"\"\n        The method for the part of the line between two indices.\n\n        :type y: float\n        :return: The x value.\n        :rtype: float\n        \"\"\"\n        m = (current_pos[1] - last_pos[1]) / (current_pos[0] - last_pos[0])\n        return (y - last_pos[1]) / m + last_pos[0]\n\n    results = {}\n    for key, node in data_object_nodes.iteritems():\n        if only_at_data_points:\n            results[key] = [p for p in node.pos if p[1] == line_pos]\n        else:\n            for i in range(1, len(node.pos)):\n                last_pos = node.pos[i - 1]\n                current_pos = node.pos[i]\n                if not (last_pos[1] <= line_pos <= current_pos[1] or current_pos[1] <= line_pos <= last_pos[1]):\n                    continue\n\n                if key not in results:\n                    results[key] = []\n                results[key].append((line_part(line_pos), line_pos))\n\n    return results",
        "sha1": "3198f9d0f2aae0d019c7b31b28257163640e439a",
        "id": 219855
    },
    {
        "content": "def _get_non_inhereted_function_names(cls):\n  \"\"\"Gets all methods that cls has that its parents don't have.\"\"\"\n  names = set(dir(cls))\n  for parent in cls.__bases__:\n    names -= set(dir(parent))\n  return list(names)",
        "sha1": "4e7a489c457ab1e04e4e54644a32672679c63f0b",
        "id": 615702
    },
    {
        "content": "from functools import reduce\nfrom operator import getitem\n\n\ndef get_dict_item(dict_name, itempath):\n    \"\"\"Access a nested object in root by item sequence.\"\"\"\n    return reduce(getitem, itempath, dict_name)",
        "sha1": "e63d4c59e61bf75fc1494830e34424803441f2d2",
        "id": 311923
    },
    {
        "content": "def camelcased_to_uppercased_spaced(camelcased: str) -> str:\n    \"\"\"Util function to transform a camelCase string to a UPPERCASED SPACED string\n    e.g: dockerImageName -> DOCKER IMAGE NAME\n    Args:\n        camelcased (str): The camel cased string to convert.\n\n    Returns:\n        (str): The converted UPPERCASED SPACED string\n    \"\"\"\n    return \"\".join(map(lambda x: x if x.islower() else \" \" + x, camelcased)).upper()",
        "sha1": "e9a01ee1fdbc6ea626ed509d68ec3ae799c2a007",
        "id": 109470
    },
    {
        "content": "def gnss_antenna_status(is_shorted, is_open):\n    \"\"\"\n    Return a valid antenna-status-enumeration from vyatta-service-gnss-v1\n    \"\"\"\n    state_table = {\n        (0, 0): \"short\",\n        (0, 1): \"unknown\",\n        (1, 0): \"OK\",\n        (1, 1): \"open\",\n    }\n    return state_table[is_shorted, is_open]",
        "sha1": "60819980b4ebc600f6c1f13a0b00203f7dde34d4",
        "id": 307572
    },
    {
        "content": "import torch\n\n\ndef get_loss_and_grads(model, train_x, train_y, flat=True, weights=None, item_loss=True,\n                       create_graph=False, retain_graph=False):\n    \"\"\"Computes loss and gradients\n    \n    Apply model to data (train_x, train_y), compute the loss\n    and obtain the gradients.\n    \n    Parameters\n    ----------\n    model : nn.Module\n        Neural network. We assume the model has a criterion attribute \n    train_x : torch.Tensor\n        Training inputs\n    train_y : torch.Tensor\n        Training targets\n    \n    Returns\n    ----------\n    loss\n        torch.Tensor of size (#params in model) containing the loss value \n        if flat=True, else a single float\n    gradients\n        torch.Tensor of size (#params in model) containing gradients w.r.t.\n        all model parameters if flat=True, else a structured list\n    \"\"\"\n    \n    model.zero_grad()\n    if weights is None:\n        weights = model.parameters()\n        out = model(train_x)\n    else:\n        out = model.forward_weights(train_x, weights)\n    \n    loss = model.criterion(out, train_y)\n    grads = torch.autograd.grad(loss, weights, create_graph=create_graph, \n                                retain_graph=retain_graph)\n    \n    if flat:\n        gradients = torch.cat([p.reshape(-1) for p in grads])\n        loss = torch.zeros(gradients.size()).to(train_x.device) + loss.item()\n    else:\n        gradients = list(grads)\n        if item_loss:\n            loss = loss.item()\n    return loss, gradients",
        "sha1": "7abb2a9758ac641c5303583e2609407e84d8f91f",
        "id": 136928
    },
    {
        "content": "def unqueue(redis):\n    \"\"\"Pop a task from the work queue.\"\"\"\n    return redis.rpop('work')",
        "sha1": "43812d6b8ee073d2be15621bfc3b80e1ba79acd0",
        "id": 171603
    },
    {
        "content": "import string\n\n\ndef to_alpha_label(n):\n    \"\"\"Convert a number to an Excel-style base-26 alphabet label.\"\"\"\n    if n < 0:\n        return \"*\"\n    a = []\n    while n > 0:\n        q, r = divmod(n, 26)\n        if r == 0:\n            q = q - 1\n            r = 26\n        n = q\n        a.append(string.ascii_uppercase[r - 1])\n    return \"\".join(reversed(a))",
        "sha1": "89e56da5872c45ab6997471efe23d8da43101334",
        "id": 410035
    },
    {
        "content": "import re\n\n\ndef ends_in_file(path):\n    \"\"\" Return True when path ends with '.%ext' or '%fn' \"\"\"\n    _RE_ENDEXT = re.compile(r\"\\.%ext[{}]*$\", re.I)\n    _RE_ENDFN = re.compile(r\"%fn[{}]*$\", re.I)\n    return bool(_RE_ENDEXT.search(path) or _RE_ENDFN.search(path))",
        "sha1": "b7087c407a474e9705aebe487a73a2daad124599",
        "id": 50908
    },
    {
        "content": "def shaped_policy(p_action, p_good, denominator):\n    \"\"\"\n    Returns the probability of an action given a state for the policy as a result of policy shaping\n    :param denominator: The sum over all p_action and p_good for every action in a given state.\n    \"\"\"\n\n    if denominator <= .00001:\n        denominator = .00001\n    if denominator >= 1000:\n        denominator = 1000\n    return (p_action * p_good) / denominator",
        "sha1": "b5eb04e5ed703149a2a6f56ba3893ff589fc9a93",
        "id": 451344
    },
    {
        "content": "import math\n\n\ndef distance_set(coord, curset):\n    \"\"\"\n    Return the distance from a coordinate to any item in the set\n    \"\"\"\n    min_dist = math.inf\n    for c in curset:\n        min_dist = min(coord.euclidean_distance(c), min_dist)\n\n    return min_dist",
        "sha1": "707f5417eb31a016fa634a40658fad74f26412b0",
        "id": 79687
    },
    {
        "content": "def reproject_extent(extent, current_extent):\n\t\"\"\"\n\t\tChanges an extent from its current spatial reference to the spatial reference on another extent object\n\t:param extent:\n\t:param current_extent:\n\t:return:\n\t\"\"\"\n\treturn extent.projectAs(current_extent.spatialReference)",
        "sha1": "b64e98c15cda62e1a4855050a63b7ff7fcd38610",
        "id": 16129
    },
    {
        "content": "def __shellquote(s):\n    \"\"\"\n    Quote and escape the supplied string for use in shell expressions.\n    \"\"\"\n    return \"'\" + s.replace(\"'\", \"'\\\\''\") + \"'\"",
        "sha1": "37cfa7840b925a7297125b6733465e7e254b7629",
        "id": 575266
    },
    {
        "content": "def sum_dicts(dict1={}, dict2={}):\n    \"\"\"\n    Merges to dictionaries. If they share keys, their values will be summed.\\n\n    Values should be numbers, otherwise results might be weird.\n\n    Parameters\n    ----------\n    dict1 : \\n\n    dict2 : \n\n    Returns\n    -------\n    `dict` : Merged dictionary with all keys and summed values.\n    \"\"\"\n    new_dict = dict1.copy()\n    dict1_keys = dict1.keys()\n    # merge dictionaries\n    new_dict.update(dict2)\n    all_keys = new_dict.keys()\n    dict2_keys = dict2.keys()\n\n    for key in all_keys:\n        if key in dict1_keys and key in dict2_keys:\n            new_dict[key] = dict1[key] + dict2[key]\n    return new_dict",
        "sha1": "a44dcb72b255b354c8c4df6da23060355737818c",
        "id": 524443
    },
    {
        "content": "import time\n\n\ndef get_balance_metrics(balance_data, ts=None):\n    \"\"\"\n    Return a list of balance metrics.\n\n    Parsed from the [balance](http://dev.datasift.com/docs/api/1/balance)\n    endpoint.\n    \"\"\"\n    ts = ts or time.time()\n\n    return [\n        ('datasift.balance.credit', (ts, balance_data['balance']['credit']))\n    ]",
        "sha1": "66db8b1c77c89e93ebc0217ce7714c6ea396a6bd",
        "id": 582556
    },
    {
        "content": "import numbers\n\n\ndef ensure_r_vector(x):\n    \"\"\"Ensures that the input is rendered as a vector in R.\n\n    It is way more complicated to define an array in R than in Python because an array\n    in R cannot end with an comma.\n\n    Examples\n    --------\n    >>> ensure_r_vector(\"string\")\n    \"c('string')\"\n    >>> ensure_r_vector(1)\n    'c(1)'\n    >>> ensure_r_vector(list(\"abcd\"))\n    \"c('a', 'b', 'c', 'd')\"\n    >>> ensure_r_vector((1, 2))\n    'c(1, 2)'\n\n    \"\"\"\n    if isinstance(x, str):\n        out = f\"c('{x}')\"\n    elif isinstance(x, numbers.Number):\n        out = f\"c({x})\"\n    elif isinstance(x, (tuple, list)):\n        mapped = map(lambda l: str(l) if isinstance(l, numbers.Number) else f\"'{l}'\", x)\n        concatenated = \", \".join(mapped)\n        out = f\"c({concatenated})\"\n    else:\n        raise NotImplementedError(\n            f\"'ensure_r_vector' is not defined for dtype {type(x)}\"\n        )\n\n    return out",
        "sha1": "14fdeb6bf73244c69d9a6ef89ba93b33aa4a66d8",
        "id": 629
    },
    {
        "content": "import json\n\n\ndef read_json(json_file):\n    \"\"\"\n    Read JSON file\n\n    Args:\n        json_file (str): The path to the JSON file\n\n    Returns:\n        dict: JSON object\n\n    \"\"\"\n    with open(json_file) as json_file:\n        json_dict = json.load(json_file)\n    return json_dict",
        "sha1": "f905168ca05a17f5e7414e0a0d20b8f8b93fc58a",
        "id": 266377
    },
    {
        "content": "def is_prod_of_two_3_digit_num(n):\n    \"\"\"Determine whether n is the product of 3-digit numbers.\"\"\"\n    result = False\n\n    for i in range(100, 1000):\n        if n % i == 0 and n // i in range(100, 1000):\n            result = True\n            break\n\n    return result",
        "sha1": "db0cb1b3ae1ecb8b15d01582f8c0599ce00ce766",
        "id": 20647
    },
    {
        "content": "def get_one_of(object, attribs):\n    \"\"\"Try to get the first one of attribs.\"\"\"\n    for a in attribs:\n        v = getattr(object, a, None)\n        if v:\n            return v",
        "sha1": "1c440e332b269c5542eac26c299bc2afc24cb2f4",
        "id": 522963
    },
    {
        "content": "def headers_to_table(markdown_headers, notebook_name):\n    \"\"\"Produces a markdown table of contents from markdown headers.\n\n    This function uses a two pass solution to turn markdown headers into a\n    table of contents. The first pass strips one # from every header and the\n    second pass turns all remaining # into spaces. Then the function produces\n    markdown links spaced in table formats.\n    \"\"\"\n    if markdown_headers is None:\n        return None\n\n    # First pass just strips #\n    stripped_headers = []\n    for header in markdown_headers:\n        # While we're here let's clean up the data.\n        header = header.replace(\"\\n\", \"\")\n        # Third argument is max replace>\n        stripped_headers.append(header.replace(\"#\", \"\", 1))\n\n    # Second pass Produces actual table of contents.\n    table_of_contents = []\n    for header in stripped_headers:\n        # Using the fact that we know markdown headers have to start on the\n        # left side.\n        pound_stripped_header = header.lstrip(\"#\")\n        nesting_count = len(header) - len(pound_stripped_header)\n\n        # Remove spaces from URL. All spaces are convered to - so -- is valid.\n        href_header = pound_stripped_header.replace(\" \", \"-\")\n        # Finally constructing toc line\n        toc_line = (u\"{spaces}* [{header_name}]\" \\\n            \"(http://localhost:8888/notebooks/{notebook_name}#{href_header})\").format(\n                spaces=\" \" * (nesting_count * 4),\n                header_name=pound_stripped_header,\n                notebook_name=notebook_name,\n                href_header=href_header,)\n        table_of_contents.append(toc_line)\n\n    return table_of_contents",
        "sha1": "3e1e05ad310056baf7167a6d4eb0f1eb4b4ea5d0",
        "id": 47768
    },
    {
        "content": "import json\n\n\ndef convert_neighbor_to_json(infn, outfn):\n  \"\"\"convert data format for nearest neighbors.\n\n  Args:\n    infn: tsv file downloaded from MSMARCO dataset.\n    outfn: path to output neighors.\n\n  Returns:\n    The output neighbor data format is as follows:\n    dict(qID=[list of ranked neighbors in the format of [pID, score]])\n  \"\"\"\n  data = {}\n  fns = infn.split(',')\n  for fn in fns:\n    for line in open(fn):\n      elements = line.split('\\t')\n      if elements[0] not in data:\n        data[elements[0]] = []\n      # no BM25 score provided in MSMARCO dataset, use 0 instead\n      neighbor = [elements[1], 0]  # replace 0 with BM25 score\n      # data[elements[0]] needs to be sorted by BM25 score\n      data[elements[0]].append(neighbor)\n\n  with open(outfn, 'w') as outfile:\n    json.dump(data, outfile)\n  print('Processed: ' + infn)\n  print('Output to: ' + outfn)\n  print('Number of entries: ' + str(len(data)))\n  return data",
        "sha1": "9104fe647e266a169c5e1a660388ef2885a6788c",
        "id": 203301
    },
    {
        "content": "from datetime import datetime\nimport uuid\n\n\ndef generate_unique_name(job_name):\n  # type: (str) -> str\n  \"\"\"Returns a unique name with time suffix and random UUID.\"\"\"\n  return '-'.join([job_name,\n                   datetime.now().strftime('%Y%m%d-%H%M%S'),\n                   str(uuid.uuid4())])",
        "sha1": "613bbfe54576af6171eebd781d453f26fd69d3da",
        "id": 561177
    },
    {
        "content": "def _indent(msg: str) -> str:\n    \"\"\" Indent a block of text (possibly with newlines) \"\"\"\n    ind = \" \"*2\n    return ind+msg.replace(\"\\n\", \"\\n\"+ind)",
        "sha1": "398b6b7b16e1039a257f4316b562e66d66effd18",
        "id": 537422
    },
    {
        "content": "import random\n\n\ndef full_jitter(value):\n    \"\"\"Jitter the value across the full range (0 to value).\n\n    This corresponds to the \"Full Jitter\" algorithm specified in the\n    AWS blog's post on the performance of various jitter algorithms.\n    (http://www.awsarchitectureblog.com/2015/03/backoff.html)\n\n    Args:\n        value: The unadulterated backoff value.\n    \"\"\"\n    return random.uniform(0, value)",
        "sha1": "0a5c233d3e0e58873d29de7e3d878fbcf3d0c47a",
        "id": 701048
    },
    {
        "content": "def count_bags(bags, bag):\n    \"\"\"Recursively count the bags of each type.\"\"\"\n    if not bags.get(bag):\n        return 1\n    else:\n        counter = 0\n        for bag_a in bags[bag]:\n            counter += count_bags(bags, bag_a) * bags[bag][bag_a]\n        return counter + 1",
        "sha1": "85d0c6d58df7d177c01fef6ba35d0128aaa7c9ed",
        "id": 671846
    },
    {
        "content": "import re\n\n\ndef check_state_keys(state, keys_regex):\n    \"\"\"check if keys exists in state using full python paths\"\"\"\n    regex = re.compile(keys_regex)\n    for k, v in state.items():\n        if regex.findall(k):\n            return True\n    return False",
        "sha1": "423bbd9a01c7240d0c73f1f7370e38164d3ae63f",
        "id": 27427
    },
    {
        "content": "def DFS_TS_subroutine(adjacency_list, explored, current_label, labels, s):\n    \"\"\"\n    Non-recursive subroutine called by DFS_TS.\n    Args:\n    adjacency_list - the graph in its adjacency list representation.\n    explored - whether each node has been explored.\n    current_label - the next label to be assigned.\n    labels - the list holding the labels of each node.\n    s - stack to assist this subroutine.\n    \"\"\"\n    # mark node i as explored\n    i = s[-1]\n    explored[i] = 1\n    exhausted = True\n    for j in adjacency_list[i]:\n        # stack each neighbor that has not yet been explored\n        if not bool(explored[j]):\n            exhausted = False\n            s.append(j)\n    if exhausted:\n        s.pop()\n        # assign label and decrement it\n        if not labels[i] >= 0:\n            labels[i] = current_label\n            current_label -= 1\n    return current_label",
        "sha1": "3bec3d0b962895fd248cd075f9d01b1267753ed9",
        "id": 318544
    },
    {
        "content": "def unordered_list(value):\n    \"\"\"\n    Recursively takes a self-nested list and returns an HTML unordered list --\n    WITHOUT opening and closing <ul> tags.\n\n    The list is assumed to be in the proper format. For example, if ``var`` contains\n    ``['States', [['Kansas', [['Lawrence', []], ['Topeka', []]]], ['Illinois', []]]]``,\n    then ``{{ var|unordered_list }}`` would return::\n\n        <li>States\n        <ul>\n                <li>Kansas\n                <ul>\n                        <li>Lawrence</li>\n                        <li>Topeka</li>\n                </ul>\n                </li>\n                <li>Illinois</li>\n        </ul>\n        </li>\n    \"\"\"\n    def _helper(value, tabs):\n        indent = '\\t' * tabs\n        if value[1]:\n            return '%s<li>%s\\n%s<ul>\\n%s\\n%s</ul>\\n%s</li>' % (indent, value[0], indent,\n                '\\n'.join([_helper(v, tabs+1) for v in value[1]]), indent, indent)\n        else:\n            return '%s<li>%s</li>' % (indent, value[0])\n    return _helper(value, 1)",
        "sha1": "6ef084e1feac992628ff995c7255d5ab05d35b8e",
        "id": 258388
    },
    {
        "content": "def _desmile_row(row):\n    \"\"\" Interpolate a single row. \"\"\"\n\n    row['x'] = row.desmiled_x\n    new_x = row.new_x\n    row = row.drop(['desmiled_x','new_x'])\n    row = row.interp(x=new_x, method='linear')\n    return row",
        "sha1": "7ea89c1d31ece49eb4216f284cb36a27f13eda1f",
        "id": 367438
    },
    {
        "content": "import math\n\n\ndef michalewicz(ind, m=10.):\n    \"\"\"Michalewicz function defined as:\n    $$ f(x) = - \\sum_{i=1}^{n} \\sin(x_i) \\sin( \\frac{i x_i^2}{\\pi} )^(2*m)$$\n    with a search domain of $0 < x_i < \\pi, 1 \\leq i \\leq n$.\n    \"\"\"\n    return - sum(( math.sin(ind[i]) * (math.sin(((i+1.)*(ind[i] **2.))/math.pi))**(2.*m) for i in range(len(ind)) )),",
        "sha1": "74128ad086d81a0cf634a891de93827e1aad1960",
        "id": 622758
    },
    {
        "content": "def to_float_list(a):\n    \"\"\"\n    Given an interable, returns a list of its contents converted to floats\n    :param a: interable\n    :return: list of floats\n    \"\"\"\n    return [float(i) for i in a]",
        "sha1": "8576c649802c9a88694097ceefb55c86ab845266",
        "id": 47161
    },
    {
        "content": "def field_help(view, field):\n    \"\"\"\n    Returns the field help for the passed in field\n    \"\"\"\n    return view.lookup_field_help(field)",
        "sha1": "84d4cad68ff7253fb3410ab0d8bf581dd8aacc1e",
        "id": 266470
    },
    {
        "content": "def filter_tokens(tokens, filters):\n    \"\"\" eliminates tokens contained in filters\n    args:\n        tokens: list of tokens to be filtered.\n        filters: list of tokens that will get removed from 'tokens'.\n    returns:\n        filtered_tokens: a list of tokens that don't contain any\n        token from filters.\n    \"\"\"\n    filtered_tokens = []\n    for token in tokens:\n        if token not in filters:\n            filtered_tokens.append(token)\n    return filtered_tokens",
        "sha1": "32fe98257e70f7d56225f78b6fb5a6e45e127af0",
        "id": 498928
    },
    {
        "content": "def calc_resize_with_apect(size, min_dimension):\n    \"\"\"calculate the dimensions needed to resize an image with the minimum dimension on one side\n    while preserving the aspect ratio.\n    size: tuple containing the original image size in pixels (w,h)\n    min_dimension: min pixel size on one size\n    \"\"\"\n    w = size[0]\n    h = size[1]\n\n    new_w = (w / min(size)) * min_dimension\n    new_h = (h / min(size)) * min_dimension\n\n    new_size = (int(new_w), int(new_h))\n\n    return new_size",
        "sha1": "fd52f50a2ce759fe4d510f6ae6b403ab2cb523e3",
        "id": 76388
    },
    {
        "content": "def scale_range(actual, actual_min, actual_max, wanted_min, wanted_max):\n    \"\"\"Scales a value from within a range to another range\"\"\"\n\n    return (wanted_max - wanted_min) * (\n        (actual - actual_min) / (actual_max - actual_min)\n    ) + wanted_min",
        "sha1": "b23a168039c98df7a59d0d66292911097b45106b",
        "id": 502614
    },
    {
        "content": "import re\n\n\ndef get_device_number(device):\n    \"\"\"Extract device number.\n\n    Ex: \"D1000\" \u2192 \"1000\"\n        \"X0x1A\" \u2192 \"0x1A\n    \"\"\"\n    device_num = re.search(r\"\\d.*\", device)\n    if device_num is None:\n        raise ValueError(\"Invalid device number, {}\".format(device))\n    else:\n        device_num_str = device_num.group(0)\n    return device_num_str",
        "sha1": "4a62aae822ed931a12574c31feafa3108bf783e3",
        "id": 80792
    },
    {
        "content": "def absolute_proportions(proportions, count):\n    \"\"\"\n    Split a given integer into n parts according to len(proportions) so they sum up to count and\n    match the given proportions.\n\n    Args:\n        proportions (dict): Dict of proportions, with a identifier as key.\n\n    Returns:\n        dict: Dictionary with absolute proportions and same identifiers as key.\n\n    Example::\n\n        >>> absolute_proportions({'train': 0.5, 'test': 0.5}, 100)\n        {'train': 50, 'test': 50}\n    \"\"\"\n\n    # first create absolute values by flooring non-integer portions\n    relative_sum = sum(proportions.values())\n    absolute_proportions = {idx: int(count / relative_sum * prop_value) for idx, prop_value in\n                            proportions.items()}\n\n    # Now distribute the rest value randomly over the different parts\n    absolute_sum = sum(absolute_proportions.values())\n    rest_value = count - absolute_sum\n    subset_keys = sorted(list(proportions.keys()))\n\n    for i in range(rest_value):\n        key = subset_keys[i % len(subset_keys)]\n        absolute_proportions[key] += 1\n\n    return absolute_proportions",
        "sha1": "06d585c0792ce93d3ea0bb0e41527b9d74c8b6b7",
        "id": 473248
    },
    {
        "content": "from typing import Tuple\n\n\ndef format_xml_property(\n    metric_type: str, summary_method: str, metric_value: float\n) -> Tuple[str, float]:\n    \"\"\"\n    Formats metric summary into XML name-value tuple in the form of\n    (metric_type[summary_method], metric_value)\n    ex: (cpu_util[avg], 88.23)\n    \"\"\"\n    return f\"{metric_type}[{summary_method}]\", metric_value",
        "sha1": "f26bfdefae85e220b9f4285a1122501ee7b7329a",
        "id": 33451
    },
    {
        "content": "def check_fields_match(msg1, msg2):\n    \"\"\"Check fields in msg1 match fields in msg2.\"\"\"\n\n    def _check_eq_or_none(fld1, fld2):\n        return fld1 is None or fld2 is None or fld1 == fld2\n\n    match = True\n    for field in msg1.fields:\n        fld1 = getattr(msg1, field.name)\n        fld2 = getattr(msg2, field.name)\n        match = match & _check_eq_or_none(fld1, fld2)\n    return match",
        "sha1": "5532d4463b5d64f9224a28811a8053c67ae9dad8",
        "id": 515502
    },
    {
        "content": "def findSelectedFields(fieldSearchList, fieldNames):\n    \"\"\" fieldSearchList is a list of fields, potentially with wild cards. fieldNames is the \n    real list of field names. Returns a list of all fields that match the SearchList.\n    \"\"\"\n    prefixes = []\n    exactMatches = []\n    for f in fieldSearchList:\n        if f.endswith(\"*\"):\n            prefixes.append(f.rstrip(\"*\"))\n        else:\n            exactMatches.append(f)\n\n    fieldsShown = []\n    for f in fieldNames:\n        if f in exactMatches:\n            fieldsShown.append(f)\n            continue\n\n        for pf in prefixes:\n            if f.startswith(pf):\n                fieldsShown.append(f)\n                break\n    return fieldsShown",
        "sha1": "31f124f4f942a957707f3d17fd6456716fc7ece3",
        "id": 191457
    },
    {
        "content": "def get_offer_based_on_quantity(offer_list, quantity):\n    \"\"\"Fetch specific offer based on offer quantity\n\n    :param offer_list: List\n    :param quantity: Integer\n\n    :rtype: Dictionary\n    \"\"\"\n    for offer in offer_list:\n        if offer[\"quantity\"] == quantity:\n            return offer",
        "sha1": "6cae69c4aee2f08ef3cf765b9cbc8daa029d7122",
        "id": 452443
    },
    {
        "content": "def _quadratic_interpolation_step(x1, x2, x3, y1, y2, y3):\n  \"\"\"Returns the step size to use when using quadratic interpolation.\n\n  This function is meant for exclusive use by the `_brent_loop_body` function.\n  It does not guard against divisions by zero, and instead assumes that `y1` is\n  distinct from `y2` and `y3`. The `_brent_loop_body` function guarantees this\n  property.\n\n  Args:\n    x1: `Tensor` of any shape and real dtype containing the first position used\n      for extrapolation.\n    x2: `Tensor` of the same shape and dtype as `x1` containing the second\n      position used for extrapolation.\n    x3: `Tensor` of the same shape and dtype as `x1` containing the third\n      position used for extrapolation.\n    y1: `Tensor` containing the value of the interpolated function at `x1`.\n    y2: `Tensor` containing the value of interpolated function at `x2`.\n    y3: `Tensor` containing the value of interpolated function at `x3`.\n\n  Returns:\n    A `Tensor` with the same shape and dtype as `x1`.\n  \"\"\"\n  r2 = (x2 - x1) / (y2 - y1)\n  r3 = (x3 - x1) / (y3 - y1)\n  return -x1 * (x3 * r3 - x2 * r2) / (r3 * r2 * (x3 - x2))",
        "sha1": "20f41d43a8fbfc1e5e8bb7535aca38b1643f6ce4",
        "id": 211980
    },
    {
        "content": "def midpoint(x1, y1, x2, y2):\n    \"\"\"\n    Computes midpoint between two points.\n    \"\"\"\n    return ((y1 + y2)/2,(x1 + x2)/2)",
        "sha1": "bc6b4a7ba22e4cecea21fb93d87e32687f30d976",
        "id": 109679
    },
    {
        "content": "from datetime import datetime\n\n\ndef time_until(next_mins):\n    \"\"\"\n    Given N minutes, this function calculates the\n    timestamp (in UTC) when the Nth minute next occurs.\n\n    Example: If the time is 16:43:02 and we run time_until(15)\n    the string 17:15:00 will be returned.\n\n    Args:\n        next_mins (int): The number of minutes past the hour to\n            get next timestamp for.\n\n    Returns:\n        str: a string timestamp (UTC) in format HH:MM:SS\n    \"\"\"\n    now_dt = datetime.utcnow()\n\n    if now_dt.minute >= next_mins:\n        hour = now_dt.hour+1\n        hour = hour if hour < 23 else 0\n        next_dt = now_dt.replace(second=0, minute=next_mins, hour=hour)\n    else:\n        next_dt = now_dt.replace(second=0, minute=next_mins)\n\n    return next_dt.strftime('%H:%M:%S')",
        "sha1": "c4009c16d81be925e02998aaabedfbffbb930166",
        "id": 646262
    },
    {
        "content": "def bool_decode(s):\n    \"\"\" Decodes a boolean. \"\"\"\n    return False if s == '0' or s == '' else True",
        "sha1": "8df338e26a6b65269e3537b234d043c6e034acd5",
        "id": 283069
    },
    {
        "content": "def interval_to_milliseconds(interval):\n    \"\"\"Convert a Binance interval string to milliseconds\n    :param interval: Binance interval string, e.g.: 1m, 3m, 5m, 15m, 30m, 1h, 2h, 4h, 6h, 8h, 12h, 1d, 3d, 1w\n    :type interval: str\n    :return:\n         int value of interval in milliseconds\n         None if interval prefix is not a decimal integer\n         None if interval suffix is not one of m, h, d, w\n    \"\"\"\n    seconds_per_unit = {\n        \"m\": 60,\n        \"h\": 60 * 60,\n        \"d\": 24 * 60 * 60,\n        \"w\": 7 * 24 * 60 * 60,\n    }\n    try:\n        return int(interval[:-1]) * seconds_per_unit[interval[-1]] * 1000\n    except (ValueError, KeyError):\n        return None",
        "sha1": "ab1745a846f7536e4e2c0c91dc482d754fa3d7f3",
        "id": 282594
    },
    {
        "content": "def latitude_bounds(EBC):\n    \"\"\"\n    Returns the standard 10 degrees of latitude to be analyzed for each system. \n    For the CalCS, HumCS, and BenCS, this comes from the Chavez 2009 EBUS Comparison\n    paper. For the CanCS, this comes from the Aristegui 2009 CanCS paper. These\n    bounds are used in the EBUS CO2 Flux comparison study to standardize latitude.\n    Parameters \n    ----------\n    EBC : str\n        Identifier for the boundary current.\n        'CalCS' : California Current\n        'HumCS' : Humboldt Current\n        'CanCS' : Canary Current\n        'BenCS' : Benguela Current\n    Returns\n    -------\n    lat1 : int\n        Minimum latitude bound.\n    lat2 : int\n        Maximum latitude bound.\n    Examples\n    --------\n    import esmtools.ebus as eb\n    y1,y2 = eb.boundaries.latitude_bounds('HumCS')\n    \"\"\"\n    if EBC == 'CalCS':\n        lat1 = 34\n        lat2 = 44\n    elif EBC == 'HumCS':\n        lat1 = -16\n        lat2 = -6\n    elif EBC == 'CanCS':\n        lat1 = 21\n        lat2 = 31\n    elif EBC == 'BenCS':\n        lat1 = -28\n        lat2 = -18\n    else:\n        raise ValueError('\\n' + 'Must select from the following EBUS strings:'\n                         + '\\n' + 'CalCS' + '\\n' + 'CanCS' + '\\n' + 'BenCS' +\n                         '\\n' + 'HumCS')\n    return lat1, lat2",
        "sha1": "960a9674e9cbfa88de37746c2739113168484775",
        "id": 446356
    },
    {
        "content": "def decode_fourcc(fourcc_code):\n    \"\"\"Decode FOURCC code into string.\"\"\"\n    fourcc_code = int(fourcc_code)\n\n    return \"\".join([chr((fourcc_code >> 8 * i) & 0xFF) for i in range(4)])",
        "sha1": "25fb1b6fff97f608c256f28fdabeb02f27bfc307",
        "id": 240832
    },
    {
        "content": "def uniquify_list(seq): \n\n    \"\"\"Uniquifies a list while preserving the original order of the list\n\n    Parameters\n    ----------\n    seq : list\n        List to uniquify \n    \n    Returns:\n    --------\n    list\n        list after removing duplicates while preserving the order of the items\n    \"\"\"\n\n    seen = {}\n    result = []\n    for item in seq:\n        if item in seen: \n            continue\n        seen[item] = 1\n        result.append(item)\n    return result",
        "sha1": "d6f5987688e551390b368d3a2c3ed62632e34a89",
        "id": 470719
    },
    {
        "content": "def sys_print_commands(\n    variables, dump_rate, filename, fix_name=\"sys_info\", append=True, print_header=True\n):\n    \"\"\"Create commands to output required system variables to a file.\"\"\"\n    commands = []\n\n    if not variables:\n        return commands\n\n    if \"step\" not in variables:\n        # always include 'step', so we can sync with the `dump` data\n        variables.insert(0, \"step\")\n\n    var_aliases = []\n    for var in variables:\n        var_alias = var.replace(\"[\", \"_\").replace(\"]\", \"_\")\n        var_aliases.append(var_alias)\n        commands.append(\"variable {0} equal {1}\".format(var_alias, var))\n\n    commands.append(\n        'fix {0} all print {1} \"{2}\" {3} {4} {5} screen no'.format(\n            fix_name,\n            dump_rate,\n            \" \".join([\"${{{0}}}\".format(v) for v in var_aliases]),\n            'title \"{}\"'.format(\" \".join(var_aliases)) if print_header else \"\",\n            \"append\" if append else \"file\",\n            filename,\n        )\n    )\n\n    return commands",
        "sha1": "89c4355e5c1caf2aafea2e7c975d2b87537341a9",
        "id": 217243
    },
    {
        "content": "def bst_search(root, value):\n  \"\"\"\n  Search a binary search tree for a value.\n\n  \"\"\"\n  if root.value == value:\n    return True\n  if root.value > value:\n    return False if root.left is None \\\n        else bst_search(root.left, value)\n  return False if root.right is None \\\n      else bst_search(root.right, value)",
        "sha1": "874d55c6e7ebe730a909382b890fc8e437ff6a92",
        "id": 585125
    },
    {
        "content": "def set_compare(a, b):\n    \"\"\"\n    Compare two iterables a and b. set() is used for comparison, so only\n    unique elements will be considered.\n\n    Parameters\n    ----------\n    a : iterable\n    b : iterable\n\n    Returns\n    -------\n    tuple with 3 elements:\n        (what is only in a (not in b),\n         what is only in b (not in a),\n         what is common in a and b)\n    \"\"\"\n    a, b = set(a), set(b)\n    return (a-b, b-a, a.intersection(b))",
        "sha1": "db8e3674198c411355486062cde0258dbaecc5fb",
        "id": 164394
    },
    {
        "content": "def joinPathSplit(pathSplit):\n    \"\"\"\n    Join the pathSplit with '/'\n    \"\"\"\n    return \"/\".join(pathSplit)",
        "sha1": "a164693547533534b8d23cec20e592cfe1d865b5",
        "id": 457298
    },
    {
        "content": "def spike_height_experimental(trending_score, x, x_old, time_boost=1.0):\n    \"\"\"\n    Compute the size of a trending spike.\n    \"\"\"\n\n    # Magnitude of trending spike\n    mag = abs(x - x_old)**0.95\n\n    # Sign of trending spike\n    sign = 1.0\n    if x < x_old:\n        sign = -1.0\n\n    alpha = 0.8\n    ell = 100.0\n    x_ = max(x, x_old)\n    mag *= ell**alpha*(x_ + ell)**(-alpha)\n\n    return time_boost*sign*mag",
        "sha1": "9dd2b8add3fdffb2a12b2973db7e6a119b5495fa",
        "id": 549001
    },
    {
        "content": "def resolve_import_alias(name, import_names):\n    \"\"\"Resolve a name from an aliased import to its original name.\n\n    :param name: The potentially aliased name to resolve.\n    :type name: str\n    :param import_names: The pairs of original names and aliases\n        from the import.\n    :type import_names: iterable(tuple(str, str or None))\n\n    :returns: The original name.\n    :rtype: str\n    \"\"\"\n    resolved_name = name\n\n    for import_name, imported_as in import_names:\n        if import_name == name:\n            break\n        if imported_as == name:\n            resolved_name = import_name\n            break\n\n    return resolved_name",
        "sha1": "645133762b28960ca7881435bbfba6982974f2e7",
        "id": 596886
    },
    {
        "content": "def _get_reaction_key(functional_groups):\n    \"\"\"\n    Return a key for :data:`._reactions`\n\n    Parameters\n    ----------\n    functional_groups : :class:`iterable`\n        An :class:`iterable` of :class:`.GenericFunctionalGroup`.\n        The correct reaction must be selected for these functional\n        groups.\n\n    Returns\n    -------\n    :class:`.frozenset`\n        A key for :data:`_reactions`, which maps to the correct\n        reaction.\n\n    \"\"\"\n\n    return frozenset(\n        functional_group.get_num_bonders()\n        for functional_group in functional_groups\n    )",
        "sha1": "0dd855b6067830554332b5653410ed9940c0cd72",
        "id": 258824
    },
    {
        "content": "def intersect_interval(interval1, interval2):\n  \"\"\"Computes the intersection of two intervals.\n\n  Parameters\n  ----------\n  interval1: tuple[int]\n    Should be `(x1_min, x1_max)`\n  interval2: tuple[int]\n    Should be `(x2_min, x2_max)`\n\n  Returns\n  -------\n  x_intersect: tuple[int]\n    Should be the intersection. If the intersection is empty returns\n    `(0, 0)` to represent the empty set. Otherwise is `(max(x1_min,\n    x2_min), min(x1_max, x2_max))`.\n  \"\"\"\n  x1_min, x1_max = interval1\n  x2_min, x2_max = interval2\n  if x1_max < x2_min:\n    # If interval1 < interval2 entirely\n    return (0, 0)\n  elif x2_max < x1_min:\n    # If interval2 < interval1 entirely\n    return (0, 0)\n  x_min = max(x1_min, x2_min)\n  x_max = min(x1_max, x2_max)\n  return (x_min, x_max)",
        "sha1": "5db8daefa083b680c89a970224e2fc67a07beb5e",
        "id": 37928
    },
    {
        "content": "def calculate_row_checksum(row):\n    \"\"\"Form the row checksum as the difference between\n    the largest and the smallest row items\"\"\"\n    sorted_row = sorted(row)[0]\n    smallest, largest = sorted_row, sorted(row)[-1]\n    return largest - smallest",
        "sha1": "b2db5cbdaf3b9f3e3c765072290418c0e6896297",
        "id": 350678
    },
    {
        "content": "def _centimeters_to_meters(length):\n    \"\"\"Convert length from centimeters to meters\"\"\"\n    return length / 100.0",
        "sha1": "4fae8d118389e93aab231118d64576074c64df7a",
        "id": 185908
    },
    {
        "content": "import torch\n\n\ndef hard_dice(input_, target, threshold=0.5, reduction='mean', epsilon=1e-8):\n    \"\"\"\n    Hard dice score coefficient after thresholding.\n\n    Arguments:\n        preds (torch tensor): raw probability outputs\n        targets (torch tensor): ground truth\n        threshold (float): threshold value, default: 0.5\n        reduction (string): one of 'none', 'mean' or 'sum'\n        epsilon (float): epsilon for numerical stability, default: 1e-8\n\n    Returns:\n        dice (torch tensor): hard dice score coefficient\n    \"\"\"\n    if not input_.shape == target.shape:\n        raise ValueError\n\n    # if not (input_.max() <= 1.0 and input_.min() >= 0.0):\n    #     raise ValueError\n\n    if not ((target.max() == 1.0 and target.min() == 0.0 and(target.unique().numel() == 2)) \n        or (target.max() == 0.0 and target.min() == 0.0 and(target.unique().numel() == 1))):\n        raise ValueError\n\n    input_threshed = input_.clone()\n    input_threshed[input_ < threshold] = 0.0\n    input_threshed[input_ >= threshold] = 1.0\n\n    intesection = torch.sum(input_threshed * target, dim=-1)\n    input_norm = torch.sum(input_threshed, dim=-1)\n    target_norm = torch.sum(target, dim=-1)\n    dice = torch.div(2.0 * intesection + epsilon,\n                     input_norm + target_norm + epsilon)\n\n    if reduction == 'none':\n        pass\n    elif reduction == 'mean':\n        dice = torch.mean(dice)\n    elif reduction == 'sum':\n        dice = torch.sum(dice)\n    else:\n        raise NotImplementedError\n\n    return dice",
        "sha1": "4a617852a5f96a895d56e1f8149e5f64c05bd0c3",
        "id": 124026
    },
    {
        "content": "def get_stepsize(traj, min_points=200):\n    \"\"\"Returns a stepsize to display at least min_points data points on the graph\"\"\"\n    n = traj.n_frames\n    if n <= min_points:\n        return 1\n    else:\n        return n // min_points",
        "sha1": "fd3b372970f6af63ef3153531abad37af6e9a117",
        "id": 55102
    },
    {
        "content": "from typing import List\nimport inspect\n\n\ndef _get_failing_lines(code, lineno: int) -> List[str]:\n    \"\"\"Get list of strings (lines of code) from lineno to lineno+3.\n\n    Ideally we'd return the exact line where the error took place, but there\n    are reasons why this is not possible without a lot of work, including\n    playing with the AST. So for now we're returning 3 lines near where\n    the error took place.\n    \"\"\"\n    source_lines, source_lineno = inspect.getsourcelines(code)\n\n    start = lineno - source_lineno\n    end = min(start + 3, len(source_lines))\n    lines = source_lines[start:end]\n\n    return lines",
        "sha1": "9256aefe6fcb950cd0b4bf822dcb3c306ac17023",
        "id": 636287
    },
    {
        "content": "def get_video_type(link):\n    \"\"\" Takes a url and decides if it's Vimeo or YouTube.\n    Returns None for unkown types. \"\"\"\n    if 'vimeo.com/' in link:\n        return 'vimeo'\n    elif 'youtube.com/' in link or 'youtu.be' in link:\n        return 'youtube'\n    return None",
        "sha1": "a6f514c9eeae211490d61b5aa1cc635177b28809",
        "id": 43923
    },
    {
        "content": "import itertools\n\n\ndef get_consensus(sets, quorum):\n    \"\"\"\n    Given an iterable of sets of items, find the set containing all items\n    which appear in at least the quorum number of sets.\n\n    Parameters\n    ----------\n    sets : iterable\n        Iterable of sets of items.\n\n    quorum : integer\n        Minimum number of sets an item must appear in.\n\n    Returns\n    -------\n    set\n        Set containing all items which appear at least the quorum number of sets.\n    \"\"\"\n    if quorum >= 1:\n\n        intersections = []\n        for i in range(quorum, len(sets)):\n            intersections += [set.intersection(*items) for items in itertools.combinations(sets, i)]\n\n        if len(intersections) > 0:\n            return set.union(*intersections)\n    return {}",
        "sha1": "497be38abc832117e3fae35d0508afabe13b0a79",
        "id": 515916
    },
    {
        "content": "def check_if_string_in_file(file_name, string_to_search):\n    \"\"\" Check if any line in the file contains given string \"\"\"\n    # Open the file in read only mode\n    with open(file_name, 'r') as read_obj:\n        # Read all lines in the file one by one\n        for line in read_obj:\n            # For each line, check if line contains the string\n            if string_to_search in line:\n                return True\n    return False",
        "sha1": "602d0d9a4e791aa29abde4ff9f50b57036366643",
        "id": 471912
    },
    {
        "content": "import torch\n\n\ndef get_elements(src, string_idx, dim=0):\n    \"\"\"Get slices of elements from tensor src.\n\n    Args:\n        string_idx: has the same convention as index-select with Python List. E.g.\n            '4'  -> src[4]\n            ':3' -> src[1, 2, 3]\n            '2:' -> src[2, 3, ... max_t]\n            '2:4', dim=1 -> src[:, [2,3,4]]\n            '1:4+6:8', dim=1 -> src[:, [1,2,3,6,7]]\n    \"\"\"\n    if \"+\" in string_idx:\n        # If there is multiple slices concatenated by \"+\":\n        string_idx_split = string_idx.split(\"+\")\n        List = []\n        for idx in string_idx_split:\n            List.append(get_elements(src, idx, dim=dim))\n        List = torch.cat(List, dim=dim)\n        return List\n\n    if string_idx == \"\":\n        return src\n    elif \":\" not in string_idx:\n        idx = eval(string_idx)\n        if dim == 0:\n            if idx < 0:\n                idx += len(src)\n            return src[idx:idx+1]\n        elif dim == 1:\n            if idx < 0:\n                idx += src.shape[1]\n            return src[:, idx:idx+1]\n        else:\n            raise\n    else:\n        if string_idx.startswith(\":\"):\n            if dim == 0:\n                return src[:eval(string_idx[1:])]\n            elif dim == 1:\n                return src[:, :eval(string_idx[1:])]\n            else:\n                raise\n        elif string_idx.endswith(\":\"):\n            if dim == 0:\n                return src[eval(string_idx[:-1]):]\n            elif dim == 1:\n                return src[:, eval(string_idx[:-1]):]\n            else:\n                raise\n        else:\n            string_idx_split = string_idx.split(\":\")\n            assert len(string_idx_split) == 2\n            if dim == 0:\n                return src[eval(string_idx_split[0]): eval(string_idx_split[1])]\n            elif dim == 1:\n                return src[:, eval(string_idx_split[0]): eval(string_idx_split[1])]\n            else:\n                raise",
        "sha1": "52f235b1f5fc187701643be27a983ef48f531150",
        "id": 479293
    },
    {
        "content": "def type_(printer, ast):\n    \"\"\"Prints \"[const|meta|...] type\".\"\"\"\n    prefixes_str = ''.join(map(lambda prefix: f'{prefix} ', ast[\"prefixes\"]))\n    type_id_str = printer.ast_to_string(ast[\"typeId\"])\n    return f'{prefixes_str}{type_id_str}'",
        "sha1": "cdf03cfb3ff0a00fa973aa4eaf7a32cb9b822191",
        "id": 701210
    },
    {
        "content": "import six\n\n\ndef to_bed12(f, db, child_type='exon', name_field='ID'):\n    \"\"\"\n    Given a top-level feature (e.g., transcript), construct a BED12 entry\n\n    Parameters\n    ----------\n    f : Feature object or string\n        This is the top-level feature represented by one BED12 line.  For\n        a canonical GFF or GTF, this will generally be a transcript.\n\n    db : a FeatureDB object\n        This is need to get the children for the feature\n\n    child_type : str\n        Featuretypes that will be represented by the BED12 \"blocks\".  Typically\n        \"exon\".\n\n    name_field : str\n        Attribute to be used in the \"name\" field of the BED12 entry.  Usually\n        \"ID\" for GFF; \"transcript_id\" for GTF.\n    \"\"\"\n    if isinstance(f, six.string_types):\n        f = db[f]\n    children = list(db.children(f, featuretype=child_type, order_by='start'))\n    sizes = [len(i) for i in children]\n    starts = [i.start - f.start for i in children]\n    fields = [\n        f.chrom,\n        f.start - 1,  # GTF -> BED coord system\n        f.stop,\n        f.attributes.get(name_field, ['.'])[0],\n        f.score,\n        f.strand,\n        f.start,\n        f.stop,\n        '0,0,0',\n        len(children),\n        ','.join(map(str, sizes)),\n        ','.join(map(str, starts))\n    ]\n    return '\\t'.join(map(str, fields)) + '\\n'",
        "sha1": "d4042b979840b0eb41d23690182a030278473b1b",
        "id": 615371
    },
    {
        "content": "def test_browser(request):\n    \"\"\" :returns Browser.NAME from --browser option \"\"\"\n    return request.config.getoption('--browser')",
        "sha1": "1ef394e63fc731216cd39dd957a4646ccd87c8da",
        "id": 275675
    },
    {
        "content": "import re\n\n\ndef find_images(document):\n    \"\"\"Returns the list of image filepaths used by the `document`.\"\"\"\n    images = []\n    for line in document:\n        match = re.match(r\"\\.\\. image::\\s+(img\\/.+)\", line)\n        if match:\n            images.append(match[1])\n    return list(set(images))",
        "sha1": "2c58b04974f5ec0d1752cb405cfd314de81a841c",
        "id": 698481
    },
    {
        "content": "def get_ancestors(parent_index, node):\n    \"\"\"\n    Return the ancestors of a node on the neuron toward the root. \n    \n    Parameters:\n    -----------\n    parent_index: numpy array\n        the parent index of the nodes\n        \n    nodes: int\n        Index of the node\n    \n    Returns:\n    --------\n    ancestors: list\n        the list of ancestors of the node.\n        \n    \"\"\"\n\n    ancestors = []\n    ancestors.append(node)\n    par = node\n    while par!=0:\n        par = parent_index[par]\n        ancestors.append(par) \n    return ancestors",
        "sha1": "7ad115af484c17a4e5a20983e5e1f3e0c519aba3",
        "id": 145207
    },
    {
        "content": "def load_h5(h5f):\n    \"\"\"\n    Load fiberbundles configurations from a hdf5 class\n\n    Parameters\n    ----------\n    h5f : hdf5 class\n        h5-file or group object\n\n    Returns\n    -------\n    res : list(list(fiber)), fibers are (n,4)-arrays with (x,y,z,radii) for each fiber point\n    \"\"\"\n\n    fiber_bundles = []\n    fb_list = list(map(int, list(h5f.keys())))\n    fb_list.sort()\n    for fb in fb_list:\n        fiber_bundles.append([])\n        f_list = list(map(int, list(h5f[str(fb)].keys())))\n        f_list.sort()\n        for f in f_list:\n            fiber_bundles[-1].append(h5f[str(fb)][str(f)][:].astype(float))\n\n    return fiber_bundles",
        "sha1": "47487b43ae375c5ade27c82ec083570ee9655e27",
        "id": 11195
    },
    {
        "content": "import importlib\n\n\ndef requires_module(module):\n    \"\"\"\n    Decorator that takes a module name as an argument and tries to import it.\n    If the module imports without issue, the function is returned, but if not,\n    a null function is returned. This is so tests that depend on certain modules\n    being imported will not fail if the module is not installed on the testing\n    platform.\n    \"\"\"\n    def ffalse(func):\n        return lambda: None\n    def ftrue(func):\n        return func\n    try:\n        importlib.import_module(module)\n    except ImportError:\n        return ffalse\n    else:\n        return ftrue",
        "sha1": "6fec997e82f0acadca0fb4a7e5215ece17857180",
        "id": 614358
    },
    {
        "content": "import typing\n\n\ndef parse_number_string(obj: str) -> typing.Union[int, float]:\n    \"\"\"Parse a string as number. First tries to parse as `int`, then as `float`. Throws a ValueError\n    if `obj` is not a string.\n\n    Args:\n        obj : the string to parse\n    \"\"\"\n    if not isinstance(obj, str):\n        raise ValueError()\n    try:\n        return int(obj)\n    except ValueError:\n        return float(obj)",
        "sha1": "051475f144daaa8105e8b1b343286182dc5bbb74",
        "id": 299195
    },
    {
        "content": "def periodize_filter_fourier(h_f, nperiods=1, aggregation='sum'):\n    \"\"\"\n    Computes a periodization of a filter provided in the Fourier domain.\n    Parameters\n    ----------\n    h_f : array_like\n        complex numpy array of shape (N*n_periods,)\n    n_periods: int, optional\n        Number of periods which should be used to periodize\n    aggregation: str['sum', 'mean'], optional\n        'sum' will multiply subsampled time-domain signal by subsampling\n        factor to conserve energy during scattering (rather not double-account\n        for it since we already subsample after convolving).\n        'mean' will only subsample the input.\n\n    Returns\n    -------\n    v_f : array_like\n        complex numpy array of size (N,), which is a periodization of\n        h_f as described in the formula:\n        v_f[k] = sum_{i=0}^{n_periods - 1} h_f[i * N + k]\n\n    References\n    ----------\n    This is a modification of\n    https://github.com/kymatio/kymatio/blob/master/kymatio/scattering1d/\n    filter_bank.py\n    Kymatio, (C) 2018-present. The Kymatio developers.\n    \"\"\"\n    N = h_f.shape[0] // nperiods\n    h_f_re = h_f.reshape(nperiods, N)\n    v_f = (h_f_re.sum(axis=0) if aggregation == 'sum' else\n           h_f_re.mean(axis=0))\n    v_f = v_f if h_f.ndim == 1 else v_f[:, None]  # preserve dim\n    return v_f",
        "sha1": "f9a2845dcf40eedce9d46faba506f1c1358ce6a5",
        "id": 703511
    },
    {
        "content": "def getModelFromData(data):\n    \"\"\"\n    get translation model from data - data consists of sets of point pairs (p,p')\n    p' = p + (tx,ty)\n    p = (x,y)\n    p' = (x',y')\n    the translation model is just [tx,ty]\n    a translation matrix T would be [[1 0 tx] [0 1 ty]]\n    \"\"\"\n    sumdeltax = 0\n    sumdeltay = 0\n    if data is None: return None\n    for pair in data:\n        p = pair[0]\n        pprime = pair[1]\n        x = p[0]\n        y = p[1]\n        xprime = pprime[0]\n        yprime = pprime[1]\n        deltax = xprime-x\n        deltay = yprime-y\n        sumdeltax += deltax\n        sumdeltay += deltay\n    npairs = len(data)\n    avgdeltax = float(sumdeltax) / npairs\n    avgdeltay = float(sumdeltay) / npairs\n    tx = avgdeltax\n    ty = avgdeltay\n    model = [tx, ty]\n    return model",
        "sha1": "d9268bb3a6a542edc2193a79422ccf79695e5f75",
        "id": 275443
    },
    {
        "content": "def scheduler_info_widget(scheduler_infos):\n    \"\"\"\n    Generates the HTML output exposing the scheduler infos. This output can be\n    consumed and exposed by CloudWatch Custom Widget.\n    \n    Params:\n        scheduler_infos (dict): the parameters to display in the widget\n    \"\"\"\n    header = '<table>\\n'\n    footer = '</table>\\n'\n    \n    rows = '<tbody>'\n    for key, value in scheduler_infos.items():\n        rows += '<tr>'\n        rows += f'<th>{key}</th>'\n        rows += f'<td>{value}</td>'\n        rows += '</tr>'\n    rows += '</tbody>\\n'\n    \n    return header + rows + footer",
        "sha1": "8559f733f27eb8f8fb51b7c4a55334fdc0656590",
        "id": 613323
    },
    {
        "content": "def buildDictionary(message):\n    \"\"\"\n    counts the occurrence of every symbol in the message and store it in a python dictionary\n      parameter:\n        message: input message string\n      return:\n        python dictionary, key = symbol, value = occurrence\n    \"\"\"\n    _dict = dict()\n    for c in message:\n        if c not in _dict.keys():\n            _dict[c] = 1\n        else:\n            _dict[c] += 1\n    return _dict",
        "sha1": "71b196aaccfb47606ac12242585af4ea2554a983",
        "id": 6348
    },
    {
        "content": "def get_all_species_alive_area(species_list, area):\n    \"\"\"Get all names of species that are alive from a certain area.\n\n    Parameters\n    ----------\n    species_list : list of strings\n        List of all species.\n\n    area : str\n        name of area for which the species should be returned.\n\n    Returns\n    -------\n    numb_total_population_alive : list of strings\n        List of all species excluding dead species from the specified area.\n    \"\"\"\n\n    total_population_alive = [\n        x for x in species_list.keys() if (\"dead\" not in x) and (area in x)\n    ]\n    numb_total_population_alive = \"+\".join(total_population_alive)\n    return numb_total_population_alive",
        "sha1": "0d49c9ec0968baa18a67e5aa8f79c36795fba05d",
        "id": 504193
    },
    {
        "content": "def nbr2(b11, b12):\n    \"\"\"\n    Normalized Burn Ratio-2 (Garc\u00eda and Caselles, 1991).\n\n    .. math:: NBR2 = (b11 - b12) / (b11 + b12)\n\n    :param b11: SWIR 1.\n    :type b11: numpy.ndarray or float\n    :param b12: SWIR 2.\n    :type b12: numpy.ndarray or float\n\n    :returns NBR2: Index value\n\n    .. Tip::\n\n        Garc\u00eda, M. J. L., Caselles, V. 1991. Mapping burns and natural \\\n        reforestation using thematic Mapper data. Geocarto International \\\n        6(1), 31-37. doi:10.1080/10106049109354290.\n    \"\"\"\n\n    NBR2 = (b11 - b12) / (b11 + b12)\n    return NBR2",
        "sha1": "8a7f6aeb3da76dc1e48c7631511c4cf3b061e149",
        "id": 186153
    },
    {
        "content": "def mps_to_mph(mps: int|float) -> float:\n    \"\"\"Convert meters per second to miles per hour\"\"\"\n    mph = round(mps * 2.237, 2)\n    return mph",
        "sha1": "8cdb94b371ac6c5a44d2769a91b2f0d61fdf6d5e",
        "id": 269565
    },
    {
        "content": "def expandtabs(self, tabsize=8):\n    \"\"\"\n    S.expandtabs(tabsize=8) -> str\n\n    Return a copy of S where all tab characters are expanded using spaces.\n    If tabsize is not given, a tab size of 8 characters is assumed.\n    \"\"\"\n    return self.replace(\"\\t\", \" \" * tabsize)",
        "sha1": "874117af75a017f08ab188a8ee471a6b708f21b7",
        "id": 290229
    },
    {
        "content": "def returnInput(*args):\n    \"\"\"\n    This is a function that returns exactly what is passed to it. Useful to hold a place in the\n    dataflow without modifying the data.\n    \"\"\"\n    if len(args) == 1:\n        # A single argument has been passed and is now packaged inside a tuple because of *.\n        # Unpack it:\n        args = args[0]\n    return args",
        "sha1": "baa4e437bcec3de3ed354ba39c7e1d48806b8f29",
        "id": 468415
    },
    {
        "content": "def get_seqid_to_species(alignment):\n    \"\"\"Get the seqid to species dictionary:\n    \n    {\"aalo@TRINITY_DN123937_c0_g1_i1.p1\": \"aalo\"}\n    \"\"\"\n    return {sequence.name: sequence.name.split(\"@\")[0] for sequence in alignment}",
        "sha1": "b69b68a0750ecc28cebe7a36916bb602d745d5af",
        "id": 447310
    },
    {
        "content": "def create_order(v):\n    \"\"\"Identify all unique, positive indices and return them sorted.\"\"\"\n    flat_v = sum(v, [])\n    x = [i for i in flat_v if i > 0]\n    # Converting to a set and back removes duplicates\n    x = list(set(x))\n    return sorted(x)",
        "sha1": "7e1b29d87e5dd3760d48c9792209d169f48a245f",
        "id": 432280
    },
    {
        "content": "def normalize_image_data(image_data):\n    \"\"\"Normalizes the given image data\"\"\"\n    result = image_data.astype(float) / 255\n    return result",
        "sha1": "cd069fbabbf168cd4254cba9286bb7921ef90d8b",
        "id": 309742
    },
    {
        "content": "def get_position (pair, s_mention) :\n\n    \"\"\"Get the position of the antecedent : {beginning (0)  , middle(1), end (2)} of the sentence \"\"\"\n\n    antd_index=int(pair[0][-1])-1\n    if antd_index == 0 :\n        return 0\n    if antd_index == len(s_mention) -1 :\n        return 2\n    return 1",
        "sha1": "ee63d63e922c76d04ace23f9f59c8182c388ceab",
        "id": 279086
    },
    {
        "content": "def find_focusable(node):\n    \"\"\"\n    Search for the first focusable window within the node tree\n    \"\"\"\n\n    if not node.children:\n        return node\n\n    if node.focus:\n        return find_focusable(node.children_dict[node.focus[0]])",
        "sha1": "9776c1a9b3ac0e96eb1bdb9ca7ded20736d5bfe2",
        "id": 306046
    },
    {
        "content": "from typing import Tuple\n\n\ndef find_prefix(root, prefix: str) -> Tuple[bool, int]:\n    \"\"\"\n    Check and return \n      1. If the prefix exsists in any of the words we added so far\n      2. If yes then how may words actually have the prefix\n    \"\"\"\n    node = root\n    # If the root node has no children, then return False.\n    # Because it means we are trying to search in an empty trie\n    if not root.children:\n        return False, 0\n    for char in prefix:\n        char_not_found = True\n        # Search through all the children of the present `node`\n        for child in node.children:\n            if child.char == char:\n                # We found the char existing in the child.\n                char_not_found = False\n                # Assign node as the child containing the char and break\n                node = child\n                break\n        # Return False anyway when we did not find a char.\n        if char_not_found:\n            return False, 0\n    # Well, we are here means we have found the prefix. Return true to indicate that\n    # And also the counter of the last node. This indicates how many words have this\n    # prefix\n    return True, node.counter",
        "sha1": "aa8d7aec5c158e7fe0df3d31e73bfe53b183dedf",
        "id": 688474
    },
    {
        "content": "def get_trimmed_glyph_name(gname, num):\n    \"\"\"\n    Glyph names cannot have more than 31 characters.\n    See https://docs.microsoft.com/en-us/typography/opentype/spec/...\n               recom#39post39-table\n    Trims an input string and appends a number to it.\n    \"\"\"\n    suffix = '_{}'.format(num)\n    return gname[:31 - len(suffix)] + suffix",
        "sha1": "a5e90163d15bd4fc0b315414fffd2ac227768ab0",
        "id": 709976
    },
    {
        "content": "def calc(kbps, burst_time=1.5):\n    \"\"\"Calculate Normal and Max Burst values\"\"\"\n    bps = kbps * 1000\n    burst_normal = int(round(bps / 8 * burst_time, 0))\n    burst_max = 2 * burst_normal\n\n    return (burst_normal, burst_max)",
        "sha1": "7476e4516e6ebe65d45f95511ba7caa0d5223d07",
        "id": 220851
    },
    {
        "content": "import re\n\n\ndef parse_remote_url(url):\n    \"\"\"Extract user, repo from URL.\"\"\"\n    repo_urls = [\n        'https://gitlab.audeering.com/',\n        'http://gitlab.audeering.local/',\n        'http://gitlab2.audeering.local/',\n        'git@srv-app-01.audeering.local:',\n        r'^.*@gitlab.audeering.com/',\n        'https://github.com/',\n        'git@github.com:',\n    ]\n    for repo_url in repo_urls:\n        url = re.sub(repo_url, '', url)\n    url_parts = url.split('/')\n    repo = url_parts[-1]\n    user = '/'.join(url_parts[:-1])\n    if repo.endswith('.git'):\n        repo = repo[:-4]\n    return (user, repo)",
        "sha1": "821a212fa43af01b956fbe226c388cb0b86ab71e",
        "id": 558655
    },
    {
        "content": "def get_D(uv_hat, n_channels):\n    \"\"\"Compute the rank 1 dictionary associated with the given uv\n\n    Parameter\n    ---------\n    uv: array (n_atoms, n_channels + n_times_atom)\n    n_channels: int\n        number of channels in the original multivariate series\n\n    Return\n    ------\n    D: array (n_atoms, n_channels, n_times_atom)\n    \"\"\"\n\n    return uv_hat[:, :n_channels, None] * uv_hat[:, None, n_channels:]",
        "sha1": "0c6518d119ace13179d86a1f7ff832245c4501c5",
        "id": 589161
    },
    {
        "content": "import importlib\n\n\ndef attempt_import(module, error_message):\n    \"\"\"Attempt import of a dependency\n\n    If the dependency is not available, raise a RuntimeError with the appropriate message\n\n    Parameters\n    ----------\n    module : string\n        the module name\n    error_message : string\n        the error to raise if the module is not importable\n\n    Returns\n    -------\n    mod : ModuleType\n        the imported module\n\n    Raises\n    ------\n    RuntimeError\n    \"\"\"\n    try:\n        return importlib.import_module(module)\n    except ImportError:\n        raise RuntimeError(error_message)",
        "sha1": "bfad50ec311f68a39ee10a49a81971670a7313df",
        "id": 148941
    },
    {
        "content": "def sanitize_file_path_for_shell(file_path):\n    \"\"\"\n    Replace characters that are ok for the filesystem but have special meaning in the shell.\n    It is assumed file_path is already passed in double quotes.\n    \"\"\"\n    file_path_sanitized = file_path.replace('\\\\', '\\\\\\\\')\n    file_path_sanitized = file_path_sanitized.replace('$', '\\\\$')\n    file_path_sanitized = file_path_sanitized.replace('\"', '\\\\\"')\n    file_path_sanitized = file_path_sanitized.replace('`', '\\\\`')\n    return file_path_sanitized",
        "sha1": "10ce60423a1cc279256f30d34205978cbf9a9f9d",
        "id": 518954
    },
    {
        "content": "import string\n\n\ndef deriveArtistFromName(name):\n    \"\"\"\n    Ensure that the given name doesnt have usual suspects like \"Featuring\" or \"Ft\" or \" with\" to attempt to return just\n    the artist name\n\n    :param name: str\n    :return: str\n    \"\"\"\n    if not name:\n        return name\n    removeParts = [\" ft. \", \" ft \", \" feat \", \" feat. \"]\n    for removePart in removeParts:\n        i = name.lower().find(removePart)\n        if i > -1:\n            name = name[:i]\n    return string.capwords(name)",
        "sha1": "5e47083819ceb3a1ec635dc53435ac13cfe61e74",
        "id": 621068
    },
    {
        "content": "import torch\n\n\ndef get_window(name, window_length, squared=False):\n    \"\"\"\n    Returns a windowing function.\n\n    Arguments:\n        window (str): name of the window, currently only 'hann' is available\n        window_length (int): length of the window\n        squared (bool): if true, square the window\n\n    Returns:\n        torch.FloatTensor: window of size `window_length`\n    \"\"\"\n    if name == \"hann\":\n        window = torch.hann_window(window_length)\n    else:\n        raise ValueError(\"Invalid window name {}\".format(name))\n    if squared:\n        window *= window\n    return window",
        "sha1": "f5cca82ae8983f0297fac601bc563d6f89d6af6e",
        "id": 628724
    },
    {
        "content": "def has_lat_lon(row):\n    \"\"\"No point in processing this row if it doesn't have coordinates\"\"\"\n    clean = {}\n    for k, v in row.items():\n        clean[k.lower()] = v # Some sheets have capital Ls, some have lower case.\n    if 'latitude' in clean.keys() and 'longitude' in clean.keys():\n        return clean['latitude'] and clean['longitude']\n    return False",
        "sha1": "cf97929f6a5b5d670fb34c67f6dbe2215d0728e7",
        "id": 502624
    },
    {
        "content": "from typing import Callable\nfrom typing import List\nfrom typing import Tuple\nfrom typing import Dict\n\n\ndef convert_schema(func: Callable, schema: List[Tuple[str, str]]) -> Dict[str, str]:\n    \"\"\"\n    Convert schema in the format of {\"col name\": \"bigint\", \"col2 name\": \"int\"}\n    applying some data types conversion function (e.g. spark2redshift)\n\n    :param func: Conversion Function (e.g. spark2redshift)\n    :param schema: Schema (e.g. {\"col name\": \"bigint\", \"col2 name\": \"int\"})\n    :return: Converted schema\n    \"\"\"\n    return {name: func(dtype) for name, dtype in schema}",
        "sha1": "68f219e8ac2eec6aa04fa354f55dc61a20b62654",
        "id": 481635
    },
    {
        "content": "def prepare_opts(opts):\n    \"\"\"\n    >>> opts = {'<module>': 'Blog'}\n    >>> prepare_opts(opts)\n    {'<module>': 'Blog', '--model': 'Blog'}\n    \"\"\"\n    rv = opts.copy()\n    rv.update({\n        '--model': opts.get('<module>')\n    })\n    return rv",
        "sha1": "c27c0dd2f091e70b1dbb3d69c29c806d8cfa4fac",
        "id": 92937
    },
    {
        "content": "import math\n\n\ndef _guess_alpha_from_vol_atm(underlying, vol_atm, beta):\n    \"\"\"_guess_alpha_from_vol_atm_\n    guess alpha from volatility at the money\n    by using Hagan's approximated atm implied volatility.\n\n    .. math::\n        \\ln\\sigma(F, F)\n            \\\\approx \\ln \\\\alpha - (1 - \\\\beta) \\ln F\n\n    :param float underlying: must be positive.\n    :param float vol_atm: volatility at the money. This must be positive.\n    :param float beta:\n\n    :return: alpha.\n    :rtype: float.\n    \"\"\"\n    assert(underlying > 0.0)\n    assert(vol_atm > 0.0)\n    ln_underlying = math.log(underlying)\n    ln_vol_atm = math.log(vol_atm)\n    one_minus_beta = 1.0 - beta\n    ln_alpha = ln_vol_atm + one_minus_beta * ln_underlying\n    return math.exp(ln_alpha)",
        "sha1": "fdc7989fdf7cdf6ec21b80585e6cee3cadeee8f7",
        "id": 361880
    },
    {
        "content": "import torch\n\n\ndef compute_errors(gt, pred):\n    \"\"\"\n    Parameters\n    ----------\n    gt : torch.Tensor\n        shape [batch, h, w]\n    pred : torch.Tensor\n        shape [batch, h, w]\n\n    Return\n    ------\n    measures : dict\n    \"\"\"\n    safe_log = lambda x: torch.log(torch.clamp(x, 1e-6, 1e6))\n    safe_log10 = lambda x: torch.log(torch.clamp(x, 1e-6, 1e6))\n    batch_size = pred.shape[0]\n    mask = gt > 0\n    gt = gt[mask]\n    pred = pred[mask]\n    thresh = torch.max(gt / pred, pred / gt)\n    a1 = (thresh < 1.25).float().mean() * batch_size\n    a2 = (thresh < 1.25 ** 2).float().mean() * batch_size\n    a3 = (thresh < 1.25 ** 3).float().mean() * batch_size\n\n    rmse = ((gt - pred) ** 2).mean().sqrt() * batch_size\n    rmse_log = ((safe_log(gt) - safe_log(pred))** 2).mean().sqrt() * batch_size\n    log10 = (safe_log10(gt) - safe_log10(pred)).abs().mean() * batch_size\n    abs_rel = ((gt - pred).abs() / gt).mean() * batch_size\n    sq_rel = ((gt - pred)**2 / gt).mean() * batch_size\n    measures = {'a1': a1, 'a2': a2, 'a3': a3, 'rmse': rmse,\n                'rmse_log': rmse_log, 'log10': log10, 'abs_rel': abs_rel, 'sq_rel': sq_rel}\n    return measures",
        "sha1": "6a1abff0ee6ca9630d633769ffa30c585594dffb",
        "id": 167228
    },
    {
        "content": "import networkx\n\n\ndef project_down(B,create_using=None,name=None):\n    \"\"\"Project a bipartite graph B down onto its \"bottom nodes\".\n\n    The nodes retain their names and are connected if they\n    share a common top node in the bipartite graph.\n\n    Returns a Graph.\n    \"\"\"\n    if create_using:\n        G=create_using\n        G.clear()\n    else:\n        G=networkx.Graph()\n    if name is not None:\n        G.name=name\n\n    for v,Bvnbrs in B.adjacency_iter():\n       if B.node_type[v]==\"Bottom\":\n          G.add_node(v)\n          for cv in Bvnbrs:\n             G.add_edges_from([(v,u) for u in B[cv] if u!=v])\n    return G",
        "sha1": "8d8d38e3058ba758f6652cd820d09aa40fb939b3",
        "id": 319614
    },
    {
        "content": "import math\n\n\ndef lin_parallaxE(ups, lil_h, R, P):\n    \"\"\"\n    Eq. 4.12\n    Calculates the linearized expression for parallax error as a function of\n    the SSP distance angle\n\n    Parameters\n    ------------\n    ups : float\n        The SSP distance angle, in degrees\n\n    lil_h : int/float\n        The cloud's altitude above the Earth's surface, in km\n\n    R : int\n        Earth's radius, in km\n\n    P : float\n        Satellite projection parameter\n\n    Returns\n    ------------\n    delta_ups : float\n        Angular parallex error, in degrees\n    \"\"\"\n    num = P * math.sin(math.radians(ups))\n    denom = P * math.cos(math.radians(ups)) - 1\n    delta_ups = (num / denom) * (lil_h / R)\n    return delta_ups",
        "sha1": "c1445da1065b8100cdcfe684cad7d1260bca7a06",
        "id": 368713
    },
    {
        "content": "def expand_list(_list):\n    \"\"\"\n    Convert list of '.' separated values to a nested dict.\n\n    Taken from SO article which I now can't find, this will take a list\n    and return a dictionary which contains an empty dict as each leaf\n    node.\n\n        >>> expand_list(['a', 'b.c'])\n        {\n            'a': {},\n            'b': {\n                'c': {}\n            }\n        }\n\n    \"\"\"\n    assert isinstance(_list, list), \"arg must be a list\"\n    tree = {}\n    for item in _list:\n        t = tree\n        for part in item.split('.'):\n            t = t.setdefault(part, {})\n    return tree",
        "sha1": "b7040d9274e47be1335220f6cf0b387100125287",
        "id": 600934
    },
    {
        "content": "import math\n\n\ndef dist2d(point1, point2):\n    \"\"\"\n    Euclidean distance between two points\n    :param point1:\n    :param point2:\n    :return:\n    \"\"\"\n\n    x1, y1 = point1[0:2]\n    x2, y2 = point2[0:2]\n\n    dist2 = (x1 - x2)**2 + (y1 - y2)**2\n\n    return math.sqrt(dist2)",
        "sha1": "0e25bc6a62c4866f6a353d73534bd5be311dc20d",
        "id": 414804
    },
    {
        "content": "def threshold_means(df, thresh_name, thresholds, comp_df=None, error_fac=1.0,\n                    use_percents=True):\n    \"\"\"Computes the means (and standard deviations) along a set of threshold\n        values. This is handy for doing the Threshold v.s. Robustness plots\n        when in comparison to DREA.\n\n    Args:\n        df (DataFrame): DataFrame of the data we want to plot. Often, this\n            needs to be filtered to be only one algorithm.\n        thresh_name (str): String representing the column name for thresholds\n        comp_df (DataFrame, optional): Data frame to compare to, percent wise.\n        error_fac (float, optional): Multiply error sizes by this number.\n            Particularly useful if we want to use confidence intervals. Default\n            is 1.0.\n        use_percents (float, optional): Return results in percents.\n\n    Returns:\n        Returns an object with properties:\n            robs -> returns a list of robustness means\n            robs_err -> returns list of robustness errors.\n            sends -> returns a list of send frequencies.\n            sends_err -> returns a list of errors of send frequencies.\n            res -> returns a list of reschedule frequencies\n            res_err -> returns a list of reschedule frequencies errors\n            runtimes -> returns a list of runtimes.\n    \"\"\"\n    if comp_df is not None:\n        comp_rob = comp_df[\"robustness\"].mean()\n        comp_res = comp_df[\"reschedule_freq\"].mean()\n        comp_run = comp_df[\"runtime\"].mean()\n        comp_send = comp_df[\"send_freq\"].mean()\n    else:\n        comp_rob = 1.0\n        comp_res = 1.0\n        comp_run = 1.0\n        comp_send = 1.0\n\n    rob_means = []\n    stderrs = []\n    sends = []\n    sends_err = []\n    reschedules = []\n    reschedules_err = []\n    runtimes = []\n    runtimes_err = []\n\n    if use_percents:\n        p = 100\n    else:\n        p = 1\n\n    for t in thresholds:\n        point = df.loc[df[thresh_name] == t]\n        mean = point[\"robustness\"].mean() / comp_rob * p\n        rob_means.append(mean)\n        se = point[\"robustness\"].sem() * p\n        stderrs.append(se * error_fac)\n        send_dat = point[\"send_freq\"].mean() / comp_send * p\n        sends.append(send_dat)\n        send_err = point[\"send_freq\"].sem() * p\n        sends_err.append(send_err * error_fac)\n        res = point[\"reschedule_freq\"].mean() / comp_res * p\n        reschedules.append(res)\n        res_err = point[\"reschedule_freq\"].sem() * p\n        reschedules_err.append(res_err * error_fac)\n        runtime = point[\"runtime\"].mean() / comp_run * p\n        runtimes.append(runtime)\n        runtime_err = point[\"runtime\"].sem() * p\n        runtimes_err.append(runtime_err * error_fac)\n\n    class ThreshResponse(object):\n        def __init__(self, robs, robs_err, sends, sends_err, res, res_err,\n                     runtimes):\n            self.robs = robs\n            self.robs_err = robs_err\n            self.sends = sends\n            self.sends_err = sends_err\n            self.res = res\n            self.res_err = res_err\n            self.runtimes = runtimes\n            self.runtimes_err = runtimes_err\n\n    return ThreshResponse(rob_means, stderrs, sends, sends_err, reschedules,\n                          reschedules_err, runtimes)",
        "sha1": "99f7855f457a2aec71a1454ac08fb656745f84d1",
        "id": 11674
    },
    {
        "content": "def epochTime(startTime: int, endTime: int):\n    \"\"\"Calculate elapsed (elapsedMins, elapsedSecs) of epoch\"\"\"\n     \n    elapsedTime = endTime - startTime\n    elapsedMins = int(elapsedTime / 60)\n    elapsedSecs = int(elapsedTime - (elapsedMins * 60))\n    return elapsedMins, elapsedSecs",
        "sha1": "728fd74c84efef0f436fbbcbde2bb4e3da669729",
        "id": 611527
    },
    {
        "content": "def humanize_seconds(seconds):\n    \"\"\"Take n seconds and return a human readable string (1h 2m 13s)\"\"\"\n    m, s = divmod(seconds, 60)\n    h, m = divmod(m, 60)\n    chunks = []\n\n    # 2nd condition is because we want 0 seconds to be represented as \"0s\" not \"\"\n    if s or seconds == 0:\n        seconds_chunk = \"%ds\" % s\n        chunks.insert(0, seconds_chunk)\n    if m:\n        minutes_chunk = \"%dm\" % m\n        chunks.insert(0, minutes_chunk)\n    if h:\n        hours_chunk = \"%dh\" % h\n        chunks.insert(0, hours_chunk)\n\n    return \" \".join(chunks)",
        "sha1": "4605b485fa4a266573d5283ccbf5e85c41e9b621",
        "id": 338017
    },
    {
        "content": "import operator\n\n\ndef sort_dictionary_by_value(dictionary):\n    \"\"\"\n    DON'T CHANGE THIS FUNCTION. This function sorts a dictionary by its values in a\n    descending fashion.\n    \n    Parameters:\n        dictionary (dict): A dictionary to be sorted.\n    Returns:\n        dict: The sorted dictionary.\n    \"\"\"\n\n    desc_dictionary = dict(sorted(dictionary.items(), key=operator.itemgetter(1), reverse=True))\n\n    return desc_dictionary",
        "sha1": "0f14c4127d8c2ea8b70313b7b589f263190da1d3",
        "id": 271076
    },
    {
        "content": "def lowercase_count(strng: str) -> int:\n    \"\"\"Returns count of lowercase characters.\"\"\"\n    return sum(c.islower() for c in strng)",
        "sha1": "98aced868e5723a0dfdf1451306a88d4d5b64b53",
        "id": 173980
    },
    {
        "content": "import re\nimport requests\nimport collections\nimport pathlib\nimport json\n\n\ndef read_json(path):\n    \"\"\"\n    Read json from a path or URL.\n    \"\"\"\n    if re.match('^(http|ftp)s?://', path):\n        response = requests.get(path)\n        obj = response.json(object_pairs_hook=collections.OrderedDict)\n    else:\n        path = pathlib.Path(path)\n        with path.open(encoding='utf-8-sig') as read_file:\n            obj = json.load(read_file, object_pairs_hook=collections.OrderedDict)\n    return obj",
        "sha1": "36b143f78284b044c53929c77fa449e0d9582864",
        "id": 317442
    },
    {
        "content": "from typing import Any\nimport pickle\n\n\ndef read(\n        path: str) \\\n        -> Any:\n    \"\"\"\n    Unpickle file at `path`.\n    \"\"\"\n    with open(path, 'rb') as file:\n        data = pickle.load(file)\n    return data",
        "sha1": "0eac0c92d2ae2a7a0f8d8b5a12f0129f9d31e71d",
        "id": 110884
    },
    {
        "content": "def check_line_intersection(line1, line2):\n    \"\"\"Given two line segments (each defined by two (x,y) pairs), return true if the two segments intersect and false if they do not.\"\"\"\n    x1, y1 = line1[0]\n    x2, y2 = line1[1]\n    x3, y3 = line2[0]\n    x4, y4 = line2[1]\n    denom = (x1-x2)*(y3-y4) - (y1-y2)*(x3-x4)\n    if denom == 0:\n        return False\n    ua = ((x1*y2 - y1*x2) * (x3-x4) - (x1-x2) * (x3*y4 - y3*x4)) / denom\n    ub = ((x1*y2 - y1*x2) * (y3-y4) - (y1-y2) * (x3*y4 - y3*x4)) / denom\n    return 0 <= ua <= 1 and 0 <= ub <= 1",
        "sha1": "65472158505b832bb77f74eb1be244c6eef9ce2c",
        "id": 574379
    },
    {
        "content": "def build_own_answer_detection_prompt(question, answer):\n    \"\"\"Builds the prompt to check if the model can distinguish between own answers vs answers generated by someone else.\n\n    Args:\n        answer: String: the model should guess if the answer was generated by the model itself\n        question: String: the question, from which the aforementioned answer was generated\n    Returns:\n        String: the complete prompt that will be given to a model\n\n    \"\"\"\n    res = f\"\"\"\n    Some time ago, we asked you the following question:\\n\\n '{question}'\\n \n    Unfortunately, we mixed up the answers from several respondents, \n    and now we are not sure which of the answers is yours.\n    For example, we have the following answer:\n    \\n\\n '{answer}'\\n\n    Is it your answer? [yes/no]\n    \"\"\"\n    return res",
        "sha1": "513752d0ddb3dbd0bd467b678fd8e755bdb1411d",
        "id": 223218
    },
    {
        "content": "def _path_append(parent, child):\n    \"\"\"Utility function for joining paths, ensuring that forward slashes are always used regardless of OS.\"\"\"\n    parent = parent.rstrip('/')\n    child = child.lstrip('/')\n    return parent + '/' + child",
        "sha1": "7b3eb334b1664ef1f99e8b65646d7bfb90035739",
        "id": 363957
    },
    {
        "content": "def chop_at_blank(row):\n    \"\"\"Chop `row` off at its first empty element.\"\"\"\n    result = []\n    for item in row:\n        if item == '':\n            break\n        result.append(item)\n    return result",
        "sha1": "ce04243a2c32c054b09a424f409a1e7fea5a2dbb",
        "id": 247762
    },
    {
        "content": "def get_all_keys(tweet, parent_key=''):\n    \"\"\"\n    Takes a tweet object and recursively returns a list of all keys contained\n    in this level and all nexstted levels of the tweet.\n\n    Args:\n        tweet (Tweet): the tweet dict\n        parent_key (str): key from which this process will start, e.g., you can\n                          get keys only under some key that is not the top-level key.\n\n    Returns:\n        list of all keys in nested dicts.\n\n    Example:\n        >>> import tweet_parser.tweet_checking as tc\n        >>> tweet = {\"created_at\": 124125125125, \"text\": \"just setting up my twttr\",\n        ...          \"nested_field\": {\"nested_1\": \"field\", \"nested_2\": \"field2\"}}\n        >>> tc.get_all_keys(tweet)\n        ['created_at', 'text', 'nested_field nested_1', 'nested_field nested_2']\n    \"\"\"\n    items = []\n    for k, v in tweet.items():\n        new_key = parent_key + \" \" + k\n        if isinstance(v, dict):\n            items.extend(get_all_keys(v, parent_key=new_key))\n        else:\n            items.append(new_key.strip(\" \"))\n    return items",
        "sha1": "bad0767f62c10f79742402cbf1952bc95e2f121b",
        "id": 373326
    },
    {
        "content": "def create_data_column(name: str, array):\n    \"\"\"\n    Creates a data column by name\n\n    :param name: the name of the data column\n    :return: the data column\n    \"\"\"\n    col = array\n    col.SetName(name)\n    return col",
        "sha1": "44a0dea2e3a3554f08c37d305daff5d0147e17e7",
        "id": 449308
    },
    {
        "content": "def is_alt_genotype(record, name):\n    \"\"\"Return if the named sample has a non-reference genotype call.\"\"\"\n    sample = record.samples[name]\n    indices = sample.allele_indices\n    return not (not indices or None in indices or indices.count(0) == len(indices) or max(indices) >= len(record.alleles))",
        "sha1": "b1f38fb2fa5dd8c4b9b9940016a1988a3b2009ea",
        "id": 94408
    },
    {
        "content": "def _calcPeakRange(sr,n, loFreq=200., hiFreq=2000.):\n\t\"\"\"\n\t_calcPeakRange - utility function to calculate parameters for peaks\t\n\tinputs:\n\t\tsr - sample rate\n\t\tn  - fft length\n\t  loFreq - lowest peak frequency\n\t  hiFreq - highest peak frequency\n\t\"\"\"\n\tfftBase = sr / float(n)\n\tpkRange = n/2 + 1\n\tminPos = int(round(loFreq/fftBase))\n\tmaxPos = int(round(hiFreq/fftBase))\n\treturn pkRange, minPos, maxPos",
        "sha1": "280c685622ffb73d6225a15c9cb138ea525c8e00",
        "id": 562964
    },
    {
        "content": "def get_transform(client, asm=None, path=None, csys=None):\n    \"\"\"Get the 3D transform for a component in an assembly.\n\n    Args:\n        client (obj):\n            creopyson Client\n        asm (str, optional):\n            Assembly name. Defaults is currently active model.\n        path (list:int, optional):\n            Path to a component in the assembly.\n            Defaults is the transform is calculated for the assembly itself.\n        csys (str, optional):\n            Coordinate system on the component to calculate the transform for.\n            Defaults is the component's default coordinate system.\n\n    Returns:\n        (obj:JLTransform): The 3D transform from the assembly to\n        the component's coordinate system.\n\n    \"\"\"\n    data = {}\n    if asm:\n        data[\"asm\"] = asm\n    if path:\n        data[\"path\"] = path\n    if csys:\n        data[\"csys\"] = csys\n    return client._creoson_post(\"file\", \"get_transform\", data)",
        "sha1": "0de7b9f55073ef4c5642182a8ec20e0122a16ed7",
        "id": 221048
    },
    {
        "content": "def sig_stars(p):\n    \"\"\"Return a R-style significance string corresponding to p values.\"\"\"\n    if p < 0.001:\n        return \"***\"\n    elif p < 0.01:\n        return \"**\"\n    elif p < 0.05:\n        return \"*\"\n    elif p < 0.1:\n        return \".\"\n    return \"\"",
        "sha1": "b806ff8c1e092dbaea22ac22c93d16fd52fed0d3",
        "id": 383740
    },
    {
        "content": "def parse_problems(lines):\n\t\"\"\" Given a list of lines, parses them and returns a list of problems. \"\"\"\n\ti = 0\n\tres = []\n\twhile i < len(lines):\n\t\tP, G = map(int, lines[i].split())\n\t\tgrudges = [tuple(map(int, lines[i + n + 1].split())) for n in range(G)]\n\t\ti += G + 1\n\t\tres.append((P, grudges))\n\treturn res",
        "sha1": "0e8a0fc38b0c16e08faf36a6ec8c60f98eb5397e",
        "id": 174663
    },
    {
        "content": "def _detect_adfs_authority(authority_url, tenant):\n    \"\"\"Prepare authority and tenant for Azure Identity with ADFS support.\n    If `authority_url` ends with '/adfs', `tenant` will be set to 'adfs'. For example:\n        'https://adfs.redmond.azurestack.corp.microsoft.com/adfs'\n        -> ('https://adfs.redmond.azurestack.corp.microsoft.com/', 'adfs')\n    \"\"\"\n    authority_url = authority_url.rstrip('/')\n\n    if authority_url.endswith('/adfs'):\n        authority_url = authority_url[:-len('/adfs')]\n        # The custom tenant is discarded in ADFS environment\n        tenant = 'adfs'\n\n    return authority_url, tenant",
        "sha1": "2f1987502c11fab33045b8b96cb934f1a2fe7d98",
        "id": 516157
    },
    {
        "content": "def createICDdictionary(filename):\n    \"\"\"This function takes the sorted_icd_codes file as an arg. and create the following dictionary:\n    ex. {'1': 'BEXT', '3': 'CALI', '2': 'BINT',....,'35': 'XTRN'}\"\"\"\n    fh = open(filename)\n    ICDdict = {}\n    for line in fh:\n        line = line.strip().split()\n        ICDdict[line[0]] = line[1]\n        #ICDdict[line[0]] = \"Q\"+line[1]   # the data set has a \"Q\" before each ICD code\n    print(ICDdict)\n    return ICDdict",
        "sha1": "34effc8a933ceea55a86fabf7e41bdbae2b8df10",
        "id": 646269
    },
    {
        "content": "from typing import Dict\nfrom typing import List\n\n\ndef annotation_inside_slice(annotation: Dict, slice_bbox: List[int]) -> bool:\n    \"\"\"Check whether annotation coordinates lie inside slice coordinates.\n\n    Args:\n        annotation (dict): Single annotation entry in COCO format.\n        slice_bbox (List[int]): Generated from `get_slice_bboxes`.\n            Format for each slice bbox: [x_min, y_min, x_max, y_max].\n\n    Returns:\n        (bool): True if any annotation coordinate lies inside slice.\n    \"\"\"\n    left, top, width, height = annotation[\"bbox\"]\n\n    right = left + width\n    bottom = top + height\n\n    if left >= slice_bbox[2]:\n        return False\n    if top >= slice_bbox[3]:\n        return False\n    if right <= slice_bbox[0]:\n        return False\n    if bottom <= slice_bbox[1]:\n        return False\n\n    return True",
        "sha1": "01168edc86c436ce878df4fc551094ab03f1b30a",
        "id": 195446
    },
    {
        "content": "def parse_list(value):\n    \"\"\"\n    Parse a string to extract a simple list of string values.\n    \"\"\"\n    if value.startswith('[') and value.endswith(']'):\n        value = value[1:len(value)-1].strip()\n    elif not value.strip():\n        return []\n    return list(value.split(','))",
        "sha1": "933dbb5af1f662bc3d4782d547c70d75ea02d6c1",
        "id": 637047
    },
    {
        "content": "from typing import Tuple\nimport torch\nfrom typing import List\n\n\ndef propagate_expectation(\n    predictions: Tuple[torch.Tensor, ...]\n) -> Tuple[torch.Tensor, ...]:\n    \"\"\"Propagates ensemble outputs by taking expectation over model predictions.\n\n    Args:\n        predictions (tuple of tensors): the predictions to propagate. Each tensor's\n            shape must be ``E x B x Od``, where ``E``, ``B``, and ``Od`` represent the\n            number of models, batch size, and output dimension, respectively.\n\n    Returns:\n        (tuple of tensors): the chosen predictions, so that\n            `output[k][i, :] = predictions[k].mean(dim=0)`\n    \"\"\"\n    output: List[torch.Tensor] = []\n    for i, predicted_tensor in enumerate(predictions):\n        assert predicted_tensor.ndim == 3\n        output.append(predicted_tensor.mean(dim=0))\n    return tuple(output)",
        "sha1": "845e53ba7a031336e32336580f1003704ced3c5e",
        "id": 312383
    },
    {
        "content": "import math\n\n\ndef _datetime_to_timestamp(dt, period, round_up=False):\n    \"\"\" Convert a datetime object to a timestamp measured in simulation periods.\n\n    Args:\n        dt (datetime): Datetime to be converted to a simulation timestamp.\n        period (int): Length of one time interval in the simulation. (minutes)\n        round_up (bool): If True, round up when casting timestamp to int, else round down.\n\n    Returns:\n        int: dt expressed as a simulation timestamp.\n    \"\"\"\n    ts = dt.timestamp() / (60 * period)\n    if round_up:\n        return int(math.ceil(ts))\n    else:\n        return int(ts)",
        "sha1": "023a7e971db655abb04fa7fb53ceb535553ab076",
        "id": 655979
    },
    {
        "content": "def valid_recipient_object(recipient):\n    \"\"\"Check the recipient object has a good email address\n\n    :param recipient: object with a property named e_mail\n    :returns: boolean True if valid\n    \"\"\"\n    if recipient is None:\n        return False\n    if not hasattr(recipient, \"e_mail\"):\n        return False\n    if recipient.e_mail is None:\n        return False\n    if recipient.e_mail is not None and str(recipient.e_mail).strip() == \"\":\n        return False\n    return True",
        "sha1": "778371a8381803c8d16a7b42fff358876f46d2a1",
        "id": 171847
    },
    {
        "content": "def clamp_value(value, minimum, maximum):\n    \"\"\"Clamp a value to fit within a range.\n\n    * If `value` is less than `minimum`, return `minimum`.\n    * If `value` is greater than `maximum`, return `maximum`\n    * Otherwise, return `value`\n\n    Args:\n        value (number or Unit): The value to clamp\n        minimum (number or Unit): The lower bound\n        maximum (number or Unit): The upper bound\n\n    Returns:\n        int or float: The clamped value\n\n    Example:\n        >>> clamp_value(-1, 2, 10)\n        2\n        >>> clamp_value(4.0, 2, 10)\n        4.0\n        >>> clamp_value(12, 2, 10)\n        10\n    \"\"\"\n    if value < minimum:\n        return minimum\n    elif value > maximum:\n        return maximum\n    else:\n        return value",
        "sha1": "1224b7ed098781d66721dceef5c837470f1195a6",
        "id": 29507
    },
    {
        "content": "import torch\n\n\ndef std_normal(size):\n    \"\"\"\n    Generate the standard Gaussian variable of a certain size\n    \"\"\"\n\n    return torch.normal(0, 1, size=size).cuda()",
        "sha1": "ed576d6c636aad3f62a0bf35f8740b7d33320f13",
        "id": 540143
    },
    {
        "content": "def pop_recursive(d, key, default=None):\n    \"\"\"dict.pop(key) where `key` is a `.`-delimited list of nested keys.\n\n    >>> d = {'a': {'b': 1, 'c': 2}}\n    >>> pop_recursive(d, 'a.c')\n    2\n    >>> d\n    {'a': {'b': 1}}\n    \"\"\"\n    if not isinstance(d, dict):\n        return default\n    if key in d:\n        return d.pop(key, default)\n    if '.' not in key:\n        return default\n    key_head, key_tail = key.split('.', maxsplit=1)\n    if key_head in d:\n        return pop_recursive(d[key_head], key_tail, default)\n    return default",
        "sha1": "5ddefd00256270890520ff9aca76507ee0f5bbc8",
        "id": 413800
    },
    {
        "content": "import re\n\n\ndef increment_bar_number(line, increment):\n    \"\"\"Increments a bar number (if found) on a line and returns the line.\"\"\"\n    # skip some stuff if it isn't relevant\n    if '\\\\barNumberCheck #' not in line and '%' not in line:\n        return line\n    # this regex actually finds the numbers and makes groups\n    regex_num = re.compile(r\"\\s([#%]\\s?)(\\d+)\")\n    num = regex_num.search(line)\n\n    if num:\n        n = int(num.group(2))\n        n = n + increment\n        line = regex_num.sub(r' ' + num.group(1) + '{}'.format(n), line)\n\n    # return line whether or not it has been touched\n    return line",
        "sha1": "0acdf8b540ae5f15930f09f03f453953fa0ced9d",
        "id": 304185
    },
    {
        "content": "def sexagesimal_angle_to_decimal(degrees, minutes=0, seconds=0, thirds=0, fourths=0):\n    \"\"\"Convert sexagesimal-parsed angles to a decimal.\n\n    Args:\n        degrees (int): Angle degrees count.\n        minutes (int): Angle minutes count.\n        seconds (int): Angle seconds count.\n        thirds (int): Angle thirds count.\n        fourths (int): Angle fourths count.\n\n    Returns:\n        float: Angle in decimal degrees.\n    \"\"\"\n    if degrees is None:\n        return None\n\n    # Degrees must be absolute or it will not sum right with subdivisions.\n    absolute_decimal = abs(float(degrees))\n    try:\n        sign_multiplier = abs(float(degrees)) / float(degrees)\n    except ZeroDivisionError:\n        sign_multiplier = 1\n    for count, divisor in [\n        (minutes, 60),\n        (seconds, 3600),\n        (thirds, 216000),\n        (fourths, 12960000),\n    ]:\n        if count:\n            absolute_decimal += float(count) / divisor\n    return absolute_decimal * sign_multiplier",
        "sha1": "b312bfe0a9d15ee3c9853d8a515d00cebae29823",
        "id": 407651
    },
    {
        "content": "from typing import IO\n\n\ndef read_vcf_header(file: IO):\n    \"\"\"Read a vcf header. Resets pointer to 0\"\"\"\n    out = []\n    for line in file:\n        if line.startswith(\"#\"):\n            out.append(line)\n        else:\n            break\n\n    file.seek(0)\n    return out",
        "sha1": "f57c7838fc78bd508f8bb3fe6556f240a38787ff",
        "id": 493956
    },
    {
        "content": "def median_manual(l):\n    \"\"\"\n    l: List of integers\n    return median of list l\n    \"\"\"\n    n = len(l)\n    l = sorted(l)\n    if n%2 == 1: return l[int((n-1)/2)]\n    return (l[int(n/2)-1] + l[int(n/2)]) / 2",
        "sha1": "63104190a38759563e8c003ac1c91ab8bbc78f89",
        "id": 673867
    },
    {
        "content": "def reindex_iloc(df, inds, copy=True):\n    \"\"\"Reindex pandas.DataFrame with iloc indices, potentially out of bound.\n\n    Parameters\n    ----------\n    df: a pandas.DataFrame\n    inds: iterable, list or pd.Series - iloc indices to reindex to\n    copy: bool, optional, default=True - whether returned data frame is a new object\n        if not, values are references; passed to copy arg of df.reindex\n\n    Returns\n    -------\n    df_ret : pd.DataFrame - df reindexed to inds\n        identical to df.iloc[inds] if inds contains no out of bound index\n        out of bound indices will result in np.nan values\n        entries are references to original DataFrame if copy=False\n\n    Example\n    -------\n    >>> X = pd.DataFrame({'a' : [1,2,3,4]}, index=[-4,7,11,14])\n    >>> reindex_iloc(X, [1, 2, 6])\n         a\n    1  2.0\n    2  3.0\n    6  NaN\n    \"\"\"\n    df_ret = df.reset_index(drop=True).reindex(inds, copy=copy)\n\n    return df_ret",
        "sha1": "0c711a07f0c22e529ab74a3feb69a62accb3f584",
        "id": 323948
    },
    {
        "content": "def join_schema_to_table_name(name: str, schema: str) -> str:\n    \"\"\"Utility function to join a table_name to a schema. If the table name already has a schema returns the original\n    name\n\n    Args:\n        name: table name to join\n        schema: schema name to join to the table name\n\n    Returns:\n        String: Reformatted table name\n    \"\"\"\n    return '.'.join(filter(None, (schema, name))) if '.' not in name else name",
        "sha1": "f42feec0f608711fc2d571d5222cf7590d80c521",
        "id": 525660
    },
    {
        "content": "import pickle\n\n\ndef load_meta(fname, data_id=''):\n    \"\"\"Load a metadata file.\n\n    Parameters\n    ----------\n    fname : str\n        Path to TFRecord folder\n\n    Returns\n    -------\n    meta : dict\n        Metadata file\n\n    \"\"\"\n    with open(fname+data_id+'_meta.pkl', 'rb') as f:\n        meta = pickle.load(f)\n    return meta",
        "sha1": "2f63d0f2fc1c25fbab59b3c667458e2daf397526",
        "id": 645841
    },
    {
        "content": "def _compose_err_msg(msg, **kwargs):\n    \"\"\"Append key-value pairs to msg, for display.\n\n    Parameters\n    ----------\n    msg: string\n        arbitrary message\n    kwargs: dict\n        arbitrary dictionary\n\n    Returns\n    -------\n    updated_msg: string\n        msg, with \"key: value\" appended. Only string values are appended.\n\n    Example\n    -------\n    >>> _compose_err_msg('Error message with arguments...', arg_num=123, \\\n        arg_str='filename.nii', arg_bool=True)\n    'Error message with arguments...\\\\narg_str: filename.nii'\n    >>>\n    \"\"\"\n    updated_msg = msg\n    for k, v in sorted(kwargs.items()):\n        if isinstance(v, str):  # print only str-like arguments\n            updated_msg += \"\\n\" + k + \": \" + v\n\n    return updated_msg",
        "sha1": "539ba888d88bd3b1bb0d1ca386bb4ee0c88756ab",
        "id": 202413
    },
    {
        "content": "def bin_coef_efficient(n: int, k: int) -> int:\n    \"\"\"\n    C(n, k) = C(n, n-k)  # fact\n    therefore, if k > n-k, change k for n-k (for easier calculation)\n    i.e. C(n, k) = n! / (k! * (n-k)!) = [(n) * (n-1) * ... (n-k+1) / k!]\n     => k terms above and k terms below\n\n    Time Complexity: O(k)\n    Space Complexity: O(1)\n    \"\"\"\n    if k > n-k:\n        k = n-k\n\n    res = 1\n\n    for i in range(k):\n        res = res * (n-i) / (k-i)\n\n    return int(res)",
        "sha1": "09508012cc6f35320d5c8b27168f76f65e71a4a6",
        "id": 659158
    },
    {
        "content": "def get_idx_dict(cat):\n    \"\"\"Create dict that returns row index of objects when given sourcename\"\"\"\n    idx_dict = {s:idx for s, idx in zip(cat.Source_Name.values,cat.index.values)}\n    return idx_dict",
        "sha1": "212c5045b0c0cb9e68ed96c5c0c11277af4480b2",
        "id": 492242
    },
    {
        "content": "def tabindex(field, index):\n    \"\"\"Set the tab index on the filtered field.\"\"\"\n    field.field.widget.attrs[\"tabindex\"] = index\n    return field",
        "sha1": "c42b64b3f94a2a8a35b8b0fa3f14fe6d44b2f755",
        "id": 818
    },
    {
        "content": "def lowtran(wls):\n    \"\"\"Railegh molecular optical depth from eq.30 of Kneizys (1980).\n    Expects wls in \u00b5m.\"\"\"\n    return 1 / ((wls ** 4 * 115.6406) - wls ** 2 * 1.335)",
        "sha1": "2aeafa0368d2e8e1923d8cb2b084891e1f00668e",
        "id": 414207
    },
    {
        "content": "import re\n\n\ndef smi_tokenizer(smi):\n    \"\"\"Tokenize a SMILES sequence or reaction\"\"\"\n    pattern = \"(\\[[^\\]]+]|Bi|Br?|Ge|Te|Mo|K|Ti|Zr|Y|Na|125I|Al|Ce|Cr|Cl?|Ni?|O|S|Pd?|Fe?|I|b|c|Mn|n|o|s|<unk>|>>|Li|p|\\(|\\)|\\.|=|#|-|\\+|\\\\\\\\|\\/|:|@|\\?|>|\\*|\\$|\\%[0-9]{2}|[0-9])\"\n    regex = re.compile(pattern)\n    tokens = [token for token in regex.findall(smi)]\n    if smi != ''.join(tokens):\n        print('ERROR:', smi, ''.join(tokens))\n    assert smi == ''.join(tokens)\n    return tokens",
        "sha1": "7cd5c7366c4897c8398cce19d36f5681863355a9",
        "id": 603559
    },
    {
        "content": "def sign(number: int | float) -> int:\n    \"\"\"\n    Return the number sign.\n\n    >>> sign(5)\n    1\n    >>> sign(-5.2)\n    -1\n    >>> sign(0)\n    0\n    \"\"\"\n\n    if number > 0:\n        return 1\n    elif number < 0:\n        return -1\n    return 0",
        "sha1": "e6360520f746db5bed8511faf6796d04f032c9df",
        "id": 607094
    },
    {
        "content": "import random\n\n\ndef rndint(a, b):\n    \"\"\"Return a random integer between a and b.\"\"\"\n    return random.randint(a, b)",
        "sha1": "5bfe312a59121974b929dac89b3ac47f39d9f31c",
        "id": 315638
    },
    {
        "content": "from typing import List\n\n\ndef get_order_videogames_columns() -> List:\n    \"\"\"Returns the order of columns for videogames rows.\"\"\"\n    return [\n        \"Rank\",\n        \"Title\",\n        \"Developer\",\n        \"Platforms\",\n        \"Average Rating\",\n        \"Number of Ratings\",\n        \"URL\",\n        \"Original Title\",\n        \"Year\",\n        \"Release Date\",\n        \"Picture URL\",\n        \"Genre\",\n        \"Description\",\n    ]",
        "sha1": "cf285b83df89d707301ae51e54e1472355ea7bac",
        "id": 465228
    },
    {
        "content": "def merge_dicts(*dicts):\n    \"\"\"Return `dicts` merged together.\n\n    If keys clash the ubsequent dictionaries have priority over preceding ones.\n\n    >>> merge_dicts() == {}\n    True\n    >>> merge_dicts({'a': 2}) == {'a': 2}\n    True\n    >>> merge_dicts({'a': 2, 'b': 3}, {'a': 1, 'c': 4}) == {'a': 1, 'b': 3, 'c': 4}\n    True\n    \"\"\"\n    try:\n        first = dicts[0]\n    except IndexError:\n        return {}\n    else:\n        rest = dicts[1:]\n        result = dict(first)\n        for d in rest:\n            result.update(d)\n        return result",
        "sha1": "646852fc235d12a8db07480fa3a237521392c49c",
        "id": 413009
    },
    {
        "content": "def remove_blank_spaces(syllables) -> list:\n    \"\"\"Given a list of letters, remove any blank spaces or empty strings.\n    >>> remove_blank_spaces(['', 'a', ' ', 'b', ' ', 'c', ''])\n    ['a', 'b', 'c']\n    \"\"\"\n    cleaned = []\n    for syl in syllables:\n        if syl == \" \" or syl == '':\n            pass\n        else:\n            cleaned.append(syl)\n    return cleaned",
        "sha1": "3cf83c5867fd002c09ac5f109744e6fd9c43a6da",
        "id": 648013
    },
    {
        "content": "import torch\n\n\ndef viterbi_decode(tag_sequence, transition):\n    \"\"\"\n    Perform Viterbi decoding in log space over a sequence given a transition matrix\n    specifying pairwise (transition) potentials between tags and a matrix of shape\n    (sequence_length, num_tags) specifying unary potentials for possible tags per\n    timestep.\n\n    Parameters\n    ==========\n    tag_sequence: torch.Tensor, required.\n        A tensor of shape (sequence_length, num_tags) representing scores for\n        a set of tags over a given sequence.\n    trans: torch.Tensor, required.\n        A tensor of shape (num_tags, num_tags) representing the binary potentials\n        for transitioning between a given pair of tags.\n\n    Returns\n    =======\n    viterbi_path: The tag indices of the maximum likelihood tag sequence\n    viterbi_score: float, The score of the viterbi path\n    \"\"\"\n    seq_len, vocab = tag_sequence.size()\n\n    path_scores = []\n    path_indices = []\n\n    path_scores.append(tag_sequence[0, :])\n\n    # Evaluate the scores for all possible paths.\n    for t in range(1, seq_len):\n\n        # Add pairwise potentials to current scores.\n        summed_potentials = path_scores[t - 1].unsqueeze(-1) + transition\n        scores, paths = torch.max(summed_potentials, 0)\n        path_scores.append(tag_sequence[t, :] + scores.squeeze())\n        path_indices.append(paths.squeeze())\n\n    # Construct the most likely sequence backwards.\n    viterbi_score, best_path = torch.max(path_scores[-1].cpu(), 0)\n    viterbi_path = [int(best_path.numpy())]\n    for backward_t in reversed(path_indices):\n        viterbi_path.append(int(backward_t[viterbi_path[-1]]))\n\n    # Reverse the backward path.\n    viterbi_path.reverse()\n    \n    return viterbi_path, viterbi_score",
        "sha1": "f45c98a78c80cbcc0f0335d69733d7cf19241688",
        "id": 59473
    },
    {
        "content": "def intersect(list1, list2):\n    \"\"\"\n    Compute the intersection of two sorted lists.\n\n    Returns a new sorted list containing only elements that are in\n    both list1 and list2.\n\n    This function can be iterative.\n    \"\"\"\n    \n    intersection = []\n    append = intersection.append\n    idx1 = 0\n    idx2 = 0\n    \n    while idx1 < len(list1) and idx2 < len(list2):\n        if list1[idx1] < list2[idx2]:\n            idx1 += 1\n        elif list1[idx1] > list2[idx2]:\n            idx2 += 1\n        else:\n            append(list1[idx1])\n            idx1 += 1\n            idx2 += 1\n    \n    return intersection",
        "sha1": "36ead411915016330387e6e589a95cac9ebb62b7",
        "id": 518222
    },
    {
        "content": "def id_from_url(url):\n    \"\"\"Get player id from URL.\"\"\"\n    return url.strip(\"/\").split(\"/\")[-1]",
        "sha1": "58a09e26f720a5f6f02656667dbb566a25288d5e",
        "id": 102581
    },
    {
        "content": "def standardize(x):\n    \"\"\"\n    Standardizes a Series or DataFrame.\n    \"\"\"\n    return (x - x.mean()) / x.std()",
        "sha1": "8fd95bea1515461e3726bd1499c26660df519226",
        "id": 657552
    },
    {
        "content": "import base64\n\n\ndef decode_bytes(s: str) -> bytes:\n    \"\"\"Decode the bytes.\n\n    Args:\n        s: Encoded binary content.\n\n    Returns:\n        Decoded binary data.\n\n    Raises:\n        binascii.Error: If the data isn't valid.\n    \"\"\"\n    return base64.b64decode(s[1:])",
        "sha1": "d8572387012e51d7eb230b3d09a1e6bfdabcaac0",
        "id": 49165
    },
    {
        "content": "def get_download_ranges_list(range_left, range_right, parts):\n    \"\"\"\n        function to get a list of ranges to be downloaded by each thread\n    \"\"\"\n    l = range(range_left, range_right + 1)\n    dividend, remainder = divmod(len(l), parts)\n    ranges_list = list( l[i * dividend + min(i, remainder) : (i+1) * dividend + min(i+1, remainder)] for i in range(parts))\n    ranges_list = [(i[0], i[-1]) for i in ranges_list]\n    return ranges_list",
        "sha1": "8a6166773e88b56f673d85fc6a581fc5f401defb",
        "id": 637997
    },
    {
        "content": "import gzip\n\n\ndef load_fasttext_class_probabilities(probability_file_path):\n    \"\"\"\n    Utility function that loads class probabilities from a previously performed prediction run.\n\n    :param probability_file_path: str, path to the output file with class probabilities for the test dataset\n    :return: list of float: probability of belonging to the positive class for each example in the test dataset\n    \"\"\"\n    probabilities = []\n    with gzip.open(probability_file_path, 'rt') as fin:\n        for line in fin:\n            cols = line.rstrip().split()\n            prob = None\n            for i, col in enumerate(cols):\n                if col == '__label__1':\n                    prob = float(cols[i + 1])\n            assert prob is not None\n            probabilities.append(prob)\n    return probabilities",
        "sha1": "3ddff33c878b6ec013b2bfb8ff1e978f602a7d91",
        "id": 27355
    },
    {
        "content": "def smart_split(line):\n    \"\"\"\n    split string like:\n    \"C C 0.0033 0.0016 'International Tables Vol C Tables 4.2.6.8 and 6.1.1.4'\"\n    in the list like:\n    ['C', 'C', '0.0033', '0.0016', 'International Tables Vol C Tables 4.2.6.8 and 6.1.1.4']\n    \"\"\"\n    flag_in = False\n    l_val, val = [], []\n    for hh in line.strip():\n        if (hh == \" \") & (not flag_in):\n            if val != []:\n                l_val.append(\"\".join(val))\n            val = []\n        elif (hh == \" \") & (flag_in):\n            val.append(hh)\n        elif ((hh == \"'\")|(hh == \"\\\"\")):\n            flag_in = not flag_in\n        else:\n            val.append(hh)\n    if val != []:\n        l_val.append(\"\".join(val))\n    return l_val",
        "sha1": "7249b6598f93c93ba17f668e5d0554ebae31a100",
        "id": 632857
    },
    {
        "content": "from pathlib import Path\nimport re\n\n\ndef find_target_file (wdir, stem, extension, no_hit_ok=True, recursive=False):\n    \"\"\"\n    Find a target file path from a path to the working directory, the stem of the file name, and the extension of the target file.\n    \"\"\"\n    if recursive:\n        paths = list(Path(wdir).glob('**/{}*'.format(stem)))\n    else:\n        paths = list(Path(wdir).glob('{}*'.format(stem)))\n    path = [ i for i in paths if re.search(extension, str(i)) ]\n    if len(path)==0:\n        if no_hit_ok:\n            ret = ''\n        else:\n            raise ValueError('No file is matched.')\n    elif len(path)>1:\n        raise ValueError('More than one file is matched.')\n    else:\n        ret = str(path[0])\n    return ret",
        "sha1": "c8715176d751aa981e990ad0afd66a40b01b315f",
        "id": 492922
    },
    {
        "content": "import torch\n\n\ndef angle_axis_to_rotation_matrix(angle_axis: torch.Tensor) -> torch.Tensor:\n    \"\"\"Convert 3d vector of axis-angle rotation to 4x4 rotation matrix\n\n    Args:\n        angle_axis (torch.Tensor): tensor of 3d vector of axis-angle rotations.\n\n    Returns:\n        torch.Tensor: tensor of 4x4 rotation matrices.\n\n    Shape:\n        - Input: :math:`(N, 3)`\n        - Output: :math:`(N, 4, 4)`\n\n    Example:\n        >>> input = torch.rand(1, 3)  # Nx3\n        >>> output = kornia.angle_axis_to_rotation_matrix(input)  # Nx4x4\n    \"\"\"\n\n    def _compute_rotation_matrix(angle_axis, theta2, eps=1e-6):\n        # We want to be careful to only evaluate the square root if the\n        # norm of the angle_axis vector is greater than zero. Otherwise\n        # we get a division by zero.\n        k_one = 1.0\n        theta = torch.sqrt(theta2)\n        wxyz = angle_axis / (theta + eps)\n        wx, wy, wz = torch.chunk(wxyz, 3, dim=1)\n        cos_theta = torch.cos(theta)\n        sin_theta = torch.sin(theta)\n\n        r00 = cos_theta + wx * wx * (k_one - cos_theta)\n        r10 = wz * sin_theta + wx * wy * (k_one - cos_theta)\n        r20 = -wy * sin_theta + wx * wz * (k_one - cos_theta)\n        r01 = wx * wy * (k_one - cos_theta) - wz * sin_theta\n        r11 = cos_theta + wy * wy * (k_one - cos_theta)\n        r21 = wx * sin_theta + wy * wz * (k_one - cos_theta)\n        r02 = wy * sin_theta + wx * wz * (k_one - cos_theta)\n        r12 = -wx * sin_theta + wy * wz * (k_one - cos_theta)\n        r22 = cos_theta + wz * wz * (k_one - cos_theta)\n        rotation_matrix = torch.cat(\n            [r00, r01, r02, r10, r11, r12, r20, r21, r22], dim=1)\n        return rotation_matrix.view(-1, 3, 3)\n\n    def _compute_rotation_matrix_taylor(angle_axis):\n        rx, ry, rz = torch.chunk(angle_axis, 3, dim=1)\n        k_one = torch.ones_like(rx)\n        rotation_matrix = torch.cat(\n            [k_one, -rz, ry, rz, k_one, -rx, -ry, rx, k_one], dim=1)\n        return rotation_matrix.view(-1, 3, 3)\n\n    # stolen from ceres/rotation.h\n\n    _angle_axis = torch.unsqueeze(angle_axis, dim=1)\n    theta2 = torch.matmul(_angle_axis, _angle_axis.transpose(1, 2))\n    theta2 = torch.squeeze(theta2, dim=1)\n\n    # compute rotation matrices\n    rotation_matrix_normal = _compute_rotation_matrix(angle_axis, theta2)\n    rotation_matrix_taylor = _compute_rotation_matrix_taylor(angle_axis)\n\n    # create mask to handle both cases\n    eps = 1e-6\n    mask = (theta2 > eps).view(-1, 1, 1).to(theta2.device)\n    mask_pos = (mask).type_as(theta2)\n    mask_neg = (mask == False).type_as(theta2)  # noqa\n\n    # create output pose matrix\n    batch_size = angle_axis.shape[0]\n    rotation_matrix = torch.eye(4).to(angle_axis.device).type_as(angle_axis)\n    rotation_matrix = rotation_matrix.view(1, 4, 4).repeat(batch_size, 1, 1)\n    # fill output matrix with masked values\n    rotation_matrix[..., :3, :3] = \\\n        mask_pos * rotation_matrix_normal + mask_neg * rotation_matrix_taylor\n    return rotation_matrix",
        "sha1": "461779558e8cf06e05a018253961e4756ada1a47",
        "id": 52576
    },
    {
        "content": "def prepare_y(y):\n    \"\"\"Make sure y is a vector. Unravel from matrix if necessary (and matrix only\n    has 1 column)\n    \"\"\"\n    if len(y.shape) > 1:\n        if y.shape[1] == 1:\n            y = y.ravel()\n        else:\n            raise ValueError(\"Y has to be a vector of response values\")\n    return y",
        "sha1": "0cf0c48cb37e33e8066f3e6e75bedd2e9952319e",
        "id": 404881
    },
    {
        "content": "def find_blend_shape_directory_by_blend_shape_idx(blend_shape, idx):\n    \"\"\"\n    Find the target directory of the given blend shape index.\n\n    If it's not found, return the root directory.\n    \"\"\"\n    for directory in blend_shape.targetDirectory:\n        # childIndices will be None if there are no entries.\n        if idx in (directory.childIndices.get() or []):\n            return directory\n\n    # If we can't find it, return the root directory.\n    return blend_shape.targetDirectory[0]",
        "sha1": "73c134fd7a6729b4b58f45a2d7a6bd696312bd00",
        "id": 453156
    },
    {
        "content": "def parse_number(x):\n    \"\"\"Parse a number from a string.\"\"\"\n    return round(float(x), 6)",
        "sha1": "5e24a16aabf58082ff2636d3b3b8b8fa48a3df0b",
        "id": 69960
    },
    {
        "content": "import requests\n\n\ndef get_sequence(UniprotID):\n    \"\"\"Get protein sequence from UniProt Fasta (using REST API).\n\n    \"\"\"\n    # collect UniProtID data\n    fasta_URL = 'https://www.uniprot.org/uniprot/'+UniprotID+'.fasta'\n    request = requests.post(fasta_URL)\n    request.raise_for_status()\n    fasta_string = request.text.split('\\n')\n    sequence = ''.join(fasta_string[1:])\n    return sequence",
        "sha1": "ede837154bda738dc8e7e83c97c5f81b284db17c",
        "id": 73574
    },
    {
        "content": "import time\n\n\ndef Event(type, _fields=None, **fields):\n    \"\"\"Create an event.\n\n    An event is a dictionary, the only required field is ``type``.\n\n    \"\"\"\n    event = dict(_fields or {}, type=type, **fields)\n    if \"timestamp\" not in event:\n        event[\"timestamp\"] = time.time()\n    return event",
        "sha1": "aaf564de2b4fcd2391130ca37a66217061ced178",
        "id": 193946
    },
    {
        "content": "def get_border_bounding_rect(h, w, p1, p2, r):\n    \"\"\"Get a valid bounding rect in the image with border of specific size.\n\n    # Arguments\n        h: image max height.\n        w: image max width.\n        p1: start point of rect.\n        p2: end point of rect.\n        r: border radius.\n    # Returns\n        rect coord\n    \"\"\"\n    x1, y1, x2, y2 = p1[0], p1[1], p2[0], p2[1]\n\n    x1 = x1 - r if 0 < x1 - r else 0\n    y1 = y1 - r if 0 < y1 - r else 0\n    x2 = x2 + r + 1 if x2 + r + 1 < w else w\n    y2 = y2 + r + 1 if y2 + r + 1 < h else h\n\n    return x1, y1, x2, y2",
        "sha1": "41135030c8759730ecb65f9243fa523a063931b5",
        "id": 268256
    },
    {
        "content": "def _align32b(i):\n    \"\"\"Return int `i` aligned to the 32-bit boundary\"\"\"\n    r = i % 4\n    return i if not r else i + 4 - r",
        "sha1": "362277b071c2d7b804744d5db35cb7d21ec7efbd",
        "id": 425911
    },
    {
        "content": "def _current_window_for_event(event):\n    \"\"\"\n    Return the `Window` for the currently focussed Buffer.\n    \"\"\"\n    return event.app.layout.current_window",
        "sha1": "4b9859c7bf7fc4b072362d2d5b9e896022769587",
        "id": 698915
    },
    {
        "content": "import logging\n\n\ndef get_queue(sqs, queue_name):\n    \"\"\"\n    Purpose:\n        Return an SQS Queue by Queue Name\n    Args:\n        sqs (SQS Resource Object): SQS Resource Object\n        queue_name (String): Name of Queue\n    Return:\n        queue (SQS Queue Object): Queue object for the queue in\n            SQS\n    \"\"\"\n\n    try:\n        return sqs.get_queue_by_name(QueueName=queue_name)\n    except Exception as err:\n        logging.exception(f\"Exception Getting Queue: {err}\")\n        raise",
        "sha1": "e01fc28b33363f5b494ebe40e22d62a701313a3f",
        "id": 388356
    },
    {
        "content": "def get_words(fh):\n    \"\"\" Get all the words from a filehandle \"\"\"\n\n    words = set()\n    for line in fh:\n        for word in line.rstrip().split():\n            words.add(word)\n\n    return words",
        "sha1": "f6216702cf8cbc05f92014c0f219c85dc72b3a82",
        "id": 139840
    },
    {
        "content": "def find_root_visual(conn):\n    \"\"\"Find the xcffib.xproto.VISUALTYPE corresponding to the root visual\"\"\"\n    default_screen = conn.setup.roots[conn.pref_screen]\n    for i in default_screen.allowed_depths:\n        for v in i.visuals:\n            if v.visual_id == default_screen.root_visual:\n                return v",
        "sha1": "930bb700bdcb141aba9fc4370d244b588357f8f1",
        "id": 703099
    },
    {
        "content": "def convert_boot_type (boot_type):\n    \"\"\"\n    Convert a next boot type parameter to the string that needs to be passed to system functions.\n    \n    :param boot_type: The boot type parameter passed to the command.  If this is None, no conversion\n    is performed.\n    \n    :return The string identifier for the boot type.  This will be empty if the boot type is not\n    valid.\n    \"\"\"\n    \n    if (boot_type is not None):\n        if ((boot_type == \"1\") or (boot_type == \"nooverride\")):\n            boot_type = \"none\"\n        elif ((boot_type == \"2\") or (boot_type == \"forcepxe\")):\n            boot_type = \"pxe\"\n        elif ((boot_type == \"3\") or (boot_type == \"forcedefaulthdd\")):\n            boot_type = \"disk\"\n        elif ((boot_type == \"4\") or (boot_type == \"forceintobiossetup\")):\n            boot_type = \"bios\"\n        elif ((boot_type == \"5\") or (boot_type == \"forcefloppyorremovable\")):\n            boot_type = \"floppy\"\n        else:\n            boot_type = \"\"\n            \n    return boot_type",
        "sha1": "966c5b7085f37d64eba57f5f662fa9cdf66191a1",
        "id": 217090
    },
    {
        "content": "def _get_package_uri_props(package_uri):\n    \"\"\"\n    Gets the properties of a debian package from its URI.\n    \"\"\"\n    uri_filename = package_uri.rsplit(\"/\", 1)[1]\n    uri_basename = uri_filename.rsplit(\".\", 1)[0]\n    uri_name, uri_version, uri_arch = uri_basename.split(\"_\")\n    return uri_filename, uri_name, uri_version, uri_arch",
        "sha1": "e4d5fbe41bfd6bd276b4c1833b79456fc1192fe2",
        "id": 363146
    },
    {
        "content": "def DF(s, k):\n    \"\"\"\n        Return a set in a list shape\n        of distinct factors of length \n        k of the string s\n        --------\n        Parameters:\n            s (str)\n            k (int)\n        \n        eg:\n        DF(\"BANANA\", 2)\n        Output:\n        ['NA', 'BA', 'AN']\n    \"\"\"\n    n=len(s)\n    return list(set([s[i:i+k] for i in range(0, max(0, n-k+1))]))",
        "sha1": "ceec3320f69ba580fd8701baa5b46bbd46181b61",
        "id": 579556
    },
    {
        "content": "from typing import Any\n\n\ndef is_resource(obj_or_type: Any) -> bool:\n    \"\"\"Return if object or type represents a resource.\"\"\"\n    return getattr(obj_or_type, \"_fondat_resource\", None) is not None",
        "sha1": "b2687abf1ed5346b0396139b07de2a1abc69a55f",
        "id": 482705
    },
    {
        "content": "from typing import Union\nfrom pathlib import Path\n\n\ndef ensure_dir(path: Union[str, Path]) -> Path:\n    \"\"\"Create directory if it doesn't exist.\"\"\"\n    path = Path(path)\n    if not path.exists():\n        path.mkdir(parents=True, exist_ok=True)\n    assert path.is_dir()\n    return path",
        "sha1": "a2888cab207ea950d05d8f842a1ba5b39e8ccc84",
        "id": 305399
    },
    {
        "content": "def to_bits(cs):\n    \"\"\"Split a string into a list of numeric and non-numeric substrings.\"\"\"\n\n    if not cs:\n        return []\n\n    out = []\n    tmp = \"\"\n    is_number = True\n    first = True\n    for c in cs:\n        if c in [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"]:\n            if is_number:\n                tmp += c\n            else:\n                if not first:\n                    out.append(tmp)\n                tmp = c\n                is_number = True\n        else:\n            if is_number:\n                if not first:\n                    out.append(tmp)\n                tmp = c\n                is_number = False\n            else:\n                tmp += c\n        first = False\n    if not first:\n        out.append(tmp)\n    return out",
        "sha1": "82ca734ddda38e792fd6992c9ae4ea4d6684d2a6",
        "id": 377225
    },
    {
        "content": "import re\nimport keyword\n\n\ndef to_identifier(val):\n    \"\"\"Convert string to valid identifier\"\"\"\n    val = str(val).strip()\n    # Replaces spaces, dashes, and slashes to underscores\n    val = re.sub(r\"[\\s\\-/\\\\]\", \"_\", val)\n    # Remove remaining invalid characters\n    val = re.sub(r\"[^0-9a-zA-Z_]\", \"\", val)\n    # Identifier can't start with digits\n    val = re.sub(r\"^[^a-zA-Z_]+\", \"\", val)\n\n    if not val or keyword.iskeyword(val):\n        raise ValueError(f\"Unable to convert to identifier: {val}\")\n\n    return val",
        "sha1": "f6c6e8a5bf7b2ed830e7f3e43505f45459ff610f",
        "id": 180707
    },
    {
        "content": "def is_valid_connection(from_node, to_node, config):\n    \"\"\"\n    Checks if a connection is valid.\n    params:\n        from_node: The node from which the connection originates.\n        to_node: The node to which the connection connects.\n        config: The settings to check against\n    returns:\n        True if the connection is valid, False otherwise.\n    \"\"\"\n    if from_node.layer == to_node.layer:\n        return False  # don't allow two nodes on the same layer to connect\n\n    if not config.allow_recurrent and from_node.layer > to_node.layer:\n        return False  # invalid\n\n    return True",
        "sha1": "68aacbbda3ae9233d3cde73141978e0a2bd25006",
        "id": 265531
    },
    {
        "content": "def request_post(request_factory):\n    \"\"\"Fixture allowing POST request object generation.\n\n    Example::\n\n        def test_this(request_post):\n            request = request_post('/some', {'a': 'b'})\n\n\n    :param str|unicode path:\n\n    :param dict data: Data to post.\n\n    :param AbstractBaseUser user: User making this request.\n\n    :param bool ajax: Make AJAX (XMLHttpRequest) request.\n\n    :param kwargs: Additional arguments for .post() method.\n\n    \"\"\"\n    def request_post_(path=None, data=None, user=None, ajax=False, **kwargs):\n        \"\"\"\n        :rtype: HttpRequest\n        \"\"\"\n        path = path or '/'\n        request = request_factory(ajax=ajax).post(path, data, **kwargs)\n        if user:\n            request.user = user\n        return request\n\n    return request_post_",
        "sha1": "fd7f764598e2a8c179807453b09cfd0689f94441",
        "id": 182921
    },
    {
        "content": "def parse_int(value):\n    \"\"\"Returns value converted to an int. Raises a ValueError if value cannot\n    be converted to an int.\n    \"\"\"\n    return int(value)",
        "sha1": "6c08e30909a5f3c44f5329637d39135350b88b56",
        "id": 524420
    },
    {
        "content": "def collapse_hemisphere_index(df):\n    \"\"\"Returns frame with 'hemisphere' index level removed from index and added\n       as a data column\"\"\"\n    updated_frame = df.copy()\n    return updated_frame.reset_index(level='hemisphere', drop=False)",
        "sha1": "43ee647d78f19c878765e997461aa47e22db0895",
        "id": 54511
    },
    {
        "content": "def rotate_layer(layer: tuple, rotation_diff: int) -> tuple:\n    \"\"\"Rotate layer by given rotation angle\"\"\"\n\n    _, _, rotation, distribution = layer\n\n    new_rotation = rotation + rotation_diff\n    new_rotation = new_rotation % 180 if distribution == \"Normal\" else new_rotation % 360\n    \n    new_layer = list(layer)\n    new_layer[2] = new_rotation\n\n    return tuple(new_layer)",
        "sha1": "41355468101de138d49700b5d595115b74201d93",
        "id": 84684
    },
    {
        "content": "def p_hid(request) -> float:\n  \"\"\"Mock hidden units dropout probability.\"\"\"\n  return request.param",
        "sha1": "88271688f46f3d6971797c6baf662702a29f87e8",
        "id": 549686
    },
    {
        "content": "import collections\n\n\ndef solution(A):\n    \"\"\"\n    This can be easily implemented by taking advantage of the\n    collections.Counter object. We can then iterate through the valid sequence\n    and return the first number that cannot be found in the counter.\n\n    Warning:\n        With this problem we need to pay special care with two valid edge cases:\n        1. Empty list (result should be 1)\n        2. List with no missing element (result should be N+1)\n\n    \"\"\"\n    # get counter for array\n    counter = collections.Counter(A)\n\n    # iterate from 1 - N\n    for i in range(1, len(A) + 1):\n\n        # found the missing element\n        if counter[i] == 0:\n            return i\n\n    # not found, must be N + 1\n    return len(A) + 1",
        "sha1": "f671b8e4516dee634103137fe7deac0f435390db",
        "id": 150887
    },
    {
        "content": "def signal_exception_handler(ex):\n    \"\"\" returns the exception without printing it, useful for expected exceptions, signaling that an exception occurred \"\"\"\n    return ex",
        "sha1": "be2fcced8883057a4c9fea6f9074b87e65cf8802",
        "id": 524786
    },
    {
        "content": "def get_array_color_mode(x):\n    \"\"\" Given a numpy array representing a single image, it returns the\n        PIL color mode that will most likely work with it \"\"\"\n    x = x.squeeze()\n    if x.ndim == 2:\n        mode = \"L\"\n    elif x.ndim == 3 and x.shape[2] == 1:\n        mode = \"L\"\n        x = x.squeeze()\n    elif x.ndim == 3:\n        mode = \"RGB\"\n    else:\n        assert False, \"Incapable of interpreting array as an image\"\n\n    return mode",
        "sha1": "77fb222f8988c69f90702d6477ce0a5715c8df89",
        "id": 270085
    },
    {
        "content": "import inspect\n\n\ndef get_signature(obj):\n    \"\"\"\n    Get signature of module/class/routine\n\n    Returns:\n        A string signature\n    \"\"\"\n    name = obj.__name__\n    if inspect.isclass(obj):\n        if hasattr(obj, \"__init__\"):\n            signature = str(inspect.signature(obj.__init__))\n            return \"class %s%s\" % (name, signature)\n        else:\n            signature = \"%s()\" % name\n    elif inspect.ismodule(obj):\n        signature = name\n    else:\n        signature = str(inspect.signature(obj))\n        return name + signature\n    return signature",
        "sha1": "9da9d7e431783b89a5e65b4940b118cd5538799c",
        "id": 17119
    },
    {
        "content": "def get_ip(conn):\n    \"\"\"Return the primary IP of a network connection.\"\"\"\n    ipcfg = conn.get('ipConfig')\n    if not ipcfg:\n        return\n    stcfg = ipcfg.get('staticIpConfig')\n    aucfg = ipcfg.get('autoIpConfig')\n    if stcfg:\n        return stcfg.get('ip')\n    elif aucfg:\n        ip = aucfg.get('allocatedIp')\n        if ip is None:\n            ip = aucfg.get('reservedIp')\n        return ip",
        "sha1": "f842536d5ee38afa90e9d530fc7961b4ae1aee32",
        "id": 346448
    },
    {
        "content": "import re\n\n\ndef include_filter(incl_filter, paths):\n    \"\"\"\n    Matches a set of paths against an include filter.\n\n    param: incl_filter: The filter to match.\n    param: paths: The set of paths to match against the filter.\n    returns: A set of paths which match the include filter.\n    \"\"\"\n    hits = set()\n    for p in paths:\n        if re.search(incl_filter, p):\n            hits.add(p)\n\n    return hits",
        "sha1": "e1be38375304f3bccb6223d762ec55e1cf41ebd8",
        "id": 288451
    },
    {
        "content": "def is_federated(resource):\n    \"\"\" Copy of BaseResource.is_federated \"\"\"\n    return resource.resource_federation_path is not None and \\\n        resource.resource_federation_path != ''",
        "sha1": "e28309b4de88f9cd64c212861faf46c40669ad18",
        "id": 367185
    },
    {
        "content": "def _to_sf_location(lx: str) -> str:\n    \"\"\"Append SF specific info to location string to\n    improve GeoLocation lookup\"\"\"\n    return lx + \", San Francisco, CA\"",
        "sha1": "cb2fe0a6e9c5e54fb9e3608b0d5adcdb8b749f23",
        "id": 438320
    },
    {
        "content": "def cypher_join(*clauses, **parameters):\n    \"\"\" Join multiple Cypher clauses, returning a (query, parameters)\n    tuple. Each clause may either be a simple string query or a\n    (query, parameters) tuple. Additional `parameters` may also be\n    supplied as keyword arguments.\n\n    :param clauses:\n    :param parameters:\n    :return: (query, parameters) tuple\n    \"\"\"\n    query = []\n    params = {}\n    for clause in clauses:\n        if clause is None:\n            continue\n        if isinstance(clause, tuple):\n            try:\n                q, p = clause\n            except ValueError:\n                raise ValueError(\"Expected query or (query, parameters) tuple \"\n                                 \"for clause %r\" % clause)\n        else:\n            q = clause\n            p = None\n        query.append(q)\n        if p:\n            params.update(p)\n    params.update(parameters)\n    return \"\\n\".join(query), params",
        "sha1": "bdfb40d1f9e793572a4612f4c71293bc96d81a30",
        "id": 489026
    },
    {
        "content": "def GetTotalDocumentNumber(inverted_index_table):\n  \"\"\"Gets the total number of documents in a document collection.\"\"\"\n  return inverted_index_table.GetRoot().n_of_doc",
        "sha1": "d0d7dc8f84d10a986d26dfe151766f4fdcb5e670",
        "id": 238454
    },
    {
        "content": "from datetime import datetime\n\n\ndef get_record(card):\n    \"\"\"\n    Extract details from a job \"card\" from Indeed.com page.\n    \"\"\"\n    atag = card.h2.a\n    job_title = atag.get('title')\n    job_url = 'https://www.indeed.com' + atag.get('href')\n\n    company = card.find('span', {'class': 'company'}).text.strip()\n    job_location = card.find('div', 'recJobLoc').get('data-rc-loc')\n    job_summary = card.find('div', 'summary').text.strip()\n\n    post_date = card.find('span', 'date').text\n    today = datetime.today().strftime('%Y-%m-%d')\n\n    # If salary range is not provided, set to empty string.\n    try:\n        job_salary = card.find('span', 'salaryText').text\n    except (AttributeError):\n        job_salary = ''\n    \n    record = (job_title, company, job_location, post_date, today, job_summary, job_salary, job_url)\n\n    return record",
        "sha1": "4778b60f96f6cf5c1a6a69c3fc267953c57e894b",
        "id": 280548
    },
    {
        "content": "def lib2name(lib):\n    \"\"\"Convert an OS dependent library name to the base name::\n        libfoo.so.0.1 => foo\n        foo.dll       => foo\n    \"\"\"\n    if lib.startswith('lib'):\n        lib = lib[4:]\n    return lib.split('.',1)[0]",
        "sha1": "a49dab72e333569631940b5e3e28d94524696cbc",
        "id": 468897
    },
    {
        "content": "def readIdLabel(path):\n    \"\"\"\n    read id and label from .csv file\n\n    Args:\n        path: path of .csv file\n\n    Returns: \n        id list, label list\n    \"\"\"\n    id_list = list()\n    label_list = list()\n    with open(path, 'r') as f:\n        next(f)\n        for line in f:\n            t = line.split(',')\n            id_list.append(t[0])\n            if t[1][:-1] != '':\n                label_list.append(t[1][:-1])\n    return id_list, label_list",
        "sha1": "1e10a76945037f0404009c3f042bf5b1bb862f66",
        "id": 272136
    },
    {
        "content": "from typing import List\n\n\ndef read_data() -> List[List[int]]:\n    \"\"\"\n    Reading a matrix of data.\n    \"\"\"\n    with open(\"data.txt\", \"r\") as file:\n        data = file.readlines()\n    return [list(map(int, line.strip())) for line in data]",
        "sha1": "16591fd2556eafb2ef96ac00629dff2f05e81dc9",
        "id": 210957
    },
    {
        "content": "def njoiner(inbox, n=None, join=\"\"):\n    \"\"\"\n    String joins and returns the first \"n\" inputs.\n    \n    Arguments:\n\n     - n(``int``) [default: ``None``] All elements in the inbox smaller then \n       this number will be joined.\n     - join(``str``) [default: ``\"\"``] String which will join the elements of \n       the inbox i.e. ``join.join()``.\n    \n    \"\"\"\n    return join.join(inbox[:n])",
        "sha1": "1541c192bb379657027ef93eebd2c01287756df8",
        "id": 625477
    },
    {
        "content": "def _get_length_length(_bytes):\n    \"\"\"Returns a count of bytes in an Ion value's `length` field.\"\"\"\n    if (_bytes[0] & 0x0F) == 0x0E:\n        # read subsequent byte(s) as the \"length\" field\n        for i in range(1, len(_bytes)):\n            if (_bytes[i] & 0x80) != 0:\n                return i\n        raise Exception(\"Problem while reading VarUInt!\")\n    return 0",
        "sha1": "624774547efa7fe3b554d408a9843a4b373e1bb0",
        "id": 455222
    },
    {
        "content": "def join_ints(ints):\n    \"\"\"Joins list of intergers into the ordered comma-separated text.\n    \"\"\"\n    return ','.join(map(str, sorted(ints)))",
        "sha1": "1e263fe00c01fd81252df0e824f5300126374cbd",
        "id": 660051
    },
    {
        "content": "from typing import List\n\n\ndef pack_bit_vector(values: List[int], bits: int):\n    \"\"\"Pack the integers in `values` into a single integer, with each entry occupying `bits` bits.\n\n    >>> pack_bit_vector([0x012, 0x234, 0x456], bits=12) == 0x456234012\n    True\n    \"\"\"\n    return sum(v << (bits * i) for i, v in enumerate(values))",
        "sha1": "40b8534d518e00f8d154b7cc628f37aff08fcd27",
        "id": 282450
    },
    {
        "content": "import six\n\n\ndef FormatTags(tags_dict):\n  \"\"\"Format a dict of tags into arguments for 'tag' parameter.\n\n  Args:\n    tags_dict: Tags to be formatted.\n\n  Returns:\n    A list of tags formatted as arguments for 'tag' parameter.\n  \"\"\"\n  return ['Key=%s,Value=%s' % (k, v) for k, v in six.iteritems(tags_dict)]",
        "sha1": "fe73d1889f3edc3f0bce1bdbdbf61ad7ff16796b",
        "id": 513324
    },
    {
        "content": "def create_individual_tests(test_set):\n    \"\"\"\n    Creates test definitions from a test set as dictionaries.\n    Assumes 'expected_outputs' is a list of tuples defining expected outputs key value pairs\n      [('schema', [SCHEMA-DICT]), ...]\n    \"\"\"\n\n    test_inputs = {k: v for k, v in test_set.items() if k != \"expected_outputs\"}\n    test_dicts = [\n        {k: v, **test_inputs}\n        for k, v in test_set.get(\"expected_outputs\", [(None, None)])\n        if v\n    ]\n    return test_dicts",
        "sha1": "981ce432f9b68fae73477aa6039e4fd71fccd302",
        "id": 227358
    },
    {
        "content": "def _inject_key(key, infix):\n    \"\"\"\n    OSM keys often have several parts, separated by ':'s.\n    When we merge properties from the left and right of a\n    boundary, we want to preserve information like the\n    left and right names, but prefer the form \"name:left\"\n    rather than \"left:name\", so we have to insert an\n    infix string to these ':'-delimited arrays.\n\n    >>> _inject_key('a:b:c', 'x')\n    'a:x:b:c'\n    >>> _inject_key('a', 'x')\n    'a:x'\n\n    \"\"\"\n    parts = key.split(':')\n    parts.insert(1, infix)\n    return ':'.join(parts)",
        "sha1": "fcbd6b25bb5fa61780ae75ea96773ed95f6ffc28",
        "id": 200669
    },
    {
        "content": "def fix_UTC_offset(date_string):\n    \"\"\"\n    Python 3.6 and lower does not like when a date string has a colon in the UTC offset, such as\n    2020-04-20T23:59:59-04:00\n    Intead, Pyton 3.6 and lower needs the colon removed:\n     2020-04-20T23:59:59-0400\n\n    We can fix this easily by simply removing the colon if it exists.\n    (Python 3.7 and later does not have this issue.)\n\n    See https://stackoverflow.com/questions/30999230/how-to-parse-timezone-with-colon for an example.\n\n    :param date_string: a date string of the format \"%Y-%m-%dT%H:%M:%S%z\"\n    :return: The date string with the UTC offset fixed\n    \"\"\"\n    if \":\" == date_string[-3:-2]:\n        date_string = date_string[:-3] + date_string[-2:]\n    return date_string",
        "sha1": "9e62fa34839e15f3d1aa9ecaba186314618930e5",
        "id": 424938
    },
    {
        "content": "from pathlib import Path\nimport logging\nimport yaml\n\n\ndef params_from_yaml(args):\n    \"\"\"Extract the parameters for preparation from a yaml file and return a dict\"\"\"\n    # Check the path exists\n    try:\n        config_file_path = Path(args.config)\n        assert config_file_path.exists()\n    except Exception:\n        logging.error(f\"Could not find config file at {args.config}\")\n        raise\n\n    # Load the data from the config file\n    try:\n        with open(config_file_path, \"r\") as f:\n            params = yaml.safe_load(f)\n    except Exception:\n        logging.error(\n            f\"Could not extract parameters from yaml file at {config_file_path}\"\n        )\n        raise\n\n    if \"verbose\" not in params.keys():\n        params[\"verbose\"] = True\n\n    return params",
        "sha1": "36beadd8fa4f27471c514a963838aac216aad434",
        "id": 702466
    },
    {
        "content": "def _wfdb_fmt(bit_res, single_fmt=True):\n    \"\"\"\n    Return the most suitable WFDB format(s) to use given signal\n    resolutions.\n\n    Parameters\n    ----------\n    bit_res : int, list\n        The resolution of the signal, or a list of resolutions, in bits.\n    single_fmt : bool, optional\n        Whether to return the format for the maximum resolution signal.\n\n    Returns\n    -------\n    fmt : str, list\n        The most suitable WFDB format(s) used to encode the signal(s).\n\n    \"\"\"\n    if isinstance(bit_res, list):\n        # Return a single format\n        if single_fmt:\n            bit_res = [max(bit_res)] * len(bit_res)\n\n        return [_wfdb_fmt(r) for r in bit_res]\n\n    if bit_res <= 8:\n        return '80'\n    elif bit_res <= 12:\n        return '212'\n    elif bit_res <= 16:\n        return '16'\n    elif bit_res <= 24:\n        return '24'\n    else:\n        return '32'",
        "sha1": "9bbf0f64efcc4aebf5ac421c60c4610f23d04647",
        "id": 132589
    },
    {
        "content": "def get_dbot_level(threat_level_id: str) -> int:\n    \"\"\"\n    MISP to DBOT:\n    4 = 0 (UNDEFINED to UNKNOWN)\n    3 = 2 (LOW to SUSPICIOUS)\n    1 | 2 = 3 (MED/HIGH to MALICIOUS)\n    Args:\n        threat_level_id (str):\n    Returns:\n        int: DBOT score\n    \"\"\"\n    if threat_level_id in ('1', '2'):\n        return 3\n    if threat_level_id == '3':\n        return 2\n    if threat_level_id == '4':\n        return 0\n    return 0",
        "sha1": "f7e89532664ee09f9d06f43e255cf7b495327b35",
        "id": 687139
    },
    {
        "content": "def count_user_type(data_list):\n    \"\"\"\n    Conta os tipos de usu\u00e1rio nos registros de uma lista.\n    Argumentos:\n        data_list: Lista de registros contendo o tipo do usu\u00e1rio em uma das colunas.\n    Retorna:\n        N\u00famero de usu\u00e1rios 'Subscriber' e 'Customer', nesta ordem.\n    \"\"\"\n\n    subscriber = 0\n    customer = 0\n\n    for sample in data_list:\n        if sample[-3] == 'Subscriber':\n            subscriber += 1\n        elif sample[-3] == 'Customer':\n            customer += 1\n\n    return [subscriber, customer]",
        "sha1": "cf464f8f276e337a0254988a58b7caa9b70083cb",
        "id": 687353
    },
    {
        "content": "def map_field(field):\n    \"\"\"\n    converts field name in text (e.g. 'SOFTWARE') to field name in class Software (e.g. 'name')\n    \"\"\"\n    field_map = { 'SOFTWARE': 'name',\n                  'VERSION': 'version',\n                  'COMMIT': 'commit',\n                  'TYPE': 'software_type',\n                  'SOURCE_URL': 'source_url' }\n    if field not in field_map:\n        print(\"field{} not in field_map.\".format(field))\n\n    return field_map.get(field)",
        "sha1": "d048f11cd3df93b23ac9193e57a014fd65f60f78",
        "id": 148772
    },
    {
        "content": "def split_list(sequence, nb_splits):\n    \"\"\" Split l in n_split. It can return unevenly sized chunks.\n\n    Parameters:\n\n    sequence: iterable\n        Iterable object to be split in `nb_splits`.\n\n    nb_splits: int\n        Number of splits.\n\n    Returns:\n\n    iterable\n        `sequence` splits in `nb_splits`.\n\n    .. codeauthor:: Angelo Ziletti <angelo.ziletti@gmail.com>\n\n    \"\"\"\n\n    return [sequence[i::nb_splits] for i in range(nb_splits)]",
        "sha1": "3443dcd1e007d244f2e1d4cb7a923409c4f4d06b",
        "id": 75662
    },
    {
        "content": "def get_tags_from_image_ids(image_ids: list=[]):\n    \"\"\"\n    Get the tags for the given image_ids\n    Note that nulls and 'latest' is ignored\n    :param image_ids:\n    :return:\n    \"\"\"\n    tags = []\n\n    for image in image_ids:\n        if ('imageTag') in image:\n            tag = image['imageTag']\n            if (tag is None) or (tag == 'latest'):\n                pass\n            else:\n                tags = tags + [tag]\n\n    return tags",
        "sha1": "021822639afff780b2da92387d104e84d78f8d8e",
        "id": 522497
    },
    {
        "content": "import logging\n\n\ndef get_logger(name: str) -> logging.Logger:\n    \"\"\" Gets the appropriate logger for this backend name. \"\"\"\n    return logging.getLogger('proxytest.' + name)",
        "sha1": "d0e0f9de13d9b603326b70a6acdbff7f3288b421",
        "id": 37409
    },
    {
        "content": "def get_extension_from_filename(name):\n    \"\"\"Extracts an extension from a filename string.\n\n    returns filename, extension\n    \"\"\"\n    name = name.strip()\n    ext = ((name.split('\\\\')[-1]).split('/')[-1]).split('.')\n    if len(ext) > 1 and ext[-1] is not '':\n        nameOnly = '.'.join(name.split('.')[:-1])\n        ext = ext[-1]\n    else:\n        nameOnly = name\n        ext = None\n    return nameOnly, ext",
        "sha1": "6a327e649738e3a2b03236c216793f81aa9405da",
        "id": 211613
    },
    {
        "content": "def split_checkpoint_step(checkpoint_dir):\n    \"\"\"Helper function to return the checkpoint index number.\n\n    Args:\n        checkpoint_dir: Path directory of the checkpoints\n\n    Returns:\n        checkpoint_id: An int representing the checkpoint index\n    \"\"\"\n\n    checkpoint_name = checkpoint_dir.split('/')[-1]\n    return int(checkpoint_name.split('-')[-1])",
        "sha1": "3413cc9cbc14ed9167621dd49651637f549f72b4",
        "id": 475616
    },
    {
        "content": "def partition_labels_from_mps(mps):\n    \"\"\"\n    Get a list of partition labels, flattening any nested meta partitions in the input and ignoring sentinels.\n\n    Parameters\n    ----------\n    mps: List[MetaPartition]\n\n    Returns\n    -------\n    partition_labels: List[str]\n    \"\"\"\n    partition_labels = []\n    for mp in mps:\n        if len(mp) > 1:\n            for nested_mp in mp:\n                if not nested_mp.is_sentinel:\n                    partition_labels.append(nested_mp.label)\n        else:\n            if not mp.is_sentinel:\n                partition_labels.append(mp.label)\n    return partition_labels",
        "sha1": "a3d168e8bbe444d2fe95e6f7354d4948084a5d34",
        "id": 202955
    },
    {
        "content": "def  oddNumbers(l, r):\n  \"\"\"\n  List odd numbers within a closed interval.\n  :param l: left interval endpoint (inclusive)\n  :param r: right interval endpoint (inclusive)\n  :return: odd numbers within [l, r].\n  \"\"\"\n  l = l if l % 2 == 1 else l + 1\n  r = r if r % 2 == 0 else r + 1\n  return list(range(l, r, 2))",
        "sha1": "aa2768b013f42030a0bae2526c169c412963f235",
        "id": 32612
    },
    {
        "content": "import toml\n\n\ndef parse_toml_file(file_object):\n    \"\"\"Parses toml data using a file-like object to a dictionary.\n\n    Args:\n        file_object: (file-like object) file like object instantiated from a toml formatted file\n\n    Returns:\n        A dictionary with parsed toml data fields.\n    \"\"\"\n    return toml.load(file_object)",
        "sha1": "5fe70fec0d35ef5cfc124c0d8ea3c185b84a8e54",
        "id": 687887
    },
    {
        "content": "def graph_search(states, goal_reached, get_successors, combine, old_states=None):\n    \"\"\"\n    Given some initial states, explore a state space until reaching the goal,\n    taking care not to re-explore previously visited states.\n\n    `states`, `goal_reached`, `get_successors`, and `combine` are identical to\n    those arguments in `tree_search`.\n    `old_states` is a list of previously encountered states--these should not\n    be re-vistited during the search.\n\n    When the goal is reached, the goal state is returned.\n    \"\"\"\n    old_states = old_states or [] # initialize, if this is the initial call\n    \n    # Check for success and failure.\n    if not states:\n        return None\n    if goal_reached(states[0]):\n        return states[0]\n\n    def visited(state):\n        # A state is \"visited\" if it's in the list of current states or has\n        # been encountered previously.\n        return any(state == s for s in states + old_states)\n\n    # Filter out the \"visited\" states from the next state's successors.\n    new_states = [s for s in get_successors(states[0]) if not visited(s)]\n\n    # Combine the new states with the existing ones and recurse.\n    next_states = combine(new_states, states[1:])\n    return graph_search(next_states, goal_reached, get_successors,\n                        combine, old_states + [states[0]])",
        "sha1": "7816cff82f23c0c0a608e8b1be0ee5015a1b5210",
        "id": 492063
    },
    {
        "content": "def get_key_ordered_dictionary_repr(dictionary, delimiter=': ',\n                                    decorator_in='{', decorator_out='}'):\n    \"\"\"Get a string representation of a dictionary ordered by keys.\n\n    Parameters\n    ----------\n    dictionary : dictionary\n        Dictionary to be represented as a string.\n    delimiter : string\n        Delimiter to put between every key-value pair.\n    decorator_in : string\n        Decorator to put at the beginning of the string.\n    decorator_out : string\n        Decorator to put at the end of the string.\n\n    Returns\n    -------\n    dict_str : string\n        String representation of the key-ordered dictionary.\n\n    Notes\n    -----\n    In the string representation the dictionary keys are displayed ordered by\n    their value and properly indented, e.g.\n    {\n        'key1': value1,\n        'key2': value2,\n    }\n\n    \"\"\"\n    dict_str = decorator_in\n    for key in sorted(dictionary.keys()):\n        dict_str += \"\\n\\t'{}'{}{}\".format(key, delimiter, dictionary[key])\n    dict_str += '\\n{}'.format(decorator_out)\n    return dict_str",
        "sha1": "25ea171bfc59268bdfb4f590231e1c7c00b64193",
        "id": 92698
    },
    {
        "content": "def generate_dat_header(name, description, version):\n    \"\"\"Return the textual DAT header from a given <name>, <description>, and <version>\"\"\"\n    header = ['clrmamepro (',\n              '\\t' + 'name \"%s\"' % name,\n              '\\t' + 'description \"%s\"' % description,\n              '\\t' + 'version %s' % version,\n              ')']\n    return '\\n'.join(header)",
        "sha1": "fd2b467c2e27554a117065c63981a2287c08ee15",
        "id": 514976
    },
    {
        "content": "import hashlib\n\n\ndef _create_finding_id(control_id, resource_name, length=20):\n  \"\"\"Hash the control and resource; repeatable (tho not strictly unique).\n\n  Needs to be repeatable such that the same control/test maps to the\n  same SCC Finding over multiple runs.\n  \"\"\"\n  input = control_id + resource_name\n  hex = hashlib.sha256(input.encode('UTF-8')).hexdigest()\n  result = int(hex, 16) % (10 ** length)\n  return str(result)",
        "sha1": "a2be7749a2a937bfbcbf381371f950fd84ee11a3",
        "id": 125949
    },
    {
        "content": "import itertools\n\n\ndef zip_points_and_children(grid, child_locations):\n    \"\"\"\n    Test helper function that transforms and then zips together the provided\n    iterables.\n\n    Args:\n        grid (Grid): A Gas Lines grid.\n        child_locations (tuple): A matrix with the exact same dimensions as the grid,\n            describing the solution to the grid in terms of the child of each point.\n            Each element of the matrix gives the coordinates of the child of the\n            corresponding point in the grid at that location in the solution or, in\n            the case of `None`, represents that that point has no child.\n\n    Returns:\n        zip: An iterable of ordered pairs, each comprising of a point in the given\n            grid and its corresponding child in the solution (if it has one, otherwise\n            `None`) for the first and second entry, respectively.\n    \"\"\"\n    # Define a closure to easily get a point in the given grid based on its location\n    def get_nullable_point(location):\n        # None represents that the point does not exist\n        if location is None:\n            return None\n        # Otherwise the location is just a pair of coordinates\n        i, j = location\n        return grid[i][j]\n\n    # Flatten both provided collections at the same time for brevity\n    points, child_locations = map(\n        itertools.chain.from_iterable,\n        (grid, child_locations),\n    )\n    # Use the previously defined closure to construct a list of child points\n    children = map(get_nullable_point, child_locations)\n    # Zip each point together with its corresponding child point\n    return zip(points, children)",
        "sha1": "5222554bed05a914bc62fa90b39a389378980a1f",
        "id": 154359
    },
    {
        "content": "def get_stationary_indicator(data, window='10s', stdtol=15 / 1000):\n    \"\"\" \n    Return a boolean pandas.Series indicating stationary (low movement) periods.\n\n    :param data: A pandas.DataFrame of acceleration time-series. It must contain\n        at least columns `x,y,z` and the index must be a DateTimeIndex.\n    :type data: pandas.DataFrame.\n    :param window: Rolling window to use to check for stationary periods. Defaults to 10 seconds (\"10s\").\n    :type window: str, optional\n    :param stdtol: Standard deviation under which the window is considered stationary. \n        Defaults to 15 milligravity (0.015).\n    :type stdtol: float, optional\n    :return: Boolean pandas.Series indexed as `data` indicating stationary periods.\n    :rtype: pandas.Series\n    \"\"\"\n\n    # What happens if there are NaNs?\n    # Ans: It evaluates to False so we're good\n    stationary_indicator = ((data[['x', 'y', 'z']]\n                            .rolling(window)\n                            .std()\n                            < stdtol)\n                            .all(axis=1))\n\n    return stationary_indicator",
        "sha1": "c91c99a9667ff1dea511f464c3c8161211dc90ba",
        "id": 574521
    },
    {
        "content": "import turtle\n\n\ndef make_window(color, title):\n    \"\"\"\n    Set up the window with the given background color and title.\n    Returns the new window.\n    \"\"\"\n    window = turtle.Screen()\n    window.bgcolor(color)\n    window.title(title)\n    return window",
        "sha1": "e3a23fddbbffd0f4851a28c327b37cebf1bcda5d",
        "id": 599196
    },
    {
        "content": "import re\n\n\ndef _QuoteString(s):\n  \"\"\"Quotes a string with appropriate quotes and escaping.\n\n  This performs lite escaping by choosing enclosing quotation marks that would\n  escape the least (either single or double quotes) and escaping those quotes\n  and the backslash. Note that this does not escape newlines. If the string\n  contains embedded newlines, they will be output verbatim.\n\n  Args:\n    s: String to quote.\n\n  Returns:\n    Quotes string (possibly multiline).\n  \"\"\"\n  single_quote_count = s.count('\\'')\n  double_quote_count = s.count('\"')\n  quote_delim = '\\'' if single_quote_count <= double_quote_count else '\"'\n  # Apply escaping to the chosen quote character and the backslash.\n  encoded = re.sub(r'([%s\\\\])' % quote_delim, r'\\\\\\1', s)\n  return quote_delim + encoded + quote_delim",
        "sha1": "67e19b01a1613ba5dbac8d6b2bb295669b9d29f5",
        "id": 602813
    },
    {
        "content": "def shape_data_remove_timestamp(shape):\n    \"\"\"\n    go.Figure complains if we include the 'timestamp' key when updating the\n    figure\n    \"\"\"\n    new_shape = dict()\n    for k in shape.keys() - set([\"timestamp\"]):\n        new_shape[k] = shape[k]\n    return new_shape",
        "sha1": "c86809a424dd26dd09446911cd6e285dc198eebd",
        "id": 450704
    },
    {
        "content": "def unquote_and_split(arg, c):\n  \"\"\"Split a string at the first unquoted occurrence of a character.\n\n  Split the string arg at the first unquoted occurrence of the character c.\n  Here, in the first part of arg, the backslash is considered the\n  quoting character indicating that the next character is to be\n  added literally to the first part, even if it is the split character.\n\n  Args:\n    arg: the string to be split\n    c: the character at which to split\n\n  Returns:\n    The unquoted string before the separator and the string after the\n    separator.\n  \"\"\"\n  head = ''\n  i = 0\n  while i < len(arg):\n    if arg[i] == c:\n      return (head, arg[i + 1:])\n    elif arg[i] == '\\\\':\n      i += 1\n      if i == len(arg):\n        # dangling quotation symbol\n        return (head, '')\n      else:\n        head += arg[i]\n    else:\n      head += arg[i]\n    i += 1\n  # if we leave the loop, the character c was not found unquoted\n  return (head, '')",
        "sha1": "79e3638c7e2870ae471cc3064615a7b7a222d6e5",
        "id": 134106
    },
    {
        "content": "def bool_to_str(x):\n    \"\"\"Converts a bool to an empty string if False and the string '1' if\n    True.\n    \"\"\"\n    return \"1\" if x else \"\"",
        "sha1": "155bd33bff609c692c580fe9bfdf25df7856af1f",
        "id": 248264
    },
    {
        "content": "def is_valid_uuid (uuid):\n    \"\"\"\n    is_valid_uuid (uuid) -> bool\n\n    returns True if uuid is a valid 128-bit UUID.\n\n    valid UUIDs are always strings taking one of the following forms:\n    XXXX\n    XXXXXXXX\n    XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX\n    where each X is a hexadecimal digit (case insensitive)\n\n    \"\"\"\n    try:\n        if len (uuid) == 4:\n            if int (uuid, 16) < 0: return False\n        elif len (uuid) == 8:\n            if int (uuid, 16) < 0: return False\n        elif len (uuid) == 36:\n            pieces = uuid.split (\"-\")\n            if len (pieces) != 5 or \\\n                    len (pieces[0]) != 8 or \\\n                    len (pieces[1]) != 4 or \\\n                    len (pieces[2]) != 4 or \\\n                    len (pieces[3]) != 4 or \\\n                    len (pieces[4]) != 12:\n                return False\n            [ int (p, 16) for p in pieces ]\n        else:\n            return False\n    except ValueError: \n        return False\n    except TypeError:\n        return False\n    return True",
        "sha1": "f7db551dc98a9f1a55dcebb1cd13cfa37ac61771",
        "id": 115810
    },
    {
        "content": "def collide_circle(sprite1, sprite2):\n    \"\"\"\n    **pyj2d.sprite.collide_circle**\n    \n    Check two sprites intersect by checking by intersection of circle around their centers.\n    Will use sprite radius attribute or circle will encompass rect attribute.\n    Can be used as spritecollide callback function.\n    \"\"\"\n    if hasattr(sprite1, 'radius'):\n        radius1 = sprite1.radius\n    else:\n        radius1 = ( (((sprite1.rect.width)**2) + ((sprite1.rect.height)**2))**0.5 ) * 0.5\n    if hasattr(sprite2, 'radius'):\n        radius2 = sprite2.radius\n    else:\n        radius2 = ( (((sprite2.rect.width)**2) + ((sprite2.rect.height)**2))**0.5 ) * 0.5\n    sx1 = (sprite1.rect.x+int(sprite1.rect.width*0.5))\n    sy1 = (sprite1.rect.y+int(sprite1.rect.height*0.5))\n    sx2 = (sprite2.rect.x+int(sprite2.rect.width*0.5))\n    sy2 = (sprite2.rect.y+int(sprite2.rect.height*0.5))\n    return ( ((sx1-sx2)**2 + (sy1-sy2)**2) ) < (radius1**2+radius2**2)",
        "sha1": "8c22b2d1ce060e2801da53a99138e22484aa4e0f",
        "id": 456541
    },
    {
        "content": "def extract_segment_types(urml_document_element, namespace):\n    \"\"\"Return a map from segment node IDs to their segment type\n    ('nucleus', 'satellite' or 'isolated').\n    \"\"\"\n    segment_types = \\\n        {namespace+':'+seg.attrib['id']: seg.tag\n         for seg in urml_document_element.iter('nucleus', 'satellite')}\n\n    for seg in urml_document_element.iter('segment'):\n        seg_id = namespace+':'+seg.attrib['id']\n        if seg_id not in segment_types:\n            segment_types[seg_id] = 'isolated'\n    return segment_types",
        "sha1": "30d2050055a9c2e66da66e3663df27a9cc6852e1",
        "id": 696196
    },
    {
        "content": "def stations_highest_rel_level(stations, N):\n    \"\"\"For Task 2C - returns a list of N stations at which relative typical water level is highest\n    List is sorted in descending order\"\"\"\n    \n    list_highest_level = []\n    for station in stations:\n        m=station.relative_water_level()\n        if m==None:\n            pass\n        else:\n            list_highest_level.append((station,m))\n    \n    sorted_list_highest_level = sorted(list_highest_level, key=lambda x: x[1], reverse=True)\n\n    i=0\n    short_list=[]\n    while i<N:\n        short_list.append(sorted_list_highest_level[i][0])\n        i+=1\n\n    return short_list",
        "sha1": "b2d75e504ee8fbcf179e2dc5239ac90cc66837b2",
        "id": 374468
    },
    {
        "content": "def get_endpoint_headers(headers):\n    \"\"\"\n    Given a dictionary-like headers object, return the names of all\n    headers in that set which represent end-to-end (and not intermediate\n    or connection headers).\n    \"\"\"\n    intermediate_headers = [\n        'connection',\n        'keep-alive',\n        'proxy-authenticate',\n        'proxy-authorization',\n        'te',\n        'trailers',\n        'transfer-encoding',\n        'upgrade',\n    ]\n    intermediate_headers.extend(\n        header.strip() for header in headers.get('connection', '')\n    )\n    return set(headers.keys()) - set(intermediate_headers)",
        "sha1": "8f2785d9090e25720f5b5bee3959dfa6024a9f8c",
        "id": 455009
    },
    {
        "content": "import torch\n\n\ndef pi_to_y(Pi):\n    \"\"\"\n    Turns a class membership matrix into a vector of class labels.\n    Args:\n        Pi: A one-hot encoded class membership matrix (N, K)\n\n    Returns:\n        A vector y of class labels (N, )\n    \"\"\"\n    return torch.argmax(\n        Pi,  # (N, K)\n        dim=1\n    )",
        "sha1": "2a2f1f0ec10e1f95fb7dac687fa4495d75edaae0",
        "id": 188361
    },
    {
        "content": "from datetime import datetime\n\n\ndef store_log(db_collection, log_dict, **params):\n    \"\"\"\n    Stores the provided log dictionary in the DB collection with the additional\n    (provided) parameters.\n    \n    Args:\n        db_collection:\n        log_dict:\n        **params: Parameters to be stored, but NOT saved in the provided dict.\n                  If you want to store the changes, then \"catch\" the returned\n                  object after calling this function.\n\n    Returns:\n        Log dictionary with the existing + new parameters!\n    \"\"\"\n    # Avoid overlaps\n    log_dict = dict(log_dict)\n    \n    # Store additional info in the log dictionary\n    for key in params.keys():\n        log_dict[key] = params[key]\n\n    log_dict[\"duration\"] = str(datetime.now() - log_dict[\"start_time\"])\n    log_dict[\"timestamp\"] = datetime.now()\n    \n    db_collection.insert_one(log_dict)  # Store info in DB collection\n    \n    return log_dict",
        "sha1": "a5ce6dd94f1cda9a0929a51c1172cf09b081cb27",
        "id": 532305
    },
    {
        "content": "def spritecollideany(sprite, group):\n    \"\"\"\n    **pyj2d.sprite.spritecollideany**\n    \n    Check if sprite intersect with any sprites in group.\n    \"\"\"\n    for _sprite in group:\n        if sprite.rect.intersects(_sprite.rect):\n            return True\n    return False",
        "sha1": "726fbf88cc0e98e8b2ba3bdd2e3aa217b3c5307e",
        "id": 162003
    },
    {
        "content": "def gpsdeg2dec(lat, lon):\n    \"\"\"Converts GPS coordinates given as (deg, min, sec, dir) pairs (dir = N/S/E/W) to decimal degrees tuple\"\"\"\n    assert len(lat) == len(lon) == 4\n    vals = [lat, lon]\n    dirs = [lat[-1], lon[-1]]\n    vals = [(v[0], int(v[1])/60.0, v[2]/3600.0) for v in vals]\n    vals = [sum(v) for v in vals]\n    vals[0] *= 1 if dirs[0].lower() == 'n' else -1\n    vals[1] *= 1 if dirs[1].lower() == 'e' else -1\n    return vals",
        "sha1": "047dec6c2baf94c83bad4e0688ab7ac0b104b7ec",
        "id": 280102
    },
    {
        "content": "def getFrequencyDict(sequence):\n    \"\"\"\n    Given a sequence of letters, convert the sequence to a dictionary of\n    letters -> frequencies. Used by containsLetters().\n\n    returns: dictionary of letters -> frequencies\n    \"\"\"\n    freq = {}\n    for x in sequence:\n        freq[x] = freq.get(x,0) + 1\n    return freq",
        "sha1": "3d8bb136242c33eaec42fe6336a2b200f6a306b8",
        "id": 415519
    },
    {
        "content": "def add_to_dict(param_dict):\n    \"\"\"\n    Aggregates extra variables to dictionary\n\n    Parameters\n    ----------\n    param_dict: python dictionary\n        dictionary with input parameters and values\n\n    Returns\n    ----------\n    param_dict: python dictionary\n        dictionary with old and new values added\n    \"\"\"\n    # This is where you define `extra` parameters for adding to `param_dict`.\n\n    return param_dict",
        "sha1": "ff77474305182be35c84a4c9ddd2d6ab3ddf1ecb",
        "id": 142965
    },
    {
        "content": "def get_tables_from_mapping(source_table, dest_fields, source_dest_map):\n    \"\"\"\n    Obtain a petl tables from a source petl table and some mappings\n    source_table: petl table\n    dest_fields: list\n    source_dest_map: dict\n    \n    Returns\n    dest_table: petl table\n    \n    \"\"\"\n    source_fields = list(source_table.header())\n\n    # Build up trns_table and spl_table from the source_table\n    dest_table = source_table.addrownumbers()\n\n    # Add the new fields one at a time.  There might be a better \n    # way to do this.\n    for field in dest_fields:\n        if field in source_dest_map:\n            dest_table = dest_table.addfield(field, source_dest_map[field])\n        else:\n            dest_table = dest_table.addfield(field, '')\n\n    # Cut out the original columns from the source_table to obtain\n    # the destination tables, trns_table and spl_table\n    dest_table = dest_table.cutout(*source_fields)\n    dest_table = dest_table.cutout('row')\n\n    return dest_table",
        "sha1": "26eab2187e6a3c0729331750d5adb50c760155a8",
        "id": 676996
    },
    {
        "content": "def pursuant_child_support(responses, derived):\n    \"\"\"\n    Return a list of child support bullet points, prefaced by the correct\n    'pursuant to' phrase.\n    \"\"\"\n\n    act = derived['child_support_acts']\n    act = 'Pursuant to %s,' % act if act != '' else act\n    try:\n        arrangements = responses.get('order_for_child_support', '').split('\\n')\n        return ['%s %s' % (act, arrangement.strip())\n                for arrangement in arrangements\n                if len(arrangement.strip()) > 0]\n    except ValueError:\n        return []",
        "sha1": "f50caab270c57684825e4180edb20eb69980caba",
        "id": 274089
    },
    {
        "content": "def apply(gate, qubit):\n    \"\"\"\n    Simply apply a gate to a single qubit\n\n    Args:\n        gate: The gate to apply\n        qubit: The target qubit\n\n    Returns: Valid QASM that represents this application\n\n    \"\"\"\n    return \"{} q[{}]\\n\".format(gate, qubit)",
        "sha1": "9723cee2fa7ae345b8eb0c38e8f9ed622d99950b",
        "id": 160492
    },
    {
        "content": "def G2mu(G, E):\n    \"\"\"compute poisson ratio from shear modulus and Youngs modulus E\n    \"\"\"\n    return E/(2*G)-1",
        "sha1": "d4f6172e165b574a4e62354c7e4c583d8cf0466b",
        "id": 436193
    },
    {
        "content": "from typing import Any\n\n\ndef sort_internal_lists(data: Any) -> Any:\n    \"\"\"\n    Sort all lists & sets within a given data structure\n    :param data: Data structure to internally sort\n    :return Data structure with sorted lists\n    \"\"\"\n    if isinstance(data, dict):\n        for key, value in data.items():\n            data[key] = sort_internal_lists(value)\n    elif isinstance(data, (set, list)):\n        return sorted(list(data))\n\n    return data",
        "sha1": "bcbe7cbf1caaeb8bdf61f575a2d06c2090c82a63",
        "id": 64281
    },
    {
        "content": "import aiohttp\nimport ssl\nimport certifi\n\n\ndef get_connector() -> aiohttp.BaseConnector:\n    \"\"\"Return the connector for aiohttp.\"\"\"\n\n    def client_context() -> ssl.SSLContext:\n        \"\"\"Return an SSL context for making requests.\"\"\"\n        context = ssl.create_default_context(\n            purpose=ssl.Purpose.SERVER_AUTH, cafile=certifi.where()\n        )\n        return context\n\n    connector = aiohttp.TCPConnector(enable_cleanup_closed=True, ssl=client_context())\n\n    return connector",
        "sha1": "705d48cf769d8b62693cd69ee1875fabe9ea628e",
        "id": 442665
    },
    {
        "content": "def _parameter_samples_from_gibbs_posterior(model, posterior_samples):\n  \"\"\"Extracts samples of model parameters from Gibbs posterior samples.\"\"\"\n  posterior_samples_dict = posterior_samples._asdict()\n  get_param = lambda param_name: posterior_samples_dict[  # pylint: disable=g-long-lambda\n      [k for k in posterior_samples_dict if k in param_name][0]]\n  return [get_param(param.name) for param in model.parameters]",
        "sha1": "dd46030e52c70db42d818a6ddeaaf3bf1a1266d5",
        "id": 343641
    },
    {
        "content": "def formatAustralianPhoneNumber(n):\n    \"\"\"Formats a string of numbers (no spaces) into an Australian mobile or landline.\"\"\"\n    if len(n)!=10:\n        return n\n    if n[1]=='4':\n        return u'{0} {1} {2}'.format(n[0:4],n[4:7],n[7:10])\n    # else assume landline\n    return u'({0}) {1} {2}'.format(n[0:2],n[2:6],n[6:10])",
        "sha1": "7cce2b9634233dbda54b9e9da0c544d795081a18",
        "id": 349094
    },
    {
        "content": "def is_bit_used_other_than_carry4_cin(design, top_module, bit, bit_to_cells):\n    \"\"\" Is the net bit specified used by any sinks other than a carry chain? \"\"\"\n    cells = design[\"modules\"][top_module][\"cells\"]\n    list_of_cells = bit_to_cells[bit]\n    assert len(list_of_cells) == 2, bit\n\n    direct_cellname, port, _ = list_of_cells[1]\n    direct_cell = cells[direct_cellname]\n    assert direct_cell['type'] == \"CARRY_CO_DIRECT\"\n    assert port == \"CO\"\n\n    # Follow to output\n    connections = direct_cell[\"connections\"][\"OUT\"]\n    assert len(connections) == 1\n\n    for cellname, port, bit_idx in bit_to_cells[connections[0]][1:]:\n        cell = cells[cellname]\n        if cell[\"type\"] == \"CARRY_COUT_PLUG\" and port == \"CIN\":\n            continue\n        else:\n            return True, direct_cellname\n\n    return False, direct_cellname",
        "sha1": "2f526eda3d74180571c3e9ce4ec65531dc4559fa",
        "id": 174685
    },
    {
        "content": "def is_relation(uri):\n    \"\"\"\n    >>> is_relation('/r/IsA')\n    True\n    >>> is_relation('/c/sv/kl\u00e4nning')\n    False\n    \"\"\"\n    return uri.startswith('/r/')",
        "sha1": "99685a92c6aa3abc3f7b618b7971437632ae09f5",
        "id": 313596
    },
    {
        "content": "def tribonacci_recursive(n):\n    \"\"\"\n    Uses recursion to calculate the nth tribonacci number\n    Args:\n        n: the number\n\n    Returns:\n        nth tribonacci number\n    \"\"\"\n    if n <= 1:\n        return 0\n    elif n == 2:\n        return 1\n    else:\n        return tribonacci_recursive(n - 1) + tribonacci_recursive(n - 2) + \\\n                tribonacci_recursive(n - 3)",
        "sha1": "cf43f058d91d6e08b4c84d37ef498e0ed1b0e403",
        "id": 601184
    },
    {
        "content": "def create_nine_digit_product(num):\n    \"\"\" Create a nine digit string resulting from the concatenation of\n    the product from num and  multipliers (1, 2, 3,).... Return 0 if string\n    cannot be length 9.\n\n    \"\"\"\n    result = ''\n    counter = 1\n    while len(result) < 9:\n        result += str(num * counter)\n        counter += 1\n    if len(result) > 9:\n        result = 0\n    return result",
        "sha1": "9c6765349edfa7e03dc8d2ffe7bf6a45155f3ad0",
        "id": 13566
    },
    {
        "content": "import re\n\n\ndef _expand_h(string):\n    \"\"\" Convert Huawei-style port ranges into list of ports \"\"\"\n    result = []\n    for element in re.split('(?<!to) (?!to)', string):\n        m = re.match('(\\d+) to (\\d+)', element)\n        if m:\n            for num in range(int(m.group(1)), int(m.group(2)) + 1):\n                result.append(str(num))\n        else:\n            result.append(element)\n    return result",
        "sha1": "37334477e7bf830a8c430040b26187bfe7c13e5d",
        "id": 52312
    },
    {
        "content": "import math\n\n\ndef transit_distance(px: float, py: float, ox: float, oy: float) -> float:\n    \"\"\"Given an object at position px, py and an observer as ox, oy this returns the\n    the distance p -> o.\"\"\"\n    d = math.sqrt((oy - py)**2 + (ox - px)**2)\n    return d",
        "sha1": "76d0175a785765d33baa7162deb1217d10a5832a",
        "id": 327919
    },
    {
        "content": "def parse_null(dictionary):\n    \"\"\"\n    Remove null, None or '' values from a dictionary\n\n    :type dictionary: dict\n    :param dictionary: dict of parameters to parse\n    :rtype: dict\n    :return: dictionary that has been sanitized of all null values\n    \"\"\"\n    # Copy the dictionary to get around deleting keys while iterating\n    parsed_dict = dict(dictionary)\n    for key, item in dictionary.items():\n        # Search for all None and \"\" items, ignore bools, 0's etc.\n        if not item and isinstance(item, (type(None), str)):\n            del parsed_dict[key]\n\n    return parsed_dict",
        "sha1": "f7a371cf5d40926924e0c62f0e22946e2c90751b",
        "id": 160177
    },
    {
        "content": "def _get_kafka_config(config: dict) -> dict:\n    \"\"\"Returns configurations for a consumer instance\n\n    Parameters\n    ----------\n    config: dict\n        Dictionary of configurations\n\n    Returns\n    ----------\n    kafka_config: dict\n        Dictionary with configurations for creating an instance of\n        a secured Kafka consumer\n    \"\"\"\n    kafka_config = {}\n    default_config = {\n        \"auto.offset.reset\": \"earliest\"\n    }\n\n    if 'username' in config and 'password' in config:\n        kafka_config[\"security.protocol\"] = \"sasl_plaintext\"\n        kafka_config[\"sasl.mechanism\"] = \"SCRAM-SHA-512\"\n        kafka_config[\"sasl.username\"] = config[\"username\"]\n        kafka_config[\"sasl.password\"] = config[\"password\"]\n\n    kafka_config[\"group.id\"] = config[\"group_id\"]\n\n    kafka_config.update(default_config)\n\n    # use servers if given\n    if 'bootstrap.servers' in config:\n        kafka_config[\"bootstrap.servers\"] = config[\"bootstrap.servers\"]\n    else:\n        # use default fink_servers\n        fink_servers = [\n            \"localhost:9093\",\n            \"localhost:9094\",\n            \"localhost:9095\"\n        ]\n        kafka_config[\"bootstrap.servers\"] = \"{}\".format(\",\".join(fink_servers))\n\n    return kafka_config",
        "sha1": "a766c29758e4bc46a39534a5fcce508046ec78c4",
        "id": 333891
    },
    {
        "content": "def is_valid_project_type(type):\n    \"\"\" Validates supported project types; currently only maven is supported; hopefully gradle in the future \"\"\"\n\n    if not type:\n        return False\n\n    if \"maven\" != type:\n        return False\n\n    return True",
        "sha1": "b450423c06d778701c639c3175b87b271ddfb5d9",
        "id": 638075
    },
    {
        "content": "def merge_dicts(*zs):\n    \"\"\"Merge one or more dictionaries into one without modifying any argument dict\n\n    In case of duplicate keys, values may be accidentally overridden.\n\n    >>> merge_dicts({1:10,2:20}, {3:30}, {4:40})\n    {1: 10, 2: 20, 3: 30, 4: 40}\n    >>> merge_dicts({1:10,2:20}, {1:21}, {1:23})\n    {1: 23, 2: 20}\n\n    \"\"\"\n    res = {}\n    for z in zs:\n        res.update(z)\n    return res",
        "sha1": "b79e288c9f216aaf94d0fbd432309129f04bad75",
        "id": 396402
    },
    {
        "content": "def get_string_commas_num( num ):\n    \"\"\"\n    This is the secret-sauce of formatting integers as strings with commas for every 3 digits. For example, ``1234`` becomes \"1,234\". I copied code from `this location`_.\n\n    :param int num: input number.\n    :returns: the nicely formatted output :py:class:`string <str>` representing an input number.\n    :type: str\n\n    .. _`this location`: https://intellipaat.com/community/2447/how-to-print-number-with-commas-as-thousands-separators\n    \"\"\"\n    return \"%s\" % f\"{num:,d}\"",
        "sha1": "4f49f4bb755ff012b3ca9bbe039919348a52285d",
        "id": 26446
    },
    {
        "content": "from typing import Tuple\nfrom typing import Optional\n\n\ndef split_id3_title(id3_title: str) -> Tuple[str, Optional[str]]:\n    \"\"\"\n    Take a 'Title (role)'-style ID3 title and return (title, role)\n    \"\"\"\n    role = None\n\n    bracket_depth = 0\n    for i in range(1, len(id3_title)+1):\n        char = id3_title[-i]\n        if char == ')':\n            bracket_depth += 1\n        elif char == '(':\n            bracket_depth -= 1\n\n        if bracket_depth == 0:\n            if i != 1:\n                role = id3_title[len(id3_title)-i:]\n            break\n\n    if role:\n        title = id3_title.replace(role, '').strip()\n        role = role[1:-1]  # strip brackets\n    else:\n        title = id3_title\n\n    return title, role",
        "sha1": "c156cef38c815ca6e971c5bbc4f29d1a6f72ffa1",
        "id": 315917
    },
    {
        "content": "import pickle\n\n\ndef load_scm_object(pkl_file_path):\n    \"\"\"\n    Deserializes SCM object from pickle file and return\n    Args:\n        pkl_file_path: str\n\n    Returns: SCM()\n\n    \"\"\"\n    with open(pkl_file_path, \"rb\") as file:\n        return pickle.load(file)",
        "sha1": "9c40e930818eee088385bb6753faad4f210e1963",
        "id": 556628
    },
    {
        "content": "def _check_ldap3_operation(conn):\n    \"\"\"Checks that the ldap3 operation succeeded/failed.\n\n    :param conn:\n        The `ldap3.Connection` that the operation was made.\n    :return:\n        `True` if the operation succeed; false otherwise\n    \"\"\"\n    result_code = conn.result['result']\n    if result_code in (0, 68):\n        return True\n\n    return False",
        "sha1": "90f8701ff3d91c5872f4ed7a6b0d1034ac7ae16d",
        "id": 348183
    },
    {
        "content": "def kassassa_rahaa(kassa):\n    \"\"\"Palauttaa kassassa olevan rahan m\u00e4\u00e4r\u00e4n.\n\n    Parametrit\n    ----------\n    pakka : array of float\n        Kassa\n\n    Palauttaa\n    ---------\n    float\n        Kassassa olevan rahan m\u00e4\u00e4r\u00e4\n    \"\"\"\n    return kassa[0]",
        "sha1": "9c6dfe083abb3f3c28fb871b9d4bf08d968768d8",
        "id": 279391
    },
    {
        "content": "def boom_7_list_comprehension(number):\n    \"\"\"itirate from zero to number with list comprehension,if number is divisable by 7 or contain 7 as a digit we'll add 'BOOM' to the list else we'll add the number\"\"\"\n    return [\"BOOM\" if i % 7 == 0 or '7' in str(i) else i for i in range(number + 1)]",
        "sha1": "2ae4c1b484b149e1818d3f7171716efe5756aa96",
        "id": 329868
    },
    {
        "content": "from typing import List\nimport re\n\n\ndef split_camel(s: str) -> List[str]:\n    \"\"\"\n    Split a string by starting chars of UpperCamelCase\n    :param s: the input string.\n    :return: a list of strings that was split by capital letters.\n    \"\"\"\n    regex = re.compile('[A-Z][^A-Z]*')\n    return regex.findall(s)",
        "sha1": "10ff8fa052cf3ce5891a75c3fcad1ed1b3b9091c",
        "id": 283489
    },
    {
        "content": "import re\n\n\ndef get_version_from_appversion_text(appversion_text):\n    \"\"\"\n    >>> # these first two could certainly be replaced\n    >>> # with more realistic examples, but I didn't have any on hand\n    >>> get_version_from_appversion_text('foofoo #102 barbar')\n    102\n    >>> get_version_from_appversion_text('foofoo b[99] barbar')\n    99\n    >>> get_version_from_appversion_text(\n    ...     'CommCare ODK, version \"2.11.0\"(29272). App v65. '\n    ...     'CommCare Version 2.11. Build 29272, built on: February-14-2014'\n    ... )\n    65\n    >>> get_version_from_appversion_text(\n    ...     'CommCare ODK, version \"2.4.1\"(10083). App v19.'\n    ...     'CommCare Version 2.4. Build 10083, built on: March-12-2013'\n    ... )\n    19\n\n    \"\"\"\n\n    patterns = [\n        r' #(\\d+) ',\n        'b\\[(\\d+)\\]',\n        r'App v(\\d+).',\n    ]\n    if appversion_text:\n        for pattern in patterns:\n            match = re.search(pattern, appversion_text)\n            if match:\n                build_number, = match.groups()\n                return int(build_number)",
        "sha1": "03b8c020c6a81e5ea5d28eb184bbdc0cabe085a6",
        "id": 135302
    },
    {
        "content": "def list_certificate_issuer_admins(client, vault_base_url, issuer_name):\n    \"\"\" List admins for a specified certificate issuer. \"\"\"\n    return client.get_certificate_issuer(\n        vault_base_url, issuer_name).organization_details.admin_details",
        "sha1": "37c8411b69c7bd3967d4ffe22c8b039236379625",
        "id": 39850
    },
    {
        "content": "def index(predicate, seq):\n    \"\"\"\n    Returns the index of the element satisfying the given predicate, or None.\n    \"\"\"\n    try:\n        return next(i for (i, e) in enumerate(seq) if predicate(e))\n    except StopIteration:\n        return None",
        "sha1": "c0a7b055e386a6766d4a280aadbfb523e1060514",
        "id": 652264
    },
    {
        "content": "def prepare_commentdoc(s):\n    \"\"\"\n    Extract documentation comment lines (starting with #:) and return them as a\n    list of lines.  Returns an empty list if there is no documentation.\n    \"\"\"\n    result = []\n    lines = [line.strip() for line in s.expandtabs().splitlines()]\n    for line in lines:\n        if line.startswith('#:'):\n            line = line[2:]\n            # the first space after the comment is ignored\n            if line and line[0] == ' ':\n                line = line[1:]\n            result.append(line)\n    if result and result[-1]:\n        result.append('')\n    return result",
        "sha1": "f6c7917a572c15c3cfde3719af2b28a8cfec6f42",
        "id": 397339
    },
    {
        "content": "import random\n\n\ndef sample_random_complex_vector(length):\n    \"\"\"Samples a random complex vector,\n\n    Samples a vector with elements of the form a + bi where a and b\n    are chosen uniformly at random from the set [0, 1).\n\n    Args:\n        length (int): Length of vector.\n\n    Returns:\n        A list of randomly sampled complex values.\n    \"\"\"\n    sample = [0] * length\n    for i in range(length):   \n        a = random.random()\n        b = random.random()\n        sample[i] = a + b * 1j\n    return sample",
        "sha1": "3ca370d6f6dfdc7ac06806ddb47bc7a98ecc429f",
        "id": 420291
    },
    {
        "content": "def get_baseline_probs(baseline_f):\n    \"\"\"\n    Read in baseline probabilities from a file that has them listed\n    in the first line non commented line. # lines are ignored as comments\n\n    Args:\n        baseline_f (str): a file containing a probability array of the form:\n            [ PrA PrC PrG PrT ]\n           Where PrA + PrC + PrG + PrT = 1 (and all are positive and non-zero)\n\n        note1: the file can contain header lines marked by #\n        note2: file format can technically be separated by arbitrary strings of\n            any whitespace characters (space, tab, newline, return, formfeed).\n            Only space tested during code development.\n        note3: Separator must be whitespace, commas converted to empty string.\n        note4: Subsequent lines ignored once a properly formatted line is found\n\n    Returns:\n        bp_array (list): List of the probabilities for each base: [ PrA, PrC, PrT, PrG ]\n\n    \"\"\"\n\n    # Default baseline probability numbers (assumes all are equally likely)\n    bp_array = [0.25, 0.25, 0.25, 0.25]\n\n    # YYY: note safer if check for invalid files everywhere\n    if baseline_f is None:\n        return bp_array\n\n    with open(baseline_f) as f:\n        try:\n            for line in f:\n                # remove header\n                if line.startswith(\"#\"):\n                    continue\n                # remove commas, brackets, and whitespace on far left and right\n                line = line.strip().replace('[', '').replace(']', '').replace(',', '')\n                if line != \"\":\n                    line = line.split()\n                    if (len(line) >= 4):\n                        for idx in range(4):\n                            bp_array[idx] = float(line[idx])\n                        return bp_array\n        except ValueError:\n            print((\"**ERROR** Baseline probability file incorrectly formatted.\\n\" +\n                  \"\\tFile should contain only [ PrA PrC PrG PrT ] \\n\" +\n                  \"\\tWhere PrA + PrC + PrG + PrT = 1 (and all are positive and non-zero)\\n\" +\n                  \"\\tContinuing with default probabilities: \" + format(bp_array)))\n            return bp_array\n\n        print((\"**ERROR** Empty baseline probability file found.\\n\" +\n              \"\\tFile should contain only [ PrA PrC PrG PrT ] \\n\" +\n              \"\\tWhere PrA + PrC + PrG + PrT = 1 (and all are positive and non-zero)\\n\" +\n              \"\\tContinuing with default probabilities: \" + format(bp_array)))\n        return bp_array",
        "sha1": "d206f4f2dfe6816b2761c21386f042b9a4b7e889",
        "id": 256099
    },
    {
        "content": "def add_up_errors(list_keyerror):\n    \"\"\"\n    #################################################################################\n    Description:\n    Adds in one string the given keyerrors from the list of keyerrors\n    #################################################################################\n\n    :param list_keyerror: list of string\n        list of keyerrors (strings or None) listing encountered errors\n\n    :return keyerror: string\n        string of all encountered errors concatenated\n    \"\"\"\n    keyerror = ''\n    for key in list_keyerror:\n        if len(keyerror) > 0 and key is not None:\n            keyerror += \" ; \"\n        if key is not None:\n            keyerror += str(key)\n    if keyerror == '':\n        keyerror = None\n    return keyerror",
        "sha1": "62e61fa97dc3bfcd5f62d312d3f573a730479dde",
        "id": 691424
    },
    {
        "content": "def d(b, a, bnds):\n    \"\"\"\n    Calculate difference between two images\n    \n    Parameters:\n        b (ee.Image): 'before' image\n        a (ee.Image): 'after' image\n        bnds (ee.List<str>): band names included in calculation\n        \n    Returns:\n        ee.Image: difference image with original bands\n    \"\"\"    \n    return b.select(bnds).subtract(a.select(bnds))",
        "sha1": "9a08abccd84ab39da7f8f4fad04f1e9d9beb4fa7",
        "id": 672872
    },
    {
        "content": "import io\n\n\ndef get_commit_readme(commit):\n    \"\"\"Return the README.md file contents at the given commit.\"\"\"\n    readme_file_blob = commit.tree / \"README.md\"\n    with io.BytesIO(readme_file_blob.data_stream.read()) as f:\n        readme_file = f.read().decode(\"utf-8\")\n    return readme_file",
        "sha1": "43bd8df27b89406c0786a5776c4b3774eff07413",
        "id": 310289
    },
    {
        "content": "def legendre_polynomial(x: float, n: int) -> float:\n    \"\"\"Evaluate n-order Legendre polynomial.\n\n    Args:\n        x: Abscissa to evaluate.\n        n: Polynomial order.\n\n    Returns:\n        Value of polynomial.\n    \"\"\"\n    if n == 0:\n        return 1\n    elif n == 1:\n        return x\n    else:\n        polynomials = [1, x]\n        for k in range(1, n):\n            new = ((2 * k + 1) * x * polynomials[-1] - k * polynomials[-2]) / (k + 1)\n            polynomials.append(new)\n        return polynomials[-1]",
        "sha1": "9c1890f54ae0a91b8d4cd8771f7a944fd3f7e7ab",
        "id": 638006
    },
    {
        "content": "def process_group(grp):\n    \"\"\"\n\n    Given a list of list of ints,\n    find the first record with the property\n    that it cannot be represented as a paired\n    sum of 25 consecutive records above it.\n\n    :param grp: The list of list of ints.\n    :return: The first entry that satisfies\n    the given property.\n\n    \"\"\"\n\n    def check_preamble(_pr, _num):\n        \"\"\"\n        Given a list of 25 consecutive entries,\n        compute if there exist two distinct entries\n        that sum to `_num`.\n\n        :param _pr: A sublist of `grp`.\n        :param _num: The target sum.\n\n        \"\"\"\n        for i in range(25):\n            for j in range(i + 1, 25):\n                if _pr[i] + _pr[j] == _num:\n                    return True\n        return False\n\n    grp = list(map(int, grp))\n    _preamble = list(map(int, grp[:25]))\n\n    for i in range(25, len(grp)):\n        current = grp[i]\n        if not check_preamble(_preamble, current):\n            return current\n        else:\n            _preamble.pop(0)\n            _preamble.append(int(current))\n\n    return -1",
        "sha1": "61810ba81455c0b3135f03bdd70b3f38cc1cbb09",
        "id": 291180
    },
    {
        "content": "def gpm2m3_s(gpm):\n    \"\"\"gpm -> m^3/s\"\"\"\n    return 6.3090196e-05*gpm",
        "sha1": "84feddeca5bffe56eb17842a97b2a436c916a2db",
        "id": 263941
    },
    {
        "content": "def get_neighbors_of(cell):\n    \"\"\"\n    Return the neighbors of cell.\n    \"\"\"\n    x = cell[0]\n    y = cell[1]\n    neighbors = set()\n    for i in range(x-1, x+2):\n        for j in range(y-1, y+2):\n            if (i, j) != cell:\n                neighbors.add((i, j))\n\n    return neighbors",
        "sha1": "5d415503e0cafa3fb834a8b82b6d230bd705b288",
        "id": 149939
    },
    {
        "content": "def format_vertical_headers(df):\n    \"\"\"Display a dataframe with vertical column headers\"\"\"\n    styles = [dict(selector=\"th\", props=[('width', '40px')]),\n              dict(selector=\"th.col_heading\",\n                   props=[(\"writing-mode\", \"vertical-rl\"),\n                          ('transform', 'rotateZ(180deg)'), \n                          ('height', '290px'),\n                          ('vertical-align', 'top')])]\n    return (df.fillna('').style.set_table_styles(styles))",
        "sha1": "b7a93a997575e1ba044f6be4de7e11fcf7c29c4a",
        "id": 461880
    },
    {
        "content": "import re\n\n\ndef _str2spec(spec):\n    \"\"\"\n    given a spec string return a dict with specs\n\n    >>> _str2spec('addition(a, b) --> (c)')\n    {'inargs': ['a', 'b'], 'inkwargs': [], 'outargs': ['c']}\n    >>> _str2spec('addition(a, b){cell} --> (c){table}')\n    {'inargs': ['a', 'b'],\n     'inkwargs': [],\n     'outargs': ['c'],\n     'inview': 'cell',\n     'outview': 'table'}\n    \"\"\"\n    # remove spaces\n    spec2 = re.sub(' ', '', spec)\n    left1, right1 = tuple(spec2.split('-->'))\n    _, inargs0, inview0 = tuple(re.split('\\\\(|\\\\)', left1))\n    inargs1 = inargs0.split(',')\n    _, outargs0, outview0 = tuple(re.split('\\\\(|\\\\)', right1))\n    outargs1 = outargs0.split(',')\n    rval = dict(inargs=inargs1, inkwargs=[], outargs=outargs1)\n    if len(inview0) > 0:\n        rval['inview'] = re.sub('\\\\{|\\\\}', '', inview0)\n    if len(outview0) > 0:\n        rval['outview'] = re.sub('\\\\{|\\\\}', '', outview0)\n    return rval",
        "sha1": "670d11c8da661dcd50287ca1e06bd9a9b51e8cba",
        "id": 157870
    },
    {
        "content": "def try_value_to_bool(value, strict_mode=True):\n    \"\"\"Tries to convert value into boolean.\n\n    strict_mode is True:\n    - Only string representation of str(True) and str(False)\n      are converted into booleans;\n    - Otherwise unchanged incoming value is returned;\n\n    strict_mode is False:\n    - Anything that looks like True or False is converted into booleans.\n    Values accepted as True:\n    - 'true', 'on', 'yes' (case independent)\n    Values accepted as False:\n    - 'false', 'off', 'no' (case independent)\n    - all other values are returned unchanged\n    \"\"\"\n    if strict_mode:\n        true_list = ('True',)\n        false_list = ('False',)\n        val = value\n    else:\n        true_list = ('true', 'on', 'yes')\n        false_list = ('false', 'off', 'no')\n        val = str(value).lower()\n\n    if val in true_list:\n        return True\n    elif val in false_list:\n        return False\n    return value",
        "sha1": "e0bf5e3f39bc8b5dfe450027f0ee5360d520d1b3",
        "id": 287239
    },
    {
        "content": "def get_last_modified(model, objects):\n  \"\"\"Get last object update time for Last-Modified header.\"\"\"\n  if hasattr(model, 'updated_at') and objects:\n    last_modified = max(obj.updated_at for obj in objects)\n  else:\n    last_modified = None\n  return last_modified",
        "sha1": "82a1c4b68cc9eeebe102a6b21b652efa34b52c1c",
        "id": 365819
    },
    {
        "content": "def all_required_args_set(args, required, definitions) -> bool:\n    \"\"\"\n    Check that all requried args are set, print details on any missing.\n    \"\"\"\n    set = True\n\n    for arg in required:\n        if not getattr(args, arg, False):\n            print('%s (%s) required, missing.' % (definitions.get(arg), arg))\n            set = False\n\n    return set",
        "sha1": "ca37c2224105dd74ace0b686ebab988f9b549bd1",
        "id": 506705
    },
    {
        "content": "import networkx as nx\n\n\ndef reachable_from(graph, node):\n    \"\"\"\n    Given an nx graph and a node within this graph,\n    return all of the nodes in the same connected component as the input node.\n    \"\"\"\n    if isinstance(graph, nx.DiGraph):\n        conn = nx.all_pairs_node_connectivity(graph)\n        return [n1 for (n1, dist) in conn[node].iteritems() if dist > 0]\n\n    elif isinstance(graph, nx.Graph):\n        for comp in nx.connected_components(graph):\n            if node in comp:\n                return comp\n\n\n    # Shouldn't get here\n    # since the node should appear in some connected component of the graph\n    raise Exception(\"Node {} not in graph\".format(node))",
        "sha1": "8364f61f602fce28798f6bf4c912fcfcc6df78a5",
        "id": 361147
    },
    {
        "content": "def ngens(x):\n    \"\"\"\n    Return the number of generators of ``x``.\n\n    EXAMPLES::\n\n        sage: R.<x,y> = SR[]; R\n        Multivariate Polynomial Ring in x, y over Symbolic Ring\n        sage: ngens(R)\n        2\n        sage: A = AbelianGroup(5, [5,5,7,8,9])\n        sage: ngens(A)\n        5\n        sage: ngens(ZZ)\n        1\n    \"\"\"\n    return x.ngens()",
        "sha1": "f348aa56bf38bc4112030e5633f6e2b6b18eed7d",
        "id": 499045
    },
    {
        "content": "def values(d):\n    \"\"\"Try to return `dict.itervalues()`, else `dict.values()`.\"\"\"\n    try:\n        return d.itervalues()\n    except AttributeError:\n        return d.values()",
        "sha1": "e5e06ea467bca7d6fc2c4971c07ffc445444d468",
        "id": 216873
    },
    {
        "content": "def space(string, length):\n    \"\"\"\n\n    :param string: '556e697432332d41'\n    :param length: 4\n    :return: 556e 6974 3233 2d41\n    \"\"\"\n    return ' '.join(\n        string[i:i + length] for i in range(0, len(string), length))",
        "sha1": "7b5273be6e2680ed8f3e2127b357085981bead66",
        "id": 445293
    },
    {
        "content": "def ShellEscape(s):\n  \"\"\"Escape a string to be passed to the shell.\n  Args:\n    s: String to escape.\n  Returns:\n    Escaped string.\n  \"\"\"\n  return \"'\" + s.replace(\"'\", \"'\\\\''\") + \"'\"",
        "sha1": "98e0702602e527040f4f71630fb231ceeb4977df",
        "id": 298461
    },
    {
        "content": "def is_delete_name(name):\n    \"\"\"\n    Determines if the specified name is flagged for deletion with the \"!\" prefix.\n    :param name: the name to be checked\n    :return: True if the name is prefixed, false otherwise\n    \"\"\"\n    return name.startswith(\"!\")",
        "sha1": "9d7055b8bafdc271cb6492af3960a75546b3af9c",
        "id": 290109
    },
    {
        "content": "def _topology_name(topology_re, topology):\n    \"\"\"Returns the name if a config's topology regex matches, None otherwise.\"\"\"\n    match = topology_re.match(topology.get('name'))\n    if match:\n        return match.group(1)\n    else:\n        return None",
        "sha1": "f08e8722c5d1115e0f954e4d6126e336a586dbca",
        "id": 503192
    },
    {
        "content": "def reverse_lookup(d, v):\n    \"\"\"Retorna uma lista com as chaves do dicion\u00e1rio 'd' que apontam para um determinado valor 'v'.\n\n    Args:\n        d (dict): dicion\u00e1rio no qual queremos encontrar todas as chaves para o valor 'v'.\n        v (any): valor pelo qual desejamos encontrar as chaves correspondentes no dicion\u00e1rio 'd'.\n\n    Returns:\n        list: Todas as chaves referentes ao valor 'v' presente no dicion\u00e1rio 'd'.\n    \"\"\"\n\n    out = []\n    for key, value in d.items():\n        if value == v:\n            out.append(key)\n    return out",
        "sha1": "b19df6a7c20b0c4f9ee8d2ec24aa206704b06357",
        "id": 119005
    },
    {
        "content": "def process_data(data_batch, cuda=False, sep_target=True):\n    \"\"\"A data pre-processing util which construct the input `Tensor` for model\n\n    Args:\n        - data_batch: a batched data from `PaddedDataset`\n        - cuda: indicates whether to put data into GPU\n        - sep_target: return separated input and target if turned on\n\n    Returns:\n        - input: the input data batch\n        - target: target data if `sep_target` is True, else a duplicated input\n        - effective_length: the useful sentence length for loss computation <s> is ignored\n        \"\"\"\n\n    batch_sentence, length = data_batch\n    if cuda:\n        batch_sentence = batch_sentence.cuda()\n        length = length.cuda()\n\n    # cut the padded sentence to max sentence length in this batch\n    max_len = length.max()\n    batch_sentence = batch_sentence[:, :max_len]\n\n    # the useful sentence length for loss computation <s> is ignored\n    effective_length = length - 1\n\n    if sep_target:\n        data = batch_sentence[:, :-1]\n        target = batch_sentence[:, 1:]\n    else:\n        data = batch_sentence\n        target = batch_sentence\n\n    data = data.contiguous()\n    target = target.contiguous()\n    effective_length = effective_length\n\n    return data, target, effective_length",
        "sha1": "71c7947382573cd63e3c28fb29f4c5e8c8aab3b0",
        "id": 339335
    },
    {
        "content": "def flattening(rad):\n    \"\"\"Flattening of a spheroid\n\n    Arguments\n    ---------\n    rad : dict\n        {'a', 'b', 'c'} Equatorial1, Equatorial2, and polar radius\n    \"\"\"\n    a, b, c = rad['a'], rad['b'], rad['c']\n    return (a-c)/a",
        "sha1": "a0be6b77d7c3f6e33c8c8434737bdfc3336e0170",
        "id": 334891
    },
    {
        "content": "import re\n\n\ndef shell_escape_filename(filename):\n    \"\"\" Escape filename for use as shell argument. \"\"\"\n    return re.sub(r'(\\s|[\\\\\\'\"|()<>{}$&#?*`!;])', r'\\\\\\1', filename)",
        "sha1": "4fb5d85956f4d4031d6069d67b8444b79a3cbab9",
        "id": 671211
    },
    {
        "content": "def message_list(transport, request, queue_name, callback=None, **kwargs):\n    \"\"\"Gets a list of messages in queue `queue_name`\n\n    :param transport: Transport instance to use\n    :type transport: `transport.base.Transport`\n    :param request: Request instance ready to be sent.\n    :type request: `transport.request.Request`\n    :param queue_name: Queue reference name.\n    :type queue_name: `six.text_type`\n    :param callback: Optional callable to use as callback.\n        If specified, this request will be sent asynchronously.\n        (IGNORED UNTIL ASYNC SUPPORT IS COMPLETE)\n    :type callback: Callable object.\n    :param kwargs: Optional arguments for this operation.\n        - marker: Where to start getting messages from.\n        - limit: Maximum number of messages to get.\n        - echo: Whether to get our own messages.\n        - include_claimed: Whether to include claimed\n            messages.\n    \"\"\"\n\n    request.operation = 'message_list'\n    request.params['queue_name'] = queue_name\n\n    # NOTE(flaper87): Assume passed params\n    # are accepted by the API, if not, the\n    # API itself will raise an error.\n    request.params.update(kwargs)\n\n    resp = transport.send(request)\n\n    if not resp.content:\n        # NOTE(flaper87): We could also return None\n        # or an empty dict, however, we're giving\n        # more value to a consistent API here by\n        # returning a compliant dict with empty\n        # `links` and `messages`\n        return {'links': [], 'messages': []}\n\n    return resp.deserialized_content",
        "sha1": "49e0aa538d5f1b83ac24545040f9409fa5193432",
        "id": 396406
    },
    {
        "content": "def unpackCommitPinUpdateMessage(msg):\n    \"\"\"if 'msg' is a commit-pin update message, return the repo, branch, and sha-hash of the updated pin.\"\"\"\n    lines = msg.split(\"\\n\")\n\n    if len(lines) < 4:\n        return None\n\n    firstline = \"Updating pin \"\n    secondline = \"New commit in pinned branch \"\n    thirdline = \"    commit \"\n\n    if not lines[0].startswith(firstline):\n        return None\n\n    if lines[1].strip():\n        return None\n\n    if not lines[2].startswith(secondline):\n        return None\n    \n    if not lines[3].startswith(thirdline):\n        return None\n    \n    hash = lines[3][len(thirdline):].strip()\n    ref_name = lines[0][len(firstline):].split(\":\")[0]\n\n    #there's a : at the end of the message\n    repoAndBranch = lines[2][len(secondline):].strip()[:-1]\n    repo = \"/\".join(repoAndBranch.split(\"/\")[:-1])\n    branch = repoAndBranch.split(\"/\")[-1]\n\n    return repo, branch, hash, ref_name",
        "sha1": "34357aa6fc53cc8ff525eaee6afa7c912284c740",
        "id": 368895
    },
    {
        "content": "def decode_authorization_header(header):\n    \"\"\" Decodes a HTTP Authorization Header to python dictionary \"\"\"\n    authorization = header.get(\"Authorization\", \"\").lstrip(\" \").lstrip(\"OAuth\")\n    tokens = {}\n\n    for param in authorization.split(\",\"):\n        try:\n            key, value = param.split(\"=\")\n        except ValueError:\n            continue\n \n        key = key.lstrip(\" \")\n        value = value.lstrip(\" \").lstrip('\"')\n        value = value.rstrip(\" \").rstrip('\"')\n\n        tokens[key] = value\n\n    return tokens",
        "sha1": "fe762bc048835d325b15ee2ba3e550f85217c455",
        "id": 68747
    },
    {
        "content": "import math\n\n\ndef binomial_coefficient(n: int, k: int) -> int:\n    \"\"\"Calculate Binomial coefficient\"\"\"\n    n_fac = math.factorial(n)\n    k_fac = math.factorial(k)\n    n_minus_k_fac = math.factorial(n - k)\n    return round(n_fac/(k_fac*n_minus_k_fac))",
        "sha1": "b15da267b67d09df0fd93ac65d088ee739a0d9c0",
        "id": 659091
    },
    {
        "content": "import html\n\n\ndef text_to_tooltip(s):\n    \"\"\"Convert plain text to tooltip\"\"\"\n    return html.escape(s).replace('\\n', '&#10;')",
        "sha1": "71575061867003d69e85beed07ba810741ed529a",
        "id": 432442
    },
    {
        "content": "def linear_fit(ab, x):\n    \"\"\"Linear function of x\n    Args:\n        ab: [a, b]\n            Constant and slope\n        x: array or float\n            Input data\n\n    Returns:\n        a + b * x\n    \"\"\"\n    return ab[0] + ab[1] * x",
        "sha1": "cd6203b22b61cfb754f79c05c9dd617bb346a393",
        "id": 236623
    },
    {
        "content": "def window_neighborhood(radius=1.0):\n    \"\"\"Window neighborhood kernel function.\n\n    Parameters\n    ----------\n    radius : float (default = 1.0)\n        radius of the window.\n\n    Returns\n    -------\n    neighborhood_fun : (d : int) => float in [0,1]\n        neighborhood function.\n    \"\"\"\n    def neighborhood_fun(d):\n        return 1.0 if d <= radius else 0.0\n    return neighborhood_fun",
        "sha1": "7d1c3b3cdf87e75d8304b32fdf3ee2646ffe8f2e",
        "id": 514563
    },
    {
        "content": "def words_in_text(histogram):\n    \"\"\"Helper function for test_stochastic_sample (below).\n       Returns all unique_words in a source text,\n       using the keys in the histogram.\n       Param: histogrma(dict)\n       Return: list\n    \"\"\"\n    return list(histogram)",
        "sha1": "98aeab01ded09da37cdc515fce8b9962076e0d36",
        "id": 347024
    },
    {
        "content": "import re\n\n\ndef find_key_and_value(s):\n    \"\"\" Find a double or single-quoted string at the end of a string (quote char must be last)\n\n    >>> s = 'A: \"Both. Start with the dataset, don\\'t close the quote'\n    >>> k, v = find_key_and_value(s)\n    >>> k\n    'A'\n    >>> v\n    '\"Both. Start with the dataset, don\\'t close the quote'\n    \"\"\"\n    s = s.strip()\n    match = re.match(r'([^:]+):\\s*(.*)$', s)\n    if match:\n        return match.groups()\n    return None, None",
        "sha1": "134e7a0d8af4544860bf7c974200b717b9d4bd9d",
        "id": 470596
    },
    {
        "content": "def calcBeepThresholds(envelopeSampleData):\n    \"\"\"\\\n    Analyses audio sample data and returns suggestions for the thresholds needed to detect the beeps.\n    \n    :param loSampleData: list of sample values, where each value is the lowest seen during that sampling period\n    :param loSampleData: list of sample values, where each value is the highest seen during that sampling period\n    :returns: tuple (rising, falling) consisting of suggested rising-edge and falling-edge detection thresholds for use in the pulse detection code.\n    \"\"\"\n    lo = min(envelopeSampleData)\n    hi = max(envelopeSampleData)\n    risingThreshold  = (lo + 2*hi) / 3.0\n    fallingThreshold = (lo*2 + hi) / 3.0\n    return risingThreshold, fallingThreshold",
        "sha1": "8b6f9d1a32314aac0cbbea6e6b29ecfa8abe44be",
        "id": 404036
    },
    {
        "content": "from typing import Sequence\n\n\ndef common_uri_prefix(uris: Sequence[str]):\n    \"\"\"\n    This is like `os.path.commonpath()`, but always expects URL paths.\n    (ie. forward slashes in all environments, and wont strip double slashes '//')\n\n    >>> common_uri_prefix(['file:///a/thing-1.txt'])\n    'file:///a/thing-1.txt'\n    >>> common_uri_prefix(['file:///a/1.txt', 'file:///a/2.txt', 'file:///a/3.txt'])\n    'file:///a/'\n    >>> # Returns the common directory, not a partial filename:\n    >>> common_uri_prefix(['file:///a/thing-1.txt', 'file:///a/thing-2.txt', 'file:///a/thing-3.txt'])\n    'file:///a/'\n    >>> common_uri_prefix(['http://example.com/things/'])\n    'http://example.com/things/'\n    >>> common_uri_prefix(['http://example.com/things/'] * 4)\n    'http://example.com/things/'\n    >>> common_uri_prefix(['http://example.com/things/', 'http://example.com/others/'])\n    'http://example.com/'\n    >>> common_uri_prefix([])\n    ''\n    \"\"\"\n    if not uris:\n        return \"\"\n    first_possibility = min(uris)\n    last_possibility = max(uris)\n\n    if first_possibility == last_possibility:\n        # All are the same\n        return first_possibility\n\n    for i, c in enumerate(first_possibility):\n        if c != last_possibility[i]:\n            result = first_possibility[:i]\n            break\n    else:\n        result = first_possibility\n\n    return result[: result.rfind(\"/\") + 1]",
        "sha1": "61f70dda336a7ddca3cb1a869cd69b8d3a7269de",
        "id": 424813
    },
    {
        "content": "import torch\n\n\ndef euclidean_dist(x, y):\n    \"\"\"\n    Calculates Euclidean distance (square of it)\n    :param x: size [n_query_total, out_dim=1600] - queries\n    :param y: size [n_ways, out_dim=1600] - prototypes\n    \"\"\"\n    n = x.size(0)  # total number of query points = n_query_total\n    m = y.size(0)  # number of classes = n_ways\n    d = x.size(1)  # dimension of pic embedding = 1600 for mini-ImageNet\n    if d != y.size(1):\n        raise ValueError(f'Pic embedding for prototype {y.size(1)} and query {d} data arent equal')\n\n    x = x.unsqueeze(1).expand(n, m, d)  # size = [n_query_total, n_ways, 1600]\n    y = y.unsqueeze(0).expand(n, m, d)  # size = [n_query_total, n_ways, 1600]\n\n    return torch.pow(x - y, 2).sum(2)",
        "sha1": "6327f150843719edb94f0d3766a1d310bb828024",
        "id": 97118
    },
    {
        "content": "import json\n\n\ndef get_abi(abi):\n    \"\"\"\n    Helper function to load abis\n    \"\"\"\n    f = open(\"./abis/alpha-homora/{}\".format(abi))\n    return json.load(f)",
        "sha1": "abec25e5d6301a2c50c87c74e2704321aed7edad",
        "id": 234921
    },
    {
        "content": "from typing import List\n\n\ndef format_status_list(status_list: list) -> List[int]:\n    \"\"\"\n    Get a status list and format it to a range of status numbers.\n    Example:\n        given: ['400-404',500,501]\n        return: [400,401,402,403,500,501]\n    Args:\n        status_list: The given status list.\n    Returns:\n        A list of statuses to retry.\n    \"\"\"\n    final_status_list = []\n    for status in status_list:\n        # Checks if the status is a range of statuses\n        if '-' in status:\n            range_numbers = status.split('-')\n            status_range = list(range(int(range_numbers[0]), int(range_numbers[1]) + 1))\n            final_status_list.extend(status_range)\n        elif status.isdigit():\n            final_status_list.append(int(status))\n    return final_status_list",
        "sha1": "def63444b2888a134e4b9e308cf92e525b8e1c06",
        "id": 171944
    },
    {
        "content": "def string_begins_with(string: str, substr: str) -> bool:\n    \"\"\"Determines whether string starts with substring or not\n    >>> string_begins_with(\"hello\", \"he\")\n    True\n    >>> string_begins_with(\"hello\", \"wo\")\n    False\n    \"\"\"\n    return string.startswith(substr)",
        "sha1": "6cf54ff75184fcad966a55eb174a8897d1212b4c",
        "id": 198597
    },
    {
        "content": "import uuid\n\n\ndef get_guid() -> str:\n    \"\"\" Returns a new guid \"\"\"\n    return uuid.uuid4().hex",
        "sha1": "5f7bc873a3e786bae10465ab4328d04d4c328df4",
        "id": 637178
    },
    {
        "content": "import torch\n\n\ndef featurize_nodes_and_compute_combo_scores(\n        node_featurizer, reactant_mol, valid_candidate_combos):\n    \"\"\"Featurize atoms in reactants and compute scores for combos of bond changes\n\n    Parameters\n    ----------\n    node_featurizer : callable, rdkit.Chem.rdchem.Mol -> dict\n        Featurization for nodes like atoms in a molecule, which can be used to update\n        ndata for a DGLGraph.\n    reactant_mol : rdkit.Chem.rdchem.Mol\n        RDKit molecule instance for reactants in a reaction\n    valid_candidate_combos : list\n        valid_candidate_combos[i] gives a list of tuples, which is the i-th valid combo\n        of candidate bond changes for the reaction.\n\n    Returns\n    -------\n    node_feats : float32 tensor of shape (N, M)\n        Node features for reactants, N for the number of nodes and M for the feature size\n    combo_bias : float32 tensor of shape (B, 1)\n        Scores for combos of bond changes, B equals len(valid_candidate_combos)\n    \"\"\"\n    node_feats = node_featurizer(reactant_mol)['hv']\n    combo_bias = torch.zeros(len(valid_candidate_combos), 1).float()\n    for combo_id, combo in enumerate(valid_candidate_combos):\n        combo_bias[combo_id] = sum([\n            score for (atom1, atom2, change_type, score) in combo])\n\n    return node_feats, combo_bias",
        "sha1": "fabaa791c2e08dec75a7d63ab9cee1cb261b3a56",
        "id": 79731
    },
    {
        "content": "def size_filter(clusters, cluster_size):\n    \"\"\"\n    Filters out clusters which are smaller than cluster_size.\n\n    Parameters\n    ----------\n    clusters : list of set of tuple of int\n        The clusters to filter.\n    cluster_size : int\n        The minimum size of a cluster needed to pass this filter.\n\n    Returns\n    -------\n    list of set of tuple of int\n        The filtered clusters.\n    \"\"\"\n    return [c for c in clusters if len(c) >= cluster_size]",
        "sha1": "cc03c06be7696bb4513b974b3b7fbb4dc9dd4f39",
        "id": 476012
    },
    {
        "content": "from typing import Any\nfrom typing import Hashable\n\n\ndef hashable(arg: Any) -> bool:\n    \"\"\"\n    Tells if the given arg is hashable.\n    \"\"\"\n\n    return isinstance(arg, Hashable)",
        "sha1": "874c6a6656940d49f55404a04b73812b913ba99e",
        "id": 471926
    },
    {
        "content": "def rgb_to_hex(rgb):\n    \"\"\"Receives (r, g, b)  tuple, checks if each rgb int is within RGB\n       boundaries (0, 255) and returns its converted hex, for example:\n       Silver: input tuple = (192,192,192) -> output hex str = #C0C0C0\"\"\"\n       \n    for num in rgb:\n        if not 0<= num <= 255:\n            raise ValueError\n            \n    r = str(hex(rgb[0]))[2:].upper().zfill(2)\n    g = str(hex(rgb[1]))[2:].upper().zfill(2)\n    b = str(hex(rgb[2]))[2:].upper().zfill(2)\n    \n    return f'#{r}{g}{b}'\n    pass",
        "sha1": "5c11c5aadf12f622eac5f77faf996c594baa8c07",
        "id": 354440
    },
    {
        "content": "def compute_coverage(statements, covered):\n    \"\"\"Compute code coverage based on number of all statemts and number of covered statements.\"\"\"\n    return 100.0 * covered / statements",
        "sha1": "b57c497fbe8ea1fef5bb88a2b285d464fa02b57b",
        "id": 329324
    },
    {
        "content": "def Swap(xs, **unused_kwargs):\n  \"\"\"Swaps two elements.\"\"\"\n  return (xs[1], xs[0])",
        "sha1": "6f8b833e32f5baf197b6dc41d848ef85babb6949",
        "id": 570400
    },
    {
        "content": "import hmac\nimport base64\nimport hashlib\n\n\ndef verify_webhook(data, secret, hmac_header):\n    \"\"\"\n    Verify the webhook and return a true or false\n\n    :param data: Payload body of the request\n    :param secret: Base64 encoded secret of the webhook.\n                   The secret is base64 encoded when read\n                   over API or copied from UI.\n    :param hmac_header: Value of the header in the request\n    \"\"\"\n    digest = hmac.new(\n        base64.b64decode(secret),\n        data.encode('utf-8'),\n        hashlib.sha256\n    ).digest()\n    computed_hmac = base64.b64encode(digest)\n    return hmac.compare_digest(\n        computed_hmac,\n        hmac_header.encode('utf-8')\n    )",
        "sha1": "ea83627d5559fde716335a51a59b95dc794fdaeb",
        "id": 453679
    },
    {
        "content": "import math\n\n\ndef plan_plot_grid(n_plots: int) -> tuple:\n    \"\"\"find optimal placement (rows, cols) for given number of plots to place\n    => min{rows - cols}  s.t. cols^2 <= n_plots <= rows^2 and cols * rows >= n_plots\n\n    :param n_plots: number of plots to place\n    :return: optimal (rows, cols)\n    \"\"\"\n    rows = math.ceil(n_plots ** 0.5)\n    cols = math.ceil(n_plots / rows)\n    return rows, cols",
        "sha1": "b2553cc3256ce848a829ab98ac1fa2b113f7d04d",
        "id": 590590
    },
    {
        "content": "def _return_default_colors_rgb(**kwargs):\n    \"\"\"Replace color in kwargs with pyfar default color if possible.\"\"\"\n\n    # pyfar default colors\n    colors = {'p': '#5F4690',  # purple\n              'b': '#1471B9',  # blue\n              't': '#4EBEBE',  # turqois\n              'g': '#078554',  # green\n              'l': '#72AF47',  # light green\n              'y': '#ECAD20',  # yellow\n              'o': '#E07D26',  # orange\n              'r': '#D83C27'}  # red\n\n    if 'c' in kwargs and isinstance(kwargs['c'], str):\n        kwargs['c'] = colors[kwargs['c']] \\\n            if kwargs['c'] in colors else kwargs['c']\n    if 'color' in kwargs and isinstance(kwargs['color'], str):\n        kwargs['color'] = colors[kwargs['color']] \\\n            if kwargs['color'] in colors else kwargs['color']\n\n    return kwargs",
        "sha1": "9f29f5c1360471d084b4b72cce152c7fa7019b8e",
        "id": 135261
    },
    {
        "content": "import functools\nimport asyncio\n\n\ndef async_test(f):\n\t\"\"\"\n\tDecorator to transform async unittest coroutines into normal test methods\n\t\"\"\"\n\t@functools.wraps(f)\n\tdef wrapper(*args, **kwargs):\n\t\tloop = asyncio.get_event_loop()\n\t\tloop.run_until_complete(f(*args, **kwargs))\n\treturn wrapper",
        "sha1": "2698381dbfbdd5611967c2418f77fc688e621f59",
        "id": 590846
    },
    {
        "content": "def valid(value):\n    \"\"\"A validator that is always valid and returns the value as-is.\"\"\"\n    return value",
        "sha1": "8ff3ab3041fa58ccd5d3b1e3cc4723db2df6d422",
        "id": 153936
    },
    {
        "content": "def partition_around_index(list_to_partition, index):\n    \"\"\"\n    Partitions a list around the given index, returning 2 lists. The first contains elements\n    before the index, and the second contains elements after the index (the given index is excluded).\n\n    Examples:\n        partition_around_index([1,2,3,4,5], 2) == ([1,2], [4,5])\n        partition_around_index([1,2,3,4,5], 0) == ([], [2,3,4,5])\n\n    Args:\n        list_to_partition (list): The list to partition\n        index (int): The index that the list should be partitioned around\n\n    Returns:\n        Tuple(list, list): The partitions of the given list\n    \"\"\"\n    list_len = len(list_to_partition)\n    if list_len <= index:\n        raise ValueError(\n            \"Index out of range: {} ({} item list)\".format(index, list_len)\n        )\n    l1, l2 = [], []\n    if index > 0:\n        l1 = list_to_partition[0:index]\n    if index < (list_len - 1):\n        l2 = list_to_partition[(index + 1) :]\n    return l1, l2",
        "sha1": "d4019d16a017596e007d2ffefa6b54e523ee3233",
        "id": 201771
    },
    {
        "content": "def fahrenheit_to_celsius(fahrenheit: float) -> float:\n    \"\"\"Converts fahrenheit to celsius.\n\n    :param fahrenheit: temperature in fahrenheit.\n    :return: temperature in celsius\n    \"\"\"\n\n    celsius = (fahrenheit - 32) * 5 / 9\n    return celsius",
        "sha1": "b3ac372d976a057b261bbaa0cccb78d666b7660e",
        "id": 551697
    },
    {
        "content": "def all_element_isinstance(obj, typ):\n    \"\"\" Reports if all elements of a collection is a type. \"\"\"\n\n    return all(isinstance(o, typ) for o in obj)",
        "sha1": "63b0c299a9b126e6e151c5e1a31fed0201f58c14",
        "id": 496858
    },
    {
        "content": "def strhash(string):\n    \"\"\"\n    Old python hash function as described in PEP 456, excluding prefix, suffix and mask.\n\n    :param string: string to hash\n    :return: hash\n    \"\"\"\n    if string == \"\":\n        return 0\n\n    x = ord(string[0]) << 7\n    for c in string[1:]:\n        x = ((1000003 * x) ^ ord(c)) & (1 << 32)\n    x = (x ^ len(string))\n    return x",
        "sha1": "27e9324c8d39d76a5cc3fc0338aeb184716dea5b",
        "id": 226384
    },
    {
        "content": "import re\n\n\ndef filter_regex(values, regexs):\n    \"\"\"Filters list of `values` by list of `regexs`.\n\n    Paramters\n    ---------\n    values: list\n        list of `str` values.\n    regexs: list\n        list of `str` regexs.\n\n    Returns\n    -------\n    list\n        Sorted `list` of values in `values` that match any regex in `regexs`.\n    \"\"\"\n    if not isinstance(values, list):\n        values = [values]\n    if not isinstance(regexs, list):\n        regexs = [regexs]\n    filtered = set()\n    for value in values:\n        for regex in regexs:\n            if re.search(regex, value):\n                filtered.add(value)\n    return sorted(list(filtered))",
        "sha1": "d5c018ba6cb9b780826c5d001af52a0c458c876d",
        "id": 67535
    },
    {
        "content": "def znorm(angle):\n    \"\"\" Normalizes an angle between -180 and 180. \"\"\"\n    angle = angle % 360\n    return angle if angle <= 180 else angle - 360",
        "sha1": "c8e1a1e6db2c0b75156d3dd72c0b29c85a391030",
        "id": 391073
    },
    {
        "content": "def _out_of_str(n1: int, n2: int) -> str:\n    \"\"\"\n    :return A string in the format [n1 / n2], where \"n1\" and \"n2\" are the passed integers padded to the same length\n    \"\"\"\n    width = len(str(max(n1, n2)))\n    return '[%s / %s]' % (str(n1).rjust(width), str(n2).rjust(width))",
        "sha1": "70398ec476405122ff799e3924d6488cea372706",
        "id": 406426
    },
    {
        "content": "def get_dil_mol_frac(\n        p_fuel,\n        p_oxidizer,\n        p_diluent\n):\n    \"\"\"\n    Parameters\n    ----------\n    p_fuel : float or un.ufloat\n        Fuel partial pressure\n    p_oxidizer : float or un.ufloat\n        Oxidizer partial pressure\n    p_diluent : float or un.ufloat\n        Diluent partial pressure\n\n    Returns\n    -------\n    float or un.ufloat\n        Diluent mole fraction\n    \"\"\"\n    return p_diluent / (p_fuel + p_oxidizer + p_diluent)",
        "sha1": "0d0c531a3153e9cb41c040997fc8ea9536aa1e05",
        "id": 158873
    },
    {
        "content": "import shutil\n\n\ndef match_stat(dest_path, source_path):\n    \"\"\" Matches stats of one fs object to that of another, see shutil.copystat \"\"\"\n    return shutil.copystat(source_path, dest_path)",
        "sha1": "822f8e99f07cb7d1b3d598e21ae3b1cc221ff89a",
        "id": 94107
    },
    {
        "content": "def a_group(trans,rots):\n    \"\"\"This subroutine combines that translations of the lattice with the\n    rotatians of the lattice.\n\n    Args:\n        trans (list): A 2D array where each row is an translation\n          of the lattice.\n\n        rots (list): A 3D array. Each row's first entry is the\n          site permutations and the second entry is the arrow\n          permutations.\n\n    Returns:\n        groupi (list): The full symmetry group.\n    \"\"\"\n    groupi = []\n    for i in trans:\n        for j in rots:\n            c = []            \n            c.append([j[0][i[l]] for l in range(0,len(i))])\n            c.append(j[1])\n            if c not in groupi:\n                groupi.append(c)\n\n    return(groupi)",
        "sha1": "218c428bd0a0e14057220b9f48417680a7da3cf4",
        "id": 576067
    },
    {
        "content": "from typing import Optional\nimport re\n\n\ndef parse_integer(number: str, default: Optional[int] = 0):\n    \"\"\"Parse a string representing an integer, ignoring commas or periods.\n\n    Parameters\n    ----------\n    number: :class:`str`\n        A string representing a number.\n    default: :class:`int`\n        The default value to use if the string is not numeric.\n        By default, 0 is used.\n\n    Returns\n    -------\n    :class:`int`\n        The represented integer, or the default value if invalid.\n    \"\"\"\n    if number is None:\n        return default\n    try:\n        number = re.sub(r'[,.]', '', number.strip())\n        return int(number)\n    except ValueError:\n        return default",
        "sha1": "d10d18f9ea2d1a3c0f8adadf7fdf3be032c2100a",
        "id": 452538
    },
    {
        "content": "from typing import Tuple\n\n\ndef _pos_from_offset(col: int, msg: bytes, offset: int) -> Tuple[int, int]:\n    \"\"\"Calculate the line and column of a given offset.\"\"\"\n    msg = msg[:offset]\n    lines = msg.split(b\"\\n\")\n\n    line = len(lines) - 1\n    col = len(lines[-1]) + (col if line == 0 else 0)\n\n    return (line, col)",
        "sha1": "01c9f35e94443b028308d3129e774b777d393723",
        "id": 498879
    },
    {
        "content": "def point_line_seg_distance(p1: tuple, p2: tuple, p: tuple, extend_line: bool = False) -> tuple:\n    \"\"\"\n    Compute distance from 2D point p to line segment (p1, p2).\n    :param p1: Start point of line in the form (x, y)\n    :param p2: End point of line in the form (x, y)\n    :param p: Point to compute distance to, in the form (x, y)\n    :param extend_line: Should the line be extended as necessary to reduce the distance.\n    :return: Tuple(Distance, (x,y) of closest point)\n    \"\"\"\n    # Code adapted from https://stackoverflow.com/questions/849211/shortest-distance-between-a-point-and-a-line-segment\n    assert len(p1) == 2\n    assert len(p2) == 2\n    assert len(p) == 2\n\n    x1, y1 = p1\n    x2, y2 = p2\n    x3, y3 = p\n    x1, x2, x3, y1, y2, y3 = float(x1), float(x2), float(x3), float(y1), float(y2), float(y3)\n    px = x2 - x1\n    py = y2 - y1\n\n    norm = px * px + py * py\n    if norm == 0:\n        dx = x1 - x3\n        dy = y1 - y3\n\n        dist = (dx * dx + dy * dy) ** .5\n        return dist, (x1, y1)\n\n    u = ((x3 - x1) * px + (y3 - y1) * py) / float(norm)\n\n    if not extend_line:\n        if u > 1:\n            u = 1\n        elif u < 0:\n            u = 0\n\n    x = x1 + u * px\n    y = y1 + u * py\n\n    dx = x - x3\n    dy = y - y3\n\n    dist = (dx * dx + dy * dy) ** .5\n\n    return dist, (x, y)",
        "sha1": "3b11ecfe130709d5ccda36f99861da6047b16a5e",
        "id": 404887
    },
    {
        "content": "def get_full_text(status):\n    \"\"\"Returns the full text of a (re)tweet\"\"\"\n    try:\n        return status.retweeted_status.full_text\n    except AttributeError:  # Not a Retweet\n        return status.full_text",
        "sha1": "d6918c62349ac1acf434e389f5c56b1f3f54e75f",
        "id": 319056
    },
    {
        "content": "from typing import List\n\n\ndef find_common_prefix_len(prefix1: List, prefix2: List) -> int:\n    \"\"\"Returns common prefix size of the given two prefix sequences.\n    \"\"\"\n    result = 0\n    for index in range(min(len(prefix1), len(prefix2))):\n        if prefix1[index] == prefix2[index]:\n            result = index + 1\n        else:\n            break\n\n    return result",
        "sha1": "95dfef4707e9d4129cba1d0eb5f9186d4d8e0ed0",
        "id": 601472
    },
    {
        "content": "import torch\n\n\ndef validate(metric,\n             net,\n             val_data,\n             use_cuda):\n    \"\"\"\n    Core validation/testing routine.\n\n    Parameters:\n    ----------\n    metric : EvalMetric\n        Metric object instance.\n    net : Module\n        Model.\n    val_data : DataLoader\n        Data loader.\n    use_cuda : bool\n        Whether to use CUDA.\n\n    Returns\n    -------\n    EvalMetric\n        Metric object instance.\n    \"\"\"\n    net.eval()\n    metric.reset()\n    with torch.no_grad():\n        for data, target in val_data:\n            if use_cuda:\n                target = target.cuda(non_blocking=True)\n            output = net(data)\n            metric.update(target, output)\n    return metric",
        "sha1": "e9612e861cc82c03eb3d0121fa9915b20bdd90de",
        "id": 464930
    },
    {
        "content": "def is_empty_macro(path):\n    \"\"\"Returns True if the given macro file only has empty lines and/or Attribute settings.\"\"\"\n    with open(path, \"rb\") as fp:\n        for line in fp:\n            # if the line is empty keep moving\n            if line.strip() == b\"\":\n                continue\n\n            # or if it starts with one of these lines\n            if line.startswith(b\"Attribute VB_\"):\n                continue\n\n            # otherwise it's something else, so return False\n            return False\n\n    return True",
        "sha1": "68350c77a13632acd4c368ffdde81d4e9241e0d3",
        "id": 555787
    },
    {
        "content": "import time\n\n\ndef time_it(f, *args, **kwargs):\n    \"\"\" Returns time taken for a function in addition it's return value\"\"\"\n    s_time = time.time()\n    result = f(*args, **kwargs)\n    return time.time() - s_time, result",
        "sha1": "a14d35ab31ae7afb014c33438a94fc8790017fa4",
        "id": 153607
    },
    {
        "content": "def uses_permission_class(view, permission_class):\n    \"\"\"\n    Determine if the provided view uses a given permission class.\n\n    Args:\n        view:\n            The view instance to test.\n        permission_class:\n            The permission class to test for the presence of.\n\n    Returns:\n        A boolean indicating if the provided view uses the given\n        permission class.\n    \"\"\"\n    return any(\n        isinstance(perm, permission_class) for perm in view.get_permissions()\n    )",
        "sha1": "39c509fcbb9adb30785417624fb3143eb8f93533",
        "id": 384649
    },
    {
        "content": "import torch\n\n\ndef huber_loss(x, delta=1.):\n    \"\"\" Standard Huber loss of parameter delta\n\n    https://en.wikipedia.org/wiki/Huber_loss\n\n    returns 0.5 * x^2 if |a| <= \\delta\n            \\delta * (|a| - 0.5 * \\delta) o.w.\n    \"\"\"\n    if torch.abs(x) <= delta:\n        return 0.5 * (x ** 2)\n    else:\n        return delta * (torch.abs(x) - 0.5 * delta)",
        "sha1": "b3493eb9d4e38fa36f92db80dc52a47c32caf3c9",
        "id": 2143
    },
    {
        "content": "def _of(smiles):\n  \"\"\" Order the fragments alphabetically. If smiles is None, returns None \"\"\"\n  if smiles is None:\n    return None\n  return '.'.join(sorted(smiles.split('.')))",
        "sha1": "f28ad26824b2050d5b7c560b2e1776aea055fa63",
        "id": 195501
    },
    {
        "content": "def hasname(ob):\n    \"\"\"Does the object have a name that begins with a letter?\"\"\" \n    return hasattr(ob, \"name\") and isinstance(ob.name, str) and len(ob.name)>0 and ob.name[0].isalpha()",
        "sha1": "35ba3c369b6ea09858eea7b682ef6d8a0fb021ff",
        "id": 253321
    },
    {
        "content": "import secrets\n\n\ndef randbelow(exclusive_upper_bound: int) -> int:\n    \"\"\"Return a random int in the range [0, n).\"\"\"\n    return secrets.randbelow(exclusive_upper_bound)",
        "sha1": "06bf74858cb0ff057cae64351159d5198fd895b0",
        "id": 428428
    },
    {
        "content": "from typing import Optional\n\n\ndef validate_economic_lifetime(economic_lifetime: Optional[int]) -> Optional[int]:\n    \"\"\"\n    Validates the economic lifetime of an object.\n    Economic lifetime is always optional.\n\n    :param economic_lifetime: The economic lifetime of the object.\n    :return: The validated economic lifetime.\n    \"\"\"\n    if economic_lifetime is None:\n        # Skip validation if no value provided\n        return None\n\n    if economic_lifetime <= 0:\n        raise ValueError(\"Economic lifetime must be positive.\")\n\n    return economic_lifetime",
        "sha1": "5b3aa11b5c737a5bc210c04bfcc025013a5fe40f",
        "id": 217581
    },
    {
        "content": "def generate_path(paths):\n    \"\"\"Generate path string from path list\"\"\"\n    path_str = \"\"\n\n    for path in paths:\n        if path_str == \"\":\n            path_str = str(path)\n        else:\n            path_str += \"->\" + str(path)\n\n    return path_str",
        "sha1": "8108d4ea1d403697d9cb7027735ae8f8f9ea5b8c",
        "id": 601025
    },
    {
        "content": "def construct_nvme_bdev(client, name, trtype, traddr, adrfam=None, trsvcid=None, subnqn=None):\n    \"\"\"Construct NVMe namespace block devices.\n\n    Args:\n        name: bdev name prefix; \"n\" + namespace ID will be appended to create unique names\n        trtype: transport type (\"PCIe\", \"RDMA\")\n        traddr: transport address (PCI BDF or IP address)\n        adrfam: address family (\"IPv4\", \"IPv6\", \"IB\", or \"FC\") (optional for PCIe)\n        trsvcid: transport service ID (port number for IP-based addresses; optional for PCIe)\n        subnqn: subsystem NQN to connect to (optional)\n\n    Returns:\n        List of created block devices.\n    \"\"\"\n    params = {'name': name,\n              'trtype': trtype,\n              'traddr': traddr}\n\n    if adrfam:\n        params['adrfam'] = adrfam\n\n    if trsvcid:\n        params['trsvcid'] = trsvcid\n\n    if subnqn:\n        params['subnqn'] = subnqn\n\n    return client.call('construct_nvme_bdev', params)",
        "sha1": "a4aacdfeb258bde06ce25e57d17415c0b1c94e90",
        "id": 295133
    },
    {
        "content": "def remove_keys_recursively(obj, fields_to_remove):\n    \"\"\"\n    Remove specified keys recursively from a python object (dict or list)\n\n    :param obj:\n    :param fields_to_remove:\n    :return:\n    \"\"\"\n    if isinstance(obj, dict):\n        obj = {\n            key: remove_keys_recursively(value, fields_to_remove) for key, value in obj.items()\n            if key not in fields_to_remove\n        }\n    elif isinstance(obj, list):\n        obj = [remove_keys_recursively(item, fields_to_remove) for item in obj if item not in fields_to_remove]\n\n    return obj",
        "sha1": "f439cb4d213e90e00b1105970929623090c5f292",
        "id": 195808
    },
    {
        "content": "def hweight(n):\n    \"\"\" Return the Hamming weight of 'n' \"\"\"\n    c = 0\n    while n>0:\n        c += (n & 1)\n        n >>= 1\n    return c",
        "sha1": "4a9c1d3aabea446093891033cfb3a24ce6badba5",
        "id": 257191
    },
    {
        "content": "def get_youtube_url(data: dict) -> str:\n    \"\"\"\n    Returns the YouTube's URL from the returned data by YoutubeDL, like\n    https://www.youtube.com/watch?v=dQw4w9WgXcQ\n    \"\"\"\n\n    return data['entries'][0]['webpage_url']",
        "sha1": "076a76f5df6ceb605a201690ac5c39972c3ddf5c",
        "id": 654326
    },
    {
        "content": "def get_coverage_color(coverage_percent: float) -> str:\n    \"\"\"\n    Returns color to represent coverage percent.\n    Args:\n        coverage_percent (float): Coverage percent.\n\n    Returns:\n        (str): Representing the color\n    \"\"\"\n    if coverage_percent <= 50.0:\n        return 'danger'\n    elif coverage_percent < 60.0:\n        return 'warning'\n    return 'good'",
        "sha1": "f112f53fc7e63eecdecdcb04282f3e5b5e4aa9df",
        "id": 484335
    },
    {
        "content": "def xgb_score(loss, preds, labels):\n    \"\"\"\n    According to xgboost scoring function\n    score = 0.5* G**2 / (H+l2_regularization)\n    \"\"\"\n    G = loss.grad(preds, labels).sum()\n    H = loss.hess(preds, labels).sum()\n    return 0.5 * (G ** 2) / (H + loss.l2_regularization)",
        "sha1": "75ee00d2d2690ce70386f8f2bbc093c49aeb257c",
        "id": 281218
    },
    {
        "content": "def make_tags(**kwargs):\n    \"\"\"\n    For each kwarg, produce a {\"Key\": key, \"Value\": value}.\n    \"\"\"\n    return [{\"Key\": key, \"Value\": value} for key, value in kwargs.items()]",
        "sha1": "057c540aab12b1b6aa22bab9cf6285114de7f4eb",
        "id": 287668
    },
    {
        "content": "def get_ratio(numerator, denominator):\n    \"\"\"Get ratio from numerator and denominator.\"\"\"\n    return (\n        0 if not denominator else round(float(numerator or 0) / float(denominator), 2)\n    )",
        "sha1": "e51a860292d54d2e44909ad878d0b1d8e66c37c2",
        "id": 709228
    },
    {
        "content": "import fnmatch\n\n\ndef find_inventory_wildcard(player, name):\n    \"\"\"\n    Find the first item in player's inventory whose name matches a wildcard\n    pattern ('*').\n\n    :param text_game_maker.player.player.Player player: player object\n    :param str name: wildcard pattern\n    :return: found item. If no matching item is found, None is returned.\n    :rtype: text_game_maker.game_objects.items.Item\n    \"\"\"\n    for item in player.inventory.items:\n        if fnmatch.fnmatch(item.name, name):\n            return item\n\n    return None",
        "sha1": "e089d435f1b219dae71b3b8e05abe3044f03e16f",
        "id": 469192
    },
    {
        "content": "def loop_detection_buffer(head):\n    \"\"\"Brute force method to determine the node at the beginning of the loop\n\n    :param   head head of the linked list\n    :returns False or node at the beginning of the loop\n\n    \"\"\"\n    if head is None:\n        return False\n\n    curr = head\n    nodes = []\n\n    while curr is not None:\n        if curr not in nodes:\n            nodes.append(curr)\n        else:\n            return curr\n        curr = curr.next\n\n    return False",
        "sha1": "f2db44ec623f3291eff8b8fb293de37d52c5f01b",
        "id": 306493
    },
    {
        "content": "def transform_tone(tone):\n    \"\"\"Transforms keyword `negative`, `neutral` or `positive` into `-1`, `0` or `1`.\n\n    :param tone: string representation of tone.\n    :type tone: str\n\n    :return: number representation of tone.\n    :rtype: str\n    \"\"\"\n    if 'negative':\n        return '-1'\n    elif 'neutral':\n        return '0'\n    else:\n        return '1'",
        "sha1": "036a3de752bffd3f97636e132c5ce80aae7ee2c2",
        "id": 644417
    },
    {
        "content": "def make_odd(number):\n  \"\"\"\n  Return the input number if its odd,\n  or return input number + 1 if its even or zero.\n  \"\"\"\n  if number == 0:\n    number += 1\n  if number % 2 == 0:\n    number += -1\n  return number",
        "sha1": "5c074d0cb981eda1d380126865714babea2c96cc",
        "id": 614236
    },
    {
        "content": "def get_story_id(logs):\n    \"\"\"Returns the job_id of a run_script job.\"\"\"\n    for log in logs['job']:\n        if log.get('job', '') == log.get('action', '') == 'run_script':\n            return log['job_id']\n    assert False",
        "sha1": "c3c60e5ad696f064665fd6282124f9c9236f5aad",
        "id": 489655
    },
    {
        "content": "def get_frequency(key=88, pitch=440.0):\n    \"\"\"\n    Get frequency from piano key number\n    :param key: key number of the piano\n    :param pitch: convert pitch of the A4 note in Hz\n    :return: frequency in hz\n    \"\"\"\n    return pow(2, float(key - 49) / 12.0) * pitch",
        "sha1": "d73aa776d6a6760335b19d966b4198488eff4c21",
        "id": 63871
    },
    {
        "content": "import math\n\n\ndef next_power_of_2(i):\n    \"\"\"Get the next power of 2 after ``i``.\"\"\"\n    return int(math.pow(2, math.floor(math.log(i, 2)) + 1))",
        "sha1": "7f07277b9de563d9fac7099280de0911396ef50e",
        "id": 335275
    },
    {
        "content": "def get_country_lat_lon_extent(country):\n    \"\"\"\n    See https://data.humdata.org/dataset/bounding-boxes-for-countries/resource/aec5d77d-095a-4d42-8a13-5193ec18a6a9\n    Args:\n        country:\n\n    Returns: longitude(left) longitude(right), latitude (bottom), latitude(top)\n\n    \"\"\"\n    #\n    # 'mexico', 'south_africa', 'spain', 'australia', 'ukraine', 'uk_of_great_britain_and_northern_ireland',\n    # 'germany','spain', 'kazakhstan', 'hungary', 'italy','indonesia'\n    if country == 'united_states_of_america':\n        return [-130, -60, 25, 48]\n    elif country == 'russian_federation':\n        # -179.985\t38.083\t179.917\t86.217\n        return [22., 90., 42., 60.]\n    elif country == 'china':\n        return [70, 138, 12, 55]\n    elif country == 'india':\n        return [68, 98, 8, 36]\n    elif country == 'pakistan':\n        return [60, 76, 23.5, 37]\n    elif country == 'afghanistan':\n        return [60, 75, 29, 39]\n    elif country == 'argentina':\n        return [-74.0, -53., -59., -21.]\n    elif country == 'brazil':\n        return [-75, -35, 5, -35]\n    elif country == 'canada':\n        return [-140, -50, 40, 70]\n    elif country == 'egypt':\n        return [13., 37., 5., 51.]\n    elif country == 'france':\n        return [-5.5, 9.0, 41.0, 51.5]\n    elif country == 'mexico':\n        return [-120, -85, 15, 35]\n    elif country == 'south_africa':\n        return [10, 35, -20, -35]\n    elif country == 'ukraine':\n        return [22, 40.5, 45, 53.]\n    elif country == 'uk_of_great_britain_and_northern_ireland':\n        return [-14., 4., 48.5, 64.5]\n    elif country == 'germany':\n        return [5.8, 15.5, 45.5, 55.5]\n    elif country == 'poland':\n        return [13., 26.500, 48., 55.5]\n    elif country == 'spain':\n        return [-18.5, 6.5, 27.5, 44.]\n    elif country == 'kazakhstan':\n        return [46.5, 90.0, 40.4, 55.5]\n    elif country == 'hungary':\n        return [16.1, 22.5, 45.5, 49.0]\n    elif country == 'italy':\n        return [1.1, 54.5, 28.5, 49.5]\n    elif country == 'indonesia':\n        return [0., 142., -11., 15.]\n    elif country == 'australia':\n        return [112.0, 168.0, -9.0, -45.0]\n    elif country in ['vietnam', 'Viet nam', 'viet_nam', 'Viet Nam']:\n        return [100., 110., 8., 24.]\n    elif country == 'kenya':\n        return [33., 42.1, -5., 5.5]\n    elif country == 'somalia':\n        return [40., 51, -2., 12]\n    elif country == 'mali':  # longitude(left) longitude(right), latitude (bottom), latitude(top)\n        return [-13, 5, 9, 26]\n    elif country == 'united_republic_of_tanzania':  # longitude(left) longitude(right), latitude (bottom), latitude(top)\n        return [29, 41, 0, -12]\n    elif country == 'zambia':  # longitude(left) longitude(right), latitude (bottom), latitude(top)\n        return [21.5, 34, -8, -18.5]\n    elif country == 'malawi':  # longitude(left) longitude(right), latitude (bottom), latitude(top)\n        return [32, 36, -9, -17]\n    elif country == 'uganda':  # longitude(left) longitude(right), latitude (bottom), latitude(top)\n        return [29, 36, -2, 5]\n    elif country == 'world':\n        return [-180, 180, -60, 85]\n    else:\n        return [-180, 180, -60, 85]",
        "sha1": "d6e5d5c833f33244ee1e02b1b1aafbcad7f836a1",
        "id": 644390
    },
    {
        "content": "import math\n\n\ndef atan(theNumber):\n    \"\"\"Returns math.atan(theNumber).\"\"\"\n    return math.atan(theNumber)",
        "sha1": "1a8285a7e16dbf9d0c071fa66956238dfb913d19",
        "id": 467659
    },
    {
        "content": "from typing import Tuple\nimport math\n\n\ndef normal_approximation_to_binomial(n: int, p: float) -> Tuple[float, float]:\n \"\"\"Returns mu and sigma corresponding to a Binomial(n, p)\"\"\"\n mu = p * n\n sigma = math.sqrt(p * (1 - p) * n)\n return mu, sigma",
        "sha1": "f7f5083a7e4cf54ac64f15283dc29593e212d127",
        "id": 565800
    },
    {
        "content": "def decryptFast(msg, cryptor):\n    \"\"\"Decrypts message with an existing `.pyelliptic.ECC` object\"\"\"\n    return cryptor.decrypt(msg)",
        "sha1": "3ac5620495ee9c2c6ee37cf96aabbf36a611084a",
        "id": 241448
    },
    {
        "content": "import torch\n\n\ndef augment(image, ref, augmenters):\n    \"\"\"Apply augmentation (only if reference is given, i.e., not at test-time).\n    \"\"\"\n    with torch.no_grad():        \n        if ref is not None:\n            for augmenter in augmenters:\n                image, ref = augmenter(image, ref)\n                \n    return image, ref",
        "sha1": "29bfa5fe85d6332b5109be31c69d12955f099e9b",
        "id": 539523
    },
    {
        "content": "def find_column(text, token):\n    \"\"\" Compute column.\n    input is the input text string\n    token is a token instance\n    \"\"\"\n    last_cr = text.rfind('\\n', 0, token.index)\n    if last_cr < 0:\n        last_cr = 0\n    column = (token.index - last_cr) + 1\n    return column",
        "sha1": "468f56fdb557b09be76ac5c2ec5145a7520eac9f",
        "id": 140708
    },
    {
        "content": "def in_degrees(graph):\n  \"\"\"Returns a list of node in-degrees in a ProgramGraph.\n\n  Args:\n    graph: A ProgramGraph.\n\n  Returns:\n    An (unsorted) list of node in-degrees.\n  \"\"\"\n  return [len(graph.incoming_neighbors(node)) for node in graph.all_nodes()]",
        "sha1": "8896614fd1c191634d04efda9d6ec153d27e5c7e",
        "id": 286189
    },
    {
        "content": "def seq_detect(seq: str) -> str:\n    \"\"\"Attempt to detect if a sequence is DNA or Protein\"\"\"\n    if all(c.lower() in \"atgc\" for c in seq):\n        return 'dna'\n    else:\n        return 'protein'",
        "sha1": "b2206e94d671f4bcc7f5977badd63af2ddfbb344",
        "id": 152504
    },
    {
        "content": "from typing import Tuple\n\n\ndef split_off_address(line: str) -> Tuple[str, str]:\n    \"\"\"Split e.g. 'beqz $r0,1f0' into 'beqz $r0,' and '1f0'.\"\"\"\n    parts = line.split(\",\")\n    if len(parts) < 2:\n        parts = line.split(None, 1)\n        if len(parts) < 2:\n            parts.append(\"\")\n    off = len(line) - len(parts[-1].strip())\n    return line[:off], line[off:]",
        "sha1": "8c86eb4f02f0cb51e583099a1d37a86ec748cac8",
        "id": 279979
    },
    {
        "content": "import json\n\n\ndef rpc_error(message = 'Invalid Request'):\n    \"\"\"\n    Generates an rpc error message\n    \"\"\"\n    return json.dumps({\"result\":None, 'error':{'message':message}, 'id':1})",
        "sha1": "4aa061e25be938b882cac71cb935f2153ad958cf",
        "id": 14935
    },
    {
        "content": "def _standardize_query(query: str) -> str:\n    \"\"\"\n    Removes string literals like \\r\\n or \\n and replaces them with whitespace.\n    \"\"\"\n    return query.replace(\"\\r\", \" \").replace(\"\\\\r\", \" \").replace(\"\\\\n\", \" \").strip()",
        "sha1": "78cc84bd63286620c71cdef077ef467f99e051ea",
        "id": 254598
    },
    {
        "content": "import collections\n\n\ndef _AggregateDicts(dicts):\n  \"\"\"Turn a sequence of dictionaries into a dictionary of lists.\"\"\"\n  result = collections.defaultdict(list)\n  for d in dicts:\n    for k, v in d.iteritems():\n      result[k].append(v)\n  return result",
        "sha1": "1c4f7045583a203ae827077837fa2bb49f27c648",
        "id": 345798
    },
    {
        "content": "def manu_year_cisco(value: str):\n    \"\"\"Calculates manufacture date from Cisco SN\"\"\"\n    date = int(value[3:5])\n    return f\"{date + 1996}\"",
        "sha1": "e1b052364653e54e9cca55adc5535a4be032b56a",
        "id": 96843
    },
    {
        "content": "from random import Random\nimport string\n\n\ndef rands(n):\n    \"\"\"Generates a random alphanumeric string of length *n*\"\"\"\n    return ''.join(Random().sample(string.ascii_letters+string.digits, n))",
        "sha1": "54aca2f57cf519fc632fb243836b25fcd8a7dc7f",
        "id": 557867
    },
    {
        "content": "def max_distinct(expr):\n    \"\"\"\n    Function ``MAX(DISTINCT expr)``\n    \"\"\"\n    return max(expr).distinct()",
        "sha1": "bc0ada46b87878954e30f2b6137814bc9eb082fe",
        "id": 501495
    },
    {
        "content": "def _get_output_file_idx(build_args):\n    \"\"\"\n        Get the index of output file from build args\n    :param build_args: arguments of gcc\n    :return: index in the build args where output file is.\n    \"\"\"\n    i = 0\n    while i < len(build_args):\n        if build_args[i] == \"-o\":\n            return i + 1\n        i += 1\n    return -1",
        "sha1": "1d8ff0b30525c1074feb88cf2646b910fe7f7db8",
        "id": 492507
    },
    {
        "content": "def fix_lon(lon):\n    \"\"\"\n    Fixes negative longitude values.\n\n    Parameters\n    ----------\n    lon: ndarray like or value\n        Input longitude array or single value\n    \"\"\"\n    if lon < 0:\n        lon += 360.\n    return lon",
        "sha1": "ab11dca9279399179242537c86cf0af85eedb60e",
        "id": 20552
    },
    {
        "content": "def get_model_params(net, name):\n    \"\"\"Get parameters of models by name.\"\"\"\n    for n, p in net.named_parameters():\n        if n == name:\n            return p",
        "sha1": "4d8e35078ba564442c2a2c726a3bf8350559d82b",
        "id": 159391
    },
    {
        "content": "import torch\n\n\ndef compute_ranking(x: torch.Tensor) -> torch.Tensor:\n    \"\"\" Given a vector of shape: (*, n) compute the ranking along the last dimension.\n\n        Note:\n            It works for arbitrary leading dimension. Each leading dimension will be treated independently.\n    \"\"\"\n    indices = torch.sort(x, dim=-1, descending=False)[1]\n\n    # this is the fast way which uses indexing on the left\n    src = torch.arange(start=0, end=x.shape[-1], step=1, dtype=indices.dtype, device=indices.device).expand_as(indices)\n    rank = torch.zeros_like(indices)\n    rank.scatter_(dim=-1, index=indices, src=src)\n    return rank",
        "sha1": "fa59456c69399fb96cdcfb7540f7677ae296207f",
        "id": 271947
    },
    {
        "content": "def _get_column_headers_and_names(document_source, column_mapping):\n    \"\"\"\n    Creates ordered lists of column headers (the 'titles of the columns), and column names (the name of the fields in\n    the ElasticSearch results, given the column mapping.\n\n    So, for example, given the following column_mapping:\n    {\n      \"\u05e2\u05d9\u05e8\": \"address.city\"\n      \"\u05ea\u05e7\u05e6\u05d9\u05d1\": \"details.budget\"\n    }\n    The returned values will be:\n    - column_headers: ['\u05e2\u05d9\u05e8', '\u05ea\u05e7\u05e6\u05d9\u05d1']\n    - column_names: ['address.city', 'details.budget']\n\n    If there is no column_mapping sent, the column_headers will be the same as the column_names\n\n    :param document_source: A single document from the Elasticsearch search result\n    :param column_mapping (dict): A dict mapping the column names in the original data, to the desired column headers\n    for the output file (see example above)\n\n    :return (tuple): A tuple consisting of column_headers and column_names, both lists (see example above)\n    \"\"\"\n\n    # In case of column mapping, set the order of the columns and then get the headers list and the names list\n    if column_mapping:\n        ordered_columns = [(field, column_mapping[field]) for field in column_mapping]\n        column_headers = [(field[1]) for field in ordered_columns]\n        column_names = [(field[0]) for field in ordered_columns]\n\n    # If no column_mapping is sent, simply set the order, and the column_names are the same as the column_headers\n    else:\n        column_headers = [field for field in document_source]\n        column_names = column_headers\n\n    return column_headers, column_names",
        "sha1": "6a147a7d60abf1c28aba4d77822ea7327a9eeafd",
        "id": 184622
    },
    {
        "content": "def get_zipname(date):\n\n    \"\"\"\n    Finds out what I want the zipfile called. Adds the date so that I can track these things\n    and replaces spaces with underscores because I am one inconsistent mofo.\n    :param date: today's date\n    :return: a nicely formatted filename for the zip file\n    \"\"\"\n\n    while True:\n        zipname = input('What do you want to call the zip file? ')\n        zipname = zipname.replace(' ', '_')\n        zipname = str(date) + '_' + str(zipname) + '.zip'\n        print('The zipfile will be called \"' + zipname + '\"')\n        proceed = input ('Happy to proceed? (y/n) ')\n        if proceed.lower() == 'y':\n            break\n\n    return zipname",
        "sha1": "7e06adec4a220dcd664af0f4d70941257639becb",
        "id": 452600
    },
    {
        "content": "def IsMolEmpty(Mol):\n    \"\"\"Check for the presence of atoms in a molecule.\n    \n    Arguments:\n        Mol (object): RDKit molecule object.\n\n    Returns:\n        bool : True - No atoms in molecule; Otherwise, false. \n\n    \"\"\"\n\n    Status = False if Mol.GetNumAtoms() else True\n    \n    return Status",
        "sha1": "80dd58ca1775e7349acfd17ccef6d8c9490ed496",
        "id": 148937
    },
    {
        "content": "def calculate_interest_amount_in_years(starting_amount, number_of_years, interest_rate, stipend_rate):\n    \"\"\"\n    After X number of years, how much would I have in the bank?\n    :param starting_amount: The amount of money the bank has to start with.\n    :type starting_amount: double\n    :param number_of_years: The amount of time to accrue interest.\n    :type number_of_years: int\n    :param interest_rate: The rate that interest is being added into the bank.\n    :type interest_rate: float\n    :param stipend_rate: The amount taken out each year for a stipend.\n    :type stipend_rate: float\n    :return: The amount in the bank, and the yearly stipends.\n    \"\"\"\n\n    # Money is not gained. Can be calculated, but not here.\n    if stipend_rate >= interest_rate:\n        return -1, -1\n\n    current_amount = starting_amount\n    stipend = {}\n    for year in range(number_of_years):\n        current_amount += (current_amount * interest_rate)\n\n        # We take the stipend out after adding new interest.\n        new_stipend_amount = current_amount * stipend_rate\n        current_amount -= new_stipend_amount\n        stipend[year+1] = round(new_stipend_amount, 2)\n\n    return current_amount, stipend",
        "sha1": "ea6a7e503c92f4b65e1ebaba8cca1bfce89ceff1",
        "id": 35996
    },
    {
        "content": "def translate_tags_IOB_to_grobid(tag: str) -> str:\n    \"\"\"\n    Convert labels from IOB2 to the ones used by GROBID (expected by the wapiti model)\n    \"\"\"\n    if tag == 'O':\n        # outside\n        return '<other>'\n    elif tag.startswith('B-'):\n        # begin\n        return 'I-' + tag[2:]\n    elif tag.startswith('I-'):\n        # inside\n        return '' + tag[2:]\n    else:\n        return tag",
        "sha1": "ed59d8a37016425dfc77af6dec3eaf6c6733bb9c",
        "id": 187600
    },
    {
        "content": "def eo_vm(x):\n    \"\"\"  Takes an integer and prints statements based on even/odd\n    :param x: INT: Any integer\n\n    No returns\n    \"\"\"\n    if type(x) != int:\n        return print('Please input a valid integer')\n\n    if x % 2 == 0:\n        print(f'{x} is even')\n        value = 'even'\n    else:\n        print(f'{x} is odd')\n        value = 'odd'\n\n    print(f'The next 9 {value} numbers are:')\n    for i in range(1, 10):\n        print(f'{x + (i*2)}')\n\n    return",
        "sha1": "1f965051e8931a86339f8330bfb7ddede7ae2d0b",
        "id": 267449
    },
    {
        "content": "from typing import List\n\n\ndef get_l_command_variations(s_command: str) -> List[str]:\n    \"\"\"\n    >>> assert get_l_command_variations('a b c') == ['a b c', 'a b', 'a']\n\n    \"\"\"\n    l_command_variations = list()\n    for n_variation in range(s_command.count(\" \") + 1):\n        l_command_variations.append(s_command.rsplit(' ', n_variation)[0])\n    return l_command_variations",
        "sha1": "9359671a14c6f9648ecd30f63ced826db42a99f4",
        "id": 270018
    },
    {
        "content": "def find_flipped_bit(s1, s2):\n    \"\"\" For two adjacent elements in a gray code, determine which bit is the \n    one that was flipped.\n    \"\"\"\n    if len(s1) == 0 or len(s2) == 0:\n        raise ValueError(\"Empty string inputted.\")\n\n    if len(s1) != len(s2):\n        raise ValueError(\"Strings compared in gray code must have the same length.\")\n    \n    if any([x != \"0\" and x != \"1\" for x in s1]) or any([x != \"0\" and x != \"1\" for x in s2]):\n        raise ValueError(f\"One of inputs {s1}, {s2} is not a valid binary string.\")\n    \n    # Sum the strings elementwise modulo 2; the sum will be 1 only in the slot \n    # where we flipped a bit        \n    string_sums = [(int(s1[i]) + int(s2[i])) % 2 for i in range(len(s1))]\n\n    if string_sums.count(1) == 0:\n        raise ValueError(f\"Strings {s1} and {s2} are the same.\")\n    elif string_sums.count(1) > 1:\n        raise ValueError(f\"Strings {s1} and {s2} are not ordered in a gray code.\")\n\n    return string_sums.index(1)",
        "sha1": "1bd4c4710ee191b1b77bb87df0085b69e3b7a886",
        "id": 436211
    },
    {
        "content": "def null_count(df):\n    \"\"\"Checks a DataFrame for nulls and returns the number of missing values\"\"\"\n    return df.isnull().sum()",
        "sha1": "606f50b15733056b76fc7bfe1dbcaf46382430b5",
        "id": 373573
    },
    {
        "content": "def model_query_scope_is_project(context, model):\n    \"\"\"Determine if a model should be scoped to a project.\n\n    :param context: The context to check for admin and advsvc rights.\n    :param model: The model to check the project_id of.\n    :returns: True if the context is not admin and not advsvc and the model\n        has a project_id. False otherwise.\n    \"\"\"\n    # Unless a context has 'admin' or 'advanced-service' rights the\n    # query will be scoped to a single project_id\n    return ((not context.is_admin and hasattr(model, 'project_id')) and\n            (not context.is_advsvc and hasattr(model, 'project_id')))",
        "sha1": "5857083c65ad5d0b65b13e41471bf051530547b6",
        "id": 172503
    },
    {
        "content": "from typing import List\nfrom typing import Dict\nfrom typing import Any\n\n\ndef _identify_trial_index_starting_step(\n    index: int,\n    trial_list: List[List[Dict[str, Any]]]\n) -> int:\n    \"\"\"Return the MCS step at the start of the trial with the given\n    index.\"\"\"\n    step = 0\n    for prior_index in range(0, index):\n        # Add 1 for the EndHabituation action step at the end of the trial.\n        step += len(trial_list[prior_index]) + 1\n    return step",
        "sha1": "16dcd82d239a7ee37dd95548afdcc538ef58aa2e",
        "id": 495421
    },
    {
        "content": "def to_full_html_document(html_template_chunk: str) -> str:\n    \"\"\"\n    Convert a HTML chunk into a full, valid html document.\n    \"\"\"\n    result = \"\"\n    result += \"<!DOCTYPE html>\\n\"\n    result += \"<html>\\n\"\n    result += \"<body>\\n\"\n    result += html_template_chunk + '\\n'\n    result += \"</body>\\n\"\n    result += \"</html>\\n\"\n\n    return result",
        "sha1": "7069b7d07ac8df5a3b02058b5fd98260ee1bef9c",
        "id": 155769
    },
    {
        "content": "from typing import Type\nimport dataclasses\n\n\ndef is_dataclassish(t: Type) -> bool:\n    \"\"\"\n    >>> is_dataclassish(int)\n    False\n    >>> is_dataclassish(tuple)\n    False\n    >>> from typing import NamedTuple\n    >>> class N(NamedTuple):\n    ...     field: int\n    >>> is_dataclassish(N)\n    True\n    >>> from dataclasses import dataclass\n    >>> @dataclass\n    ... class D:\n    ...     field: str\n    >>> is_dataclassish(D)\n    True\n    \"\"\"\n    if dataclasses.is_dataclass(t):\n        return True\n    b = t.__bases__\n    if len(b) != 1 or b[0] != tuple:\n        return False\n    f = getattr(t, '_fields', None)\n    if not isinstance(f, tuple):\n        return False\n    # pylint: disable=unidiomatic-typecheck\n    return all(type(n) == str for n in f)",
        "sha1": "7c37584a127f1a93ad01d21a6d105540291c5926",
        "id": 261956
    },
    {
        "content": "def get_calc_loc(target_name, calc_locs):\n    \"\"\"\n    This is a helper method that helps you pick out a certain calculation\n    from an array of calc_locs.\n\n    There are three modes:\n        - If you set target_name to a String, search for most recent calc_loc\n            with matching nameget_\n        - Otherwise, return most recent calc_loc overall\n\n    Args:\n        target_name: (bool or str) If str, will search for calc_loc with\n            matching name, else use most recent calc_loc\n        calc_locs: (dict) The dictionary of all calc_locs\n\n    Returns:\n        (dict) dict with subkeys path, filesystem, and name\n    \"\"\"\n\n    if isinstance(target_name, str):\n        for doc in reversed(calc_locs):\n            if doc[\"name\"] == target_name:\n                return doc\n        raise ValueError(\"Could not find the target_name: {}\".format(target_name))\n    else:\n        return calc_locs[-1]",
        "sha1": "d1bb48571779106c42f5b2f5d36fc5eaad1fc6c4",
        "id": 401173
    },
    {
        "content": "from typing import Callable\n\n\ndef threw(func: Callable, *args, **kwargs) -> bool:\n\t\"\"\"Return true if function threw\n\n\t:param func: function to be called\n\t:param args: args for function\n\t:param kwargs: kwargs for function\n\t:return: true if function threw\n\t\"\"\"\n\ttry:\n\t\tfunc(*args, **kwargs)\n\t\treturn False\n\texcept Exception:\n\t\treturn True",
        "sha1": "029d72194e61682eebcc46c5b5907cf0b88f382f",
        "id": 327613
    },
    {
        "content": "def human_to_bed_chrom_start_stop(start, stop):\n    \"\"\"\n    Returns a tuple containing chromosome coords in BED convention.\n\n    :param start: first bp coordinate of subsequence (chromosome starts with 1 not 0).\n    :param stop: last bp coordinate of subsequence (chromosome starts with 1 not 0).\n    :return: bed_coords = (bed_start, bed_stop)\n    \"\"\"\n    bed_start = start-1\n    bed_stop = stop\n\n    bed_coords = (bed_start, bed_stop)\n\n    return bed_coords",
        "sha1": "bc2e8349b5e9e9a56098377bf66c69c3ce3aaefb",
        "id": 293242
    },
    {
        "content": "import math\n\n\ndef euler_to_orientation( yaw=0.0, pitch = 0.0 ):\n    \"\"\"convert euler angles into orientation vector\"\"\"\n    z = math.sin( pitch )\n    xyr = math.cos( pitch )\n    y = xyr*math.sin( yaw )\n    x = xyr*math.cos( yaw )\n    return x, y, z",
        "sha1": "62120d5800ec889162540e7acd1d0cc36f352f09",
        "id": 262725
    },
    {
        "content": "import ast\n\n\ndef _parse_mock_imports(mod_ast, expanded_imports):\n  \"\"\"Parses a module AST node for import statements and resolves them against\n  expanded_imports (such as you might get from _expand_mock_imports).\n\n  If an import is not recognized, it is omitted from the returned dictionary.\n\n  Returns a dictionary suitable for eval'ing a statement in mod_ast, with\n  symbols from mod_ast's imports resolved to real objects, as per\n  expanded_imports.\n  \"\"\"\n  ret = {}\n\n  for node in mod_ast.body:\n    if isinstance(node, ast.Import):\n      for alias in node.names:\n        if alias.name in expanded_imports:\n          ret[alias.asname or alias.name] = expanded_imports[alias.name]\n    elif isinstance(node, ast.ImportFrom):\n      if node.level == 0:\n        for alias in node.names:\n          fullname ='%s.%s' % (node.module, alias.name)\n          if fullname in expanded_imports:\n            ret[alias.asname or alias.name] = expanded_imports[fullname]\n\n  return ret",
        "sha1": "50cb8e4e2469b7bf63deacf0a5085afcded0b5e3",
        "id": 8287
    },
    {
        "content": "def pop_rain(series):\n    \"\"\"\n    Converts a series of rain values into 0 or 1 depending on whether there is measurable rain\n    :param series:\n    :return:\n    \"\"\"\n    new_series = series.copy()\n    new_series[series >= 0.01] = 1.\n    new_series[series < 0.01] = 0.\n    return new_series",
        "sha1": "c071c81217d58e68ba42e0bb31179d0675064638",
        "id": 360108
    },
    {
        "content": "def pairwise_distance(data1, data2=None):\n    \"\"\"\n    Compute pairwise distances between data1 and data2, if data2 is None, then\n    pairwise distance between data1 and itself is computed.\n    \"\"\"\n    if data2 is None:\n        data2 = data1\n    A = data1.unsqueeze(dim=1)\n    B = data2.unsqueeze(dim=0)\n    dis = (A-B)**2.0\n    dis = dis.sum(dim=-1).squeeze()\n    return dis",
        "sha1": "e905820a4d836f1a3c102404aede822b9b57f771",
        "id": 604638
    },
    {
        "content": "def search_ancestor(node, *node_types):\n    \"\"\"\n    Recursively looks at the parents of a node and returns the first found node\n    that matches node_types. Returns ``None`` if no matching node is found.\n\n    :param node: The ancestors of this node will be checked.\n    :param node_types: type names that are searched for.\n    :type node_types: tuple of str\n    \"\"\"\n    while True:\n        node = node.parent\n        if node is None or node.type in node_types:\n            return node",
        "sha1": "709cbda37e591088c600736c36cf5c09b06f2d89",
        "id": 662327
    },
    {
        "content": "def replace_string_contents(raw_string, renaming_dictionary):\n    \"\"\"\n    Takes a string and replaces it with any changes provided in a renaming dictionary\n    :param raw_string: a raw string with which to pass through the renaming dictioanary\n    :param renaming_dictionary: a dictionary containing keys to be replaced with their associated values\n    :return: string identical to the raw string provided with all changes made based on the renaming dictionary\n    \"\"\"\n\n    # Renames a string using a provided renaming dictionary\n    replacement_string = raw_string\n\n    # Iterate over keys in renaming dictionary\n    for key in renaming_dictionary:\n\n        # For any keys found in the provided string, replace them using the value of the provided dictionary for that key\n        if key in replacement_string:\n            replacement_string = replacement_string.replace(key, renaming_dictionary[key])\n\n    # return the modified string\n    return replacement_string",
        "sha1": "87d022ef0b0f7a7b97d68ece9d4472f063fad0dd",
        "id": 125091
    },
    {
        "content": "import json\n\n\ndef load_category_map(category_map_file):\n  \"\"\"\n  Loads mapping between payees (as repored in the statement) and\n  expenses categories.\n  The input file is expected to be a json file with categories as\n  keys and payees as values.\n\n  E.g.\n  {\n    \"Expenses:PublicTransport\": [\"OysterCard\"]\n  }\n  \"\"\"\n  category_map = {}\n  with open(category_map_file) as f:\n    for (category, payees) in json.loads(f.read()).items():\n      for payee in payees:\n        category_map[payee] = category\n  return category_map",
        "sha1": "45ddcd159368a92975c167a51e959b71d4c50ae8",
        "id": 325062
    },
    {
        "content": "def splitter_fn(delim):\n    \"\"\"Simple string spliter function builder.\n\n    Creates a very simple string splitter function that splits an input string on the given `delim` character, then\n    strips whitespace, and yields the resultant values one at a time.\n\n    :param delim: delimiter character (e.g., ',')\n    :return: splitter function\n    \"\"\"\n    def splitter(s):\n        if s:\n            for v in s.split(delim):\n                yield v.strip()\n    return splitter",
        "sha1": "42a980c97316c102402b9ecd09f74e47551d8450",
        "id": 250390
    },
    {
        "content": "import string\n\n\ndef convert_to_title(s: str) -> str:\n    \"\"\"Formats string as a title, such that the input string has no punctuation,\n    is titlecased, and has no whitespace.\n\n    Args:\n        s: any string.\n\n    Returns:\n        The input string as a title.\n    \"\"\"\n    # Remove punctuation\n    s = s.translate(s.maketrans('', '', string.punctuation))\n    s = s.title()\n    # Remove whitespace\n    s = s.translate(s.maketrans('', '', string.whitespace))\n    return s",
        "sha1": "ac178bfad76170da7ad8a4888dc0e5cf43473602",
        "id": 312009
    },
    {
        "content": "def convert_eq(list_of_eq):\n    \"\"\"\n    Convert equality constraints in the right format\n    to be used by unification library.\n    \"\"\"\n    lhs = []\n    rhs = []\n    for eq in list_of_eq:\n        lhs.append(eq.lhs)\n        rhs.append(eq.rhs)\n    return tuple(lhs), tuple(rhs)",
        "sha1": "1366c0fa26b690cace85cb4feb2a646600850543",
        "id": 636950
    },
    {
        "content": "from datetime import datetime\n\n\ndef get_today_string(form: str = \"%d/%m/%Y\"):\n    \"\"\"Get a string with the present date.\n\n    :param form: The date format. Use %Y for year, %m for months and %d for daus, defaults to \"%d/%m/%Y\"\n    :type form: str, optional\n    :return: The present data in string format\n    :rtype: `str`\n    \"\"\"\n\n    # today = date.today()\n    today = datetime.now()\n    return str(today.strftime(form))",
        "sha1": "96f673936e68fe39212cccdfa0f1d2c341562147",
        "id": 152182
    },
    {
        "content": "def _to_micros(dur):\n    \"\"\"Convert duration 'dur' to microseconds.\"\"\"\n    return int(dur.total_seconds() * 10e5)",
        "sha1": "5d30e1dad26fc2922f6efb4c89fc599c0d5e50e2",
        "id": 194571
    },
    {
        "content": "from typing import Union\nfrom typing import Tuple\nfrom typing import Optional\n\n\ndef utm_region_code(\n    epsg: Union[int, Tuple[int, int, int]], tidx: Optional[Tuple[int, int]] = None\n) -> str:\n    \"\"\"\n    Construct UTM gridspec identifier.\n\n    Examples:\n    - 32751          -> \"51S\"\n    - 32633, 10, 2   -> \"33N_10_02\"\n    - 32603, (1, 2)  -> \"03N_01_02\"\n    \"\"\"\n    if isinstance(epsg, tuple):\n        tidx = epsg[1:]\n        epsg = epsg[0]\n\n    if 32601 <= epsg <= 32660:\n        zone, code = epsg - 32600, \"N\"\n    elif 32701 <= epsg <= 32760:\n        zone, code = epsg - 32700, \"S\"\n    else:\n        raise ValueError(\n            f\"Not a utm epsg: {epsg}, valid ranges [32601, 32660] and [32701, 32760]\"\n        )\n\n    if tidx is None:\n        return f\"{zone:02d}{code}\"\n\n    return f\"{zone:02d}{code}_{tidx[0]:02d}_{tidx[1]:02d}\"",
        "sha1": "41fe8ffa6c195d70bd26bc6b71ab0c12b6929b2f",
        "id": 654127
    },
    {
        "content": "def metric_source_slug(data_model, metric, source) -> str:\n    \"\"\"Return a slug for the metric source combination.\"\"\"\n    source_name = data_model[\"sources\"][source][\"name\"]\n    return f\"#{metric['name']} from {source_name}\".lower().replace(\" \", \"-\")",
        "sha1": "fbe59cf13a6d9dbf158960308930bfd306503323",
        "id": 553794
    },
    {
        "content": "import torch\n\n\ndef transform_vectors(matrix: torch.Tensor, vectors4: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Left-multiplies MxM @ NxM. Returns NxM.\n    \"\"\"\n    res = torch.matmul(vectors4, matrix.T)\n    return res",
        "sha1": "2ee0aa6a35cf31918851ccc62679d3f09bf1d378",
        "id": 133385
    },
    {
        "content": "def get_protected_attr_values(attr, df, privileged_group, privileged=True):\n    \"\"\"Retrieves all values given the privileged_group argument.\n\n    If privileged is True and privileged_group[attr] is a list then it returns\n    the list, if it's a function then values of df[attr] for which the function\n    returns True. \n\n    If privileged is False and privileged_group[attr] is a list then it returns\n    values of df[attr] not in the list else if it's a function returns values \n    of df[attr] for which the function returns False. \n\n    Parameters\n    ----------\n    attr: str\n        Protected attribute which is a key of the privileged_group\n        dictionnary\n    df: pd.DataFrame\n        Dataframe to extract privilieged group from.\n    privileged_group: dict\n        Dictionnary with protected attribute as key (e.g. age or gender)\n        and a list of favorable value (like ['Male']) or a function\n        returning a boolean corresponding to a privileged group\n    privileged: bool (default True)\n        Boolean prescribing whether to\n        condition this metric on the `privileged_groups`, if `True`, or\n        the `unprivileged_groups`, if `False`.\n\n    Returns\n    -------\n    list:\n        List of privileged values of the protected attribyte attr \n        if privileged is True else unprivileged values\n\n    Raises\n    ------\n    ValueError:\n        attr must be in privileged_group\n    \"\"\"\n    if attr not in privileged_group:\n        raise ValueError('attr must be in privileged_group')\n\n    val = privileged_group[attr]\n    if type(val) == list:\n        if privileged:\n            return val\n\n        def fn(x): return x in val\n    else:\n        fn = val\n\n    cond = df[attr].apply(fn) == privileged\n    return df[cond][attr].unique().astype(str).tolist()",
        "sha1": "5bff379b4564917f6786abeede86e32b10a0f5a1",
        "id": 239105
    },
    {
        "content": "def joinStrs(strlist):\n    \"\"\"Join a list of strings into a single string (in order).\"\"\"\n\n    return ''.join(strlist)",
        "sha1": "4d465f426e19490e3afc4f0cc4dc9af83960e8e0",
        "id": 247614
    },
    {
        "content": "def format_unit_name(unit):\n    \"\"\"\n        Formats the name of a production unit \n        so that different date sources coincide.\n \n        :param unit: The name of the unit\n        :type unit: string\n        :return: The formatted name of the unit\n        :rtype: string\n    \"\"\"\n        \n    ans = (unit.title()\n               .replace(\"'\", ' ')\n               .replace('-', ' ')\n               .replace('(Le) ','')\n               .replace('(L ) ','')\n               .replace('(La) ', '')\n               .replace(' B ', '')\n               .replace(' ( Sur Seine)', '')\n               .replace('St Maurice ', '')\n               .replace('Des Eaux', '')\n               .replace('Spem Ccg', 'Spem')\n               .replace('Combigolfe Ccg', 'Combigolfe')\n               .replace('Provence 4 Biomasse', 'Provence 4')\n               .replace('Lucy 3', 'Lucy')\n               .replace('Porcheville', 'Porcheville ')\n               .replace('Chooz', 'Chooz ')\n               .replace('Chinon', 'Chinon ')\n               .upper()\n               .lstrip()\n               .replace('  ', ' ')\n               )\n    return ans",
        "sha1": "7d6c510e62f1b817148827f73830f49b15c4bbef",
        "id": 283302
    },
    {
        "content": "import math\n\n\ndef segdist(P1, P2):\n    \"\"\"\n    Distance between two points, used for triangle\n    \"\"\"\n    seg = math.sqrt(math.pow(P2[0]-P1[0],2) + math.pow(P2[1]-P1[1],2) + math.pow(P2[2]-P1[2],2))\n    return seg",
        "sha1": "b3d440d94a85d3c239f99cd8f6d872c83c030f70",
        "id": 461783
    },
    {
        "content": "def calCentroid(xmn, ymn, xmx, ymx):\n    \"\"\"\n        Function to calculate the centroid of a given image\n    \"\"\"\n    xmid = (xmx + xmn) / 2\n    ymid = (ymx + ymn) / 2\n    return xmid, ymid",
        "sha1": "98212262d8eff2b55f14a1feabfa49b00c33912a",
        "id": 182385
    },
    {
        "content": "def inc(n: int) -> int:\n    \"\"\"\n    Increments n by 1\n\n    >>> inc(10)\n    11\n    \"\"\"\n    return n + 1",
        "sha1": "65a05a7b4fe241dbcf8f312ce3a638b64621e26e",
        "id": 314949
    },
    {
        "content": "import re\n\n\ndef format_wsl_path(path: str) -> str:\n    \"\"\"Format windows path to the WSL equivalent\n\n    Args:\n        path (str): Windows path to convert\n\n    Returns:\n        str: WSL path, i.e. /mnt/c/my/path.sh\n    \"\"\"\n    out_path = path.replace(\"\\\\\", \"/\", -1)\n\n    if re.search(\"^[a-z]:/\", out_path, re.IGNORECASE):\n        drive, _, rest = out_path.partition(\":\")\n        out_path = f\"/mnt/{drive.lower()}{rest}\"\n\n    return out_path",
        "sha1": "71f37009404c2875b9694b2ef8f98085a5c13542",
        "id": 478693
    },
    {
        "content": "def extra_same_elem(list1: list, list2: list) -> list:\n    \"\"\"\n\n    extract the same part between 2 lists\n\n    Args:\n        list1: list 1\n        list2: list 2\n\n    Returns:\n        same part between 2 lists\n\n    \"\"\"\n    set1 = set(list1)\n    set2 = set(list2)\n    same_elem = set1.intersection(set2)\n\n    return list(same_elem)",
        "sha1": "d60ca4779bb8a7e061e9dfd62258c8c1cc3c196b",
        "id": 529705
    },
    {
        "content": "def readlines_without_newlines(path):\n    \"\"\"\n    Read file into a list of lines, this exists because open(path).readlines() will preserve trailing newlines\n    \"\"\"\n    with open(path) as file:\n        return [line.strip() for line in file]",
        "sha1": "b050d61fdcde4f6f724c7089e3b947b02f6c7771",
        "id": 665597
    },
    {
        "content": "import random\n\n\ndef get_experiments(space_dict, number_of_exp):\n    \"\"\"populate a dictionary of hyperparameters\n       across dimensions in space_dict\n       for number_of_exp trials\n\n       for each parameter:\n           if there is a single value:\n               it is used and returned for every experiment\n           if thre are two values:\n               they are searched with random.uniform\n               to generate a value for a given experiment\n           if there is a list of values:\n               the list is searched with random.sample\n               to generate a value for a given experiment\n       for example:\n\n           search_space = {\n                'hyperparameter_1' : [0, 1, 2],\n                'hpyerparameter_2' : [0.5, 1.5]}\n           number_of_exp = 4\n           get_experiments(search_space, number_of_exp)\n\n           returns something like:\n\n               {'exp_0': {'hyperparameter_1': 0, 'hpyerparameter_2': 1.451704640798038},\n                'exp_1': {'hyperparameter_1': 2, 'hpyerparameter_2': 0.9641265687806964},\n                'exp_2': {'hyperparameter_1': 0, 'hpyerparameter_2': 0.6768719701707783},\n                'exp_3': {'hyperparameter_1': 1, 'hpyerparameter_2': 0.5655527538628148}}\n    \"\"\"\n#\n    experiments = dict()\n    for i in range(number_of_exp):\n        experiments.update({'exp_' + str(i) : dict()})\n        for key in space_dict:\n            if len(space_dict[key]) > 2:\n                value = random.sample(space_dict[key], 1)[0]\n            elif len(space_dict[key]) == 1:\n                value = space_dict[key][0]\n            else:\n                value = random.uniform(space_dict[key][0], space_dict[key][1])\n            experiments['exp_' + str(i)].update({key : value})\n    return experiments",
        "sha1": "99a308942f5b9a55c3f004d0374880555a40326a",
        "id": 550079
    },
    {
        "content": "def power_law(a,b,x):\n    \"\"\"\n    Power law function\n\n    Parameters\n    ----------\n    a : float\n        Power law coefficient.\n    b : float\n        Power law exponent.\n\n    \"\"\"\n    return a*x**(b)",
        "sha1": "dd2d57820ae228b29730675c9dcfcb665c5084cb",
        "id": 399304
    },
    {
        "content": "def columnvectorize(X):\n    \"\"\"Convert a 1D numpy array to a 2D column vector of size (N,1)\"\"\"\n    return X.reshape((X. size, 1)) if X.ndim == 1 else X",
        "sha1": "a596550b34551682af8cb179f4fe52f3f919e88d",
        "id": 475962
    },
    {
        "content": "def map_bool(to_bool) -> bool:\n    \"\"\"Maps value to boolean from a string.\n\n    Parameters\n    ----------\n    to_bool: str\n        Value to be converted to boolean.\n\n    Returns\n    -------\n    mapped_bool: bool\n        Boolean value converted from string.\n\n    Example\n    -------\n    >>> boolean_string = \"True\" # can also be lower case\n\n    >>> boolean_value = map_bool(boolean_string)\n    \"\"\"\n    try:\n        boolean_map = {\"true\": True, \"false\": False}\n        mapped_bool = boolean_map[to_bool.lower()]\n    except KeyError:\n        raise KeyError(\"Boolean Value Expected got '{}'\".format(to_bool))\n\n    return mapped_bool",
        "sha1": "4e3bb175f653174a56cb6ddc72ba7bcc56755826",
        "id": 703831
    },
    {
        "content": "def get_all_followings(igapi, target_userid):\n    \"\"\"\n    Returns a list of {\"username\": \"\", \"userid\": 0, \"is_private\": false, \"full_name\": \"\"}\n    \"\"\"\n    following_list = []\n    current_followings_list = igapi.getTotalFollowings(target_userid)\n    for following in current_followings_list:\n        user = {\n            \"username\": following[\"username\"],\n            \"userid\": following[\"pk\"],\n            \"is_private\": following[\"is_private\"],\n            \"full_name\": following[\"full_name\"],\n        }\n        following_list.append(user)\n    return following_list",
        "sha1": "c428d2044071a2430b05e793d5ecbf5326479767",
        "id": 665502
    },
    {
        "content": "def is_close(a, b, rel_tol=1e-09, abs_tol=0.0):\n    \"\"\"\n    Determines whether one float value is approximately equal or \"close\"\n    to another float value.\n\n    Copied from PEP 485.\n\n    Args:\n        a (float): Specifies the first value to be tested for relative\n            closeness.\n        b (float): Specifies the second value to be tested for relative\n            closeness.\n        rel_tol (float): Specifies the relative tolerance -- it is the\n            amount of error allowed, relative to the larger absolute\n            value of a or b. For example, to set a tolerance of 5%, use\n            rel_tol=0.05. The default tolerance is 1e-9, which assures\n            that the two values are the same within about 9 decimal\n            digits. rel_tol must be greater than 0.0.\n        abs_tol (float): Specifies a minimum absolute tolerance level --\n            useful for comparisons near zero.\n\n    Returns:\n        bool: Indicates whether the first float value is approximately\n            equal or \"close\" to the second float value.\n    \"\"\"\n    return abs(a-b) <= max(rel_tol * max(abs(a), abs(b)), abs_tol)",
        "sha1": "eb0cf402504882ed90ea238b0a86a7d821cec258",
        "id": 142254
    },
    {
        "content": "def process_error_helper(root, base_dir, process_func, errors_to_handle=(),\n                         **func_kwargs):\n    \"\"\"Wrapper which applies process_func and handles some common errors so one\n    bad run does not spoil the whole batch.\n\n    Useful errors to handle include:\n\n    OSError: if you are not sure if all the files exist\n    AssertionError: if some of the many assertions fail for known reasons;\n    for example is there are occasional problems decomposing runs into threads\n    due to limited numerical precision in logls.\n\n    Parameters\n    ----------\n    root: str\n        File root.\n    base_dir: str\n        Directory containing file.\n    process_func: func\n        Function for processing file.\n    errors_to_handle: error type or tuple of error types\n        Errors to catch without throwing an exception.\n    func_kwargs: dict\n        Kwargs to pass to process_func.\n\n    Returns\n    -------\n    run: dict\n        Nested sampling run dict (see the module docstring for more\n        details) or, if an error occured, a dict containing its type\n        and the file root.\n    \"\"\"\n    try:\n        return process_func(root, base_dir, **func_kwargs)\n    except errors_to_handle as err:\n        run = {'error': type(err).__name__,\n               'output': {'file_root': root}}\n        return run",
        "sha1": "a224920905b075490e57bc6e51cdee7d3e6bf972",
        "id": 525589
    },
    {
        "content": "import math\nimport statistics\n\n\ndef _get_average_da(streams_da: dict) -> dict:\n    \"\"\"Calculate average data availability among all data streams\"\"\"\n    total_results = {}\n    for k, v in streams_da.items():\n        for i, j in v.items():\n            if i not in total_results:\n                total_results[i] = []\n            total_results[i].append(j)\n\n    return {k: math.ceil(statistics.mean(v)) for k, v in total_results.items()}",
        "sha1": "db2fde9e13b4cbb5ce43d5f3c2d2ff2abd30f487",
        "id": 8857
    },
    {
        "content": "def nWaveRiseTime(pmax, patm=101e3, csnd=341, lamb=6.8e-8):\n    \"\"\"\n    Calculate N-wave rise time\n\n    Parameters\n    ----------\n    pmax -- N-wave overpressure amplitude in Pa\n    patm -- atmospheric pressure in Pa\n    csnd -- speed of sound in m/s\n    lamb -- air molecular mean free path in m\n\n    Returns\n    -------\n    trise -- N-wave rise time in s\n    \"\"\"\n    trise = (lamb / csnd) * (patm / pmax)\n    return trise",
        "sha1": "cbcae9d8e3ddaeb6daab5a02a722cd7302ebf562",
        "id": 14402
    },
    {
        "content": "import re\n\n\ndef clean_tag(tag):\n    \"\"\"Normalizes a tag to a plain lowercase identifier.\"\"\"\n    return re.sub('[^A-Za-z0-9]', '', tag).lower()",
        "sha1": "caaffc24fd93e8cd7a37a6a01651e95d5cc307bc",
        "id": 368589
    },
    {
        "content": "def should_reformat(value, line_length):\n    \"\"\"\n    Do we want to reformat this string.\n    \n    Only bother if it's a string and longer than the current line length target\n    or if it currently has a break\n    \"\"\"\n    return isinstance(value, str) and (\"\\n\" in value or len(value) > line_length)",
        "sha1": "fe208d72012c2bb8895604b3a8da2b1a1d7f9c6d",
        "id": 496717
    },
    {
        "content": "from typing import Dict\nfrom pathlib import Path\nfrom typing import List\n\n\ndef make_true_relatives(\n    probes: Dict[str, Path], galleries: Dict[str, List[Path]]\n) -> Dict[str, List[int]]:\n    \"\"\"\n    Make a list of true relatives for each probe. Each true relative faces will be\n    represented by its index in the gallery array.\n\n    Parameters\n    ----------------\n    probes:\n        A dictionary mapping FIDs to probes.\n    galleries:\n        A dictionary mapping split names to faces in the gallery for that split.\n\n    Returns\n    ---------------\n    A dictionary whose keys are FIDs, and who values are lists of ints. Each key represents\n    a probe, since each family has one probe. Each value represents a list of true relatives of\n    that probe in the gallery as indices in the gallery.\n\n    \"\"\"\n    true_relatives = {fid: [] for fid in probes}\n    for split_name in galleries:\n        for idx, face in enumerate(galleries[split_name]):\n            fid = face.parent.parent.name\n            true_relatives[fid].append(idx)\n\n    return true_relatives",
        "sha1": "b34081e011d5617e560ffd409a60ab0d15e57165",
        "id": 581767
    },
    {
        "content": "def _generate_source_tree(sources, sizes):\n  \"\"\"Generates a dict equivalent to the source tree. Each element is either a\n  file (so its value is its size) or a folder (so its value is a dictionary of\n  all the files or folders found inside it).\n\n  |sources| is a list of files to build the source tree out of, and |sizes|\n  has the size for every file in |sources|.\n\n  An example of a dict that might get returned:\n  {\n    'c': {\n        'file1': 18.0,\n        'folder': {'file2': 20}\n         }\n  }\n  \"\"\"\n  source_tree = {}\n  for filepath, size in zip(sources, sizes):\n    split_path = filepath.split('\\\\')\n\n    # Ensure that all the parent folders have been created.\n    parent = source_tree\n    for section in split_path[:-1]:\n      parent = parent.setdefault(section, {})\n\n    # Set the appropriate size for the file.\n    parent[split_path[-1]] = size\n\n  return source_tree",
        "sha1": "4a2f4686006ed137168fd57602c5e0be78d334d7",
        "id": 223328
    },
    {
        "content": "def single_option(list_like):\n    \"\"\"\n    checks if a single option is true\n    :param list_like: a list-like of bools\n    :return: true if exactly 1 is true, else returns false\n    \"\"\"\n    trues = 0\n    num = 0\n    for i, b in enumerate(list_like):\n        if b:\n            trues += 1\n            num = i + 1\n            if trues > 1:\n                return 0\n    return num",
        "sha1": "9e2698a9f38895703ae7b8084332294705bb8fb5",
        "id": 441266
    },
    {
        "content": "def constrain(table, column, minval, maxval, shift=0):\n    \"\"\" Constrains values like so:\n        - removes rows with values < minval\n        - caps column values at maxval\n        - shift values by shift (e.g. to allow for 0 to mean 1 bedroom)\n    \"\"\"\n    table = table[table[column] >= minval]\n    table.loc[table[column] > maxval, column] = maxval\n    table[column] = table[column] + shift\n    return table",
        "sha1": "d72e5922651de5e72bc7f0994e4f0f1b9f805f07",
        "id": 418870
    },
    {
        "content": "def pose_from_frame(frame):\n    \"\"\"Returns a PyBullet pose from a frame.\n\n    Parameters\n    ----------\n    frame : :class:`compas.geometry.Frame`\n\n    Returns\n    -------\n    point, quaternion : tuple\n    \"\"\"\n    return list(frame.point), frame.quaternion.xyzw",
        "sha1": "7024dfb0528159817bac6ccaa0bfdcccf05fd9df",
        "id": 407920
    },
    {
        "content": "def is_remote(path):\n    \"\"\"Determine if a uri is located on a remote server\n    First figures out if there is a ':' in the path (e.g user@host:/path)\n    Then looks if there is a single @ in part preceding the first ':'\n    :param path: uri/path\n    :type path: str\n    :return: True if path points to remote server, False otherwise\n    :rtype: bool\n    \"\"\"\n    dot = path.split(\":\")[0]\n    at = dot.split(\"@\")\n    if len(at) == 1:\n        return False\n    return True",
        "sha1": "979acfc548f5d550d249fceadad6f9ece4f40035",
        "id": 65124
    },
    {
        "content": "def gauss_sum_of_squares(n):\n    \"\"\"Calculate sum(x * x for x in range(1, n+1)) by formula.\"\"\"\n    return (2 * n + 1) * (n + 1) * n // 6",
        "sha1": "ae937630d377c5748658aa9f01361b81807614a2",
        "id": 455584
    },
    {
        "content": "import math\n\n\ndef _sine_sample(amp, freq, rate, i) -> float:\n    \"\"\"\n    Generates a single audio sample taken at the given sampling rate on \n    a sine wave oscillating at the given frequency at the given amplitude.\n\n    :param float amp The amplitude of the sine wave to sample\n    :param float freq The frequency of the sine wave to sample\n    :param int rate The sampling rate\n    :param int i The index of the sample to pull\n\n    :return float The audio sample as described above\n    \"\"\"\n    return float(amp) * math.sin(2.0 * math.pi * float(freq)\n        * (float(i) / float(rate)))",
        "sha1": "223a48fcbea6f9ef98418d9a3d161f37f64f2a05",
        "id": 129029
    },
    {
        "content": "def tp10k_transform(DGE,norm_factor=1.0e4):\n    \"\"\"normalize columns of pandas dataframe to sum to a constant, by default 10,000\"\"\"\n    return(norm_factor*(DGE / DGE.sum()))",
        "sha1": "04a98017f4a593aa1b398a9fe378dc523d0fb4d3",
        "id": 450597
    },
    {
        "content": "def split_cubic_into_two(p0, p1, p2, p3):\n    \"\"\"Split a cubic Bezier into two equal parts.\n\n    Splits the curve into two equal parts at t = 0.5\n\n    Args:\n        p0 (complex): Start point of curve.\n        p1 (complex): First handle of curve.\n        p2 (complex): Second handle of curve.\n        p3 (complex): End point of curve.\n\n    Returns:\n        tuple: Two cubic Beziers (each expressed as a tuple of four complex\n        values).\n    \"\"\"\n    mid = (p0 + 3 * (p1 + p2) + p3) * .125\n    deriv3 = (p3 + p2 - p1 - p0) * .125\n    return ((p0, (p0 + p1) * .5, mid - deriv3, mid),\n            (mid, mid + deriv3, (p2 + p3) * .5, p3))",
        "sha1": "064465614c96c769bd585e7fd0238180ac306de4",
        "id": 483568
    },
    {
        "content": "import collections\n\n\ndef readAsIntervals( gff_iterator, with_values = False, with_records = False,\n                     use_strand = False ):\n    \"\"\"read tuples of (start, end) from a GFF file.\n\n    If with_values is True, a value is added to the tuples.\n    If with_records is True, the record is added to the tuples.\n    with_values and with_records are exclusive.\n    \n    If use_strand is True, intervals will be grouped by contig and strand.\n    The default is to group by contig only.\n\n    Returns a dictionary of intervals by contig.\n    \"\"\"\n\n    assert not (with_values and with_records), \"both with_values and with_records are true.\"\n    intervals = collections.defaultdict(list)\n    if use_strand:\n        keyf = lambda x: (x.contig, x.strand)\n    else:\n        keyf = lambda x: x.contig\n\n    if with_values:\n        for gff in gff_iterator:\n            intervals[keyf(gff)].append( (gff.start,gff.end,gff.score) )\n    elif with_records:\n        for gff in gff_iterator:\n            intervals[keyf(gff)].append( (gff.start,gff.end,gff) )\n    else:\n        for gff in gff_iterator:\n            intervals[keyf(gff)].append( (gff.start,gff.end) )\n    return intervals",
        "sha1": "1959dbc10e90f4af32aa11f3babdd1622416d9a0",
        "id": 132453
    },
    {
        "content": "from typing import Mapping\n\n\ndef mapping_update(mapping, *args, **kwargs):\n    \"\"\"Deep update a nested mapping data structure.\n\n    Unlike Python's ``dict.update()`` method, if the same key is present in\n    both the target and this mapping and the value is a dictionary, it\n    will be updated instead of being overwritten.\n\n    .. code-block:: python\n\n        >>> d1 = {'baz': {'foo': 'foo'}, 'fizz': 'buzz'}\n        >>> d2 = {'baz': {'bar': 'bar'}, 'fizz': 'fizzbuzz'}\n        >>> mapping_update(d1, d2)\n        {'baz': {'foo': 'foo', 'bar': 'bar'}, 'fizz': 'fizzbuzz'}\n\n\n    :param mapping: The mapping data structure to be updated.\n    :type mapping: ~collections.abc.Mapping\n\n\n    :returns: The updating mapping.\n    :rtype mapping: ~collections.abc.Mapping\n\n    \"\"\"\n    for k, v in mapping.__class__(*args, **kwargs).items():\n        if isinstance(v, Mapping):\n            mapping[k] = mapping_update(mapping.get(k, {}), v)\n        else:\n            mapping[k] = v\n\n    return mapping",
        "sha1": "2a3013847593f76fea56df2a4cc7163b7eb7091a",
        "id": 346890
    },
    {
        "content": "from pathlib import Path\n\n\ndef epacems_parquet_path(\n    pudl_settings_fixture,\n    pudl_engine,  # implicit dependency; ensures .parquet files exist\n):\n    \"\"\"Get path to the directory of EPA CEMS .parquet data.\"\"\"\n    out_dir = Path(pudl_settings_fixture['parquet_dir'], 'epacems')\n    return out_dir",
        "sha1": "7fa1b8ef2cfa589fb38076dd84d999019bd8b0a3",
        "id": 263394
    },
    {
        "content": "def merge_blocks(ivs, dist=0):\n    \"\"\" Merge blocks\n\n    Args:\n        ivs (list): List of intervals. Each interval is represented by a tuple of\n            integers (start, end) where end > start.\n        dist (int): Distance between intervals to be merged. Setting dist=1 will merge\n            adjacent intervals\n\n    Returns:\n        list: Merged list of intervals\n\n    Examples:\n        >>> merge_interval_list([])\n        []\n        >>> merge_interval_list([(1,10)])\n        [(1, 10)]\n        >>> merge_interval_list([(4, 9), (10, 14), (1, 3)])\n        [(1, 3), (4, 9), (10, 14)]\n        >>> merge_interval_list([(4, 9), (10, 14), (1, 3)], dist=1)\n        [(1, 14)]\n    \"\"\"\n    if len(ivs)<= 1: return ivs\n    ivs.sort(key=lambda x:x[0])\n    ret = [ivs[0]]\n    for iv in ivs[1:]:\n        if iv[0] - ret[-1][1] > dist:\n            ret.append(iv)\n        else:\n           ret[-1] = (ret[-1][0], max(iv[1],ret[-1][1]))\n    return ret",
        "sha1": "549cd0b4cffdf3df3053566324b1bd097eb7a6af",
        "id": 470096
    },
    {
        "content": "def in_3d_box(box, coords):\n    \"\"\"\n    Check if point is in a box\n\n    Args:\n      box (tuple): ((x0, x1), (y0, y1), (z0, z1)).\n      coords (tuple): (x, y, z).\n\n    Returns\n      bool\n    \"\"\"\n    cx = coords[0] >= box[0][0] and coords[0] <= box[0][1]\n    cy = coords[1] >= box[1][0] and coords[1] <= box[1][1]\n    cz = coords[2] >= box[2][0] and coords[2] <= box[2][1]\n    return cx and cy and cz",
        "sha1": "4580e67c89b02565b0ac4d1b5c1d11dd5396f74a",
        "id": 25840
    },
    {
        "content": "def strip_latex_delimiters(source):\n    \"\"\"Remove LaTeX math delimiters that would be rendered by the math block.\n\n    These are: ``\\(\u2026\\)``, ``\\[\u2026\\]``, ``$\u2026$``, and ``$$\u2026$$``.\n    This is necessary because sphinx does not have a dedicated role for\n    generic LaTeX, while Jupyter only defines generic LaTeX output, see\n    https://github.com/jupyter/jupyter-sphinx/issues/90 for discussion.\n    \"\"\"\n    source = source.strip()\n    delimiter_pairs = (pair.split() for pair in r\"\\( \\),\\[ \\],$$ $$,$ $\".split(\",\"))\n    for start, end in delimiter_pairs:\n        if source.startswith(start) and source.endswith(end):\n            return source[len(start) : -len(end)]\n\n    return source",
        "sha1": "2a5e945dceac3e6d6dc30f20e12d8d9a64c2b490",
        "id": 354207
    },
    {
        "content": "from typing import List\n\n\ndef process_reverse_chain_params(resource: str, reverse_chains: List[dict]) -> str:\n    \"\"\"\n    Creates a query string based on the given reverse chain parameters for the queried resource (querying based on\n    other resources that refer to the resource.\n    :param resource: the main resource on which to query references\n    :param reverse_chains: list of reverse chain resources and parameters\n    :return: query string to use in fhir search\n    \"\"\"\n\n    resource_prop = resource.lower()\n    reverse_chain_string = \"\"\n    # add all the resources given as reverse chain parameters to the query url\n    for i, chain_resource in enumerate(reverse_chains):\n        reverse_chain_string += f\"_has:{chain_resource['resource']}:{resource_prop}:{chain_resource['property']}=\"\n\n        # Check if there are multiple conditions given and if so join them with commas\n        if isinstance(chain_resource[\"params\"], list):\n            reverse_chain_string += \",\".join(chain_resource[\"params\"])\n        else:\n            reverse_chain_string += chain_resource[\"params\"]\n\n        if i < len(reverse_chains) - 1:\n            reverse_chain_string += \"&\"\n    return reverse_chain_string",
        "sha1": "fa954bda820c489ee1c37d7aaa0ba1322416e30e",
        "id": 319997
    },
    {
        "content": "def is_list_of_dict(l):\n    \"\"\"\n    Checks if a list is entirely composed of dictionaries\n\n        l ([]): List of any type\n\n        returns (bool): True if l is entirely composed of dicts\n    \"\"\"\n    return all(type(x) == dict for x in l)",
        "sha1": "eb9d159d0b130b2070141daba6f9bc494da4589f",
        "id": 166392
    },
    {
        "content": "def xaxis3D(\n    xaxis3d_type=None,\n    xaxis3d_name=\"\",\n    xaxis3d_name_size=16,\n    xaxis3d_name_gap=20,\n    xaxis3d_min=None,\n    xaxis3d_max=None,\n    xaxis3d_interval=\"auto\",\n    xaxis3d_margin=8,\n    **kwargs\n):\n    \"\"\"\n    3D x \u8f74\u914d\u7f6e\u9879\n\n    :param xaxis3d_type:\n        3D x \u8f74\u7c7b\u578b\n    :param xaxis3d_name:\n        x \u8f74\u540d\u79f0\uff0c\u9ed8\u8ba4\u4e3a \"\"\n    :param xaxis3d_name_size:\n        x \u8f74\u540d\u79f0\u4f53\u5927\u5c0f\uff0c\u9ed8\u8ba4\u4e3a 16\n    :param xaxis3d_name_gap:\n        x \u8f74\u540d\u79f0\u4e0e\u8f74\u7ebf\u4e4b\u95f4\u7684\u8ddd\u79bb\uff0c\u9ed8\u8ba4\u4e3a 25\n    :param xaxis3d_min:\n        x \u5750\u6807\u8f74\u523b\u5ea6\u6700\u5c0f\u503c\uff0c\u9ed8\u8ba4\u4e3a\u81ea\u9002\u5e94\u3002\n    :param xaxis3d_max:\n        x \u5750\u6807\u8f74\u523b\u5ea6\u6700\u5927\u503c\uff0c\u9ed8\u8ba4\u4e3a\u81ea\u9002\u5e94\u3002\n    :param xaxis3d_interval:\n        x \u8f74\u523b\u5ea6\u6807\u7b7e\u7684\u663e\u793a\u95f4\u9694\uff0c\u5728\u7c7b\u76ee\u8f74\u4e2d\u6709\u6548\u3002\u9ed8\u8ba4\u4f1a\u91c7\u7528\u6807\u7b7e\u4e0d\u91cd\u53e0\u7684\u7b56\u7565\u95f4\u9694\u663e\u793a\u6807\u7b7e\u3002\n        \u8bbe\u7f6e\u6210 0 \u5f3a\u5236\u663e\u793a\u6240\u6709\u6807\u7b7e\u3002\u8bbe\u7f6e\u4e3a 1\uff0c\u8868\u793a\u300e\u9694\u4e00\u4e2a\u6807\u7b7e\u663e\u793a\u4e00\u4e2a\u6807\u7b7e\u300f\uff0c\u5982\u679c\u503c\n        \u4e3a 2\uff0c\u8868\u793a\u9694\u4e24\u4e2a\u6807\u7b7e\u663e\u793a\u4e00\u4e2a\u6807\u7b7e\uff0c\u4ee5\u6b64\u7c7b\u63a8\n    :param xaxis3d_margin:\n        x \u8f74\u523b\u5ea6\u6807\u7b7e\u4e0e\u8f74\u7ebf\u4e4b\u95f4\u7684\u8ddd\u79bb\u3002\u9ed8\u8ba4\u4e3a 8\n    \"\"\"\n    _xaxis3D = {\n        \"name\": xaxis3d_name,\n        \"nameGap\": xaxis3d_name_gap,\n        \"nameTextStyle\": {\"fontSize\": xaxis3d_name_size},\n        \"type\": xaxis3d_type,\n        \"min\": xaxis3d_min,\n        \"max\": xaxis3d_max,\n        \"axisLabel\": {\"margin\": xaxis3d_margin, \"interval\": xaxis3d_interval},\n    }\n    return _xaxis3D",
        "sha1": "be5a05ddbf5b0c4d0bb93eb6384c5a260905ca8e",
        "id": 344248
    },
    {
        "content": "from pathlib import Path\n\n\ndef upload_fop_image(obj):\n    \"\"\"\n    Note: obj must be instance of ProductImage model\n\n    Takes the parent product_image name e.g. /LOBLAWS/20190101/something.jpg and morphs it into\n    /LOBLAWS/20190101/something_fop.jpg\n\n    Reference for how ImageField attributes can be accessed:\n    https://docs.djangoproject.com/en/2.2/topics/files/\n    \"\"\"\n    parent_path = Path(obj.product_image.image_path.name)  # e.g. /LOBLAWS/20190101/something.jpg\n    suffix = parent_path.suffix\n    fop_path = parent_path.with_suffix(f'_FOP{suffix}')\n    return fop_path",
        "sha1": "0dd860569424ada8b305b084eb3a9e347e5e9c89",
        "id": 444134
    },
    {
        "content": "def areinstance(tokens, classes):\n    \"\"\"\n    >>> tokens = (TimeToken(15), TimeToken(16))\n    >>> areinstance(tokens, TimeToken)\n    True\n    >>> tokens = (TimeToken(15), DayToken(7, 5, 2018))\n    >>> areinstance(tokens, TimeToken)\n    False\n    >>> areinstance(tokens, (TimeToken, DayToken))\n    True\n    \"\"\"\n    assert isinstance(classes, type) or isinstance(classes, tuple), \\\n        \"Classes must either be a tuple or a type.\"\n    if isinstance(classes, type):\n        classes = (classes,)\n    return all([\n        any([isinstance(token, cls) for cls in classes]) for token in tokens])",
        "sha1": "27cf41a668dfcd52975becd0ae96c81d2c9ab385",
        "id": 20782
    },
    {
        "content": "def getCenter(x, y, blockSize):\n    \"\"\"\n    Determines center of a block with x, y as top left corner coordinates and blockSize as blockSize\n    :return: x, y coordinates of center of a block\n    \"\"\"\n    return (int(x + blockSize/2), int(y + blockSize/2))",
        "sha1": "9398aa6438b5921c625381898a7666b4203d2e73",
        "id": 35010
    },
    {
        "content": "def _clear_dash(s):\n  \"\"\"Returns s if it is not '-', otherwise returns ''.\"\"\"\n  return s if s != '-' else ''",
        "sha1": "282e475029c8bbfac9d94735f87d0a43d4405a51",
        "id": 437144
    },
    {
        "content": "def overl(l1, l2):\n    \"\"\"\n    calculate overlap of two lists\n    \"\"\"\n    return len(set(l1).intersection(l2)) / max([len(l1), len(l2)])",
        "sha1": "b78cf188179465bec6e26f29da1b7e9a0b16f90f",
        "id": 144432
    },
    {
        "content": "def format_website_string(website):\n    \"\"\"Returns a formatted website string to be used for siteinfo.\n    Args:\n        website (string): user-supplied.\n    Returns:\n        string: lower-case, prefixes removed.\n    \"\"\"\n    formatted = website.lower()\n    prefixes = [\"https://\", \"http://\", \"www.\"]\n    for prefix in prefixes:\n        if formatted.startswith(prefix):\n            formatted = formatted[len(prefix) :]\n    return formatted",
        "sha1": "35598dcc8ce8a9b9ce6cdc00be9fa98d5981def4",
        "id": 277340
    },
    {
        "content": "def type_name(cls):\n    \"\"\"Get the name of a class.\"\"\"\n    if not cls:\n        return '(none)'\n    return '{}.{}'.format(cls.__module__, cls.__name__)",
        "sha1": "708ef5940d415500789dc64d5308632ed9676a7e",
        "id": 100685
    },
    {
        "content": "from pathlib import Path\n\n\ndef iterdir(path):\n    \"\"\"Return directory listing as generator\"\"\"\n    return Path(path).iterdir()",
        "sha1": "54e370f7a9a99f267b5a91a85219ce90f1e6dafd",
        "id": 679484
    },
    {
        "content": "def scan_dir(d):\n    \"\"\"Look in the supplied directory for a vendor or WORKSPACE entry.\n\n    Args:\n      d: full path to directory to be scanned.\n\n    Returns:\n      Success (boolean) and Full Path.\n    \"\"\"\n    success = False\n    path = \"\"\n\n    if d.get_child(\"vendor\").exists:\n        # vendor exists, this is our match.  Return it to the caller as success\n        path = d.get_child(\"vendor\")\n        success = True\n    elif d.get_child(\"WORKSPACE\").exists:\n        # We should stop looking if we've reached the workspace root\n        path = \"\"\n        success = True\n\n    return (success, path)",
        "sha1": "57c5483f99cba99a89000d03799be6db9837a232",
        "id": 270397
    },
    {
        "content": "def letter_to_num(letter):\n\t\"\"\"Returns the distance from 'a' of a character\n\tletter_to_num('a') == 0, letter_to_num('y') == 24\"\"\"\n\treturn ord(letter) - 97",
        "sha1": "2af1d058df1b3b1282d900039d93c2ab6cd5a2aa",
        "id": 157654
    },
    {
        "content": "def find_init(n, init_scalar, list_idx):\n\t\"\"\"\n\tFind the list scalar products for functions from init_scalar. \n\n\tParameters:\n\tn \t\t\t-- integer\n\tinit_scalar -- list of Decimal\n\tlist_idx    -- list of list\n\n\tReturn:\n\tlist_init -- list of Decimal\n\t\"\"\"\t\n\tlist_init = []\n\n\tfor j in range(len(list_idx)):\n\t\tinit = [init_scalar[i][list_idx[j][i]] for i in range(n)]\n\t\tlist_init.append(init)\n\n\treturn list_init",
        "sha1": "4626841d3182bf1d5c44ade5474f94a32185bd04",
        "id": 316530
    },
    {
        "content": "def select_model_features(df, categorical_col_list, numerical_col_list, PREDICTOR_FIELD, grouping_key='patient_nbr'):\n    \"\"\"\n        INPUT:\n            - df (Pandas DataFrame): dataset\n            - categorical_col_list (string array): array of all categorical feature names\n            - numerical_col_list (string array): array of all numerical feature names\n            - PREDICTOR_FIELD: the label which is time_in_hospitalization\n            - grouping_key: feature to be grouped on later\n        \n        OUTPUT:\n            - returns the DataFrame for all this feautres\n    \"\"\"\n    selected_col_list = [grouping_key] + [PREDICTOR_FIELD] + categorical_col_list + numerical_col_list   \n    return df[selected_col_list]",
        "sha1": "b711140faf3a48b7a95caf3f8da075c411c8b3bb",
        "id": 266350
    },
    {
        "content": "import click\n\n\ndef map_setup_options(f):\n    \"\"\"Create common options for making 2D maps of the data set\"\"\"\n    f = click.option('--peratom',\n                     help='Save the per-atom projection.',\n                     default=False, is_flag=True)(f)\n    f = click.option('--adjusttext/--no-adjusttext',\n                     help='Adjust the annotation texts so they do not overlap.',\n                     default=False)(f)\n    f = click.option('--annotate', '-a',\n                     help='Location of tags to annotate the samples.',\n                     default='none', type=str)(f)\n    f = click.option('--aspect_ratio', '-ar',\n                     help='Aspect ratio of the plot',\n                     show_default=True, default=2, type=float)(f)\n    f = click.option('--style', '-s',\n                     type=click.Choice(['default', 'journal'], case_sensitive=False),\n                     help='Style of the plot.',\n                     show_default=True, default='default')(f)\n    return f",
        "sha1": "dee4f4244cb486acf1646fdcd7ebd42a710a10f0",
        "id": 405048
    },
    {
        "content": "def GetCbsdRegistrationRequest(cbsd):\n  \"\"\"Returns a CBSD registration request from the |Cbsd| object.\n  \"\"\"\n  return {\n      'cbsdCategory': cbsd.category,\n      'installationParam': {\n          'latitude': cbsd.latitude,\n          'longitude': cbsd.longitude,\n          'height': cbsd.height_agl,\n          'heightType': 'AGL',\n          'indoorDeployment': cbsd.is_indoor,\n          'antennaBeamwidth': cbsd.antenna_beamwidth,\n          'antennaAzimuth': cbsd.antenna_azimuth,\n          'antennaGain': cbsd.antenna_gain,\n      }\n  }",
        "sha1": "2720960f20e9eb792f0cc4dc5564c85a5dd69743",
        "id": 635041
    },
    {
        "content": "def find_card_number(patron):\n    \"\"\"\n    Gets the card number from the patron record\n    :param patron: patron record dictionary\n    :return: card_number\n    \"\"\"\n    return patron.get(\"id\")",
        "sha1": "17f8a5d9f723243848094e69031d47b5c0b17502",
        "id": 120667
    },
    {
        "content": "import re\nfrom textwrap import dedent\n\n\ndef normalize_repr_output(expression):\n    \"\"\"\n    Enable string comparison between :func:`repr()` output on different Python versions.\n\n    This function enables string comparison between :func:`repr()` output on\n    Python 2 (where Unicode strings have the ``u`` prefix) and Python 3 (where\n    Unicode strings are the default and no prefix is emitted by\n    :func:`repr()`).\n    \"\"\"\n    return re.sub(r'\\bu([\\'\"])', r'\\1', dedent(expression).strip())",
        "sha1": "2a0b152171be4bb9b1027f9b6e8641a3e3516b3d",
        "id": 268489
    },
    {
        "content": "def get_value(tablerow):\n    \"\"\"Extract the value of a tablerow.\n\n    Parameters:\n        tablerow (bs4.BeautifulSoup): A tablerow from the wikipedia infobox in html\n\n    Returns:\n        value (str): Value of the tablerow\n\n    Raises:\n        None\n    \"\"\"\n    tabledata = tablerow.find('td')\n    if tabledata:\n        value = tabledata.get_text(separator=\" \", strip=True)\n    else:\n        value = ''\n    return value",
        "sha1": "d96c373d354dbd86834086fd27d2425092ec6dec",
        "id": 484150
    },
    {
        "content": "import re\n\n\ndef agent_pid_to_ip(agent_pid: str) -> str:\n    \"\"\"Convert the agent PID from Mesos into an IP address\n\n    :param: agent pid (this is in the format 'slave(1)@10.40.31.172:5051')\n    :returns: ip address\n    \"\"\"\n    m = re.match(r\".+?@([\\d\\.]+):\\d+\", agent_pid)\n    assert m\n    return m.group(1)",
        "sha1": "303a144e054b9205547694de37417c10c396b4bb",
        "id": 224190
    },
    {
        "content": "def to_force(weight):\n    \"\"\"\n    Converts weight in grams to force in N at standard earth gravity.\n    \"\"\"\n    return 9.81 * weight / 1000",
        "sha1": "f173c3e1598edcb7255ec5aff3389ae2de054fc5",
        "id": 71698
    },
    {
        "content": "def rgb2bgr(x):\n    \"\"\"\n    Convert clip frames from RGB mode to BRG mode.\n    Args:\n        x (Tensor): A tensor of the clip's RGB frames with shape:\n            (channel, time, height, width).\n\n    Returns:\n        x (Tensor): Converted tensor\n    \"\"\"\n    return x[[2, 1, 0], ...]",
        "sha1": "c5672fe1290d641fede5395184be7b8d86e08d8f",
        "id": 224116
    },
    {
        "content": "import time\n\n\ndef _block(predicate, timeout):\n    \"\"\"\n    Block until a predicate becomes true.\n\n    ``predicate`` is a function taking no arguments. The call to\n    ``_block`` blocks until ``predicate`` returns a true value. This\n    is done by polling ``predicate``.\n\n    ``timeout`` is either ``True`` (block indefinitely) or a timeout\n    in seconds.\n\n    The return value is the value of the predicate after the\n    timeout.\n    \"\"\"\n    if timeout:\n        if timeout is True:\n            timeout = float('Inf')\n        timeout = time.time() + timeout\n        while not predicate() and time.time() < timeout:\n            time.sleep(0.1)\n    return predicate()",
        "sha1": "0d2058e47885aaf7a20034cc6678623e2656f7a8",
        "id": 417096
    },
    {
        "content": "def quizn_to_index(quizn):\n  \"\"\"See: https://github.com/fielddaylab/jo_wilder/blob/master/src/scenes/quiz.js\n\n  For some reason there are 5 quizzes, but there is no quiz numbered 1.\n  \n  Returns:\n    The correct quiz number for quizzes 2-5, or 0 for quiz 0.\n  \"\"\"\n  return quizn - 1 if quizn >= 2 else quizn",
        "sha1": "b57df8c103d3124872be02eb487772787cb8131e",
        "id": 62078
    },
    {
        "content": "def spacing(area, shape):\n    \"\"\"\n    Returns the spacing between grid nodes\n\n    Parameters:\n\n    * area\n        ``(x1, x2, y1, y2)``: Borders of the grid\n    * shape\n        Shape of the regular grid, ie ``(ny, nx)``.\n\n    Returns:\n\n    * ``[dy, dx]``\n        Spacing the y and x directions\n\n    \"\"\"\n    x1, x2, y1, y2 = area\n    ny, nx = shape\n    dx = float(x2 - x1) / float(nx - 1)\n    dy = float(y2 - y1) / float(ny - 1)\n    return [dy, dx]",
        "sha1": "71e803ac463651f67d4f77f876af201cae719059",
        "id": 209015
    },
    {
        "content": "def box(points):\n    \"\"\"Obtain a tight fitting axis-aligned box around point set\"\"\"\n    xmin = min(points, key=lambda x: x[0])[0]\n    ymin = min(points, key=lambda x: x[1])[1]\n    xmax = max(points, key=lambda x: x[0])[0]\n    ymax = max(points, key=lambda x: x[1])[1]\n    return (xmin, ymin), (xmax, ymax)",
        "sha1": "8efb486150aa6d9f74f011c37302b57cf78d46f2",
        "id": 255219
    },
    {
        "content": "def lesser(tup,value):\n    \"\"\"\n    Returns the number of elements in tup strictly less than value\n\n    Examples:\n        lesser((5, 9, 1, 7), 6) returns 2\n        lesser((1, 2, 3), -1) returns 0\n\n    Parameter tup: the tuple to check\n    Precondition: tup is a non-empty tuple of ints\n\n    Parameter value:  the value to compare to the tuple\n    Precondition:  value is an int\n    \"\"\"\n    assert type(tup) == tuple, 'tup isnt a tuple'\n    assert len(tup) >= 1, 'tuple cant be empty'\n    assert type(value) == int, 'value isnt an int'\n\n    count = 0\n\n    for index in tup:      # assigns index as a element in tup equiv to index = tup[:]?\n        if index < value:\n            count += 1\n\n    return count\n    # pass",
        "sha1": "199895708250e993b80c63669c3b68806a5aaec1",
        "id": 680878
    },
    {
        "content": "def subtree_ids(treeview, x, level=0):\n    \"\"\"\n    Return a list of tuples containing the ids and levels for *x* and every element below it in the Treeview *treeview*.\n\n    The level of *x* is 0, children of *x* are 1, and so forth.\n    \"\"\"\n    id_list = list()\n    id_list.append((x, level))\n    for y in treeview.get_children(x):\n        id_list.extend(subtree_ids(treeview, y, level + 1))\n    return id_list",
        "sha1": "b637a0bba5ca3aff9dfa1665984b7cd4cd56790d",
        "id": 655706
    },
    {
        "content": "def strip_dash(text):\n    \"\"\" Strip leading dashes from 'text' \"\"\"\n    if not text:\n        return text\n\n    return text.strip(\"-\")",
        "sha1": "cecc28be2173492732e96d70270de6c1a83fbc3d",
        "id": 485056
    },
    {
        "content": "from typing import ChainMap\n\n\ndef unpack_dictionaries(dictionaries):\n    \"\"\"Flattens/unpacks a list of dictionaries into\n    a single dictionary.\n\n    Parameters\n    ----------\n    dictionaries : list\n        List containing multiple dictionaries\n\n    Returns\n    -------\n    unpacked : dict\n        Dictionary containing all keys/values of\n        all dictionaries in the input list.\n\n    Example\n    -------\n        >>> # Define dictionaries\n        >>> week_1 = {'radiance-week1': 200}\n        >>> week_2 = {'radiance-week2': 300}\n        >>> # Create list of dictionaries\n        >>> week_list = [week_1, week_2]\n        >>> week_list\n        [{'radiance-week1': 200}, {'radiance-week2': 300}]\n        >>> # Unpack dictionaries\n        >>> unpacked = unpack_dictionaries(week_list)\n        {'radiance-week1': 200, 'radiance-week2': 300}\n    \"\"\"\n    # Reverse input list\n    dictionaries_reversed = list(reversed(dictionaries))\n\n    # Flatten/unpack all semester dictionaries into single dictionary\n    unpacked = dict(ChainMap(*dictionaries_reversed))\n\n    # Return unpacked dictionary\n    return unpacked",
        "sha1": "bf1bf14d298dea6b453fbb47efd14542fff9d5fc",
        "id": 250695
    },
    {
        "content": "def mejorar_receta(pizza, topping):\n    \"\"\"\n    (list of str, str) -> list of str\n\n    Agrega un nuevo ingrediente a la pizza\n\n    >>> mejorar_receta(['queso', \"jamon\"], \"champi\u00f1on\")\n    ['champi\u00f1on', 'jamon', 'queso']\n    >>> mejorar_receta(['queso', \"jamon\"], \"jamon\")\n    ['jamon', 'queso']\n\n    :param pizza: list of str La pizza actual\n    :param topping: str El topping a agregar\n    :return: La pizza con el nuevo topping ordenada\n    \"\"\"\n    nueva_pizza = pizza.copy()\n    if not (topping in pizza):\n        nueva_pizza.append(topping)\n    return sorted(nueva_pizza)",
        "sha1": "0f4197482c3c553e942bc7e23c1a14dbf222a2f2",
        "id": 531233
    },
    {
        "content": "def idToMQTT(id:str) -> str:\n\t\"\"\"\tConvert a oneM2M ID to an MQTT compatible path element.\n\t\"\"\"\n\treturn f'{id.lstrip(\"/\").replace(\"/\", \":\")}'",
        "sha1": "073caae3c4d0a5dd19c20e497191015337282319",
        "id": 522853
    },
    {
        "content": "def digitize(n):\n    \"\"\"Convert integer to reversed list of digits.\"\"\"\n    stringy = str(n)\n    return [int(s) for s in stringy[::-1]]",
        "sha1": "8e3e763cffa1718b9d494b868d1dd551a8f5f59e",
        "id": 392126
    },
    {
        "content": "from typing import List\n\n\ndef get_cache_types() -> List[str]:\n    \"\"\"\n    Return the types (aka levels) of the cache.\n    \"\"\"\n    return [\"mem\", \"disk\"]",
        "sha1": "8548be4a15166097079914b76f0db815b60233ae",
        "id": 624279
    },
    {
        "content": "def strip_end(text: str, suffix: str, case_insensitive: bool = False) -> str:\n    \"\"\"Strips the suffix from a string if present.\n    \n    https://stackoverflow.com/a/1038999\n\n    Arguments:\n        text {str} -- String to check for suffix\n        suffix {str} -- Suffix to look for.\n    \n    Keyword Arguments:\n        case_insensitive {bool} -- Do a case insensitive match. (default: {False})\n    \n    Returns:\n        str -- Suffix to look for.\n    \"\"\"\n    if case_insensitive:\n        if not text.lower().endswith(suffix.lower()):\n            return text\n    else:\n        if not text.endswith(suffix):\n            return text\n    return text[: len(text) - len(suffix)]",
        "sha1": "9503130cd310ad98351039372eafca4d5de7388e",
        "id": 163372
    },
    {
        "content": "from typing import Tuple\n\n\ndef ntp2parts(ntp: int) -> Tuple[int, int]:\n    \"\"\"Split NTP time into seconds and fraction.\"\"\"\n    return ntp >> 32, ntp & 0xFFFFFFFF",
        "sha1": "729b3f9ce912e1be54c0c5bafd9c5577a78091b9",
        "id": 691877
    },
    {
        "content": "def strip_end(text: str, suffix: str, case_insensitive: bool = False) -> str:\n    \"\"\"Strips the suffix from a string if present.\n\n    https://stackoverflow.com/a/1038999\n\n    :param text: String to check for suffix\n    :param suffix: Suffix to look for.\n    :param case_insensitive: Do a case insensitive match. Defaults to False.\n    :returns: The resulting string.\n    \"\"\"\n    if case_insensitive:\n        if not text.lower().endswith(suffix.lower()):\n            return text\n    else:\n        if not text.endswith(suffix):\n            return text\n    return text[: len(text) - len(suffix)]",
        "sha1": "3c36201fae79aa1d54e8ddeefa9c4bede9aff614",
        "id": 445163
    },
    {
        "content": "def canonical_name(k, config):\n    \"\"\"Return the canonical name for a key.\n\n    Handles user choice of '-' or '_' conventions by standardizing on whichever\n    version was set first. If a key already exists in either hyphen or\n    underscore form, the existing version is the canonical name. If neither\n    version exists the original key is used as is.\n    \"\"\"\n    try:\n        if k in config:\n            return k\n    except TypeError:\n        # config is not a mapping, return the same name as provided\n        return k\n\n    altk = k.replace('_', '-') if '_' in k else k.replace('-', '_')\n\n    if altk in config:\n        return altk\n\n    return k",
        "sha1": "fc96ac043a384200f57881bafa3cb84c22f4e0f4",
        "id": 188139
    },
    {
        "content": "import re\n\n\ndef clear_sent(sent):\n    \"\"\"Remove all non-letters symbols, except `-`\n\n    Args:\n        sent (str): Sentence\n\n    Returns:\n        str: Cleared sentence\n    \"\"\"\n    sent = re.sub(r'[^-\\w]+', ' ', sent)\n    sent = re.sub(r'(\\s-\\s*|-\\s+)', ' ', sent)\n    return sent.strip()",
        "sha1": "c3ab8addc0dc48daefe4e85e381b5cfd66955d1d",
        "id": 582440
    },
    {
        "content": "import re\n\n\ndef email_is_valid(email: str) -> bool:\n    \"\"\"\n    Check if email address adheres to expected structure of an email address (prefix@domain).\n    :param email: Email address as a string.\n    :return: True if email is a valid email address, False otherwise.\n    \"\"\"\n    if not re.fullmatch(\"[^@]+@[^@]+\\\\.[^@]+\", email):\n        return False\n    return True",
        "sha1": "7d966883629e3620866f34a9fde3c8e988322b9f",
        "id": 184250
    },
    {
        "content": "def indent(amount: int, s: str) -> str:\n  \"\"\"Indents `s` with `amount` spaces.\"\"\"\n  prefix = amount * \" \"\n  return \"\\n\".join(prefix + line for line in s.splitlines())",
        "sha1": "201dd126cfb46748fd5b0b0cefb3fc01f5fdb5f4",
        "id": 423455
    },
    {
        "content": "def value(tab, i):\n    \"\"\"Return 3 digit value starting at position i[index starts from 1].\"\"\"\n    return tab[i - 1] * 100 + tab[i] * 10 + tab[i + 1]",
        "sha1": "1d9820922183bf2814b3807a6131a7f34dc2b599",
        "id": 323323
    },
    {
        "content": "import typing\n\n\ndef get_percentage(now: int, needed: int, /, *, save_float: bool = False) -> typing.Union[int, float]:\n    \"\"\"``function``\n\n    Method for getting the current percentage of progress.\n\n    Parameters:\n    -----------\n    now: :class:`int` [Positional only]\n        Current progress parameter.\n\n    needed: :class:`int` [Positional only]\n        Needed progress parameter.\n\n    save_float: :class:`bool` = False [Keyword only]\n        If True, fill return :class:`float` percentage.\n\n    Returns:\n    --------\n    percentage: :class:`typing.Union[int, float]`\n        Percentage of progress.\n    \"\"\"\n    if not save_float:\n        return int((now / needed) * 100)\n    else:\n        return (now / needed) * 100",
        "sha1": "190d72bfd4522b00d98169943b77c91d6fda04f4",
        "id": 114792
    },
    {
        "content": "def flatten_list_of_lists(some_list, remove_duplicates=False, sort=False):\n    \"\"\"\n    Convert a list of lists into a list of all values\n    :param some_list: a list such that each value is a list\n    :type some_list: list\n    :param remove_duplicates: if True, return a unique list, otherwise keep duplicated values\n    :type remove_duplicates: bool\n    :param sort: if True, sort the list\n    :type sort: bool\n    :return: a new object containing all values in teh provided\n    \"\"\"\n    data = [item for sublist in some_list for item in sublist]\n\n    if remove_duplicates:\n        if sort:\n            return list(set(data))\n        else:\n            ans = []\n            for value in data:\n                if value not in ans:\n                    ans.append(value)\n            return ans\n    elif sort:\n        return sorted(data)\n\n    return data",
        "sha1": "20fe701870fa087706afa393c0bb25b73f3f3bae",
        "id": 517624
    },
    {
        "content": "def _linestring_to_segments(arr):\n    \"\"\"\n    Translates line strip to segments\n    :param arr: numpy.array\n        Array of line strip vertices\n    :return: numpy.array\n        Line segments\n    \"\"\"\n    return [arr[i / 2] for i in range(0, len(arr) * 2)][1:-1]",
        "sha1": "e0c642bd245a9f42cc55a8f5e53686a939ba6e3c",
        "id": 647939
    },
    {
        "content": "def extract_abstracts(papers):\n    \"\"\"Extract an abstract list from the paper list.\"\"\"\n    abstracts = []\n    for paper in papers:\n        abstracts.append(paper['abstract'])\n    return abstracts",
        "sha1": "44b7c1f6a22844b755ad4e9c8c863e8d1a35bb14",
        "id": 277405
    },
    {
        "content": "import hashlib\n\n\ndef get_typeid(typeinfo):\n    \"\"\"Compute the CallSiteTypeId from a typeinfo string\"\"\"\n    return int.from_bytes(hashlib.md5(typeinfo.encode(\"ascii\")).digest()[:8], \"little\")",
        "sha1": "bb4a0e24aa549770a1cb6fb0a93612724296acf4",
        "id": 389626
    },
    {
        "content": "def V_tank_Reflux(Reflux_mass, tau, rho_Reflux_20, dzeta_reserve):\n    \"\"\"\n    Calculates the tank for waste.\n    Parameters\n    ----------\n    Reflux_mass : float\n        The mass flowrate of Reflux, [kg/s]\n    tau : float\n        The time, [s]\n    rho_Reflux_20 : float\n        The destiny of waste for 20 degrees celcium, [kg/m**3]\n    dzeta_reserve : float\n        The coefificent of reserve, [dismensionless]\n    Returns\n    -------\n    V_tank_Reflux : float\n        The tank for Reflux, [m**3]\n    References\n    ----------\n    &&&&&&&&&&&&\n    \"\"\"     \n    return Reflux_mass * tau * dzeta_reserve / rho_Reflux_20",
        "sha1": "3e1adc446bbe2dd936663af895c59222cd000a48",
        "id": 7419
    },
    {
        "content": "def ReadFile(file_path):\n  \"\"\"Reads the contents of a given file path.\n\n  Args:\n    file_path: The path to the file to read.\n\n  Returns:\n    The contents of the file.\n  \"\"\"\n  with open(file_path, 'rb') as stream:\n    return stream.read()",
        "sha1": "16527ed3c98ba3498c9c897c3aab511a4296b406",
        "id": 420818
    },
    {
        "content": "def get_ele_list_from_struct(struct):\n    \"\"\"\n    Get elements list from pymatgen structure objective\n\n    Parameters\n    ----------\n        struct : pymatgen objective\n            The structure\n    Returns\n    -------\n        ele_list : [str]\n            The list of elements\n    \"\"\"\n    ele_list = []\n    for ele in struct.species:\n        ele_list.append(str(ele))\n    return ele_list",
        "sha1": "4a6fa05518ff0725d85b270b8d8b0003eb139e5e",
        "id": 24328
    },
    {
        "content": "import random\n\n\ndef partitionByState(ser, holdouts=1):\n  \"\"\"\n  Creates training and test indexes by randomly selecting\n  a indices for each state.\n  :param pd.DataFrame ser: Classes for instances\n  :param int holdouts: number of holdouts for test\n  :return list-object, list-object: test, train\n  \"\"\"\n  classes = ser.unique().tolist()\n  classes.sort()\n  test_idxs = []\n  for cls in classes:\n    ser_cls = ser[ser == cls]\n    if len(ser_cls) <= holdouts:\n      raise ValueError(\n          \"Class %s has fewer than %d holdouts\" %\n          (cls, holdouts))\n    idxs = random.sample(ser_cls.index.tolist(),\n        holdouts)\n    test_idxs.extend(idxs)\n  #\n  train_idxs = list(set(ser.index).difference(test_idxs))\n  return train_idxs, test_idxs",
        "sha1": "c98d7ee7d7ddeafa97285db9df339db1a01a188f",
        "id": 33872
    },
    {
        "content": "def _choice_evaluator(choice_array, choice_condition):\n    \"\"\"\n    Determines which rows in `choice_array` meet the given `choice_condition`,\n    where `choice_condition` is in the set `{0.0, 1.0}`.\n\n    Parameters\n    ----------\n    choice_array : 1D ndarray of ints that are either 0 or 1.\n    choice_condition : int in `{0, 1}`.\n\n    Returns\n    -------\n    bool_mask : 1D ndarray of bools.\n        Equal to `choice_array == choice_condition`\n    \"\"\"\n    if choice_condition in [0.0, 1.0]:\n        return choice_array == choice_condition\n    else:\n        msg = \"choice_condition MUST be either a 0 or a 1\"\n        raise ValueError(msg)",
        "sha1": "82e1ae37884b486342c375e630c567387766d5fd",
        "id": 573150
    },
    {
        "content": "def get_byte_order(ei_data):\n    \"\"\"Get the endian-ness of the header.\"\"\"\n    if ei_data == b'\\x01':\n        return 'little'\n    elif ei_data == b'\\x02':\n        return 'big'\n    else:\n        raise ValueError",
        "sha1": "c92dff638c9863fa5e9840c2d43eb196771ad51c",
        "id": 513996
    },
    {
        "content": "import cmath\nimport math\n\n\ndef twiddles(n):\n    \"\"\"Returns an array of n twiddle factors.\"\"\"\n    return [cmath.rect(1, -2 * math.pi * k / n) for k in range(0, n)]",
        "sha1": "3136d8341f629b2eb7d932c1c5ea167d884d1c62",
        "id": 316111
    },
    {
        "content": "def create_nested_list_data() -> list[list]:\n    \"\"\"Return a small dataset in a nested list format.\n\n    Each nested list is composed of items with type: str, str, float, in that order.\n        - The first str is the title (i.e., name) of a book\n        - The second str is the name of the author\n        - the float is the average rating for the book\n\n    INSTRUCTIONS: Do NOT change this function.\n    \"\"\"\n    return [\n        ['Shogun', 'James Clavell', 4.39],\n        ['1984', 'George Orwell', 4.4],\n        ['Animal Farm', 'George Orwell', 3.96],\n        ['The Ex Hex', 'Erin Sterling', 3.76]\n    ]",
        "sha1": "b12f461cdacae7e464dd3a71d131a7624a4f9f8c",
        "id": 495768
    },
    {
        "content": "def loss_fn(model, batch):\n    \"\"\" loss_fn as required by do_train \"\"\"\n    return model(batch['image'], batch['annotations'])",
        "sha1": "ac5f99a6cea610594dffd5fdf56c4c685acf5241",
        "id": 32955
    },
    {
        "content": "def _get_perf_hint(hint, index: int, _default=None):\n    \"\"\"\n    Extracts a \"performance hint\" value -- specified as either a scalar or 2-tuple -- for\n    either the left or right Dataset in a merge.\n\n    Parameters\n    ----------\n    hint : scalar or 2-tuple of scalars, optional\n    index : int\n        Indicates whether the hint value is being extracted for the left or right Dataset.\n        0 = left, 1 = right.\n    _default : optional\n        Optional default value, returned if `hint` is None.\n\n    Returns\n    -------\n    Any\n        The extracted performance hint value.\n    \"\"\"\n    if hint is None:\n        return _default\n    elif isinstance(hint, tuple):\n        return hint[index]\n    else:\n        return hint",
        "sha1": "d67a70d526934dedaa9f571970e27695404350f2",
        "id": 705849
    },
    {
        "content": "def is_custom_session(session):\n    \"\"\"Return if a ClientSession was created by pyatv.\"\"\"\n    return hasattr(session, '_pyatv')",
        "sha1": "741057221f80f0285b8b744e635d419dd9a0a38b",
        "id": 666663
    },
    {
        "content": "def set_fpn_weights(training_model, inference_model, verbose=False):\n    \"\"\"\n    Set feature pyramid network (FPN) weights from training to inference graph\n    Args:\n        training_model:   MaskRCNN training graph, tf.keras.Model\n        inference_model:  MaskRCNN inference graph, tf.keras.Model\n        verbose:          Print layers that get weights, bool\n\n    Returns: inference_model\n\n    \"\"\"\n    fpn_layers = ['fpn_c5p5', 'fpn_c4p4', 'fpn_c3p3', 'fpn_c2p2',\n                  'fpn_p5', 'fpn_p4', 'fpn_p3', 'fpn_p2',\n                  ]\n    for layer_name in fpn_layers:\n        # Get weights from training graph\n        layer_weights = training_model.get_layer(layer_name).get_weights()\n        # Set weights in inference graph\n        inference_model.get_layer(layer_name).set_weights(layer_weights)\n\n        if verbose:\n            print(f'Set weights: {layer_name}')\n\n    return inference_model",
        "sha1": "29aadfcd0dcb50edb41938433ff644b1b62f209e",
        "id": 8150
    },
    {
        "content": "def loglik_nats(model, x):\n    \"\"\"Compute the log-likelihood in nats.\"\"\"\n    return - model.log_prob(x).mean()",
        "sha1": "f929be38cb70fe56b6bb1a0e5cc21cf02fead3b6",
        "id": 696072
    },
    {
        "content": "def fuzzy_list_match(line, ldata):\n    \"\"\"\n    Searches for a line in a list of lines and returns the match if found.\n\n    Examples\n    --------\n\n    >>> tmp = fuzzy_list_match(\"data tmp\", [\"other\", \"data\", \"else\"])\n    >>> print(tmp)\n    (True, \"data\")\n\n    >>> tmp = fuzzy_list_match(\"thing\", [\"other\", \"else\"])\n    >>> print(tmp)\n    (False, None)\n\n    \"\"\"\n\n    for match in ldata:\n        if match in line:\n            return True, match\n\n    return False, None",
        "sha1": "8cbc92634859b991ac77b6bfab7c8825b9b108bb",
        "id": 32309
    },
    {
        "content": "def _filter_df_day_shot(\n        df,\n        day_shot_list,\n        return_mask=False\n):\n    \"\"\"\n    Filters a dataframe by date and shot number for an arbitrary number of\n    date/shot combinations. Returns the indices (for masking) and the filtered\n    dataframe.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        dataframe to filter. Must have columns for \"date\" and \"shot\".\n    day_shot_list : List[Tuple[Str, Int, Int]]\n        List of tuples containing date, start shot, and end shot. Date should\n        be a string in ISO-8601 format, and start/end shots numbers should be\n        integers:\n        [(\"YYYY-MM-DD\", start_shot, end_shot)]\n    return_mask : bool\n        if true, mask will be returned as the second item, which can be used to\n        update data (e.g. inserting a spatial calibration)\n\n    Returns\n    -------\n    Union[Tuple[pd.DataFrame, np.array], Tuple[pd.DataFrame]]\n        (filtered dataframe,) or (filtered dataframe, mask)\n    \"\"\"\n    mask_list = [((df[\"date\"] == date) &\n                  (df[\"shot\"] <= end_shot) &\n                  (df[\"shot\"] >= start_shot))\n                 for (date, start_shot, end_shot) in day_shot_list]\n    mask = [False for _ in range(len(df))]\n    for m in mask_list:\n        mask = m | mask\n    if return_mask:\n        return df[mask], mask\n    else:\n        return df[mask],",
        "sha1": "13e5bc29d18ae183ffafc0879ebced00d69e9d4c",
        "id": 400451
    },
    {
        "content": "import requests\n\n\ndef status_handler(status_code):\n    \"\"\"\n    determine if the response is good or bad\n    :param int status_code: status code of response\n    :return: whether response is good\n    :rtype: bool\n    \"\"\"\n    return (status_code == requests.codes.ok)",
        "sha1": "faf248251020230e018ffabb2ef0eb502ca9bf20",
        "id": 190201
    },
    {
        "content": "def get_class_path(cls, use_tfds_prefix=True):\n    \"\"\"Returns path of given class or object. Eg: `tfds.image.cifar.Cifar10`.\"\"\"\n    if not isinstance(cls, type):\n        cls = cls.__class__\n    module_path = cls.__module__\n    if use_tfds_prefix and module_path.startswith('tensorflow_datasets'):\n        module_path = 'tfds' + module_path[len('tensorflow_datasets'):]\n    return '.'.join([module_path, cls.__name__])",
        "sha1": "089aaad202cfa15d8f349e92968a5e613dd1050a",
        "id": 185069
    },
    {
        "content": "def generate_ascending_descending(power):\n    \"\"\"\n    Generates lists that have ascending and descending elements\n    :param power: power of 2\n    :return: generated lists\n    \"\"\"\n    ascending = []\n    for i in range(2 ** power):\n        ascending.append(i)\n    descending = ascending[::-1]\n    return ascending, descending",
        "sha1": "b4a5d1984cccc5e2c9fe7a8dd7326056ded26a5f",
        "id": 305340
    },
    {
        "content": "def _num_items_2_ridge_ylimit(n):\n    \"\"\" uses linear regression model to infer adequate figsize\n        from the number of boxes in a boxplot\n    Data used for training:\n        X = [1, 3, 4, 6, 8, 11, 14, 16, 19, 22, 24]\n        y = [.15, 0.5, 0.6, 0.9, 1.18, 1.7, 2.1, 2.4, 2.85, 3.3, 3.7]\n    Parameters\n    ----------\n    n : int\n        number of items\n    Returns\n    -------\n    (w,h) : tuple\n        the width and the height of the figure\n    \"\"\"\n    return 0.15134*n + 0.00076",
        "sha1": "4da4ddd34d92f4143daa23498198be051aa36c87",
        "id": 341162
    },
    {
        "content": "import itertools\n\n\ndef partition_range(stop, annotations=None):\n    \"\"\"\n    Partition the range from 0 to `stop` based on annotations.\n\n        >>> partition_range(50, annotations=[[(0, 21), (30, 35)],\n        ...                                  [(15, 32), (40, 46)]])\n        [(0, 15, {0}),\n         (15, 21, {0, 1}),\n         (21, 30, {1}),\n         (30, 32, {0, 1}),\n         (32, 35, {0}),\n         (35, 40, set()),\n         (40, 46, {1}),\n         (46, 50, set())]\n\n    :arg stop: End point (not included) of the range (similar to the `stop`\n        argument of the built-in :func:`range` function).\n    :type stop: int\n    :arg annotations: For each annotation level, a list of (`start`, `stop`)\n        pairs defining an annotated region.\n    :type annotations: list\n\n    :return: Partitioning of the range as (`start`, `stop`, `levels`) tuples\n        defining a region with a set of annotation levels.\n    :rtype: list\n\n    All regions (`start`, `stop`) are defined as in slicing notation, so\n    zero-based and `stop` is not included.\n\n    The `annotations` argument is a list of annotations. An annotation is a\n    list of regions as (`start`, `stop`) tuples. The level of each annotation\n    is its index in `annotations`.\n\n    Annotation regions can overlap (overlap within one level is ignored) and\n    do not need to be sorted.\n    \"\"\"\n    annotations = annotations or []\n\n    partitioning = []\n    part_start, part_levels = 0, None\n\n    # We loop over the range, only touching positions where levels potentially\n    # change.\n    for p in sorted(set(itertools.chain([0, stop],\n                                        *itertools.chain(*annotations)))):\n        if p == stop:\n            partitioning.append( (part_start, p, part_levels) )\n            break\n\n        # Annotation levels for position p.\n        levels = {level for level, regions in enumerate(annotations)\n                  if any(x <= p < y for x, y in regions)}\n\n        if p == 0:\n            part_levels = levels\n            continue\n\n        if levels != part_levels:\n            partitioning.append( (part_start, p, part_levels) )\n            part_start, part_levels = p, levels\n\n    return partitioning",
        "sha1": "98bd25ede466064cdb0ef66b4db82ca8cfb91a39",
        "id": 317573
    },
    {
        "content": "def read_file(filename: str, binary_mode: bool = False) -> bytes | str:\n    \"\"\"Read and return the contents of a file in a single file.read().\n\n    :param filename: The filename of the file to read.\n    :param binary_mode: Read from file as bytes or unicode.\n    :returns: The contents of the file.\n    \"\"\"\n    mode = \"rb\" if binary_mode else \"r\"\n    with open(filename, mode) as f:\n        content: bytes | str = f.read()\n        return content",
        "sha1": "a06a8548cec0c6a24c13d0809252930a1a6b9713",
        "id": 453989
    },
    {
        "content": "def semi_split(s):\n\n    \"\"\" Split 's' on semicolons.\n    \"\"\"\n    return map(lambda x: x.strip(), s.split(';'))",
        "sha1": "f81eff42e170c7b760a75d86d5d0d9326365ca8e",
        "id": 701275
    },
    {
        "content": "def timedelta2millisecond(td):\n    \"\"\"Get milliseconds from a timedelta.\"\"\"\n    milliseconds = td.days * 24 * 60 * 60 * 1000\n    milliseconds += td.seconds * 1000\n    milliseconds += td.microseconds / 1000\n    return milliseconds",
        "sha1": "72cbddf2209053e57efaa805175d501cfad38db4",
        "id": 562941
    },
    {
        "content": "def get_region_of_all_exchanges(scenario: dict) -> dict:\n    \"\"\"Returns {ID: region_name, ...} map for `EnergyExchange` in given `scenario`. If no region is found, the string `Region` is applied\"\"\"\n    try:\n        exchanges = [\n            {exchange[\"Id\"]: exchange[\"Attributes\"][\"Region\"]}\n            for exchange in scenario[\"Agents\"]\n            if exchange[\"Type\"] == \"EnergyExchange\"\n        ]\n    except KeyError:\n        exchanges = [\n            {exchange[\"Id\"]: \"Region\"} for exchange in scenario[\"Agents\"] if exchange[\"Type\"] == \"EnergyExchange\"\n        ]\n    output = {}\n    for exchange in exchanges:\n        output.update(exchange)\n    return output",
        "sha1": "475e7ef769c28fa9ad8653460ece3f0150dda244",
        "id": 74526
    },
    {
        "content": "def humanized_time(second):\n    \"\"\"\n    :param second: time in seconds\n    :return: human readable time (hours, minutes, seconds)\n    \"\"\"\n    m, s = divmod(second, 60)\n    h, m = divmod(m, 60)\n    return \"%dh %02dm %02ds\" % (h, m, s)",
        "sha1": "af5f2017f8981f58afffab4c036d5436ea93fc91",
        "id": 579270
    },
    {
        "content": "import re\n\n\ndef get_page(str):\n    \"\"\"\n    \u83b7\u53d6\u8be5\u9879\u9875\u7801\n    :param : str\n    :return: int page\n    \"\"\"\n\n    # page = str.split()[-1]\n    page = re.findall(r\"[0-9]{1,3}$\", str)[0]  # list \u6b63\u5219$\u8868\u672b\u5c3e\u67e5\u627e\n    temp = page\n    if temp.strip().lstrip('-').isdigit():\n        return int(page)\n    else:\n        return \"\u8be5\u884c\u76ee\u5f55\u9519\u8bef: \" + str",
        "sha1": "ba310e793b0dd4c1738178cdb8478a1ed6575086",
        "id": 183119
    },
    {
        "content": "def contains(text, pattern):\n    \"\"\"Return a boolean indicating whether pattern occurs in text.\n    Runtime, worst case: O(n^2), n is the len(text)\n    Runtime, best case: O(1), if text == ''\n    Space Complexity: O(n), n is the len(text)\n    \"\"\"\n    assert isinstance(text, str), 'text is not a string: {}'.format(text)\n    assert isinstance(pattern, str), 'pattern is not a string: {}'.format(text)\n    \n    if text == '':\n        return False\n    elif pattern == '':\n        return True\n    \n    if text[0] == pattern[0]:               # if the first characters match\n        if len(pattern) == 1:               # and the pattern is only 1 character\n            return True \n        for i in range(1, len(pattern)):    # loop 1 through length of pattern\n            if len(text) < len(pattern):        # check there are enough characters left\n                return False\n\n            if text[i] == pattern[i]:       # if the pattern matches\n                if i == len(pattern) - 1:     # check if the pattern is over\n                    return True\n            else:                           # if the pattern stops matching\n                text = text[i:]                 # set text to after current index\n                return contains(text, pattern)  # start again\n    else:                                   # if no match\n        text = text[1:]                     # set text to after current index\n        return contains(text, pattern)      # start again",
        "sha1": "ab930961de326fec8b7c4259d55388b0a6ecdc73",
        "id": 557997
    },
    {
        "content": "def is_ws4py_error_logging(event):\n    \"\"\"Filter out all error messages logged by ws4py.\"\"\"\n    return event.logger == \"ws4py\"",
        "sha1": "1382fd9e6dfa3849a55a4957eafc754e801ca639",
        "id": 485645
    },
    {
        "content": "import math\n\n\ndef _gradf_and_inv_hessf(data, z):\n    \"\"\"\n    Gradient and inverse of the Hessian of f\n\n    Parameters\n    ----------\n    data : (n, d)\n    z: (d, )\n\n    Returns\n    -------\n    gx : float\n        First coefficient of the gradient\n    gy : float\n        Second coefficient of the gradient\n    a11 : float\n        ``invhess[0, 0]``\n    a12 : float\n        ``invhess[0, 1]``\n    a22 : float\n        ``invhess[1, 1]``\n    \"\"\"\n    x = z[0]\n    y = z[1]\n    gx = 0.0\n    gy = 0.0\n    a11 = 0.0\n    a12 = 0.0\n    a22 = 0.0\n    for i in range(len(data)):\n        tx = x - data[i][0]\n        ty = y - data[i][1]\n        inv_l2_t = 1 / math.sqrt(tx ** 2 + ty ** 2)\n        gx += inv_l2_t * tx\n        gy += inv_l2_t * ty\n        inv_l2_t3 = inv_l2_t ** 3\n        a11 += inv_l2_t - inv_l2_t3 * tx ** 2\n        a12 -= (tx * ty) * inv_l2_t3\n        a22 += inv_l2_t - inv_l2_t3 * ty ** 2\n\n    # Calculate invert:\n    invdet = 1 / (a11 * a22 - a12 * a12)\n\n    return gx, gy, a22 * invdet, -a12 * invdet, a11 * invdet",
        "sha1": "4bd5400fef7f0d0acc7edd2e79c7ea7374e46bec",
        "id": 441749
    },
    {
        "content": "def mul_matrix_pt(m, point): # pylint:disable=invalid-name\n    \"\"\"Multiplies the matrix with the point. Returns a new point.\"\"\"\n    x, y = point\n    x2 = x * m.a + y * m.c + 1 * m.e\n    y2 = x * m.b + y * m.d + 1 * m.f\n    return x2, y2",
        "sha1": "c303d2f65778b0842505890099a5ae8ee550d84c",
        "id": 247949
    },
    {
        "content": "def _grad_j(q_j, A_j, b_j, b_j_norm, a_1_j, a_2_j, m):\n    \"\"\"Compute the gradient with respect to one of the coefficients.\"\"\"\n    return (A_j.t() @ q_j / (-m)) + (b_j * (a_1_j / b_j_norm + a_2_j))",
        "sha1": "d051883d848386af9eeb40a10029351158b075c0",
        "id": 651750
    },
    {
        "content": "def _is_layer(nn_layer, layer_type):\n    \"\"\"\n    :param nn_layer : NN layer proto message\n    :param layer_type : str Layer type to check against\n    :returns True if nn_layer is of type `layer_type` otherwise False\n    \"\"\"\n    return nn_layer.WhichOneof(\"layer\") == layer_type",
        "sha1": "149dec6bb73d80c2de84b1051b0400a04014c5cc",
        "id": 557230
    },
    {
        "content": "def qualys_login(session_object, config):\n    \"\"\"\n    Logs in the Qualys API v2\n    :param session_object: requests session object of the current session.\n    :param config: confiture object that contains the quart configuration.\n    :return: requests session oject, answer of the server.\n    \"\"\"\n    payload = {\n        'action': 'login',\n        'username': config.subsection('qualys').get('user'),\n        'password': config.subsection('qualys').get('password')\n    }\n    return session_object.post(\n        'https://qualysapi.qualys.eu/api/2.0/fo/session/',\n        data=payload)",
        "sha1": "6f9b07562f1876856c6132c184b920218f8a725e",
        "id": 298875
    },
    {
        "content": "def dict_to_filter_params(d, prefix=\"\"):\n    \"\"\"\n    Flattens a dictionary of attributes to a set of parameters suitable for filtering\n    a `QuerySet` that uses `__` as separator.\n    \"\"\"\n    params = {}\n\n    for key, val in d.items():\n        k = prefix + key\n        if isinstance(val, dict):\n            params.update(dict_to_filter_params(val, k + \"__\"))\n        else:\n            params[k] = val\n\n    return params",
        "sha1": "f43b598019ce1004746a03012371a62c84d33169",
        "id": 506911
    },
    {
        "content": "def _sparse_gradient(vol, positions):\n    \"\"\"Gradient of a 3D volume at the provided `positions`.\n\n    For SIFT we only need the gradient at specific positions and do not need\n    the gradient at the edge positions, so can just use this simple\n    implementation instead of numpy.gradient.\n    \"\"\"\n    p0 = positions[..., 0]\n    p1 = positions[..., 1]\n    p2 = positions[..., 2]\n    g0 = vol[p0 + 1, p1, p2] - vol[p0 - 1, p1, p2]\n    g0 *= 0.5\n    g1 = vol[p0, p1 + 1, p2] - vol[p0, p1 - 1, p2]\n    g1 *= 0.5\n    g2 = vol[p0, p1, p2 + 1] - vol[p0, p1, p2 - 1]\n    g2 *= 0.5\n    return g0, g1, g2",
        "sha1": "6676014543066dbc75e1a9e069b959cc30e5bd50",
        "id": 451072
    },
    {
        "content": "def get_project_folder_name(project_file_name: str) -> str:\n    \"\"\"\n    Get the name of a folder associated with a project.\n    :param project_file_name: Name of the project.\n    :return: Name of the project folder according to Simio's nomenclature.\n    \"\"\"\n    project_name = project_file_name.split('.')[0]\n    folder_name = '.'.join([project_name, 'Files'])\n    return folder_name",
        "sha1": "4540fcbb40395918b75a29e81b207d1c773d28f0",
        "id": 73407
    },
    {
        "content": "def get_2pttype_for_dictkey(dictkey):\n    \"\"\"\n    Convert key in blinding factor dictionary to sets of strings used\n    in the fits file to designate which kind of 2pt function is being analyzed.\n    \"\"\"\n    if dictkey == 'gal_gal_cl':\n        return 'GPF','GPF'\n    elif dictkey == 'gal_shear_cl':\n        return 'GPF','GEF'\n    elif dictkey == 'shear_shear_cl':\n        return 'GEF','GEF'\n    elif dictkey == 'gal_gal_xi':\n        return 'GPR','GPR'\n    elif dictkey == 'gal_shear_xi':\n        return 'GPR','G+R'\n    elif dictkey == 'shear_shear_xip':\n        return 'G+R','G+R'\n    elif dictkey == 'shear_shear_xim':\n        return 'G-R','G-R'",
        "sha1": "5d61c11fa576e058173c03a602c0c5527770b399",
        "id": 591443
    },
    {
        "content": "def vdecomp(v, m=None, minlen=None, maxlen=None):\n    \"\"\"\n    Decompose a vector into components. an nD stack of m-element vectors will return a tuple with up to m elements,\n    each of which will be an nD stack of scalars\n\n    :param v: nD stack of m-element vectors, a numpy (n+1)D array with shape\n        (n_stack0,n_stack1,...,n_stackn-2,m,n_stackn-1)\n    :param minlen: If passed, this will pad out the returned vector components with zero scalars\n        such that the returned tuple has minlen components. We do zero scalars rather than zero arrays\n        of the same size as the other components to save memory, since a scalar is compatible by\n        broadcasting with an array of any size.\n    :param maxlen: If passed, this will restrict the returned vector components to the given\n        size, even if the input vector has more components.\n    :param m: If passed, treat the input as if it were an nD stack of m-element vectors. If the actual\n              stack has more components, don't return them. If it has less, return scalar zeros for the\n              missing components\n    :return: A tuple. Each element is a vector component. Vector components pulled from the vector will be\n        an nD stack of scalars, a numpy nD array with shape (n_stack0,n_stack1,...,n_stackn-2,n_stackn-1).\n        Vector components which are made up will be scalar zeros.\n\n    Note: If you pass maxlen<minlen, the result is still well-defined, since the maxlen is used first,\n          then the minlen. If you pass a vector with m=4, a minlen of 7, and a maxlen of 2, you will get\n          a result with the first two components of the vector, followed by 5 zeros. I'm not sure if this\n          is useful, but there it is.\n    Example:\n        v=np.zeros((24,3,50)) #Suitable for holding multiple trajectories\n\n        #OR\n\n        v0=np.zeros((3,50)) #Initial conditions for 50 trajectories\n\n        t=np.arange(24)     #Time steps\n\n        v=rk4(x0=v0,t=t)    #Numerically integrate multiple trajectories. Result shape will be (t.size,)+v0.shape,\n\n                            #IE (24,3,50)\n\n        x,y,z=vcomp(v) #after this, x, y, and z are each numpy arrays of shape (24,50)\n\n    \"\"\"\n    if maxlen is None and m is not None:\n        maxlen = m\n    if minlen is None and m is not None:\n        minlen = m\n    ndStack = len(v.shape) > 2\n    efflen = v.shape[-2 if ndStack else 0]\n    if maxlen is not None and maxlen < efflen:\n        efflen = maxlen\n    result = tuple([v[..., i, :] if ndStack else v[i, ...] for i in range(efflen)])\n    if minlen is not None and minlen > efflen:\n        result = result + (0,) * (minlen - efflen)\n    return result",
        "sha1": "cbc846be6a07082711e5c9faa191181c8cdc75f8",
        "id": 698736
    },
    {
        "content": "import json\n\n\ndef get_placeholder_value_given_type(value):\n    \"\"\"Given an a parameter type, return a placeholder value\"\"\"\n    # string, array, object, boolean, integer, float, or datetime.\n    if value.lower() == \"string\":\n        return json.dumps(\"\")\n    elif value.lower() == \"array\":\n        return []\n    elif value.lower() == \"object\":\n        return {}\n    elif value.lower() == \"boolean\":\n        return \"false\"\n    elif value.lower() == \"integer\":\n        return 0\n    elif value.lower() == \"float\":\n        return 0\n    elif value.lower() == \"datetime\":\n        return \"2021-04-01T00:00:00.fffffffZ\"",
        "sha1": "e8e8b6f6730b6ed301e57aac0938714588ce8467",
        "id": 461916
    },
    {
        "content": "def f1score(precision_value, recall_value, eps=1e-5):\n    \"\"\"\n    Calculating F1-score from precision and recall to reduce computation\n    redundancy.\n    Args:\n        precision_value: precision (0-1)\n        recall_value: recall (0-1)\n    Returns:\n        F1 score (0-1)\n    \"\"\"\n    numerator = 2 * (precision_value * recall_value)\n    denominator = precision_value + recall_value + eps\n    return numerator / denominator",
        "sha1": "ffb9978a2da77dcb9328cb5f7c07db81c2612035",
        "id": 179866
    },
    {
        "content": "def get_state_feature(num_states_in_group, num_groups, state):\n    \"\"\"\n    Given state, return the feature of that state\n\n    Args:\n        num_states_in_group [int]\n        num_groups [int]\n        state [int] : 1~500\n\n    Returns:\n        one_hot_vector [numpy array]\n    \"\"\"\n\n    ### Generate state feature\n    # Create one_hot_vector with size of the num_groups, according to state\n    # For simplicity, assume num_states is always perfectly divisible by num_groups\n    # Note that states start from index 1, not 0!\n\n    # Example:\n    # If num_states = 100, num_states_in_group = 20, num_groups = 5,\n    # one_hot_vector would be of size 5.\n    # For states 1~20, one_hot_vector would be: [1, 0, 0, 0, 0]\n\n    one_hot_vector = [0] * num_groups\n    for k in range(num_groups):\n        group_end = num_states_in_group * (k + 1)\n        group_start = group_end - num_states_in_group + 1\n        if (state <= group_end) & (state >= group_start):\n            one_hot_vector[k] = 1\n        else:\n            one_hot_vector[k] = 0\n\n    return one_hot_vector",
        "sha1": "1dc74f21d83693e77f5867640af4a03a8cdfe2c5",
        "id": 391292
    },
    {
        "content": "def get_pgroupvolume(module, array):\n    \"\"\"Return Protection Group Volume or None\"\"\"\n    try:\n        pgroup = array.get_pgroup(module.params['name'])\n        for volume in pgroup['volumes']:\n            if volume == module.params['restore']:\n                return volume\n    except Exception:\n        return None",
        "sha1": "f27289ad3c9e72333eba04ee3af7e2fd4ea331b9",
        "id": 576092
    },
    {
        "content": "def match_all_attrs(token, attrs_to_match):\n    \"\"\"\n    Return true if all the values in the given dict match the morphology attributes of the token.  So attrs_to_match\n    is a way of filtering out which tokens will be matched.\n    Returns true for an empty dict.\n    :param token:\n    :param attrs_to_match:\n    :return:\n    \"\"\"\n    if attrs_to_match is None:\n        return True\n    token_attrs = token.morph.to_dict()\n    for attr in attrs_to_match:\n        if attr not in token_attrs or attrs_to_match[attr] != token_attrs[attr]:\n            return False\n    return True",
        "sha1": "0e5490a1da012a945db897b2315f79fd69dccc7b",
        "id": 663313
    },
    {
        "content": "from typing import Counter\n\n\ndef percentile(data: list, p=0.5):\n    \"\"\"\n    :param data: origin list\n    :param p: frequency percentile\n    :return: the element at frequency percentile p\n    \"\"\"\n    assert 0 < p < 1\n    boundary = len(data) * p\n    counter = sorted(Counter(data).items(), key=lambda x: x[0])\n    keys, counts = zip(*counter)\n    accumulation = 0\n    for i, c in enumerate(counts):\n        accumulation += c\n        if accumulation > boundary:\n            return keys[i]\n    return None",
        "sha1": "ac0a3a4705579c1b6a5165b91e6dcad65afcd1f4",
        "id": 13851
    },
    {
        "content": "from typing import Any\nfrom typing import Iterable\n\n\ndef to_iterable(val: Any) -> Iterable:\n    \"\"\"Get something we can iterate over from an unknown type\n\n    >>> i = to_iterable([1, 2, 3])\n    >>> next(iter(i))\n    1\n\n    >>> i = to_iterable(1)\n    >>> next(iter(i))\n    1\n\n    >>> i = to_iterable(None)\n    >>> next(iter(i)) is None\n    True\n\n    >>> i = to_iterable('foobar')\n    >>> next(iter(i))\n    'foobar'\n\n    >>> i = to_iterable((1, 2, 3))\n    >>> next(iter(i))\n    1\n    \"\"\"\n    if isinstance(val, Iterable) and not isinstance(val, (str, bytes)):\n        return val\n    return (val,)",
        "sha1": "6c24de85d822a5511adb26149ec863197164c61b",
        "id": 23531
    },
    {
        "content": "def transform_rotation(x):\n    \"\"\"Get rotation from transform\"\"\"\n    return x[..., :4]",
        "sha1": "6cbbbdb3110b3bfe1a6f21ed07919e9fa5cc5a46",
        "id": 358268
    },
    {
        "content": "def win_split(cmdline):\n    \"\"\" Minimal implementation of shlex.split for Windows following\n    https://msdn.microsoft.com/en-us/library/windows/desktop/17w5ykft.aspx.\n    \"\"\"\n    def split_iter(cmdline):\n        in_quotes = False\n        backslashes = 0\n        arg = ''\n        for c in cmdline:\n            if c == '\\\\':\n                # MSDN: Backslashes are interpreted literally, unless they\n                # immediately precede a double quotation mark.\n                # Buffer them until we know what comes next.\n                backslashes += 1\n            elif c == '\"':\n                # Quotes can either be an escaped quote or the start of a quoted\n                # string. Paraphrasing MSDN:\n                # Before quotes, place one backslash in the arg for every pair\n                # of leading backslashes. If the number of backslashes is odd,\n                # retain the double quotation mark, otherwise interpret it as a\n                # string delimiter and switch state.\n                arg += '\\\\' * (backslashes // 2)\n                if backslashes % 2 == 1:\n                    arg += c\n                else:\n                    in_quotes = not in_quotes\n                backslashes = 0\n            elif c in (' ', '\\t') and not in_quotes:\n                # MSDN: Arguments are delimited by white space, which is either\n                # a space or a tab [but only outside of a string].\n                # Flush backslashes and return arg bufferd so far, unless empty.\n                arg += '\\\\' * backslashes\n                if arg:\n                    yield arg\n                    arg = ''\n                backslashes = 0\n            else:\n                # Flush buffered backslashes and append.\n                arg += '\\\\' * backslashes\n                arg += c\n                backslashes = 0\n\n        if arg:\n            arg += '\\\\' * backslashes\n            yield arg\n\n    return list(split_iter(cmdline))",
        "sha1": "d1aa8230c36fa52537c0b6f8d8e2bcf89513a5ea",
        "id": 421556
    },
    {
        "content": "from typing import List\n\n\ndef find_unused_bank(edge_banks: List[int]) -> int:\n    \"\"\"\n    Given a list of banks used in one clock, find a bank index that isn't used, or -1 if all are used\n    :param edge_banks:\n    :return:\n    \"\"\"\n    for i in list(range(len(edge_banks))):\n        if i not in edge_banks:\n            return i\n    return -1",
        "sha1": "b59e622fde507ae13231c02f29334760796091d8",
        "id": 603381
    },
    {
        "content": "def get_batch_asset_task_info(ctx):\n    \"\"\"Parses context data from webpublisher's batch metadata\n\n        Returns:\n            (tuple): asset, task_name (Optional), task_type\n    \"\"\"\n    task_type = \"default_task_type\"\n    task_name = None\n    asset = None\n\n    if ctx[\"type\"] == \"task\":\n        items = ctx[\"path\"].split('/')\n        asset = items[-2]\n        task_name = ctx[\"name\"]\n        task_type = ctx[\"attributes\"][\"type\"]\n    else:\n        asset = ctx[\"name\"]\n\n    return asset, task_name, task_type",
        "sha1": "11fac7519e72635ea2e6c553dd61e53fd2cf5d99",
        "id": 138458
    },
    {
        "content": "def parallel_mean(mean_a, count_a, mean_b, count_b):\n    \"\"\"Compute the mean based on stats from two partitions of the data.\n\n    See \"Parallel Algorithm\" in\n    https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance\n\n    Args:\n        mean_a: the mean of partition a\n        count_a: the number of elements in partition a\n        mean_b: the mean of partition b\n        count_b: the number of elements in partition b\n\n    Return:\n        the mean of the two partitions if they were combined\n    \"\"\"\n    mean = (count_a * mean_a + count_b * mean_b) / (count_a + count_b)\n    return mean",
        "sha1": "7f09e422431cb34b3cbdf72baf1ba4a69ecdd050",
        "id": 507327
    },
    {
        "content": "def remove_suffix(s: str, suffix: str) -> str:\n    \"\"\"\n    Removes a suffix from a string.\n\n    :param s: string to be processed\n    :param suffix: suffix to be removed\n    :return: processed string\n    \"\"\"\n\n    if suffix and s.endswith(suffix):\n        return s[0:-len(suffix)]\n    else:\n        return s",
        "sha1": "d056009a9e290d64987de8dd066e16bad5b51bee",
        "id": 221006
    },
    {
        "content": "def parse_and_append(cls, string, index, match):\n    \"\"\"Utility function which wraps a call to parse_from.\n\n    Call parse_from on the specified class, append the new instance to\n    the match list, and return the original return value of parse_from.\n    This can be used in complex parse_from implementations to avoid\n    writing repetitive list append statements for each parsed child.\n    \"\"\"\n    instance, index = cls.parse_from(string, index)\n    match.append(instance)\n    return instance, index",
        "sha1": "930ee53f024b9c98656a5f4bf968c110b7262354",
        "id": 53334
    },
    {
        "content": "from pathlib import Path\n\n\ndef get_child_path(directory: Path, child_name: str) -> Path:\n    \"\"\"Returns path of file child_name placed in directory.\"\"\"\n\n    return directory.joinpath(child_name)",
        "sha1": "212dab374664054376040bf7dcdc80e273108ee5",
        "id": 513762
    },
    {
        "content": "def _cleanBlank(value):\n    \"\"\" Converts blank string to Python None \"\"\"\n    if value != None and str(value).strip() == '':\n        return None\n    return value",
        "sha1": "7353ec40f88db8880c43249b84bdb87fd546e924",
        "id": 288487
    },
    {
        "content": "from typing import Union\nfrom typing import Any\n\n\ndef gte(query: Union[int, float], field_name: str, object: Any) -> bool:\n    \"\"\"\n    Check if value of object is greater than or equal to value of query\n    \"\"\"\n    return float(getattr(object, field_name)) >= float(query)",
        "sha1": "26f8378a008e20b0b17e2f54d99d088b46f1c722",
        "id": 447157
    },
    {
        "content": "async def get_message(ctx, message:int):\n    \"\"\"\n    Returns a previous message for a given channel and location (relative to context).\n\n    Parameters:\n        ctx (context): Passthrough for context from command.\n        message (int): Location of message relative to sent command.\n\n    Returns:\n        Message: The message object of the selected message.\n    \"\"\"\n    channel = ctx.message.channel\n    history = await channel.history().flatten()\n\n    returnMsg = history[message]\n    return returnMsg",
        "sha1": "9ceb5499fd8afb9ec25ae010c7645b4bda9f4c17",
        "id": 659970
    },
    {
        "content": "import re\n\n\ndef get_advanced_name(classic_name):\n    \"\"\"\n        Helper function to get a valid Advanced identifier\n        corresponding to a Classic identifier.\n        Note that this does not deal with reserved words.\n        Nor does it check for duplicates.\n    \"\"\"\n    adv_name = \"nspepi_adv_\" + re.sub(r'[^a-zA-Z0-9_]', '_', classic_name)\n    return adv_name",
        "sha1": "7486a62842aaa670393326faa0a609b27489bb3e",
        "id": 348181
    },
    {
        "content": "def repo_visibility(repo):\n    \"\"\"Convert repo to its visibility attribute as string\n\n    :param repo: Repository to show its visibility as string\n    :type repo: ``repocribro.models.Repository``\n    :return: Text representation of repo visibility\n    :rtype: str\n    \"\"\"\n    if repo.is_public:\n        return 'Public'\n    if repo.is_hidden:\n        return 'Hidden'\n    if repo.is_private:\n        return 'Private'",
        "sha1": "a63670502e6f36fcb4189d383b9e6876272811a6",
        "id": 586051
    },
    {
        "content": "import math\n\n\ndef sym_ceil(x, tol=1e-08):\n    \"\"\"Round down for negative, round up for positive.\"\"\"\n    if abs(x - round(x)) < tol:\n        return round(x)\n    elif x > 0:\n        return math.ceil(x)\n    else:\n        return math.floor(x)",
        "sha1": "0c57a1389e99557802e47599e336657700f8cd46",
        "id": 609785
    },
    {
        "content": "import pickle\n\n\ndef unpickle_file(picklefile, **kwargs):\n    \"\"\"\n    Load data from `picklefile` with Python's :mod:`pickle` module.\n\n    :param picklefile: either target file path as string or file handle\n    :param kwargs: further parameters passed to :func:`pickle.load`\n    :return: data stored in `picklefile`\n    \"\"\"\n\n    if isinstance(picklefile, str):\n        with open(picklefile, 'rb') as f:\n            return pickle.load(f, **kwargs)\n    else:\n        return pickle.load(picklefile, **kwargs)",
        "sha1": "793267e872784c25558959e72aeb8014294772a6",
        "id": 389748
    },
    {
        "content": "import torch\n\n\ndef vtln_warp_freq(vtln_low_cutoff, vtln_high_cutoff, low_freq, high_freq, vtln_warp_factor, freq):\n    \"\"\"\n    This computes a VTLN warping function that is not the same as HTK's one, but has similar inputs (this function has\n    the advantage of never producing empty bins).\n\n    This function computes a warp function F(freq), defined between low_freq and high_freq inclusive, with the following\n    properties:\n        F(low_freq) == low_freq\n        F(high_freq) == high_freq\n    The function is continuous and piecewise linear with two inflection points.\n    The lower inflection point (measured in terms of the unwarped frequency) is at frequency l, determined as described\n    below. The higher inflection point is at a frequency h, determined as described below.\n    If l <= f <= h, then F(f) = f/vtln_warp_factor.\n    If the higher inflection point (measured in terms of the unwarped frequency) is at h, then max(h, F(h)) ==\n    vtln_high_cutoff. Since (by the last point) F(h) == h/vtln_warp_factor, then max(h, h/vtln_warp_factor) ==\n    vtln_high_cutoff, so\n        h = vtln_high_cutoff / max(1, 1/vtln_warp_factor) = vtln_high_cutoff * min(1, vtln_warp_factor).\n    If the lower inflection point (measured in terms of the unwarped frequency) is at l, then min(l, F(l)) ==\n    vtln_low_cutoff. This implies that\n        l = vtln_low_cutoff / min(1, 1/vtln_warp_factor) = vtln_low_cutoff * max(1, vtln_warp_factor)\n\n    Args:\n        vtln_low_cutoff (float): Lower frequency cutoffs for VTLN\n        vtln_high_cutoff (float): Upper frequency cutoffs for VTLN\n        low_freq (float): Lower frequency cutoffs in mel computation\n        high_freq (float): Upper frequency cutoffs in mel computation\n        vtln_warp_factor (float): Vtln warp factor\n        freq (Tensor): given frequency in Hz\n    Returns:\n        Tensor: Freq after vtln warp\n    \"\"\"\n    assert vtln_low_cutoff > low_freq, 'be sure to set the vtln_low option higher than low_freq'\n    assert vtln_high_cutoff < high_freq, 'be sure to set the vtln_high option lower than high_freq [or negative]'\n    low = vtln_low_cutoff * max(1.0, vtln_warp_factor)\n    high = vtln_high_cutoff * min(1.0, vtln_warp_factor)\n    scale = 1.0 / vtln_warp_factor\n    f_low = scale * low  # F(l)\n    f_high = scale * high  # F(h)\n    assert low > low_freq and high < high_freq\n    # slope of left part of the 3-piece linear function\n    scale_left = (f_low - low_freq) / (low - low_freq)\n    # [slope of center part is just \"scale\"]\n    # slope of right part of the 3-piece linear function\n    scale_right = (high_freq - f_high) / (high_freq - high)\n    res = torch.empty_like(freq)\n    outside_low_high_freq = torch.lt(freq, low_freq) | torch.gt(freq, high_freq)  # freq < low_freq || freq > high_freq\n    before_l = torch.lt(freq, low)  # freq < l\n    before_h = torch.lt(freq, high)  # freq < h\n    after_h = torch.ge(freq, high)  # freq >= h\n    # order of operations matter here (since there is overlapping frequency regions)\n    res[after_h] = high_freq + scale_right * (freq[after_h] - high_freq)\n    res[before_h] = scale * freq[before_h]\n    res[before_l] = low_freq + scale_left * (freq[before_l] - low_freq)\n    res[outside_low_high_freq] = freq[outside_low_high_freq]\n    return res",
        "sha1": "13016eff1fc17644834a9241d883915aecce7f2a",
        "id": 362024
    },
    {
        "content": "import attr\n\n\ndef get_attr_type(class_: type, attr_name: str) -> type:\n    \"\"\"Get type of attribute in a attr class.\"\"\"\n    return getattr(attr.fields(class_), attr_name).type",
        "sha1": "edbd8be9adfaf99dc3c7a4a3d523a813fed6a5bc",
        "id": 428529
    },
    {
        "content": "import pathlib\nfrom typing import Dict\nfrom typing import List\nimport re\n\n\ndef parse_schema(sql_file: pathlib.Path) -> Dict[str, List[str]]:\n    \"\"\"\n    Parses the SQL schema creation script (schemas.sql)\n    \"\"\"\n    in_comment = False\n    in_table_schema = False\n    schema = {}\n    table_schema: List[str] = []\n    with open(sql_file, \"r\", encoding=\"utf-8\") as in_fd:\n        for line in in_fd:\n            line = line.strip().lower()\n            if \"/*\" in line:\n                # Start of comment\n                in_comment = True\n                continue\n            elif \"*/\" in line:\n                # End of comment\n                in_comment = False\n                continue\n            elif in_comment:\n                # Ignore comments\n                continue\n            elif line == \");\":\n                # End of schema\n                in_table_schema = False\n            elif line.startswith(\"create table \"):\n                # Starting a table\n                match = re.search(r\"create table (\\w+) \\(\", line)\n                if match is not None:\n                    in_table_schema = True\n                    table = match.group(1)\n                    table_schema = []\n                    schema[table] = table_schema\n            elif in_table_schema:\n                # Line in the current schema\n                col_name = line.split()[0]\n                table_schema.append(col_name)\n\n    return schema",
        "sha1": "bb9cb25e5e0f21c208bd739f627d862a706dd8ef",
        "id": 61690
    },
    {
        "content": "import functools\nimport inspect\n\n\ndef auto_defer(hidden=False):\n    \"\"\"A decorator for auto deferring a interaction. This decorator has to be placed before the main decorator\n    \n    Parameters\n    ----------\n    hidden: :class:`bool`, optional\n        Whether the interaction should be deferred hidden; default ``False``\n\n    Example\n    --------\n\n    .. code-block::\n\n        from discord_ui import ext\n\n        @ui.slash.command()\n        @ext.auto_defer()\n        async def my_command(ctx):\n            \\\"\\\"\\\"This command will be deferred automatically\\\"\\\"\\\"\n            ...\n    \n    \"\"\"\n    # https://stackoverflow.com/questions/69076152/how-to-inject-a-line-of-code-into-an-existing-function#answers-header\n    def decorator(func):\n        func.__auto_defer__ = True\n        @functools.wraps(func)\n        async def wraper(*args, **kwargs):\n            # if there is self param use the next one\n            ctx = args[1 if list(inspect.signature(func).parameters.keys())[0] == \"self\" else 0]\n            # use defer for \"auto_defering\"\n            await ctx.defer(hidden=hidden)\n            return await func(*args, **kwargs)\n        return wraper\n    return decorator",
        "sha1": "3bc8ef6f768962ac5cefa06e131b69156f46fc32",
        "id": 344078
    },
    {
        "content": "def normalize_vect_len(e1, e2):\n    \"\"\"\n        Return the shortest and the longest edges.\n\n        :param e1: Matrix of coordinates of points composing the first edge\n        :param e2: Matrix of coordinates of points composing the second edge\n        :return: Matrix of coordinates, Matrix of coordinates\n    \"\"\"\n\n    longest = e1 if len(e1) > len(e2) else e2\n    shortest = e2 if len(e1) > len(e2) else e1\n    return shortest, longest",
        "sha1": "56babf084215a1f1a0b807ff619ffdff301ee2d0",
        "id": 248817
    },
    {
        "content": "def decompile_scriptPubKey(asm):\n    \"\"\"\n    >>> decompile_scriptPubKey('OP_DUP OP_HASH160 cef3550ff9e637ddd120717d43fc21f8a563caf8 OP_EQUALVERIFY OP_CHECKSIG')\n    '76a914cef3550ff9e637ddd120717d43fc21f8a563caf888ac'\n    \"\"\"\n    asm = asm.split(\" \")\n    hex = \"\"\n    if asm[0] == 'OP_DUP':\n        hex += \"76\"\n    if asm[1] == 'OP_HASH160':\n        hex += 'a9'\n    if len(asm[2]) == 40:\n        hex += asm[2]\n    if asm[3] == 'OP_EQUALVERIFY':\n        hex += '88'\n    if asm[4] == 'OP_CHECKSIG':\n        hex += 'ac'\n    return hex",
        "sha1": "b54f613f07f18a06114cefaa33def810dfed8078",
        "id": 104586
    },
    {
        "content": "import asyncio\n\n\nasync def wait_for_event(evt, timeout): # pylint: disable=invalid-name\n    \"\"\"Wait for an event with a timeout\"\"\"\n    try:\n        await asyncio.wait_for(evt.wait(), timeout)\n    except asyncio.TimeoutError:\n        pass\n\n    return evt.is_set()",
        "sha1": "b8b2616f36f12db092c8f12a402115df863dff6f",
        "id": 663043
    },
    {
        "content": "def retrieve_ego_id(base_data_dict):\n    \"\"\"\n    Retrieve the ego vehicle id from sample(origin format).\n\n    Parameters\n    ----------\n    base_data_dict : dict\n        Data sample in origin format.\n\n    Returns\n    -------\n    ego_id : str\n        The id of ego vehicle.\n    \"\"\"\n    ego_id = None\n\n    for cav_id, cav_content in base_data_dict.items():\n        if cav_content['ego']:\n            ego_id = cav_id\n            break\n    return ego_id",
        "sha1": "fe88116de0ce102ae6c3d6e121d7d82ba855bad3",
        "id": 393212
    },
    {
        "content": "import re\n\n\ndef getStringsBetween(start, end, source):\n    \"\"\"\n    get the string between start string and end string for given source string\n    source string is one line string (no new line charactor in it)\n    \"\"\"\n    regex = r\"\\%s(.*?)\\%s\" % (start, end)\n    result = re.match(regex, source)\n    if result:\n        return result.group(1).strip()\n    else:\n        return None",
        "sha1": "1e928cd8650ca0c09a9a4e28c5dbb797c4b398fa",
        "id": 238869
    },
    {
        "content": "def bin2hex(strbin):\n    \"\"\"\n    Convert a string representing a binary number into a string\n    representing the same value in hexadecimal format.\n    \"\"\"\n    dic = { \"0000\":\"0\",\n            \"0001\":\"1\",\n            \"0010\":\"2\",\n            \"0011\":\"3\",\n            \"0100\":\"4\",\n            \"0101\":\"5\",\n            \"0110\":\"6\",\n            \"0111\":\"7\",\n            \"1000\":\"8\",\n            \"1001\":\"9\",\n            \"1010\":\"A\",\n            \"1011\":\"B\",\n            \"1100\":\"C\",\n            \"1101\":\"D\",\n            \"1110\":\"E\",\n            \"1111\":\"F\"\n    }\n    while strbin.__len__()%4 != 0:\n        strbin = '0' + strbin\n    strh = \"\"\n    for i in range(0, strbin.__len__()/4):\n        strh = strh + dic[str(strbin[i*4:i*4+4])]\n    return strh",
        "sha1": "7069dba59e699acbeb53cf9ea78c8e169ce60de9",
        "id": 677150
    },
    {
        "content": "def is_consistent(x, e):\n    \"\"\"\n    Checks for consistency\n    (e.g. if e[n] is v, then x[n] must also be v)\n    \"\"\"\n    consistent = True\n    for n in x:\n        if n in e and x[n] != e[n]:\n            consistent = False\n    return consistent",
        "sha1": "f0622ac66d9ad1871ed08e068074a1104acba48b",
        "id": 110968
    },
    {
        "content": "def expected_hamming(p1, p2=None):\n    \"\"\"\n    Return the expected hamming distance between two random vectors X and Y\n    where X_i ~ Bernoulli(p1) and Y_i ~ Bernoulli(p2) (defaults to p1 if p2\n    isn't specified), under the following assumptions:\n    1. P(X_i = Y_i) = P(X_j = Y_j). In words, this means (X_i, Y_i) and\n    (X_j, Y_j) are identically jointly distributed. In other words, all data\n    points are equally easy (or hard) to learn (this is an empirically false\n    assumption).\n    2. X_i and Y_i are conditionally independent (conditioned on i). In other\n    words, the predictions between any two learned models on the same test\n    example are independent (obviously false assumption).\n    \"\"\"\n    if p2 is None:\n        return 2 * p1 * (1 - p1)\n    else:\n        return p1 + p2 - 2 * p1 * p2",
        "sha1": "394b4502873255d096c720671c4a73e540cdbc49",
        "id": 139254
    },
    {
        "content": "def profile_probability(kmer, profile_matrix):\n    \"\"\"\n    A function to return the probability of a kmer based on a profile matrix\n    Args:\n        kmer: the kmer\n        profile_matrix: the profile matrix\n    Return:\n        The probability of kmer computed w.r.t the profile matrix\n    \"\"\"\n    matrix_row = {\"A\": 0, \"C\": 1, \"G\": 2, \"T\": 3}\n    probability = 1\n    for i in range(len(kmer)):\n        probability = probability*profile_matrix[matrix_row[kmer[i]]][i]\n    return probability",
        "sha1": "8824952b2a8556131dddfb8fa91b3db30ae4cc99",
        "id": 597009
    },
    {
        "content": "def label_filter(label_key, label_value):\n  \"\"\"Return a valid label filter for operations.list().\"\"\"\n  return 'labels.\"{}\" = \"{}\"'.format(label_key, label_value)",
        "sha1": "fe6efe0fdce70a1c36e3f070205c3b0d4279dd62",
        "id": 636932
    },
    {
        "content": "def intensity_factor(n_power, ftp):\n    \"\"\"\n    Compute the intensity factor with the following:\n    IF = Intensity Factor\n    NP = Normalized Power\n    FTP = Functional Threshold Power\n\n    by doing: IF = NP / FTP\n    \"\"\"\n    return round(n_power / ftp, 2)",
        "sha1": "9c962bdfe6849eb65cf7c93db7518822bd31f6b6",
        "id": 608998
    },
    {
        "content": "def _snake_to_camel(word):\n    \"\"\"Converts snake_case into camelCase\"\"\"\n    words = word.split('_')\n    return ''.join([words[0]] + [x.capitalize() for x in words[1:]])",
        "sha1": "067c9d1983851ce0045da9ff601df067c5fac2b9",
        "id": 603135
    },
    {
        "content": "async def fetch_http(session, url, **kwargs):\n    \"\"\"Uses aiohttp to make http GET requests\"\"\"\n\n    async with session.get(url, **kwargs) as response:\n        return await response.json()",
        "sha1": "3253292fa1c967d0514c337ae32ddcd353d613cf",
        "id": 429643
    },
    {
        "content": "def greet(event: str):\n    \"\"\"Greets appropriately (from http://blog.ketchum.com/how-to-write-10-common-holiday-greetings/)  \"\"\"\n    greetings = \"Happy\"\n    if event == \"Christmas\":\n        greetings = \"Merry\"\n    elif event == \"Kwanzaa\":\n        greetings = \"Joyous\"\n    elif event == \"wishes\":\n        greetings = \"Warm\"\n\n    return \"{greetings} {event}!\".format(**locals())",
        "sha1": "9a50856a2662a31c9b287ecead55cdadcdab7d12",
        "id": 405087
    },
    {
        "content": "def decode_dataset_id(dataset_id):\n    \"\"\"Decode a dataset ID encoded using `encode_dataset_id()`.\n    \"\"\"\n    dataset_id = list(dataset_id)\n    i = 0\n    while i < len(dataset_id):\n        if dataset_id[i] == '_':\n            if dataset_id[i + 1] == '_':\n                del dataset_id[i + 1]\n            else:\n                char_hex = dataset_id[i + 1:i + 3]\n                dataset_id[i + 1:i + 3] = []\n                char_hex = ''.join(char_hex)\n                dataset_id[i] = chr(int(char_hex, 16))\n        i += 1\n    return ''.join(dataset_id)",
        "sha1": "08b31b17f8bda58379e7541eaedb1d1a128e9777",
        "id": 41679
    },
    {
        "content": "def predict(model, X):\n\t\"\"\"Predicts the endogenous variable y of the specified model on the specified exogenous\n\tvariables X.\"\"\"\n\treturn model.predict(X).ravel()",
        "sha1": "4e6a81e6c74e4106bf522a196b594f5c8219eebc",
        "id": 314589
    },
    {
        "content": "def emulator_default_visibility(emulator):\n    \"\"\"The default visibility for this emulator.\"\"\"\n    return emulator[\"default_visibility\"]",
        "sha1": "d279920f4c401b8bf68bf1224432badec4658ebe",
        "id": 23179
    },
    {
        "content": "import hashlib\n\n\ndef seeded_auth_token(client, service, seed):\n    \"\"\"Return an auth token based on the client+service+seed tuple.\"\"\"\n    hash_func = hashlib.md5()\n    token = ','.join((client, service, seed)).encode('utf-8')\n    hash_func.update(token)\n    return hash_func.hexdigest()",
        "sha1": "2a6663006a4d094b2ee9af002068d6cb8fdfaf62",
        "id": 443965
    },
    {
        "content": "def is_int(x):\n    \"\"\"Tests if something is an integer\"\"\"\n    return isinstance(x, int)",
        "sha1": "8de9fab5745ff09628467442db5d3c510b3f0031",
        "id": 343588
    },
    {
        "content": "def get_leftmost_coord(geom):\n    \"\"\"\n    gets the leftmost coordinate of a geometry object\n    :param geom: shapely.geometry.LineString/MultiLineString/Point\n    :return: float\n    \"\"\"\n    return min(geom.xy[0])",
        "sha1": "8cf49a2e4988e67edbea505a8cf5e7e2e9690901",
        "id": 266543
    },
    {
        "content": "import struct\n\n\ndef skip(fmt, buf):\n    \"\"\"Return the data remaining after skipping the given elements.\"\"\"\n    return buf[struct.calcsize(fmt):]",
        "sha1": "3e56ee83dc7a9ae6f0af56c138ba22672230b90d",
        "id": 151158
    },
    {
        "content": "def _is_part_dsn(msg):\n    \"\"\"\n    Receive a MIME part and returns True if it is a DSN\n    False is returned otherwise\n    \"\"\"\n    if len(msg) <= 1:\n        return False\n    if msg[1].get_content_type() != 'message/delivery-status':\n        return False\n    return True",
        "sha1": "31f6f8472676abfc0e94add96c47111cf19a4a74",
        "id": 640056
    },
    {
        "content": "import tarfile\n\n\ndef result_files_from_archive(tar_archive: tarfile.TarFile):\n    \"\"\"\n    Extracts the result files from the given archive returning the files as well as the director structure contained\n    in the tar archive for later reconstruction\n\n    :param tar_archive: the tar archive from which to extract the files\n    :return: List of file object extracted from the tar archive\n    \"\"\"\n\n    file_members = []\n    for member in tar_archive.getmembers():\n        if member.isreg():  # skip if the TarInfo is not files\n            file_members.append(member)\n\n    files = []\n    for file_member in file_members:\n        files.append(tar_archive.extractfile(file_member))\n    return files, file_members, tar_archive.getmembers()",
        "sha1": "ee1cc2aa22e868e4d1bf32d40c222cacdc6b4aff",
        "id": 188353
    },
    {
        "content": "def get_det_prefix(pv_info):\n    \"\"\"\n    Determines which prefix will be passed into the detector init.\n\n    Parameters\n    ----------\n    pv_info: ``str``\n        The second element in a camview cfg line.\n\n    Returns\n    -------\n    detector_prefix: ``str``\n        The prefix to use in the detector init.\n    \"\"\"\n    pv_info = pv_info.split(';')\n    try:\n        detector_prefix = pv_info[1]\n    except IndexError:\n        # Not provided in config, guess from image base\n        detector_prefix = ':'.join(pv_info[0].split(':')[:-1])\n    return detector_prefix + ':'",
        "sha1": "0b973808092f14b0eeeb6d20f5e3c207a83a7068",
        "id": 136620
    },
    {
        "content": "def scaleadd(origin, offset, vectorx):\n    \"\"\"\n    From a vector representing the origin,\n    a scalar offset, and a vector, returns\n    a Vector3 object representing a point \n    offset from the origin.\n\n    (Multiply vectorx by offset and add to origin.)\n    \"\"\"\n    multx = vectorx * offset\n    return multx + origin",
        "sha1": "4a46490503c357ba12155bd3f5d77d4b856c86f7",
        "id": 242897
    },
    {
        "content": "def to_camel(snake):\n    \"\"\"time_skill -> TimeSkill\"\"\"\n    return snake.title().replace(\"_\", \"\")",
        "sha1": "d486cd202aa9cfea124d04d309393b40eab60576",
        "id": 228511
    },
    {
        "content": "def is_correlated(corr_matrix, feature_pairs, rho_threshold=0.8):\n    \"\"\"\n    Returns dict where the key are the feature pairs and the items\n    are booleans of whether the pair is linearly correlated above the\n    given threshold.\n    \"\"\"\n    results = {}\n    for pair in feature_pairs:\n        f1, f2 = pair.split(\"__\")\n        corr = corr_matrix[f1][f2]\n        results[pair] = round(corr, 3) >= rho_threshold\n    return results",
        "sha1": "18afa0cc24f5d9205cde3c8ad23f70d73b5c395b",
        "id": 706932
    },
    {
        "content": "import time\n\n\ndef callAndTime(input_):\n\t\"\"\"\n\tWrapper for calling and timing a function\n\targs:\n\t\tinput: List \n\t\t\tfunc at zeroth index and\n\t\t\trest are arguments\n\t\"\"\"\n\tfunc= input_[0]\n\targs = input_[1:]\n\t\n\tstart = time.time()\n\tret = func(*args)\n\ttimetaken = time.time() - start\n\treturn ret, timetaken",
        "sha1": "f5da12dba74b9dc9f05ec32fe839e61247477d8b",
        "id": 200459
    },
    {
        "content": "def countN(x: int, y: int, fld: list) -> int:\n    \"\"\" Count all alive neighbours \"\"\"\n\n    mw, mh = len(fld), len(fld[0])\n    \"\"\" check 3 row elements above, check 2 sides, check 3 row elements below \"\"\"\n    return fld[(x-1) % mw][(y-1)% mh] + fld[x][(y-1)% mh] + fld[(x+1) % mw][(y-1)% mh] +\\\n        fld[(x-1) % mw][y] + fld[(x+1) % mw][y] +\\\n        fld[(x-1) % mw][(y+1)% mh] + fld[x][(y+1)% mh] + fld[(x+1) % mw][(y+1)% mh]",
        "sha1": "20a3f3a3046aa1a8d41bcce55ff6a19db143abb3",
        "id": 248908
    },
    {
        "content": "def convert_email(value: str) -> str:\n    \"\"\"Convert email domain from student to employee or vice versa\"\"\"\n    user, domain = value.split('@')\n    if domain.startswith('st.'):\n        return f\"{user}@{domain[3:]}\"\n    else:\n        return f\"{user}@st.{domain}\"",
        "sha1": "b927a521f379698bcc57c125c39de63a5552d9a6",
        "id": 678568
    },
    {
        "content": "def pair_has_proper_orientation(read_pair):\n    \"\"\"\n    Returns True if the given pysam reads map towards each other (i.e., read\n    with lowest coordinate is in forward orientation and other read is in\n    reverse).\n    \"\"\"\n    return len(read_pair) == 2 and not read_pair[0].is_reverse and read_pair[1].is_reverse",
        "sha1": "1d26ec8c5bf1aef8dea4e67388879530d2a336c6",
        "id": 372435
    },
    {
        "content": "def manual_order(qs, pks, pk_name='id'):\n    \"\"\"\n    Given a query set and a list of primary keys, return a set of objects from\n    the query set in that exact order.\n    \"\"\"\n    if not pks:\n        return qs.none()\n    return qs.filter(id__in=pks).extra(\n            select={'_manual': 'FIELD(%s, %s)'\n                % (pk_name, ','.join(map(str, pks)))},\n            order_by=['_manual'])",
        "sha1": "1720711681c03f1375469949c97d318415eb77e7",
        "id": 187177
    },
    {
        "content": "from typing import List\nfrom pathlib import Path\n\n\ndef filter_paths_to_existing(*iterables) -> List[str]:\n    \"\"\"\n    Filter paths to only existing.\n    \"\"\"\n    return [path for path in iterables if Path(path).exists()]",
        "sha1": "43e31638b0eba1705000e6cf7d657c761cb83d93",
        "id": 466644
    },
    {
        "content": "def call_reply(msg, call_return):\n    \"\"\"Construct message used by CtrlServer when replying to API calls.\n\n    :param msg: Description of API call.\n    :type msg: string\n    :param call_return: Return value of API call.\n    :type call_return: string\n    :returns: Constructed call_reply dict, ready to be sent over the wire.\n\n    \"\"\"\n    return {\"type\": \"call_reply\", \"msg\": msg, \"call_return\": call_return}",
        "sha1": "eadbaf02bf182af228e0c4a1a31603ca5bfa0eb2",
        "id": 624358
    },
    {
        "content": "def selection_with_period(raw, only_1516=False, only_17=False, only_18=False):\n    \"\"\"Augment a selection to require a specific data taking period.\n\n    Parameters\n    ----------\n    raw : str\n        Raw selection string.\n    only_1516 : bool\n        Require 2015/2016\n    only_17 : bool\n        Require 2017\n    only_18 : bool\n        Require 2018\n\n    Returns\n    -------\n    str\n        Updated selection string\n    \"\"\"\n    if only_1516:\n        return f\"({raw}) && (isMC16a == 1)\"\n    elif only_17:\n        return f\"({raw}) && (isMC16d == 1)\"\n    elif only_18:\n        return f\"({raw}) && (isMC16e == 1)\"\n    else:\n        return raw",
        "sha1": "d72261bd637d104e93010f503c679793970f4759",
        "id": 355968
    },
    {
        "content": "def simple_cpu(size, data_in):\n    \"\"\"Simple CPU implementation.\"\"\"\n    return [data_in[i] * data_in[i] + data_in[i] for i in range(size)]",
        "sha1": "bc5aa7e1ef3d3ab5fd01c2be3057e01d2d04350c",
        "id": 557272
    },
    {
        "content": "import json\nimport zlib\n\n\ndef open_file(dataset_path):\n  \"\"\"\n  Open a JSON dataset in the dataset path\n\n  Args :\n    - dataset_path (str) :  the dataset path\n\n  Returns :\n    - dict : the loaded dataset\n  \"\"\"\n  try :\n    with open(dataset_path, 'r') as file:\n      return json.load(file)\n  except UnicodeDecodeError:\n    with open(dataset_path, 'rb') as file:\n      read_file = file.read()\n      return json.loads(zlib.decompress(read_file), encoding='utf-8')",
        "sha1": "20c673e71ca8731246915379754d23eeb5be543e",
        "id": 111006
    },
    {
        "content": "def build_version(release_version: str) -> str:\n    \"\"\"Given 'X.Y.Z[-rc.N]', return 'X.Y.Z'.\"\"\"\n    return release_version.split('-')[0]",
        "sha1": "06258630ff67d3a16d8642bfc297d9f7196c0c07",
        "id": 85279
    },
    {
        "content": "def center_crop(x, y=None, crop_size=None, data_format='channels_last'):\n    \"\"\"\n    Takes a pair of numpy arrays (image and label) and returns a pair of matching center crops\n    :param x: image in numpy array format\n    :param y: label in numpy array format\n    :param crop_size: (height, width) tuple\n    :param data_format: 'channels_first' or 'channels_last'\n    :return: (cropped image, cropped label) tuple\n    \"\"\"\n    if crop_size is None:\n        return x if y is None else x, y\n\n    if data_format == 'channels_first':\n        centerh, centerw = x.shape[1] // 2, x.shape[2] // 2\n    elif data_format == 'channels_last':\n        centerh, centerw = x.shape[0] // 2, x.shape[1] // 2\n    else:\n        raise NotImplementedError()\n    crop_size = (2 * centerh, 2 * centerw) if crop_size is None else crop_size\n    lh, lw = crop_size[0] // 2, crop_size[1] // 2\n    rh, rw = crop_size[0] - lh, crop_size[1] - lw\n\n    start_h, end_h = centerh - lh, centerh + rh\n    start_w, end_w = centerw - lw, centerw + rw\n    if data_format == 'channels_first':\n        cropped_x = x[:, start_h:end_h, start_w:end_w]\n        if y is None:\n            return cropped_x\n        else:\n            cropped_y = y[:, start_h:end_h, start_w:end_w]\n            return cropped_x, cropped_y\n    elif data_format == 'channels_last':\n        cropped_x = x[start_h:end_h, start_w:end_w, :]\n        if y is None:\n            return cropped_x\n        else:\n            cropped_y = y[start_h:end_h, start_w:end_w, :]\n            return cropped_x, cropped_y",
        "sha1": "37ccee7858a74da8d95f1dcdb934550fa525e8a9",
        "id": 614427
    },
    {
        "content": "def find_index(arr, pred):\n    \"\"\" Find index of the first item that satisfies a predicate \"\"\"\n    for index, elem in enumerate(arr):\n        if pred(elem):\n            return index\n    return -1",
        "sha1": "4bd65d40061f5102348d3c38fff6bd7fe669de90",
        "id": 602176
    },
    {
        "content": "def tally(predicate, iterable):\n  \"\"\"Count how many times the predicate is true.\n\n  Taken from the Python documentation. Under the PSF license.\n\n  :param predicate:\n      Predicate function.\n  :param iterable:\n      Iterable sequence.\n  :returns:\n      The number of times a predicate is true.\n  \"\"\"\n  return sum(map(predicate, iterable))",
        "sha1": "1717fd7c581772fc47a7d371bffcf43067b338aa",
        "id": 598858
    },
    {
        "content": "import torch\n\n\ndef npvec_to_tensorlist(vec, params):\n    \"\"\" Convert a numpy vector to a list of tensor with the same dimensions as params\n\n        Args:\n            vec: a 1D numpy vector\n            params: a list of parameters from net\n\n        Returns:\n            rval: a list of tensors with the same shape as params\n    \"\"\"\n    loc = 0\n    rval = []\n    for p in params:\n        numel = p.data.numel()\n        rval.append(torch.from_numpy(vec[loc:loc+numel]).view(p.data.shape).float())\n        loc += numel\n    assert loc == vec.size, 'The vector has more elements than the net has parameters'\n    return rval",
        "sha1": "3cbed80b3896d6f0610a057903f09728ccae0a30",
        "id": 39846
    },
    {
        "content": "def resize_image(img, new_width, new_height):\n    \"\"\"Resize image to a ``new_width`` and ``new_height``.\n    \n    Args:\n        img (PIL image): An image.\n        new_width (int): New width.\n        new_height (int): New height.\n    \n    Returns:\n        PIL image: A resized image.\n    \n    Examples:\n        >>> img = Image.open('share/Lenna.png')\n        >>> img_resized = resize_image(img, 256, 256)\n        >>> img_resized.size\n        (256, 256)\n    \"\"\"\n    img_new = img.resize((new_width, new_height))\n    return img_new",
        "sha1": "00d9c13f36cc639da667e12b3b7e2679912b4ddc",
        "id": 264301
    },
    {
        "content": "def get_category_to_id_mapping(data, column):\n    \"\"\"Creates two mappings for id and categorical value and vice verse for given column.\n    Id is a unique identifier of categorical value. Starting from 0.\n\n    Args:\n      data: dataframe that contains categorical values\n      column: a column of dataframe that contains categorical values for which a mapping from categorical value\n    to id is needed\n\n    Returns:\n      id_to_category: dictionary of ids and categories\n      category_to_id: dictionary of categories and ids\n\n    \"\"\"\n    categories = sorted(data[column].unique())\n    print(\"There are {} unique categories\".format(len(categories)))\n    id_to_category = {i: categories[i] for i in range(len(categories))}\n    category_to_id = {categories[i]: i for i in range(len(categories))}\n    return id_to_category, category_to_id",
        "sha1": "c06d829536ce2a1fa8f24e8388e25102c053e311",
        "id": 54727
    },
    {
        "content": "def convertArrayInTupleList(array):\n    \"\"\"\n    Convert an array (or a list) of element in a list of tuple where each element is a tuple with two sequential element of the original array/list\n\n    Parameters\n    ----------\n    array : numpy array/list\n\n    Returns\n    -------\n    tuple_list. List of tuple\n        Given the input array = [a, b, c, d ...] the tuple_list will be [(a, b), (b, c), (c, d) ...]\n\n    \"\"\"\n    \n    tuple_list = []\n    \n    for i in range(len(array) - 1):\n        tmp_tuple = (array[i], array[i + 1])\n        \n        tuple_list.append(tmp_tuple)\n        \n    return tuple_list",
        "sha1": "c48a94b53814fd50264dd6975c0e8db64ed013a8",
        "id": 108797
    },
    {
        "content": "def log_dir(request) -> str:\n    \"\"\"Retrieve user-provided logging directory on the command line.\"\"\"\n    return request.config.getoption(\"--log-dir\")",
        "sha1": "b8d00e0282170ab858a9b7a39f367549739bb5c8",
        "id": 566024
    },
    {
        "content": "def split_cdl(cdl_string):\n    \"\"\"\n    Accepts a comma delimited list of values as a string,\n    and returns a list of the string elements.\n    \"\"\"\n    return [x.strip() for x in cdl_string.split(',')]",
        "sha1": "8f73bdfdc1a70f513ed8f381a93bdd7392e4b332",
        "id": 564590
    },
    {
        "content": "import re\n\n\ndef get_daligner_job_descriptions(run_jobs_stream, db_prefix, single=False):\n    \"\"\"Return a dict of job-desc-tuple -> HPCdaligner bash-job.\n\n    Comments are ignored.\n\n    E.g., each item will look like:\n      ('.2', '.1', '.2', '.3'): 'daligner\n\n    Rationale\n    ---------\n    For i/o efficiency, we want to daligner calls with LAsort/LAmerge lines. But\n    Gene has done this himself too. So now, we only want the daligner calls here.\n\n    Later, we will do the extra LAmerge lines, grouped by A-read.\n    \"\"\"\n    re_block_dali = re.compile(r'%s(\\.\\d+|)' % db_prefix)\n\n    def blocks_dali(line):\n        \"\"\"Return ['.1', '.2', ...]\n        Can return [''] if only 1 block.\n        \"\"\"\n        return [mo.group(1) for mo in re_block_dali.finditer(line)]\n    # X == blocks[0]; A/B/C = blocks[...]\n\n    lines = [line.strip() for line in run_jobs_stream]\n    # in case caller passed filename, not stream\n    assert any(len(l) > 1 for l in lines), repr('\\n'.join(lines))\n\n    lines_dali = [l for l in lines if l.startswith(\n        'daligner')]  # could be daligner_p\n    result = {}\n    for dali in lines_dali:\n        id = tuple(blocks_dali(dali))\n        early_checks = [\n            \"LAcheck -v {db_prefix} *.las\".format(db_prefix=db_prefix)]\n        script = '\\n'.join([dali] + early_checks) + '\\n'\n        result[id] = script\n    return result",
        "sha1": "66a2216bf1131102b0683f158a0d2d692470eec7",
        "id": 177485
    },
    {
        "content": "from pathlib import Path\n\n\ndef list_source_files(base_dir: Path) -> list[Path]:\n    \"\"\"List source files.\n\n    Args:\n        base_dir (Path): Base directory.\n\n    Returns:\n        list[Path]: Paths of source files.\n    \"\"\"\n\n    files = []\n    for child in base_dir.iterdir():\n        if child.is_dir():\n            files = files + list_source_files(child)\n        elif child.is_file() and str(child.suffix) == \".cpp\":\n            files = files + [child]\n    return files",
        "sha1": "7199550fa23b3db2526a80de610148e408b8a8cd",
        "id": 114593
    },
    {
        "content": "def build_cgi_environ(wsgi_environ, git_project_root, user=None):\n    \"\"\"Build a CGI environ from a WSGI environment:\n\n    CONTENT_TYPE\n    GIT_PROJECT_ROOT = directory containing bare repos\n    PATH_INFO (if GIT_PROJECT_ROOT is set, otherwise PATH_TRANSLATED)\n    QUERY_STRING\n    REMOTE_USER\n    REMOTE_ADDR\n    REQUEST_METHOD\n\n    The git_project_root parameter must point to a directory that contains\n    the git bare repo designated by PATH_INFO. See the git documentation.\n\n    The git repo (my-repo.git) is located at GIT_PROJECT_ROOT + PATH_INFO\n    (if GIT_PROJECT_ROOT is defined) or at PATH_TRANSLATED.\n\n    If REMOTE_USER is set in wsgi_environ, you should normally leave user\n    alone.\n    \"\"\"\n    cgi_environ = dict(wsgi_environ)\n    none_string_keys = []\n\n    for key, value in cgi_environ.items():  # NOT iteritems, due to \"del\"\n        if not isinstance(value, str):\n            none_string_keys.append(key)\n\n    for key in none_string_keys:\n        del cgi_environ[key]\n\n    cgi_environ['GIT_HTTP_EXPORT_ALL'] = '1'\n    cgi_environ['GIT_PROJECT_ROOT'] = git_project_root\n\n    if user:\n        cgi_environ['REMOTE_USER'] = user\n    cgi_environ.setdefault('REMOTE_USER', 'unknown')\n    return cgi_environ",
        "sha1": "da802e892fd61dad6345271f2045e05e59012b18",
        "id": 115440
    },
    {
        "content": "from typing import Counter\n\n\ndef vcounts(arr, normalize=True):\n    \"\"\"Equivalent of pandas_htools vcounts method that we can apply on lists\n    or arrays. Basically just a wrapper around Counter but with optional\n    normalization.\n\n    Parameters\n    ----------\n    arr: Iterable\n        Sequence of values to count. Typically a list or numpy array.\n    normalize: bool\n        If True, counts will be converted to percentages.\n\n    Returns\n    -------\n    dict: Maps unique items in `arr` to the number of times (or % of times)\n        that they occur in `arr`.\n    \"\"\"\n    counts = dict(Counter(arr))\n    if normalize:\n        length = len(arr)\n        counts = {k: v/length for k, v in counts.items()}\n    return counts",
        "sha1": "2327f221b9df21761d34721aeb4b95b0f997b5b1",
        "id": 486279
    },
    {
        "content": "import textwrap\n\n\ndef ind(text: str, tabs=1):\n    \"\"\"Indent text with the given number of tabs.\"\"\"\n    return textwrap.indent(text, \"    \" * tabs)",
        "sha1": "d8d1e08d0b2acaee8aec797b3514fb30493f960d",
        "id": 365971
    },
    {
        "content": "def calc_death_rate(confirmed, deaths):\n    \"\"\"\n    Calculates the daily death rate in confirmed cases.\n    :param confirmed: DataFrame of confirmed cases\n    :param deaths: DataFrame of deaths\n    :return: DataFrame of daily death rate\n    \"\"\"\n    death_rate = (deaths / confirmed) * 100\n    death_rate = death_rate.fillna(0.0)\n    return death_rate",
        "sha1": "61f6e445d52b4495c3d07cf115692620c047fc75",
        "id": 659405
    },
    {
        "content": "def truncate(text: str, maxlen: int = 1024) -> str:\n    \"\"\"Truncate a paragraph to a specific length.\n\n    Args:\n        text: The paragraph to truncate.\n        maxlen: The maximum length of the paragraph.\n\n    Returns:\n        The truncated paragraph.\n    \"\"\"\n    etc = \"[\u2026]\"\n    return f\"{text[:maxlen - len(etc)]}{etc}\" if len(text) > maxlen - len(etc) else text",
        "sha1": "83f806ff67205f8d79a17605593e705ba60016c6",
        "id": 513056
    },
    {
        "content": "def get_deployment_labels(deployments):\n    \"\"\"\n    Get labels for a given list of deployments.\n\n    Returns a dictionary in the below format:\n\n    { deployment_name: {dictionary of deployment labels}, ...}\n    \"\"\"\n    deployment_dictionary = {}\n    for deployment in deployments:\n        deployment_dictionary[deployment['metadata']['name']] = deployment['spec']['template']['metadata']['labels']\n    return deployment_dictionary",
        "sha1": "418786845c1f9313e6f1e670927f3f600553dab7",
        "id": 661457
    },
    {
        "content": "def adjust(center, neighbors):\n    \"\"\"Calculates the mean coordinates of a list of (x, y) tuples\n\n    Used to calculate the new position of a cluster center\n    \"\"\"\n    if len(neighbors) == 0:\n        return center\n    # Watch out for overflow...\n    avg_x = sum([n[0] for n in neighbors]) / len(neighbors)\n    avg_y = sum([n[1] for n in neighbors]) / len(neighbors)\n    return (avg_x, avg_y)",
        "sha1": "b92edf2264955de2d362d25e724fe4c37b98d0c4",
        "id": 229677
    },
    {
        "content": "def jinja_filter_param_value_str(value, str_quote_style=\"\", bool_is_str=False):\n    \"\"\"Convert a parameter value to string suitable to be passed to an EDA tool\n\n    Rules:\n\n    - Booleans are represented as 0/1 or \"true\"/\"false\" depending on the\n      bool_is_str argument\n    - Strings are either passed through or enclosed in the characters specified\n      in str_quote_style (e.g. '\"' or '\\\\\"')\n    - Everything else (including int, float, etc.) are converted using the str()\n      function.\n    \"\"\"\n    if type(value) == bool:\n        if bool_is_str:\n            return \"true\" if value else \"false\"\n        else:\n            return \"1\" if value else \"0\"\n    elif type(value) == str:\n        return str_quote_style + str(value) + str_quote_style\n    else:\n        return str(value)",
        "sha1": "d814ddad7fcfac73e0ddebd126698898faf31cc6",
        "id": 156629
    },
    {
        "content": "def to_binary(text, encoding='ascii'):\n    \"\"\"Return the binary representation of string (if not already binary).\"\"\"\n    if not isinstance(text, bytes):\n        text = text.encode(encoding)\n    return text",
        "sha1": "c702dcfb8b14e66ec0db157b48b87febc49056c9",
        "id": 300678
    },
    {
        "content": "def list_extract(items, arg):\n    \"\"\"Extract items from a list of containers\n\n    Uses Django template lookup rules: tries list index / dict key lookup first, then\n    tries to getattr. If the result is callable, calls with no arguments and uses the return\n    value..\n\n    Usage: {{ list_of_lists|list_extract:1 }} (gets elt 1 from each item in list)\n           {{ list_of_dicts|list_extract:'key' }} (gets value of 'key' from each dict in list)\n    \"\"\"\n    def _extract(item):\n        try:\n            return item[arg]\n        except TypeError:\n            pass\n        attr = getattr(item, arg, None)\n        return attr() if callable(attr) else attr\n\n    return [_extract(item) for item in items]",
        "sha1": "23fb863a7032f37d029e8b8a86b883dbfb4d5e7b",
        "id": 707517
    },
    {
        "content": "def hex_sans_prefix(number):\n    \"\"\"Generates a hexadecimal string from a base-10 number without the standard '0x' prefix.\"\"\"\n    return hex(number)[2:]",
        "sha1": "6faaec36b2b3d419e48b39f36c1593297710a0a4",
        "id": 27869
    },
    {
        "content": "def parse_line(line):\n    \"\"\"Takes a line formatted as three comma seperated integers. Returns three\n    integers.\n    \"\"\"\n    n, p1, p2 = line.strip().split(',')\n    return int(n), int(p1), int(p2)",
        "sha1": "c6d41a18582fca388510105f51d05cdd00f88a46",
        "id": 283167
    },
    {
        "content": "def add_encoding(css: str) -> str:\n    \"\"\"Add @charset 'UTF-8'; if missing.\n    \"\"\"\n    return '@charset utf-8;' + css if '@charset' not in css.lower() else css",
        "sha1": "e8e20080e1048a548109a21f3a77e79c52376e09",
        "id": 150008
    },
    {
        "content": "def admin_urlname(value, arg):\n    \"\"\"Given model opts (model._meta) and a url name, return a named pattern.\n    URLs should be named as: customadmin:app_label:model_name-list\"\"\"\n    pattern = \"customadmin:%s:%s-%s\" % (value.app_label, value.model_name, arg)\n    # print(pattern)\n    return pattern",
        "sha1": "85b5cef15fca003328f3ad662132de85e7a1b5a4",
        "id": 133678
    },
    {
        "content": "import re\n\n\ndef strip_ns(tag):\n    \"\"\"\n    strips the namespace from a string\n    \"\"\"\n    return re.sub(\"^\\{.*\\}\", '', tag)",
        "sha1": "0be31b10ae5b3eb89598a79dd207055bf9520f49",
        "id": 310401
    },
    {
        "content": "def get_text(answer: list) -> str:\n    \"\"\"Extract only the text from the answers.text column\n\n    Args:\n        answer: the answer.\n    \"\"\"\n    return answer[0]",
        "sha1": "eddf15f182a869ff7a862e66ddb31b9238a869d3",
        "id": 103275
    },
    {
        "content": "def val_in_array(val, arr):\n    \"\"\"Brute force check for a value in an array.\"\"\"\n    for a in arr:\n        if val == a:\n            return True\n    return False",
        "sha1": "a9da1f837d936d9e9131c376772a1102ed70c8c0",
        "id": 182317
    },
    {
        "content": "import re\n\n\ndef process_replaces(pre_process_str, replaces):\n    \"\"\"\n    Performs string replaces in the original ocr_result in the pre processing\n    stage of the OCR result.\n\n    Args:\n        pre_process_str (str): OCR document str in the pre process stage;\n        replaces (list): List of tuples in the following format:\n                         [(regexp, replace_str), (regexp, replace_str), ...]\n\n    Returns:\n        (str): The pre processed OCR result string after the DRM option\n               replaces.\n    \"\"\"\n    for regexp, replacement in replaces:\n        pre_process_str = re.sub(regexp, replacement, pre_process_str)\n\n    return pre_process_str",
        "sha1": "e92352bfe6810494d9033cd6a6c7382d79da9d96",
        "id": 500561
    },
    {
        "content": "def max_digits(x):\n    \"\"\"\n    Return the maximum integer that has at most ``x`` digits:\n\n        >>> max_digits(4)\n        9999\n        >>> max_digits(0)\n        0\n    \"\"\"\n    return (10 ** x) - 1",
        "sha1": "3f0ffdfbbb3fdaec8e77889b3bfa14c9b9829b2e",
        "id": 699184
    },
    {
        "content": "from dateutil import tz\nfrom datetime import datetime\n\n\ndef format_timestamp(ts):\n        \"\"\"\n        Format the UTC timestamp for Elasticsearch\n        eg. 2014-07-09T08:37:18.000Z\n        @see https://docs.python.org/2/library/time.html#time.strftime\n        \"\"\"\n        tz_info = tz.tzutc()\n        return datetime.fromtimestamp(ts, tz=tz_info).strftime(\"%Y-%m-%dT%H:%M:%S.000Z\")",
        "sha1": "d1573fd48dbd1ba2ac895f46c6ed467bdbc262ce",
        "id": 197427
    },
    {
        "content": "import pathlib\n\n\ndef _stringify_path(path_or_buffer):\n    \"\"\"Convert path like object to string\n\n    Args:\n        path_or_buffer: object to be converted\n\n    Returns:\n        string_path_or_buffer: maybe string version of path_or_buffer\n    \"\"\"\n\n    try:\n\n        _PATHLIB_INSTALLED = True\n    except ImportError:\n        _PATHLIB_INSTALLED = False\n\n    if hasattr(path_or_buffer, \"__fspath__\"):\n        return path_or_buffer.__fspath__()\n\n    if _PATHLIB_INSTALLED and isinstance(path_or_buffer, pathlib.Path):\n        return str(path_or_buffer)\n\n    return path_or_buffer",
        "sha1": "172e0faab6ca67f7d0f15577909e4027a33d9d8c",
        "id": 676730
    },
    {
        "content": "def convert_spot_coordinates(spots, resolution_z, resolution_yx):\n    \"\"\"\n    Convert spots coordinates in nanometer.\n\n    Parameters\n    ----------\n    spots : np.ndarray, np.int64\n        Coordinates of the detected spots with shape (nb_spots, 3).\n    resolution_z : int or float\n        Height of a voxel, along the z axis, in nanometer.\n    resolution_yx : int or float\n        Size of a voxel on the yx plan, in nanometer.\n\n    Returns\n    -------\n    spots_nanometer : np.ndarray, np.int64\n        Coordinates of the detected spots with shape (nb_spots, 3), in\n        nanometer.\n\n    \"\"\"\n    # convert spots coordinates in nanometer, for each dimension, according to\n    # the pixel size of the image\n    spots_nanometer = spots.copy()\n    spots_nanometer[:, 0] *= resolution_z\n    spots_nanometer[:, 1:] *= resolution_yx\n\n    return spots_nanometer",
        "sha1": "a67bd68e4af2db848dc5427b3e4ca66b716cd67a",
        "id": 464887
    },
    {
        "content": "def make_mmvt_boundary_definitions(cv, milestone):\n    \"\"\"\n    Take a Collective_variable object and a particular milestone and\n    return an OpenMM Force() object that the plugin can use to monitor\n    crossings.\n    \n    Parameters\n    ----------\n    cv : Collective_variable()\n        A Collective_variable object which contains all the information\n        for the collective variable describine this variable. In fact,\n        the boundaries are contours of the function described by cv.\n        This variable contains information like the groups of atoms\n        involved with the CV, and the expression which describes the\n        function.\n        \n    milestone : Milestone()\n        A Milestone object which describes the boundary between two\n        Voronoi cells. This variable contains information like the \n        values of the variables which will be entered into the \n        Force() object.\n        \n    Returns\n    -------\n    myforce : openmm.Force()\n        An OpenMM force object which does not affect atomic motion, but\n        allows us to conveniently monitor a function of atomic \n        position.\n    \"\"\"\n    myforce = cv.make_force_object()\n    myforce.setForceGroup(1)\n    variable_names_list = cv.add_parameters(myforce)\n    cv.add_groups_and_variables(myforce, cv.get_variable_values_list(\n                                    milestone))\n    return myforce",
        "sha1": "45baaaa70ea24cb564c529cd885597415561a25d",
        "id": 704283
    },
    {
        "content": "def __none_mult(x, y):\n    \"\"\"PRIVATE FUNCTION, If x or y is None return None, else return x * y\"\"\"\n    if x is not None and y is not None:\n        return x * y\n    return None",
        "sha1": "88cd9fca1fbb3f2ab10dec22a73e792201b20578",
        "id": 679653
    },
    {
        "content": "def get_separation_score(results):\n    \"\"\"\n    For a Solr response, compute the separation score which is derived from the\n    ratio between the two first Solr scores.\n    \"\"\"\n    score1 = results[0]['score']\n    score2 = results[1]['score']\n\n    return (1 - score2 / score1)",
        "sha1": "1fc53bb05eb012d15ea1abb26838e2c4a6c04847",
        "id": 620831
    },
    {
        "content": "import torch\n\n\ndef compute_score(logits, labels):\n    \"\"\"Computes score of the prediction.\n\n    Ground truth soft scores for labels is calculated as per:\n    https://github.com/hengyuan-hu/bottom-up-attention-vqa/blob/master/tools/compute_softscore.py#L80\n    and http://visualqa.org/evaluation.html\n    Args:\n        logits:\n            Tensor containing predicted score for each answer candidiate\n            predicted. Scores are pre-sigmoid and not between 0 and 1.\n            Tensor is of dimension (batch size, answer vocabulary size).\n        labels:\n            Ground truth soft scores for each answer in the set of candidiate\n            answers.\n            Soft scores/ accuracies are between [0, 1].\n            Tensor is of dimension (batch size, answer vocabulary size).\n    \"\"\"\n    # logits has dimension [batch_size, num_ans_candidiates]\n    # Each sample in the batch is of dimension num_ans_candidiates and each\n    # element is the predicted score for the corresponding answer.\n\n    # Finding the element index (offset) with max score for each sample in the\n    # batch (i.e. argmax).\n    # logits will now have dimension [batch_size] i.e. batch_size * 1.\n    logits = torch.max(logits, 1)[1].data\n\n    # Making zero vector of dimension [batch_size, num_ans_candidiates].\n    one_hots = torch.zeros(*labels.size()).cuda()\n\n    # Making a one-hot vector of dimension [batch_size, num_ans_candidiates].\n    # For each sample the value at an index = index with max score, will be\n    # set to 1.\n    # This one hot vector is essentially encodes the most probable answer in\n    # the vocabulary as predicted by the model.\n    one_hots.scatter_(1, logits.view(-1, 1), 1)\n\n    # Score will be a vector of dimension [batch_size, num_ans_candidiates].\n    # Each sample will be a vector containing all zeroes, except at the index\n    # of the predicted answer which contains the soft score/accuracy of that\n    # answer calculated as per http://visualqa.org/evaluation.html.\n    scores = one_hots * labels\n    return scores",
        "sha1": "37dc135e998bbb4116ae93c322ff6ea857411c31",
        "id": 125528
    },
    {
        "content": "from typing import Sequence\nfrom typing import Pattern\nfrom typing import Optional\n\n\ndef matches_variable_ref(patterns: Sequence[Pattern],\n                         string: str) -> Optional[Sequence[str]]:\n    \"\"\"\n    Matches the string against the patterns, returning a 3-tuple on match and\n    None on no match.\n\n    :param patterns: A series of patterns returned from variable_ref_patterns().\n    :param string:   The string against which to match.\n\n    :return: None if no match. If match, a 3-tuple containing three elements:\n             the portion of the string preceding the variable reference,\n             the variable reference, and the portion of the string following\n             the variable reference\n    \"\"\"\n\n    for p in patterns:\n        m = p.match(string)\n        if m:\n            return m.groups()\n    else:\n        return None",
        "sha1": "859f068fea4714d4ec3817aedc2ce7f2b406e0be",
        "id": 259521
    },
    {
        "content": "import json\n\n\ndef load_json_file(path_to_json_file: str) -> str:\n    \"\"\"\n    Return json as string from selected file.\n\n    Args:\n        path_to_json_file: (str) path to file with json\n    Returns:\n        File content as string (str)\n    \"\"\"\n    with open(path_to_json_file) as json_file:\n        data = json.load(json_file)\n        return json.dumps(data)",
        "sha1": "5a79a6d0674af2105919ad6dc56a60275451c79e",
        "id": 529792
    },
    {
        "content": "import yaml\n\n\ndef load_TS_class(filename,\n                  print_info=True):\n    \"\"\" Load parameters and rules from yaml file to create time series.\n\n    Returns dict.\n    \"\"\"\n\n    with open(filename, 'r') as ymlfile:\n        TS_def = yaml.load(ymlfile, Loader=yaml.SafeLoader)\n\n    if print_info:\n        print(TS_def['class_name'])\n        print(TS_def['description'])\n        print('n_channels:', TS_def['n_channels'])\n        print('n_timepoints:', TS_def['n_timepoints'])\n\n    return TS_def",
        "sha1": "f316ae454fd652d0afd5ca24f055c646617a7879",
        "id": 84988
    },
    {
        "content": "from datetime import datetime\n\n\ndef _epoch_utc_to_datetime(epoch_utc):\n    \"\"\"\n    Helper function for converting epoch timestamps (as stored in JWTs) into\n    python datetime objects (which are easier to use with sqlalchemy).\n    \"\"\"\n    return datetime.fromtimestamp(epoch_utc)",
        "sha1": "8618c7aa9b673887503fa5ee981e7c6c045ad813",
        "id": 299221
    },
    {
        "content": "def cipher(text, shift, encrypt=True):\n    \"\"\"\n    Encrypts words by shifting the direction of letters.\n\n    Parameters:\n    -----------\n    text = Word whose letters will be converted. String.\n    shift = Number of positions up/down the alphabet to shift letters. Int.\n    encrypt = Direction of encryption. Boolean.\n\n    Examples:\n    -----------\n    cipher(\"testing\", 1)\n    \"uftjoh\"\n\n    cipher(\"uftjoh\", -1)\n    \"testing\"\n\n    cipher(\"uftjoh\", 1, encrypt = False)\n    \"testing\"\n    \"\"\"\n    alphabet = 'abcdefghijklmnopqrstuvwyzABCDEFGHIJKLMNOPQRSTUVWYZ'\n    new_text = ''\n    for c in text:\n        index = alphabet.find(c)\n        if index == -1:\n            new_text += c\n        else:\n            new_index = index + shift if encrypt == True else index - shift\n            new_index %= len(alphabet)\n            new_text += alphabet[new_index:new_index+1]\n    return new_text",
        "sha1": "e05a2c8da9e4428b414a7e89ed4dc409c045a2b6",
        "id": 126122
    },
    {
        "content": "def replace_if_none(to_be_checked, replacement_string):\n    \"\"\"Return a replacement is to be checked is empty (None or empty string)\"\"\"\n    if to_be_checked:\n        return to_be_checked\n    return replacement_string",
        "sha1": "677453b0e1d2faf312f26d4ad6b3b299495a3e2d",
        "id": 465349
    },
    {
        "content": "def askQuestionList(prompt, choices):\n    \"\"\"Ask a question with multiple choices, return the choice.\"\"\"\n    while True:\n        answer = input(\"%s [%s] \" % (prompt, '/'.join(choices)))\n        if answer.lower() in choices:\n            return answer.lower()\n        else:\n            print(\"Invalid choices. Please select one of the choices: \" + ', '.join(choices))",
        "sha1": "3d6ae0ac09508f6f936b402f289868abcbab9075",
        "id": 460901
    },
    {
        "content": "def get_image_url(date):\n  \"\"\" Return the URL to the frontpage of the NYT on a certain date.\n\n  :param date: a `datetime.date` object.\n  :returns: a `str`, the URL to the image of the frontpage of the NYT on the\n      requested date.\n  \"\"\"\n\n  _frontpage_url_template = ('http://www.nytimes.com/images'\n                             '/{year}/{month}/{day}/nytfrontpage/scan.jpg')\n  return _frontpage_url_template.format(\n      year=date.year,\n      month=str(date.month).zfill(2),\n      day=str(date.day).zfill(2),\n    )",
        "sha1": "3f845c695e2db210be032971cfd71dbc6db96c72",
        "id": 33870
    },
    {
        "content": "def _verify_names(sampler, var_names, arg_names):\n    \"\"\"Make sure var_names and arg_names are assigned reasonably.\n\n    This is meant to run before loading emcee objects into InferenceData.\n    In case var_names or arg_names is None, will provide defaults. If they are\n    not None, it verifies there are the right number of them.\n\n    Throws a ValueError in case validation fails.\n\n    Parameters\n    ----------\n    sampler : emcee.EnsembleSampler\n        Fitted emcee sampler\n    var_names : list[str] or None\n        Names for the emcee parameters\n    arg_names : list[str] or None\n        Names for the args/observations provided to emcee\n\n    Returns\n    -------\n    list[str], list[str]\n        Defaults for var_names and arg_names\n    \"\"\"\n    # There are 3 possible cases: emcee2, emcee3 and sampler read from h5 file (emcee3 only)\n    if hasattr(sampler, \"args\"):\n        num_vars = sampler.chain.shape[-1]\n        num_args = len(sampler.args)\n    elif hasattr(sampler, \"log_prob_fn\"):\n        num_vars = sampler.get_chain().shape[-1]\n        num_args = len(sampler.log_prob_fn.args)\n    else:\n        num_vars = sampler.get_chain().shape[-1]\n        num_args = 0  # emcee only stores the posterior samples\n\n    if var_names is None:\n        var_names = [\"var_{}\".format(idx) for idx in range(num_vars)]\n    if arg_names is None:\n        arg_names = [\"arg_{}\".format(idx) for idx in range(num_args)]\n\n    if len(var_names) != num_vars:\n        raise ValueError(\n            \"The sampler has {} variables, but only {} var_names were provided!\".format(\n                num_vars, len(var_names)\n            )\n        )\n\n    if len(arg_names) != num_args:\n        raise ValueError(\n            \"The sampler has {} args, but only {} arg_names were provided!\".format(\n                num_args, len(arg_names)\n            )\n        )\n    return var_names, arg_names",
        "sha1": "afdbcfe38e11c0a1c181ed0b6af1fe446b13b8b9",
        "id": 230833
    },
    {
        "content": "def _get_segmentation_strategy(segmentation):\n    \"\"\"Get the baci string for a geometry pair strategy.\"\"\"\n    if segmentation:\n        return 'segmentation'\n    else:\n        return 'gauss_point_projection_without_boundary_segmentation'",
        "sha1": "267a10a7237b2d42f1988ad9e35eddc7febbe789",
        "id": 645748
    },
    {
        "content": "import re\n\n\ndef is_valid_id(question_id: str) -> bool:\n    \"\"\"Checks whether a given question_id contains only accepted characters.\"\"\"\n    return not bool(re.search(r\"[^a-z0-9.]\", question_id))",
        "sha1": "5c3ad06636e678d568dbc7f49191a3514c9f22b7",
        "id": 521930
    },
    {
        "content": "def h2r(_hex):\n    \"\"\"\n    Convert a hex string to an RGB-tuple.\n    \"\"\"\n    if _hex.startswith('#'):\n        l = _hex[1:]\n    else:\n        l = _hex\n    return list(bytes.fromhex(l))",
        "sha1": "2cc6b758ef819b9e90b24e941e332ad610b76ecd",
        "id": 630699
    },
    {
        "content": "import re\n\n\ndef normalize_input_string(input):\n    \"\"\"Performs following actions on input, then returns the input:\n    - make all characters lowercase\n    - strip leading and trailing whitespace\n    - replace underscores with spaces\n    - substitute one or more whitespaces with a space\n    \"\"\"\n    input = input.lower().strip()\n    input = input.replace('_', ' ')\n    input = re.sub(r'\\s+', ' ', input)\n\n    return input",
        "sha1": "60dc84ee309c15d40b31d2504eb053cf197701a7",
        "id": 77822
    },
    {
        "content": "from typing import Set\nimport glob\n\n\ndef get_globbed(*file_patterns: str) -> Set[str]:\n  \"\"\"Returns the set of files returned from globbing `file_patterns`\"\"\"\n  return {filename\n          for pattern in file_patterns\n          for filename in glob.glob(pattern, recursive=True)}",
        "sha1": "eed238783d15d78c55388a0c2ada5eecd5047203",
        "id": 349339
    },
    {
        "content": "def identity(x):\n    \"\"\"\n    A named identity function is nicer than `lambda x: x`.\n    \"\"\"\n    return x",
        "sha1": "f99054c91cfd524aee1d196be8d70b506946d6a7",
        "id": 112850
    },
    {
        "content": "import random\n\n\ndef uniform_in_bounds(bounds):\n    \"\"\"Generates a random uniform sample between ``bounds``.\n\n    :param bounds: the bounds we must adhere to\n    :type bounds: dict {\"name\": [lb ub], ...}\n    \"\"\"\n    return map(random.uniform, *zip(*bounds.values()))",
        "sha1": "19d19e54fdc12de0f0facb4dabc13ca9edd20986",
        "id": 164564
    },
    {
        "content": "def getDayOfMonth(date):\n    \"\"\"Extracts the day of the month from a date. The first day of the\n    month is day 1.\n\n    Args:\n        date (Date): The date to use.\n\n    Returns:\n        int: An integer that is representative of the extracted value.\n    \"\"\"\n    return date.day",
        "sha1": "53d890d032be2acfea014212df6e8f0ca4eba5f9",
        "id": 638448
    },
    {
        "content": "def extract_ids(records):\n    \"\"\"\n    Given a list of records from a remote server, return a list of ids contained therein.\n    \"\"\"\n    ids = []\n    for record in records:\n        ids.append(record['id'])\n\n    return ids",
        "sha1": "28b63c521d499e02a5b78466e5fc4845bed0ed2a",
        "id": 588012
    },
    {
        "content": "def calcAcWcCorrelationAndDispersion( wcT1, acT2, acT3, wcT4, wcPrecision, acPrecision ):\n    \"\"\"\\\n    Returns correlation and dispersion at the time of the correlation given\n    t1, t2, t3, t4 from a clock sync request-response exchange and knowledge of\n    the precision of both clocks.\n    \n    :param wcT1: t1 measurement (of Wall clock) in nanoseconds\n    :param acT2: t2 measurement (of Arduino clock) in nanoseconds\n    :param acT3: t3 measurement (of Arduino clock) in nanoseconds\n    :param wcT4: t4 measurement (of Wall clock) in nanoseconds\n    :param wcPrecision: measurement precision of Wall Clock in nanoseconds\n    :param acPrecision: measurement precision of Arduino Clock in nanoseconds\n    \n    :returns: ( (acTimeNanos, wcTimeNanos), dispersionNanos )\n    \"\"\"\n    correlation = ( (acT2 + acT3) / 2.0, (wcT1 + wcT4) / 2)\n    rtt = (wcT4 - wcT1) - ( acT3 - acT2 )\n    dispersion = rtt / 2.0 + wcPrecision + acPrecision\n    return correlation, dispersion",
        "sha1": "ab62ea0850cba7aa6ae3e6b91f13d33fc4d69985",
        "id": 146231
    },
    {
        "content": "def is_status_creator(user, status=None):\n    \"\"\"\n    Check for ownership of the provided status.\n\n    Arguments:\n        user (User): The user for whom a permission check is being made.\n        status (UserTaskStatus): The status record to check ownership for.\n\n    Returns\n    -------\n        bool: True if no status is provided or the user owns the given status.\n\n    \"\"\"\n    if not status:\n        return True\n    return status.user_id == user.id",
        "sha1": "ccb55af9ef6a49d670b9e352f80900e233672a75",
        "id": 407407
    },
    {
        "content": "def yy2yyyy(yy):\n    \"\"\"Add '20' to timestamp.\n\n    E.g. '21081812' will become '2021081812'\n\n    Args:\n        yy (string)\n\n    Returns:\n        yyyy (string)\n\n    \"\"\"\n    return f\"20{yy}\"",
        "sha1": "aba042779f9af0c0105e13d10fc74fa56d7609dc",
        "id": 482974
    },
    {
        "content": "def likes(list_of_names: list) -> str:\n    \"\"\"\n    >>> likes([])\n    'no one likes this'\n    >>> likes([\"Python\"])\n    'Python likes this'\n    >>> likes([\"Python\", \"JavaScript\", \"SQL\"])\n    'Python, JavaScript and SQL like this'\n    >>> likes([\"Python\", \"JavaScript\", \"SQL\", \"JAVA\", \"PHP\", \"Ruby\"])\n    'Python, JavaScript and 4 others like this'\n    \"\"\"\n\n    if len(list_of_names) == 0:\n        return \"no one likes this\"\n    if len(list_of_names) == 1:\n        return f\"{list_of_names[0]} likes this\"\n    if len(list_of_names) == 2:\n        return f\"{list_of_names[0]} and {list_of_names[1]} like this\"\n    if len(list_of_names) == 3:\n        return (\n            f\"{list_of_names[0]}, {list_of_names[1]} and {list_of_names[2]} like this\"\n        )\n    else:\n        return f\"{list_of_names[0]}, {list_of_names[1]} and {len(list_of_names) - 2} others like this\"",
        "sha1": "20d46cfcd319b7bef3733a86aa6e57769381d67b",
        "id": 121882
    },
    {
        "content": "def mod_rec_exponentiate(number, exponent, mod):\n    \"\"\"\n    Modular exponentiation - recursive method\n    Complexity: O(logEXPONENT)\n\n    Sometimes, when the number can be extremely big, we find\n    the answer modulo some other number. We can do it in both\n    the recursive and the serial way. For simplicity we just\n    implement it in the recursive method.\n    \"\"\"\n    if exponent == 0:\n        return 1\n    elif exponent % 2 == 0:\n        return mod_rec_exponentiate((number**2) % mod, exponent//2, mod)\n    else:\n        return number * mod_rec_exponentiate((number**2) % mod, exponent//2, mod) % mod",
        "sha1": "d7f0af22a54abdc367d8bed12c0cffecad44d0db",
        "id": 119390
    },
    {
        "content": "def argmax(array):\n    \"\"\"\n    Returns the index of the biggest element in array.\n    \"\"\"\n    return array.index(max(array))",
        "sha1": "94bceb72fb36a5bd4e6aafbe3eb8b75ec1833a79",
        "id": 570028
    },
    {
        "content": "def is_self_loop(dst=None, src=None):\n    \"\"\"\n    Returns whether or not dst and src are the same and thus a self loop\n    \"\"\"\n    return dst == src",
        "sha1": "56964a090aba3bbd24141c02774910a31297f5b0",
        "id": 558501
    },
    {
        "content": "def reference(data, dim=\"f2\", old_ref=0, new_ref=0):\n    \"\"\"Function for referencing NMR spectra\n\n    Args:\n        data (DNPData): Data for referencing\n        dim (str): dimension to perform referencing down. By default this dimension is \"f2\".\n        old_ref (float): Value of old reference\n        new_ref (float): New reference value\n\n    Returns:\n        DNPData: referenced data\n    \"\"\"\n\n    data = data.copy()\n\n    data.coords[dim] -= old_ref - new_ref\n\n    return data",
        "sha1": "7f63b478ae524e2a4ccb6cc9626479efd182696b",
        "id": 183283
    },
    {
        "content": "def _convert_nested_dict_values_to_to_sets(nested_dict):\n    \"\"\"\n    Given a nested dictionary from kmer -> ID -> list of positions\n    convert it to a dictionary from kmer -> ID -> set of positions.\n\n    This simplifies code which wants to use these positions as a set\n    against which membership is repeatedly checked.\n    \"\"\"\n    return {\n        kmer: {\n            key: set(position_list)\n            for (key, position_list) in id_to_positions_dict.items()}\n        for (kmer, id_to_positions_dict)\n        in nested_dict.items()\n    }",
        "sha1": "db955fc5ab0b76d761bdb72ed41e901e3804e38c",
        "id": 92703
    },
    {
        "content": "def equilibrium_temperature(T_star, R_star, albedo, semi_major_axis):\n    \"\"\"\n    Calculate the equilibrium temperature of a planet.\n    \"\"\"\n    return T_star * (1 - albedo)**.25 * (R_star/(2*semi_major_axis))**.5",
        "sha1": "f6a4e3d662cfa772c0154bf065de25ef700a59cf",
        "id": 461498
    },
    {
        "content": "from typing import Any\nimport asyncio\nimport inspect\n\n\ndef isfuturistic(obj: Any) -> bool:\n    \"\"\"Returns True when obj is an asyncio Future or can be wrapped into one.\n\n    Futuristic objects can be passed into `asyncio.ensure_future` and methods\n    that accept awaitables such as `asyncio.gather` and `asyncio.wait_for`\n    without triggering a `TypeError`.\n    \"\"\"\n    return asyncio.isfuture(obj) or asyncio.iscoroutine(obj) or inspect.isawaitable(obj)",
        "sha1": "f9d4897efae43816b2f420b674557fe42b193f0b",
        "id": 560914
    },
    {
        "content": "def pad_pkcs7(key, block_len=16):\n    \"\"\"Pad given key using PKCS#7.\n    Args\n        key: byte array of key to pad\n        block_len: length of block to which to pad key\n    Returns\n        bytearray of padded key\n    \"\"\"\n    if block_len == len(key):\n        pad_len = block_len\n    else:\n        pad_len = block_len - len(key)\n    return key + bytearray([pad_len] * pad_len)",
        "sha1": "db54287440e859bc60ec497c9526947afa171688",
        "id": 203908
    },
    {
        "content": "from typing import Tuple\n\n\ndef cut_at_line(text: str, marker: str) -> Tuple[str, str]:\n    \"\"\"\n    Split text into two parts: up to the line with marker, and lines after.\n\n    If `marker` isn't in the text, return (\"\", text)\n    \"\"\"\n    lines = text.splitlines(keepends=True)\n    for i, line in enumerate(lines):\n        if marker in line:\n            return \"\".join(lines[: i + 1]), \"\".join(lines[i + 1 :])\n    return (\"\", text)",
        "sha1": "8724cb32b2293871321068769650014d2c2ce4b1",
        "id": 485003
    },
    {
        "content": "def Not(query):\n    \"\"\"The negation of a query\"\"\"\n    return '(NOT %s)' % (query,)",
        "sha1": "9bddebbd3c3c8e4c2e8e74b6c87049f0e4706dcb",
        "id": 635044
    },
    {
        "content": "import hashlib\n\n\ndef get_hash_str(value, alg='sha224'):\n    \"\"\"\n    Return hash from string\n    :param value: string\n    :param alg: algorithm, default sha224\n    :return: hash\n    \"\"\"\n    res = None\n\n    try:\n        if isinstance(value, str):\n            res = getattr(hashlib, alg)(value.encode('utf-8')).hexdigest()\n    except UnicodeEncodeError:\n        pass\n\n    return res",
        "sha1": "40a7291f00cf3f5acb2fa0d0354687dca051593d",
        "id": 150373
    },
    {
        "content": "def delSyno_no_tax(connection):\n    \"\"\"\n    Suppresses the synonyms which are not associated with any accepted taxa\n\n    Parameters:\n    -----------\n    connection: psycopg2 Connection\n        connection to the database\n    Returns:\n    --------\n    List of suppressed taxon identifiers\n    \"\"\"\n    cur=connection.cursor()\n    SQL= \"WITH a AS (SELECT t.cd_tax FROM taxon t LEFT JOIN taxon ta ON t.cd_syno=ta.cd_tax WHERE t.status='SYNONYM' AND ta.cd_tax IS NULL) DELETE FROM taxon WHERE cd_tax IN (SELECT cd_tax FROM a) RETURNING cd_tax\"\n    cur.execute(SQL)\n    res=cur.fetchall()\n    deleted=[r[0] for r in res]\n    cur.close()\n    connection.commit()\n    return deleted",
        "sha1": "1ae3afa2c3bf6094170d008aba007b2ad5f7b999",
        "id": 576022
    },
    {
        "content": "def is_there_overlap_wlimits(w1, w2):\n    \"\"\"Check if there is overlap between 2 consecutive telluric regions.\"\"\"\n    ret = False\n    for i in range(len(w1)-1):\n        if w2[i] >= w1[i+1]:\n            ret = True\n            break\n    return ret",
        "sha1": "b500e36b32254240d633ebc4af2f40a59220949d",
        "id": 439771
    },
    {
        "content": "def shift_leftward(n, nbits):\n    \"\"\"Shift positive left, or negative right.  Same as n * 2**nbits\"\"\"\n    if nbits < 0:\n        return n >> -nbits\n    else:\n        return n << nbits",
        "sha1": "d9f83bb8a3c6389d9b56ff4b2397f42ecabb327a",
        "id": 465730
    },
    {
        "content": "def calc_absent(marks):\n    \"\"\" Function which returns the count of absent students. \"\"\"\n    absent_count = 0\n    for mark in marks:\n        if mark == 0:\n            absent_count += 1\n\n    return absent_count",
        "sha1": "444c56dcabe4824c76f44bf07119fb14eedec15f",
        "id": 38514
    },
    {
        "content": "def read_D(filename):\n    \"\"\"reads an upper triangular matrix with phylogentic distances\n       between species.\n       Returns a dictionary with distances for species names.\"\"\"\n    D = {}\n    f = open(filename)\n    cols = f.readline().rstrip().split('\\t')\n    for line in f:\n        elems = line.strip().split('\\t')\n        for i,e in enumerate(elems):\n            if e!='' and i>0:\n                D[(cols[i],elems[0])] = float(e)\n                D[(elems[0],cols[i])] = float(e)\n    f.close()\n    return D",
        "sha1": "f542d939ee1ceed29c4265a1f77c2ccb5c99d00a",
        "id": 676411
    },
    {
        "content": "def parse_utf(s):\n    \"\"\" Decode a utf-8 encoded string. \"\"\"\n    return s.strip().decode('utf-8') or None",
        "sha1": "933319627f7b0905af2b2b7a79080fbff4d4b033",
        "id": 285917
    },
    {
        "content": "def check_utilization(nn, np, ppn, threshold=0.9, name=None):\n    \"\"\"Check whether the calculated node utilization is below threshold.\n\n    This function raises a :class:`RuntimeError` if the calculated\n    node utilization is below the given threshold or if the number\n    of calculated required nodes is zero.\n\n    :param nn:\n        Number of requested nodes.\n    :param np:\n        Number of required processing units (e.g. CPUs, GPUs).\n    :param ppn:\n        Number of processing units available per node.\n    :param threshold:\n        The minimally required node utilization.\n    :param name:\n        A human-friendly name for the tested processing unit\n        to be used in the error message, for example: CPU or GPU.\n    :returns:\n        The number of calculated nodes.\n    :raises RuntimeError:\n        Raised if the node utilization is below the given threshold.\n    \"\"\"\n    if not (0 <= threshold <= 1.0):\n        raise ValueError(\"The value for 'threshold' must be between 0 and 1.\")\n\n    # Zero nodes are just returned and possible utilization or validation checks\n    # must be performed elsewhere.\n    if nn == 0:\n        return 0\n\n    # The utilization is the number of processing units (np) required divided\n    # by the product of the number of nodes (nn) and the number of processing\n    # units per node (ppn).\n    utilization = np / (nn * ppn)\n\n    # Raise RuntimeError if the utilization is below the specified threshold.\n    if utilization < threshold:\n        raise RuntimeError(\n            \"Low{name} utilization warning: {util:.0%}\\n\"\n            \"Total resources requested would require {nn} node(s), \"\n            \"but each node supports up to {ppn}{name} task(s).\\n\"\n            \"Requesting {np} total{name} task(s) would result in node underutilization. \"\n            \"Use --force to ignore the warning, but users are encouraged to use --pretend to \"\n            \"confirm that the submission script fully utilizes the compute resources before \"\n            \"force submission\" .format(\n                util=utilization, np=np, nn=nn, ppn=ppn,\n                name=' {}'.format(name) if name else ''))\n\n    # Everything fine, return number of nodes (nn).\n    return nn",
        "sha1": "a9d6f22ecca25d663bf8abd179f2f473debf5eea",
        "id": 284716
    },
    {
        "content": "def emu_to_px(x: int) -> int:\n    \"\"\"Converts EMU to pixels\n\n    :param x: EMU\n    :type x: int\n    :return: Pixels\n    :rtype: int\n\n    \"\"\"\n    return int(x * 220 / (914400))",
        "sha1": "22fb4589bc9fbe4d3d9f6b3033cc2efca72b1285",
        "id": 131344
    },
    {
        "content": "def to_int(s, default=None):\n    \"\"\"Attempts to convert the provided string to an integer.\n\n    :param s: The text to convert\n    :param default: Default value to return if cannot be converted\n    :returns: The integer if converted, otherwise the default value\n    \"\"\"\n    try:\n        return int(s)\n    except ValueError:\n        pass\n\n    return default",
        "sha1": "9f8214efc65035b433af22e9872cb4fe1e4e1cf7",
        "id": 700408
    },
    {
        "content": "import re\n\n\ndef _strip_newlines_from_pnx_string(pnx_string):\n    \"\"\" The original pnx record contains \"\\n\", which cause problems for ElementTree, so we must strip them \"\"\"\n    pnx_string_without_newlines = re.sub('\\n', '', pnx_string)\n    return(pnx_string_without_newlines)",
        "sha1": "f43db5a32890addfd2b8c6e92441399da6f753f0",
        "id": 296524
    },
    {
        "content": "from typing import Iterable\nfrom typing import Any\n\n\ndef superposicion_basico(lista_1: Iterable[Any], lista_2: Iterable[Any]) -> bool:\n    \"\"\"Toma dos listas y devuelve un booleano en base a si tienen al menos 1\n    elemento en com\u00fan.\n\n    Restricci\u00f3n: Utilizar bucles anidados.\n    \"\"\"\n    for elem in lista_1:\n        for elem_ in lista_2:\n            if elem == elem_:\n                return True\n    return False",
        "sha1": "07647cb383a39a3da34d5c3b5430ade126822d25",
        "id": 294315
    },
    {
        "content": "def cal_path_distance(graph, path):\n    \"\"\"\n    calculate path distance.\n    \"\"\"\n    distance = 0.0\n\n    for i, station in enumerate(path):\n        if i == 0:\n            continue\n\n        distance += graph.edges[path[i-1], path[i]]['distance']\n\n    return distance",
        "sha1": "fcc352b709819381275099370dac4e77ecfbb76a",
        "id": 185892
    },
    {
        "content": "def get_highest_bench_points(bench_points):\n    \"\"\"\n    Returns a tuple of the team with the highest scoring bench\n    :param bench_points: List [(team_name, std_points)]\n    :return: Tuple (team_name, std_points) of the team with most std_points\n    \"\"\"\n    max_tup = (\"team_name\", 0)\n    for tup in bench_points:\n        if tup[1] > max_tup[1]:\n            max_tup = tup\n    return max_tup",
        "sha1": "26cd6fd04f29bdd87e24fa6df92d971e833cab54",
        "id": 562427
    },
    {
        "content": "import re\n\n\ndef construct_metadata_url(infer_url: str) -> str:\n    \"\"\"Construct v2 metadata endpoint from v2 infer endpoint\"\"\"\n    return re.sub(r\"/infer$\", \"\", infer_url)",
        "sha1": "59559d4de4c8cd59ad9bcecd307efcacecc7a656",
        "id": 182276
    },
    {
        "content": "def list_to_err(errs):\n    \"\"\"convert list of error strings to a single string\n\n    :param errs: list of string errors\n    :type errs: [str]\n    :returns: error message\n    :rtype: str\n    \"\"\"\n\n    return str.join('\\n\\n', errs)",
        "sha1": "a01e12d282ed3419f1c3cccf4dbfc70704c32efe",
        "id": 675067
    },
    {
        "content": "def attach(data, session):\n    \"\"\"\n    Add all data objects to a database session.\n\n    \"\"\"\n    instances = []\n    for name in dir(data):\n        if name.startswith('_'):\n            continue\n\n        instance = getattr(data, name)\n        session.add(instance)\n        instances.append(instance)\n\n    return data",
        "sha1": "b1660d825e278166e19fe5262009901aefa8347e",
        "id": 167199
    },
    {
        "content": "def user(request):\n    \"\"\"The user to use for fetching.\"\"\"\n    return request.config.getoption('--user')",
        "sha1": "b19ca8c5b90e1fd8fff8fbbce1b15dcc32ef0cd7",
        "id": 612696
    },
    {
        "content": "import itertools\n\n\ndef flatten_metas(meta_iterables):\n    \"\"\"\n    Take a collection of metas, and compose/flatten/project into a single list.\n\n    For example:\n\n        A: pkg1, pkg2a\n        B: pkg2b, pkg3\n\n        Flattened([A, B]) => [pkg1, pkg2a, pkg3]\n        Flattened([B, A]) => [pkg1, pkg2b, pkg3]\n\n    The resulting list of metas will not be ordered in any particular way.\n\n    \"\"\"\n    visited = {}\n    for metas in meta_iterables:\n        visited_this_depth = {}\n        for meta in metas:\n            if meta.name() not in visited:\n                visited_this_depth.setdefault(meta.name(), []).append(meta)\n        for name, metas in visited_this_depth.items():\n            visited.setdefault(name, []).extend(metas)\n    return itertools.chain.from_iterable(visited.values())",
        "sha1": "4125a7bea44989140909e91392cd3adb740e26f1",
        "id": 47004
    },
    {
        "content": "import torch\n\n\ndef l2_penalty(var):\n    \"\"\"\n    L2/Ridge regularization penalty.\n\n    Parameters\n    ----------\n    var : torch.Variable\n        Torch variable representing the weight matrix over which the penalty\n        should be computed\n\n    Returns\n    -------\n    penalty : torch.Variable\n        Torch variable containing the penalty as a single floating point value\n\n    Notes\n    -----\n    The penalty is derived from the L2-norm, which has a square root. The exact\n    optimization can also be done without the square root, but this makes no\n    difference in the actual output of the optimization because of the scaling\n    factor used along with the penalty.\n\n    \"\"\"\n    return torch.sqrt(torch.pow(var, 2).sum())",
        "sha1": "2952721742a3acfcd7d8645f95da7955ef5f4f06",
        "id": 596819
    },
    {
        "content": "def comment_cleanup(lines):\n    \"\"\"Cleans-up comments and empty lines from textual data read from files.\"\"\"\n\n    no_comments = [k.partition(\"#\")[0].strip() for k in lines]\n    return [k for k in no_comments if k]",
        "sha1": "5b4d07542435e4c39f33a6bf3d011031c764d258",
        "id": 140638
    },
    {
        "content": "def greet(name: str) -> str:\n    \"\"\"Greet in Spanish\"\"\"\n\n    return f\"\u00a1Hola, {name}!\"",
        "sha1": "66db38b28132feb2df4c4ce58f84da99625d008b",
        "id": 276574
    },
    {
        "content": "def pddl_to_tarski_type(typename):\n    \"\"\" Translate a few PDDL types into their corresponding Tarski names\n        (e.g. the FSTRIPS type \"int\" corresponds to the Tarski type \"Integer\").\n    \"\"\"\n    translations = {\"int\": \"Integer\", \"real\": \"Real\", \"number\": \"Real\"}\n    return translations.get(typename, typename)",
        "sha1": "ea97590ce785abd63d5d2a091ff4b9780f48fd28",
        "id": 281144
    },
    {
        "content": "from typing import List\nimport re\n\n\ndef split_sequence(sequence: str, separator: str, *args, **kwargs) -> List[str]:\n    \"\"\"\n    Take a sequence as string and split it with the provided regex separator\n    Sequence, separator, args and kwargs are passed to re.split() method.    \n    \n    :param sequence: Sequence to split\n    :param separator: Regex used to split sequence\n    :return: List of token in the sequence\n    \"\"\"\n\n    tokens = re.split(separator, sequence, *args, **kwargs)\n    if len(tokens) == 0:\n        raise ValueError(\"No tokens are returned after split\")\n    else:\n        return tokens",
        "sha1": "372e35f4fbe1b0b1fadf2388b5324ee9708fa7cd",
        "id": 529518
    },
    {
        "content": "def _parse_settings_args(args: list):\n    \"\"\" Parse settings command args and get settings option and signal. \"\"\"\n    if not args:\n        return 'common', []  # 'common' is the option, when user does not use any option.\n    else:\n        try:\n            option, signal, *_ = args\n        except ValueError:\n            return args[0], ''\n    return option, signal",
        "sha1": "1fdd3e124d7a7b9a456533593b9768814c6553c1",
        "id": 96453
    },
    {
        "content": "def estimate_probability(word, previous_n_gram, \n                         n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=1.0):\n    \"\"\"\n    Estimate the probabilities of a next word using the n-gram counts with k-smoothing\n    \n    Args:\n        word: next word\n        previous_n_gram: A sequence of words of length n\n        n_gram_counts: Dictionary of counts of n-grams\n        n_plus1_gram_counts: Dictionary of counts of (n+1)-grams\n        vocabulary_size: number of words in the vocabulary\n        k: positive constant, smoothing parameter\n    \n    Returns:\n        A probability\n    \"\"\"\n    # convert list to tuple to use it as a dictionary key\n    previous_n_gram = tuple(previous_n_gram)\n    #print(previous_n_gram)\n    \n    # Set the denominator\n    # If the previous n-gram exists in the dictionary of n-gram counts,\n    # Get its count.  Otherwise set the count to zero\n    # Use the dictionary that has counts for n-grams\n    previous_n_gram_count = n_gram_counts.get(previous_n_gram,0)\n        \n    # Calculate the denominator using the count of the previous n gram\n    # and apply k-smoothing\n    denominator = previous_n_gram_count + vocabulary_size*k\n\n    # Define n plus 1 gram as the previous n-gram plus the current word as a tuple\n    n_plus1_gram = previous_n_gram + (word,)\n    #print(n_plus1_gram)\n  \n    # Set the count to the count in the dictionary,\n    # otherwise 0 if not in the dictionary\n    # use the dictionary that has counts for the n-gram plus current word\n    n_plus1_gram_count = n_plus1_gram_counts.get(n_plus1_gram,0)\n        \n    # Define the numerator use the count of the n-gram plus current word,\n    # and apply smoothing\n    numerator = n_plus1_gram_count + k\n\n    # Calculate the probability as the numerator divided by denominator\n    probability = numerator/denominator\n    \n    return probability",
        "sha1": "69d680ef3f833a823d3773293dd595c5530d809c",
        "id": 656798
    },
    {
        "content": "import hashlib\n\n\ndef hash160(msg):\n    \"\"\"RIPEMD160(SHA256(msg)) -> bytes\"\"\"\n    h = hashlib.new('ripemd160')\n    h.update(hashlib.sha256(msg).digest())\n    return h.digest()",
        "sha1": "98d46a3ad11f83e3dc7f46789e0dc936df7d46a3",
        "id": 595711
    },
    {
        "content": "def odd_even(x):\n    \"\"\"Classify a number as odd or even.\n    \n    >>> odd_even(4)\n    'even'\n    >>> odd_even(3)\n    'odd'\n    \"\"\"\n    if x % 2==0:\n        return 'even'\n    else:\n        return 'odd'",
        "sha1": "048007a22ba02aa985f6a4a9a687baea35bbb143",
        "id": 363650
    },
    {
        "content": "def get_file_name(input_file_name):\n    \n    \"\"\"\n\n    Parses apart the pathway from the raw file name, so the file name can be passed\n    to the rest of the program as a simple variable\n\n    : Param input_file_name: Full pathway of file being analyzed\n    \n    : Return: The file name with pathway removed\n   \n    \"\"\"\n\n    input_file_name = input_file_name.split(\"\\\\\")\n    input_file_name = input_file_name[-1]\n    input_file_name = input_file_name.split(\"/\")\n    file_name = input_file_name[-1]\n    return (file_name)",
        "sha1": "b6bbfd12584acd0ace89e10759c60d43687b9510",
        "id": 336157
    },
    {
        "content": "def make_alignment_line(strand, kmer, prob, event):\n    \"\"\"Convert strand, kmer, probability and event to a correctly formatted alignment file line\n    :param strand: 't' or 'c' representing template or complement strand of read\n    :param kmer: nucleotide kmer\n    :param prob: probability of kmer coming from certain event\n    :param event: mean of the corresponding event\n    :return: correctly formatted alignment line\n    \"\"\"\n    assert strand in ['c', 't'], \"Strand must be either 'c' or 't'. strand: {}\".format(strand)\n    assert isinstance(prob, float), \"probability must be a float: prob {}\".format(prob)\n    assert isinstance(kmer, str), \"kmer must be a string: kmer {}\".format(kmer)\n    assert isinstance(event, float), \"event must be a float: event {}\".format(event)\n    entry_line = \"blank\\t0\\tblank\\tblank\\t{strand}\\t0\\t0.0\\t0.0\\t0.0\\t{kmer}\\t0.0\\t0.0\\t{prob}\\t{event}\\t0.0\\n\"\n    return entry_line.format(strand=strand, kmer=kmer, prob=prob, event=event)",
        "sha1": "551f3739bc46aae88d266605604ec12a0f5a1e4f",
        "id": 443757
    },
    {
        "content": "def last_index(S, char):\n    \"\"\"Return the HIGHEST index in S where character char is found. Kind\n    of like str.find(sub) but starts from the end of the string,\n    instead of the beginning.\n    \"\"\"\n    indices = list(range(len(S)))\n    indices.reverse() # Like [4, 3, 2, 1, 0]\n    for i in indices:\n        if S[i] == char:\n            return i\n    return -1",
        "sha1": "6f419947b9c271dbcbaa319b1decfb55036365ae",
        "id": 364202
    },
    {
        "content": "def union(bbox1, bbox2):\n    \"\"\"Create the union of the two bboxes.\n\n    Parameters\n    ----------\n    bbox1\n        Coordinates of first bounding box\n    bbox2\n        Coordinates of second bounding box\n\n    Returns\n    -------\n    [y0, y1, x0, x1]\n        Coordinates of union of input bounding boxes\n\n    \"\"\"\n    y0 = min(bbox1[0], bbox2[0])\n    y1 = max(bbox1[1], bbox2[1])\n    x0 = min(bbox1[2], bbox2[2])\n    x1 = max(bbox1[3], bbox2[3])\n    return [y0, y1, x0, x1]",
        "sha1": "0cb11ca0925bfbb191070b701e032abeca32eea5",
        "id": 690529
    },
    {
        "content": "def build_prefix_table(pattern):\n\t\"\"\"\n\tHelper function to build a prefix table - stores length of longest prefix suffix ending at each index in pattern\n\n\tTime complexity: O(n), n = Length of pattern\n\n\tSpace complexity: O(n), n = Length of pattern\n\t\"\"\"\n\n\tprefix_table = []\n\tif len(pattern) > 0:\n\t\tprefix_table.append(0)\n\t\tprevious_length = 0\n\t\ti = 1\n\t\twhile i < len(pattern):\n\t\t\tif pattern[i] == pattern[previous_length]:\n\t\t\t\tprefix_table.append(previous_length + 1)\n\t\t\t\tprevious_length += 1\n\t\t\t\ti += 1\n\t\t\telse:\n\t\t\t\tif previous_length != 0:\n\t\t\t\t\tprevious_length = prefix_table[previous_length - 1]\n\t\t\t\telse:\n\t\t\t\t\tprefix_table.append(0)\n\t\t\t\t\ti += 1\n\treturn prefix_table",
        "sha1": "a02317edf0a22d0226b9ded54af914213f7996a6",
        "id": 71171
    },
    {
        "content": "import resource\n\n\ndef used_memory(unit: int = 1024 * 1024 * 1024) -> float:\n    \"\"\"\n    Get the memory usage of the current process.\n\n    :param unit: Unit of the memory, default in Gigabytes.\n    :return: Memory usage of the current process.\n    \"\"\"\n\n    return resource.getrusage(resource.RUSAGE_SELF).ru_maxrss / unit",
        "sha1": "8b7ebdbdc0cac7a754d110d2ad805128578128cc",
        "id": 261998
    },
    {
        "content": "def ssh_no_error(ssh, cmd, sudo=False):\n    \"\"\"Execute a command over ssh channel, and log and exit if the command\n    fails.\n\n    :param ssh: SSH() object connected to a node.\n    :param cmd: Command line to execute on remote node.\n    :param sudo: Run command with sudo privileges.\n    :type ssh: SSH() object\n    :type cmd: str\n    :type sudo: bool\n    :returns: stdout from the SSH command.\n    :rtype: str\n    :raises RuntimeError: In case of unexpected ssh command failure\n    \"\"\"\n    if sudo:\n        ret, stdo, stde = ssh.exec_command_sudo(cmd, timeout=60)\n    else:\n        ret, stdo, stde = ssh.exec_command(cmd, timeout=60)\n\n    if ret != 0:\n        print('Command execution failed: \"{}\"'.format(cmd))\n        print('stdout: {0}'.format(stdo))\n        print('stderr: {0}'.format(stde))\n        raise RuntimeError('Unexpected ssh command failure')\n\n    return stdo",
        "sha1": "1f674e9e4cea60605a815e19f3b7ab6288e7993e",
        "id": 378199
    },
    {
        "content": "import re\n\n\ndef is_cpc_code (item):\n    \"\"\"Check if an item is a Cooperative Patent Classification code.\n    Should also work for IPC codes because they have same format.\n\n    Examples:\n    H04W52/00 => True\n    H04W => False\n    H04W005202 => False\n    \n    Args:\n        item (str): String to be checked.\n    \n    Returns:\n        bool: True if input string is a CPC code, False otherwise.\n    \"\"\"\n    if not isinstance(item, str):\n        return False\n    pattern = r'^[ABCDEFGHY]\\d\\d[A-Z]\\d+\\/\\d+$'\n    return True if re.fullmatch(pattern, item) else False",
        "sha1": "d536ee4e1f53ced0119b9dc28c62296fe1807eb8",
        "id": 537756
    },
    {
        "content": "def steps(number: int) -> int:\n    \"\"\"Return the number of steps to reach 1 for Collatz Conjecture\"\"\"\n    if number < 1:\n        raise ValueError(f\"{number} is not a natural number\")\n\n    _n = number\n    count = 0\n\n    while _n != 1:\n        count += 1\n        if _n % 2 == 0:\n            _n /= 2\n        elif _n != 1:\n            _n = _n*3 + 1\n\n    return count",
        "sha1": "41b1f36e66fce581d89bd3570ca20f083c4a9883",
        "id": 78477
    },
    {
        "content": "def end_chat(input_list):\n    \"\"\" End chat \n   \n    Parameters\n    ----------\n    input_list : list\n        List containing 'quit' to end chat.\n    \n    Returns\n    -------\n    True or False : boolean\n        Boolean assures whether to end chat based on whether the input contains 'quit'.\n   \"\"\"\n    \n    if 'quit' in input_list:\n        return True\n    else: \n        return False",
        "sha1": "bcf96aa2d3dc43c1d8464b9775c7450ebcf7984b",
        "id": 665045
    },
    {
        "content": "def _match_hash(x, y, no_match=None):\n    \"\"\"Returns an array of the positions of (first) matches of y in x\n    \n    This is similar to R's `match` or Matlab's `[Lia, Locb] = ismember`\n    \n    As in R's `match`, a hash (i.e. dictionary) is built to do the\n    mapping.\n    \"\"\"\n\n    x_val2ind = {v: i for i, v in enumerate(x)}\n    y_ind = [x_val2ind.get(v, no_match) for v in y]\n\n    return y_ind",
        "sha1": "9c339d45622d0b15316887bd61aab434463477f6",
        "id": 603677
    },
    {
        "content": "import torch\n\n\ndef gelu_impl(x):\n     \"\"\"OpenAI's gelu implementation.\"\"\"\n     return 0.5 * x * (1.0 + torch.tanh(0.7978845608028654 * x *\n                                        (1.0 + 0.044715 * x * x)))",
        "sha1": "fe04756f5ca215586e582f3ac00ac1e4d85910ac",
        "id": 464320
    },
    {
        "content": "def sortedby(item_list, key_list, reverse=False):\n    \"\"\" sorts ``item_list`` using key_list\n\n    Args:\n        list_ (list): list to sort\n        key_list (list): list to sort by\n        reverse (bool): sort order is descending (largest first)\n                        if reverse is True else acscending (smallest first)\n\n    Returns:\n        list : ``list_`` sorted by the values of another ``list``. defaults to\n        ascending order\n\n    SeeAlso:\n        sortedby2\n\n    Examples:\n        >>> list_    = [1, 2, 3, 4, 5]\n        >>> key_list = [2, 5, 3, 1, 5]\n        >>> result = sortedby(list_, key_list, reverse=True)\n        >>> print(result)\n        [5, 2, 3, 1, 4]\n\n    \"\"\"\n    assert len(item_list) == len(key_list), (\n        'Expected same len. Got: %r != %r' % (len(item_list), len(key_list)))\n    sorted_list = [item for (key, item) in\n                   sorted(list(zip(key_list, item_list)), reverse=reverse)]\n    return sorted_list",
        "sha1": "8efca95c32256e3f7b96c9e101f3d7b596bb572f",
        "id": 664531
    },
    {
        "content": "def convert_identifier(identifier):\n    \"\"\"\n    old:\n        identifier=='bg, a, ...'\n        return  1\n\n    identifier== bu-FD-J, 51234, 13212-01\n    return bu-FD-J, 51234, 13212\n\n    \"\"\"\n    if \"-\" in identifier:\n        ln = identifier.split(\"-\")[0]\n        try:\n            ln = int(ln)\n            identifier = str(ln)\n        except ValueError:\n            return identifier\n\n    return identifier",
        "sha1": "790bc0da8e91943d656f65d53b75efc6704aa3d8",
        "id": 396472
    },
    {
        "content": "def merge_relevancies(a, b):\n    \"\"\"\n    The second element of the returned tuple is True\n    iff the relevancies are compatible and can be merged.\n    Two relevancies are incompatible if one of them is a\n    \"strong\" one, i.e. either \"harmful\" or \"vital\", and\n    the other is \"weak\" and has the opposite sign (\"relevant\"\n    and \"irrelevant\" respectively).\n\n    If a and b are compatible, then the first element of the tuple is\n    the resulting relevance. The less intuitive merge results are:\n      strong, weak -> strong\n      relevant, irrelevant -> relevant\n    \"\"\"\n    RELEVANCIES = [\"harmful\", \"irrelevant\", \"relevant\", \"vital\"]\n    id_a = RELEVANCIES.index(a)\n    id_b = RELEVANCIES.index(b)\n    if id_a > id_b:\n        a, b = b, a\n        id_a, id_b = id_b, id_a\n    if id_a == 0 and id_b <= 1:\n        return a, True\n    if id_a == 1 and id_b <= 2:\n        return b, True\n    if id_a > 1:\n        return b, True\n    return a, False",
        "sha1": "57d75c91ef2f7594c3c9c6c72b2672e216f86eed",
        "id": 398969
    },
    {
        "content": "def is_local_variable(variable_name):\n    \"\"\"\n    Is a variable name a local or fully qualified?\n\n    >>> is_local_variable('FOO')\n    True\n    >>> is_local_variable('HH__FOO')\n    False\n\n    :param variable_name: str\n    :return: bool\n    \"\"\"\n    return '__' not in variable_name",
        "sha1": "4da43b9a572304ce14d16a656094f75dca7f452e",
        "id": 129280
    },
    {
        "content": "def check_required_hash(md5_hash):\n    \"\"\"Check if the Md5 hash satisfies the required conditions\"\"\"\n\n    # Check if MD5 hash starts with 5 zeroes\n    if md5_hash.find('00000') == 0:\n        return True\n    return False",
        "sha1": "3854fdcdcf5fc602617f15736b354b8c95dcc108",
        "id": 392363
    },
    {
        "content": "import fnmatch\n\n\ndef is_copy_only_path(path, context):\n    \"\"\"Check whether the given `path` should only be copied and not rendered.\n\n    Returns True if `path` matches a pattern in the given `context` dict,\n    otherwise False.\n\n    :param path: A file-system path referring to a file or dir that\n        should be rendered or just copied.\n    :param context: cookiecutter context.\n    \"\"\"\n    try:\n        for dont_render in context['cookiecutter']['_copy_without_render']:\n            if fnmatch.fnmatch(path, dont_render):\n                return True\n    except KeyError:\n        return False\n\n    return False",
        "sha1": "9dd45350a0a4df42923e50c06aca110fbcf40f38",
        "id": 668521
    },
    {
        "content": "def safeRemoveItemsFromList(listWithItems, itemsToBeRemoved):\n    \"\"\"Removes items from other list with check if it is inside.\n\n    :param list listWithItems: list to be updated\n    :param list itemsToBeRemoved: items to be removed\n    :return: list without items\n    :rtype: list\n\n    >>> safeRemoveItemsFromList([1, 2, 3, 4], [2, 5])\n    [1, 3, 4]\n\n    \"\"\"\n    if itemsToBeRemoved:\n        for item in itemsToBeRemoved:\n            if item in listWithItems:\n                listWithItems.remove(item)\n\n    return listWithItems",
        "sha1": "aec3252773342a838f617d7a0efbb137a90708e2",
        "id": 343564
    },
    {
        "content": "def format_header(header_values):\n    \"\"\"\n    Formats a row of data with bolded values.\n\n    :param header_values: a list of values to be used as headers\n    :return: a string corresponding to a row in enjin table format\n    \"\"\"\n    header = '[tr][td][b]{0}[/b][/td][/tr]'\n    header_sep = '[/b][/td][td][b]'\n    return header.format(header_sep.join(header_values))",
        "sha1": "5b7cd734a486959660551a6d915fbbf52ae7ef1e",
        "id": 6258
    },
    {
        "content": "def map_float(min_input, max_input, min_output, max_output, value):\n    \"\"\"Map a value from one range to another.\"\"\"\n    return min_output + (max_output - min_output) * (value - min_input) / (max_input - min_input)",
        "sha1": "8058082abdc80c41e6586db4acf981d2f8cf9458",
        "id": 247712
    },
    {
        "content": "def remove_prefix(string: str, prefix: str):\n    \"\"\"\n    Removes a prefix from a string if present.\n\n    Args:\n        string (`str`): The string to remove the prefix from.\n        prefix (`str`): The prefix to remove.\n\n    Returns:\n        The string without the prefix.\n    \"\"\"\n    return string[len(prefix) :] if string.startswith(prefix) else string[:]",
        "sha1": "598e1b9b863d342e757e54cf94035da63e3ace1f",
        "id": 11990
    },
    {
        "content": "def water_concentration(T):\n    \"\"\"Return water concentration for temperature range 253.15K < T < 383.15K.\n    Uses interpolation equation (eq. 2) for specific volume found in\n    the doucment by The International Association for the Properties of\n    Water and Steam from 2011\n    <http://www.iapws.org/relguide/LiquidWater.pdf>`__.\n    Pressure = 0.1 MPa\n\n    Parameters\n    ----------\n    T : float\n        Temperature in K\n\n    Returns\n    -------\n    conc : float\n        Water conentration at T in mol/l\n\n    Examples\n    --------\n    >>> round(water_concentration(273.15), 3)\n    55.498\n    >>> round(water_concentration(298.15), 3)\n    55.343\n    \"\"\"\n    if not 253.15 <= T <= 383.15:\n        raise ValueError(\"Temperature is outside of allowed range.\")\n    p0 = 10.0**5    # Pa\n    R = 8.31464     # J/mol/K\n    Tr = 10.0\n    Ta = 593.0\n    Tb = 232.0\n    a = [1.93763157E-2,\n         6.74458446E+3,\n        -2.22521604E+5,\n         1.00231247E+8,\n        -1.63552118E+9,\n         8.32299658E+9]\n    b = [5.78545292E-3,\n        -1.53195665E-2,\n         3.11337859E-2,\n        -4.23546241E-2,\n         3.38713507E-2,\n        -1.19946761E-2]\n    n = [None, 4., 5., 7., 8., 9.]\n    m = [1., 2., 3., 4., 5., 6.]\n    def alpha(T):\n        return Tr/(Ta - T)\n    def beta(T):\n        return Tr/(T - Tb)\n    coef = a[0] + b[0]*beta(T)**m[0]\n    for i in range(1, 6):\n        coef += a[i]*alpha(T)**n[i] + b[i]*beta(T)**m[i]\n    v0 = R*Tr/p0*coef  # m3/mol\n    return 1/(v0*1000)    # mol/L",
        "sha1": "4bfc18af868fb02864cdd0bdbd32e656c3bc2934",
        "id": 398591
    },
    {
        "content": "def date_fmt(date: str):\n    \"\"\"Convert m/d/y to Y-M-D\"\"\"\n    try:\n        month, day, year = date.split(\"/\")\n        return f\"{year}-{month}-{day}\"\n\n    except:\n        return None",
        "sha1": "051f7d60d227f0daa4fafbb3ce21678b459161c2",
        "id": 384378
    },
    {
        "content": "def translate_coord_to_move(row, column):\n    \"\"\"\n    Translates a pair of board indices to a move on a board. For example: (0,0) ==> {'Row':A,'Column':1}\n    :param row: 1st index\n    :param column: 2nd index\n    :return: dict of the row and column.\n    \"\"\"\n    return {\"Row\": chr(row + 65), \"Column\": (column + 1)}",
        "sha1": "de2f0bba20549d5b480fa2552180caaaf26e36d1",
        "id": 146657
    },
    {
        "content": "def bytes_2_str(bytes_str):\n    \"\"\"\n    \u7528\u4e8e\u5c06bytes \u7c7b\u578b\u8f6c\u6362\u4e3a str \u7c7b\u578b\n    :param bytes_str:\n    :return:\n    \"\"\"\n    return str(bytes_str, encoding='GBK')",
        "sha1": "c7e37a63cb6af5dfc695ef986bba5d2a8c8720cd",
        "id": 593342
    },
    {
        "content": "def parse_sources(source):\n    \"\"\"\n    Parse the input string for sources and separate based on config file type.\n\n    Parameters\n    ----------\n    source : str\n        Input source specification.\n\n    Returns\n    -------\n    sources : dict\n        Dictionary of parse sources. Keys are config file names.\n    \"\"\"\n    sources = {\"setup.cfg\": [], \"pyproject.toml\": []}\n    valid_sources = {\n        \"install\": {\"file\": \"setup.cfg\", \"option\": \"install_requires\"},\n        \"extras\": {\"file\": \"setup.cfg\", \"option\": \"options.extras_require\"},\n        \"build\": {\"file\": \"pyproject.toml\", \"option\": \"build-system\"},\n    }\n    for entry in sorted(source.strip().split(\",\")):\n        if entry not in valid_sources:\n            raise ValueError(\n                f\"Invalid source '{entry}'. Must be one of {list(valid_sources.keys())}.\"\n            )\n        sources[valid_sources[entry][\"file\"]].append(valid_sources[entry][\"option\"])\n    return sources",
        "sha1": "fa4c3fef5c88f865ca2c2e03192bc29d7336a3df",
        "id": 137850
    },
    {
        "content": "def zif_bank_code(zif_pinout: list):\n    \"\"\"\n    Return a list encoding even/odd ZIF pins in the data acquisition order.\n    Empirically, this has been useful for finding differential offsets in array maps\n    (although the ZIF even-/odd-ness tends to overlap with other pin breakouts).\n\n    >>> print(zif_pintout(zif_by_intan64))\n    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, -1, -1, -1, -1, -1, ...]\n\n    Parameters\n    ----------\n    zif_pinout: list\n        The sequence numbering ZIF pins by data acquisition order.\n\n    Returns\n    -------\n    even_odd_zif: list\n        Even (+1) and odd (-1) ZIF pins in acquisition order\n\n    \"\"\"\n    return [1 - 2 * (c % 2) for c in zif_pinout if c is not None]",
        "sha1": "195e8a96c1a2aa985c3e87563ed298aded4a6517",
        "id": 625296
    },
    {
        "content": "def hideAllFootnotes(obj, i, j, numrows, numcols, section, more, custom):\n    \"\"\"Hide all footnotes in the table\"\"\"\n    \n    # Returns False to prevent being called multiple times\n    # This function is much more efficient than using hideFootnotes\n    more.thetable.SelectAllFootnotes()\n    more.thetable.HideFootnote()\n    return False",
        "sha1": "5a4c374eebb9e1df94dc3ae9b250f7b3637e0d4e",
        "id": 504509
    },
    {
        "content": "import re\n\n\ndef remove_url(text, replace_token):\n    \"\"\"\n    Removal of URLs\n\n    :param str text: Input string of text\n    :param str replace_token: The token to be replaced\n    :return str string: A new text\n    \"\"\"\n\n    regex = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n    return re.sub(regex, replace_token, text)",
        "sha1": "702ad4641d00fc74a4996b5aaec19b028e0f0040",
        "id": 313359
    },
    {
        "content": "def generate_by_embedding_model(lex, embedding_model, k=25):\n    \"\"\"Described in \"Exploring Word Embeddings for Unsupervised Textual User-Generated\n    Content Normalization, Bertaglia and Nunes(2016)\"\n\n    Args:\n        lex (dict): The lexicon dictionary.\n        embedding_model (obj): The embedding model in word2vec format. Must be readable by gensim.\n        k (int): Number of neares neighbours to evaluate (all experiments ran with k=25).\n\n    Returns:\n        dict(str: list(str)): A list of possible corrections for each word.\n    \"\"\"\n    cands = {}\n    corrs = {}\n\n    cands = {\n        word: [\n            sims[0]\n            for sims in embedding_model.most_similar(word, topn=k)\n            if sims[0] not in lex\n        ]\n        for word in lex\n        if word in embedding_model\n    }\n\n    for word in cands:\n        for c in cands[word]:\n            if c not in corrs:\n                corrs[c] = []\n            corrs[c].append(word)\n\n    return corrs",
        "sha1": "3d5ec8d9769b6cd7858d71c67d6f7daddb473218",
        "id": 408310
    },
    {
        "content": "import re\n\n\ndef extract_page_nr(some_string):\n    \"\"\" extracts the page number from a string like `Seite 21`\n\n    :param some_string: e.g. `Seite 21`\n    :type some_string: str\n\n    :return: The page number e.g. `21`\n    :rtype: str\n    \"\"\"\n\n    page_nr = re.findall(r'\\d+', some_string)\n    if len(page_nr) > 0:\n        return \"-\".join(page_nr)\n    else:\n        return some_string",
        "sha1": "6d39314de89c8f4bf4d931f2dc329fe394a10091",
        "id": 9404
    },
    {
        "content": "def semifactorial(x):\n    \"\"\"Compute the semifactorial function x!!.\n\n    x!! = x * (x-2) * (x-4) *...\n\n    Args:\n        x: positive int\n    Returns:\n        float for x!!\n    \"\"\"\n    y = 1.\n    for n in range(x, 1, -2):\n        y *= n\n    return y",
        "sha1": "3b4272680470e43b2c89971ee2d672a1f35b7f51",
        "id": 364675
    },
    {
        "content": "def contains_no_complete_reductions(reductions):\n\t\"\"\"Checks whether reductions contains a reduction with the empty word.\"\"\"\n\tfor reduction in reductions:\n\t\tif reduction[-1] == \"\":\n\t\t\treturn False\n\treturn True",
        "sha1": "7fc3b6f032762d238861839d2b10ca9873ee0884",
        "id": 630001
    },
    {
        "content": "from typing import Mapping\n\n\ndef _recursive_update(d, u):\n    \"\"\"Recursively update a nested dict with another.\"\"\"\n    # https://stackoverflow.com/a/3233356/1595060\n    for k, v in u.items():\n        if isinstance(v, Mapping):\n            d[k] = _recursive_update(d.get(k, {}), v)\n        else:\n            d[k] = v\n    return d",
        "sha1": "0562fa333b2667db02675382c95ce09bcbd23758",
        "id": 538505
    },
    {
        "content": "def safe_get(collection, key, default=None):\n    \"\"\"Get values from a collection without raising errors\"\"\"\n\n    try:\n        return collection.get(key, default)\n    except TypeError:\n        pass\n\n    try:\n        return collection[key]\n    except (IndexError, TypeError):\n        pass\n\n    return default",
        "sha1": "b96629f32fc11599d6ea902f5d00848ddc4aac84",
        "id": 55333
    },
    {
        "content": "def get_attr(obj, attr, default=None):\n    \"\"\"Recursive get object's attribute. May use dot notation.\n\n    >>> class C(object): pass\n    >>> a = C()\n    >>> a.b = C()\n    >>> a.b.c = 4\n    >>> get_attr(a, 'b.c')\n    4\n\n    >>> get_attr(a, 'b.c.y', None)\n\n    >>> get_attr(a, 'b.c.y', 1)\n    1\n    \"\"\"\n    if '.' not in attr:\n        return getattr(obj, attr, default)\n    else:\n        L = attr.split('.')\n        return get_attr(getattr(obj, L[0], default), '.'.join(L[1:]), default)",
        "sha1": "a14b821ed3d5ea61abc9b66cb80df4391519055d",
        "id": 464170
    },
    {
        "content": "import random\n\n\ndef secure_randint(min_value, max_value, system_random=None):\n    \"\"\" Return a random integer N such that a <= N <= b.\n\n        Uses SystemRandom for generating random numbers.\n        (which uses os.urandom(), which pulls from /dev/urandom)\n    \"\"\"\n    if not system_random:\n        system_random = random.SystemRandom()\n    return system_random.randint(min_value, max_value)",
        "sha1": "f4b61457c6e384e6185a5d22d95539001903670d",
        "id": 704949
    },
    {
        "content": "def tile_position(tile_ref, tile_size):\n    \"\"\"Returns the x and y coordinates of the top-left corner of a tile, given the tile's row and column.\n       tile_ref is a 2-item sequence representing the column and row of the tile\n       tile_size is the size of each tile as an int.\"\"\"\n       \n    return tile_ref[0]*tile_size, tile_ref[1]*tile_size",
        "sha1": "10f8ca747caf1d2e81dfb20efe950e2a1e9cf2b7",
        "id": 469475
    },
    {
        "content": "def make_word(parts, sub, i):\n    \"\"\"Replace a syllable in a list of words, and return the joined word.\"\"\"\n    j = 0\n    for part in parts:\n        for k in range(len(part)):\n            if i == j:\n                part[k] = sub\n                return ' '.join(''.join(p for p in part) for part in parts)\n            j += 1",
        "sha1": "869aa94f60b819b38ad7ddc315b65d862cf525e0",
        "id": 8017
    },
    {
        "content": "def get_subindex(string, ch):\n    \"\"\"\n    Finds all indexes of the specified character in the string. \n    \"\"\"\n    return [c for (c, i) in enumerate(string) if i == ch]",
        "sha1": "b0cc60f76258993edd5db4788d8df6a26b21e3fe",
        "id": 152870
    },
    {
        "content": "def boolean(value):\n    \"\"\"Parse the string \"true\" or \"false\" as a boolean (case insensitive)\"\"\"\n    value = value.lower()\n    if value == 'true':\n        return True\n    if value == 'false':\n        return False\n    raise ValueError(\"Invalid literal for boolean(): {}\".format(value))",
        "sha1": "194024bda50548bbee128ceb09e3a6db46a07c14",
        "id": 227464
    },
    {
        "content": "def is_main_google_link(soup_tag):\n    \"\"\"\n    This function checks if the given BeautifulSoup4 tag is of a main link in Google page\n    \"\"\"\n    return 'data-ved' in soup_tag.attrs",
        "sha1": "28a4c61adc44be6932c0062c97e0c2dbcfb2b72f",
        "id": 180122
    },
    {
        "content": "def get_message_type(msg_or_cls):\n    \"\"\"Returns ROS1 message type name, like \"std_msgs/Header\".\"\"\"\n    return msg_or_cls._type",
        "sha1": "42db77fbb59ada627e66a5878247fcd030e4773a",
        "id": 196437
    },
    {
        "content": "def Report(build_statuses):\n  \"\"\"Generate the stdout description of a given build.\n\n  Args:\n    build_statuses: List of build_status dict's from FetchBuildStatus.\n\n  Returns:\n    str to display as the final report.\n  \"\"\"\n  result = ''\n\n  for build_status in build_statuses:\n    result += '\\n'.join([\n        'cidb_id: %s' % build_status['id'],\n        'buildbucket_id: %s' % build_status['buildbucket_id'],\n        'status: %s' % build_status['status'],\n        'artifacts_url: %s' % build_status['artifacts_url'],\n        'toolchain_url: %s' % build_status['toolchain_url'],\n        'stages:\\n'\n    ])\n    for stage in build_status['stages']:\n      result += '  %s: %s\\n' % (stage['name'], stage['status'])\n    result += '\\n'  # Blank line between builds.\n\n  return result",
        "sha1": "c295c0e330f57b62bcd657914ff6a176449bc0d2",
        "id": 358077
    },
    {
        "content": "def notas(*notas:float, situa\u00e7\u00e3o:bool = False):\n\n    \"\"\"\n    Uma fun\u00e7\u00e3o feita para mostrar:\n    A quantidade de notas\n    A maior nota\n    A menor nota\n    A media da turma\n    A situa\u00e7\u00e3o da turma\n\n    :param notas: Uma ou mais notas de alunos\n    :param situa\u00e7\u00e3o: A situa\u00e7\u00e3o da turma (opcional)\n    :return: Dicionario com todas as informa\u00e7\u00f5es mostradas acima\n\n    \"\"\"\n\n    totalnota = dict()\n    quantidenotas = len(notas)\n    maiornota = 0\n    menornota = 10\n    media = 0\n\n    for x in notas:\n        if(x > maiornota):\n            maiornota = x\n        if(x < menornota):\n            menornota = x\n        media += x\n    media /= quantidenotas\n\n    totalnota['quantidade'] = quantidenotas\n    totalnota['maiornota'] = maiornota\n    totalnota['menornota'] = menornota\n    totalnota['media'] = media\n\n    if(situa\u00e7\u00e3o == True):\n        if(media < 4):\n            totalnota['situa\u00e7\u00e3o'] = 'horrivel'\n        elif(media > 7):\n            totalnota['situa\u00e7\u00e3o'] = 'otima'\n        else:\n            totalnota['situa\u00e7\u00e3o'] = 'rasoavel'\n\n    return totalnota",
        "sha1": "c4617eea28de0c58ae5b677c9991afd1d0185c61",
        "id": 113810
    },
    {
        "content": "def winphone(alert, title=None, _open_page=None, extras=None):\n    \"\"\"MPNS specific platform override payload.\n\n    Must include exactly one of ``alert``, ``title``, ``_open_page``, or ``extras``.\n\n    \"\"\"\n    if len(list(filter(None, (alert, _open_page, title)))) != 1:\n        raise ValueError(\"MPNS payload must have one notification type.\")\n    payload = {}\n    if alert is not None:\n        payload['alert'] = alert\n    if title is not None:\n        payload['title'] = title\n    if _open_page is not None:\n        payload['_open_page'] = _open_page\n    if extras is not None:\n        payload['extras'] = extras\n    return payload",
        "sha1": "6cedbb59c7a12a01c9832173d6020ce4732fae86",
        "id": 555856
    },
    {
        "content": "def get_parent_child_dict(company,parent,children_list):\n    \"\"\"\n        Build a dictionary that contains parent company, subsidiary company information\n        for a certain company\n\n        Parameters\n        ----------\n        company: str\n            A company name to build dictionary for\n        parent: str\n            The parent company name for the company\n        children_list: list of strings\n            A list of subsidiary names of the company\n            \n        Returns\n        -------\n        dict\n            parent(str): the parent company name\n            child(list): a list of subsidiary names of the company\n    \"\"\"\n    parent_child_dict = {}\n    parent_child_dict['parent'] = parent\n    parent_child_dict['children'] = children_list\n    return parent_child_dict",
        "sha1": "c5325c1b0ff96b42a5c0b5efcd02d54ab2ed22fa",
        "id": 322349
    },
    {
        "content": "def multiplicative_persistence(num: int) -> int:\n    \"\"\"\n    Return the persistence of a given number.\n\n    https://en.wikipedia.org/wiki/Persistence_of_a_number\n\n    >>> multiplicative_persistence(217)\n    2\n    >>> multiplicative_persistence(-1)\n    Traceback (most recent call last):\n        ...\n    ValueError: multiplicative_persistence() does not accept negative values\n    >>> multiplicative_persistence(\"long number\")\n    Traceback (most recent call last):\n        ...\n    ValueError: multiplicative_persistence() only accepts integral values\n    \"\"\"\n\n    if not isinstance(num, int):\n        raise ValueError(\"multiplicative_persistence() only accepts integral values\")\n    if num < 0:\n        raise ValueError(\"multiplicative_persistence() does not accept negative values\")\n\n    steps = 0\n    num_string = str(num)\n\n    while len(num_string) != 1:\n        numbers = [int(i) for i in num_string]\n\n        total = 1\n        for i in range(0, len(numbers)):\n            total *= numbers[i]\n\n        num_string = str(total)\n\n        steps += 1\n    return steps",
        "sha1": "a68a573dca42d9f70a22fd44c0c7d60f2676ea02",
        "id": 615208
    },
    {
        "content": "def _extract_fields_from_row(row, element):\n    \"\"\"Pluck data from the provided row and element.\"\"\"\n    row_data = []\n    fields = row.find_all(element)\n    for raw_field in fields:\n        field = raw_field.text.strip()\n        row_data.append(field)\n    return row_data",
        "sha1": "081473db76f139eab4687b60f309801b40f1c69a",
        "id": 677035
    },
    {
        "content": "def bbox_from_xywh(xywh):\n    \"\"\"Convert a bounding box from a numeric dict to a numeric list representation.\"\"\"\n    return (\n        xywh['x'],\n        xywh['y'],\n        xywh['x'] + xywh['w'],\n        xywh['y'] + xywh['h']\n    )",
        "sha1": "a11d0e9b0915b7513c874056ee28780ddd5553eb",
        "id": 582579
    },
    {
        "content": "def safe_cast(x, type, default = 0):\n    \"\"\"\n    Safely casts a variable to another type. If the cast fails, the default value is returned.\n\n    Parameters\n    ----------\n    x : [type]\n        The variable that will be casted\n    type : [type]\n        The type the variable will be casted to\n    default : [type], optional\n        The default value that will be returned if the cast fails, by default None\n\n    Returns\n    -------\n    [type]\n        The resulting value of the operation\n    \"\"\"\n    try:\n        return type(x)\n    except:\n        return default",
        "sha1": "27e42eedfae66a646441d7ff710985532f80730f",
        "id": 449571
    },
    {
        "content": "def differential(field, f, t=None):\n    \"\"\"\n    Return the differential `fdt` on the function field.\n\n    INPUT:\n\n    - ``field`` -- function field\n\n    - ``f``, ``t`` -- elements of the function field\n\n    If ``t`` is not given, then return the differential `fdx` where\n    `x` is the generator of the base rational function field.\n\n    EXAMPLES::\n\n        sage: K.<x> = FunctionField(GF(4)); _.<Y> = K[]\n        sage: L.<y> = K.extension(Y^3 + x + x^3*Y)\n        sage: x.differential()\n        d(x)\n        sage: y.differential()\n        (x*y^2 + 1/x*y) d(x)\n    \"\"\"\n    f = field(f)\n    if t is not None:\n        t = field(t)\n\n    return field.space_of_differentials().element_class(field, f, t)",
        "sha1": "d41a7ce86ee0f50df108f70c95e92cb96d5d1901",
        "id": 331231
    },
    {
        "content": "def substitute_file_extension(filename, extension):\n    \"\"\"Substitutes file extension, respecting known shader extensions.\n\n    foo.vert -> foo.vert.[extension] [similarly for .frag, .comp, etc.]\n    foo.glsl -> foo.[extension]\n    foo.unknown -> foo.[extension]\n    foo -> foo.[extension]\n    \"\"\"\n    if filename[-5:] not in ['.vert', '.frag', '.tesc', '.tese',\n                             '.geom', '.comp', '.spvasm']:\n        return filename.rsplit('.', 1)[0] + '.' + extension\n    else:\n        return filename + '.' + extension",
        "sha1": "2fae485ca9d39e48ea12650d838aa749574274e6",
        "id": 550627
    },
    {
        "content": "import re\n\n\ndef remove_ansi_escape_sequences(line):\n    \"\"\"\n    Remove ANSI escape sequences that are used for adding colors in\n     terminals, so that only the text is logged in a file\n    \"\"\"\n    ansi_escape = re.compile(r\"(\\x9B|\\x1B\\[)[0-?]*[ -\\/]*[@-~]\")\n    return ansi_escape.sub(\"\", line)",
        "sha1": "0bf00667f268ead0beefd1706e7d7549c11129a4",
        "id": 474383
    },
    {
        "content": "def rgb2rgba(rgb):\n    \"\"\"Take a row of RGB bytes, and convert to a row of RGBA bytes.\"\"\"\n    rgba = []\n    for i in range(0, len(rgb), 3):\n        rgba += rgb[i:i+3]\n        rgba.append(255)\n\n    return rgba",
        "sha1": "5a8107649a2442fb3a13e7e167a5cecc0ee57e29",
        "id": 136006
    },
    {
        "content": "def get_date_string(start_date, end_date):\n\t\"\"\"\n\t\tTurns the dates into a string for the API\n\t\"\"\"\n\t# first convert dates\n\tstart_date = start_date.replace('/', '')\n\tend_date = end_date.replace('/','')\n\n\tif start_date == 'off' and end_date == 'off':\n\t\treturn \"\"\n\telif start_date == 'off' and end_date != 'off':\n\t\treturn \"date:r::\" + end_date\n\telif start_date != 'off' and end_date == 'off':\n\t\treturn \"date:r:\" + start_date + \":\"\n\telse:\n\t\treturn \"date:r:\" + start_date + \":\" + end_date",
        "sha1": "0b0d31eca0511a2f1347eb50979e6484eee024e2",
        "id": 340883
    },
    {
        "content": "def _return_ordered_config(unordered_config: dict, template_config: dict) -> dict:\n    \"\"\"Orders the keys in unordered_config according to the order in template_config.\n\n    Note this function assumes all keys in both dictionaries are identical,\n    including all nested dictionaries.\n    \"\"\"\n\n    # create a dictionary for the ordered config\n    ordered_config = {}\n\n    # loop through keys in the order of the template config\n    for key in template_config.keys():\n\n        # get the value from the unordered config\n        val = unordered_config[key]\n\n        # if the value is a dictionary, it needs to be ordered too\n        if isinstance(val, dict):\n            ordered_config[key] = _return_ordered_config(val, template_config[key])\n        # otherwise just save the value\n        else:\n            ordered_config[key] = val\n\n    return ordered_config",
        "sha1": "f74f62973511bf26b188c77e6ee6d31367c8fcf6",
        "id": 647410
    },
    {
        "content": "def build_style_name(width='', weight='', custom='', is_italic=False):\n    \"\"\"Build style name from width, weight, and custom style strings\n    and whether the style is italic.\n    \"\"\"\n\n    return ' '.join(\n        s for s in (custom, width, weight, 'Italic' if is_italic else '') if s\n    ) or 'Regular'",
        "sha1": "8a1070337195f28cf58182e184db32c4fbc88fef",
        "id": 440764
    },
    {
        "content": "def element_strip(elem):\n    \"\"\"formatting of element name to match colnames of reference material\"\"\"\n    elem = elem.replace('(LR)', '').replace('(MR)', '').replace('(HR)', '')\n    elem = ''.join([c for c in elem if c.isalpha()])\n    return elem",
        "sha1": "747b4d69a73979dff4769c07dffe51b9a0cb0771",
        "id": 428667
    },
    {
        "content": "def crossproduct(a, b):\n    \"\"\"Return cross product (a X b)\n\n    Parameters\n    ----------\n    a : 3-element list of floats\n        first set of values in cross-product calculation\n\n    b : 3-element list of floats\n        second set of values in cross-product calculation\n\n    Returns\n    -------\n    c :  3-element list of floats\n        result of cross-product calculation\n    \"\"\"\n    c = [0]*3\n    c[0] = a[1]*b[2] - a[2]*b[1]\n    c[1] = a[2]*b[0] - a[0]*b[2]\n    c[2] = a[0]*b[1] - a[1]*b[0]\n    return c",
        "sha1": "1a3b7233a881af03c64fceab029317f92abf489c",
        "id": 460964
    },
    {
        "content": "def deepcopy_nested_dict(nested_dict_to_deepcopy: dict):\n    \"\"\"\n    Deepcopy of a nested dictionary of two levels, e.g. {k1:{...}, k2:{...}, ..., kN:{...}}\n    :param nested_dict_to_deepcopy: The nested dictionary to return a deepcopy of\n    :return: A deepcopy of a nested dictionary\n    \"\"\"\n    # Copy the upper level\n    deepcopied_nested_dict = nested_dict_to_deepcopy.copy()\n\n    # Coppy the lower level\n    for k, d in nested_dict_to_deepcopy.items():\n        assert type(d) is dict\n        deepcopied_nested_dict[k] = d.copy()\n\n    return deepcopied_nested_dict",
        "sha1": "09e3c0d4eecf88613b25a7344ad3ca9f4ea8c23b",
        "id": 108605
    },
    {
        "content": "def parse_position(string):\n    \"\"\"\n    >>> parse_position('3,000,000')\n    3000000\n    >>> parse_position('3M')\n    3000000\n    >>> parse_position('3kk')\n    3000000\n    \"\"\"\n    units = ''\n    for index, char in enumerate(string):\n        if char.lower() in ['k', 'm']:\n            units = string[index:]\n            string = string[:index]\n            break\n    pos = float(string.replace(',', ''))\n    for char in units:\n        if char.lower() == 'k':\n            pos *= 1000\n        if char.lower() == 'm':\n            pos *= 1000000\n    pos = int(pos)\n    return pos",
        "sha1": "893aef37ab347bcab81f1ced2019ce29f5d29bd4",
        "id": 645695
    },
    {
        "content": "def _filter_unique(tuple_list):\n    \"\"\"For a list of tuples [(distance, value), ...] - filter out duplicate\n    values.\n    Args:\n        tuple_list: List of tuples. (distance, value)\n    \"\"\"\n\n    added = set()\n    ret = []\n    for distance, value in tuple_list:\n        if not value in added:\n            ret.append((distance, value))\n            added.add(value)\n    return ret",
        "sha1": "07e20482db03475519a3cb8d90ef1037ccbe610c",
        "id": 451785
    },
    {
        "content": "def _RunMakeCTLogListTests(input_api, output_api):\n  \"\"\"Runs make_ct_known_logs_list unittests if related files were modified.\"\"\"\n  files = (input_api.os_path.normpath(x) for x in\n           ('components/certificate_transparency/tools/make_ct_known_logs_list.py',\n            'components/certificate_transparency/tools/make_ct_known_logs_list_unittest.py',\n            'components/certificate_transparency/data/log_list.json'))\n  if not any(f in (af.LocalPath() for af in input_api.change.AffectedFiles())\n             for f in files):\n    return []\n  test_path = input_api.os_path.join(input_api.PresubmitLocalPath(),\n                                     'make_ct_known_logs_list_unittest.py')\n  cmd_name = 'make_ct_known_logs_list_unittest'\n  cmd = [input_api.python_executable, test_path]\n  test_cmd = input_api.Command(\n    name=cmd_name,\n    cmd=cmd,\n    kwargs={},\n    message=output_api.PresubmitPromptWarning)\n  return input_api.RunTests([test_cmd])",
        "sha1": "fec70841cbefa111360030ac2e0a1e36f69d98b6",
        "id": 362061
    },
    {
        "content": "from typing import Dict\nfrom typing import Any\nfrom typing import Optional\n\n\ndef find_namespace(\n        namespace: str,\n        discovery_map_data: Dict[str, Any],\n) -> Optional[Dict[str, Any]]:\n    \"\"\"Find the service-colors list for the given namespace.\"\"\"\n    for namespace_obj in discovery_map_data['namespaces']:\n        assert isinstance(namespace_obj, dict)\n        if namespace_obj['namespace'] == namespace:\n            return namespace_obj\n    return None",
        "sha1": "3192434d77e3bbd59fcb36bed98198105564cdaa",
        "id": 661009
    },
    {
        "content": "def straight_line_from_points(a, b):\n    \"\"\"\n    Generate a geojson LineString object from two geojson points.\n\n    Parameters\n    ----------\n    a : geojson\n        Point A\n    b : geojson\n        Point B\n\n    Returns\n    -------\n    line : geojson\n        A geojson LineString object.\n\n    \"\"\"\n    line = {\n        'type': 'Feature',\n        'geometry': {\n            'type': 'LineString',\n            'coordinates': [\n                (\n                    a['geometry']['coordinates'][0],\n                    a['geometry']['coordinates'][1]\n                ),\n                (\n                    b['geometry']['coordinates'][0],\n                    b['geometry']['coordinates'][1]\n                ),\n            ]\n        },\n        'properties': {\n            'id': 'terrain path'\n        }\n    }\n\n    return line",
        "sha1": "39d0b9dfd5aa9b96a237a76f4653fdc02475dc99",
        "id": 517117
    },
    {
        "content": "import re\n\n\ndef remove_links(text):\n    \"\"\"Remove links/urls from text.\n\n    Twitter automatically shortens all links to https://t.co/<hash>. We\n    take advantage of this fact to strip links with a very simple regex.\n\n    Args:\n        text (str): Text from which to remove links.\n\n    Returns:\n        str: The same text with links such as \"https://t.co/FakeLink\" removed.\n\n    Examples:\n        >>> remove_links('No links here.')\n        'No links here.'\n\n        >>> remove_links(r'There was a link: https://t.co/FakeLink')\n        'There was a link: '\n    \"\"\"\n    return re.sub(r\"http[s]?://\\S+\", \"\", text)",
        "sha1": "b86a3b21d7ec9b71b18735543679377baf572165",
        "id": 544721
    },
    {
        "content": "def get_commit(repository):\n    \"\"\"\n    Get current commit ID.\n\n    :param repository: repository\n    :type repository: git.Repo\n    :return: commit ID\n    :rtype: str or unicode\n    \"\"\"\n    return repository.head.commit.hexsha",
        "sha1": "c6fd3bc4871f135770818a57882e75c163ad4016",
        "id": 467894
    },
    {
        "content": "import yaml\n\n\ndef load_config_data(path: str) -> dict:\n    \"\"\"Load a config data from a given path\n    :param path: the path as a string\n    :return: the config as a dict\n    \"\"\"\n    with open(path) as f:\n        cfg: dict = yaml.load(f, Loader=yaml.FullLoader)\n    return cfg",
        "sha1": "77a10fb93c2d69af2e4ae628868336e7095842d5",
        "id": 263930
    },
    {
        "content": "def update_graphs_header(pathname):\n    \"\"\"\n    Update the graphs page header on page load or refresh.\n\n    Parameters\n    ----------\n    pathname : str\n        The pathname of the url in window.location\n\n    Returns\n    -------\n    header : str\n        The header for the graphs page that includes the hostname\n    desc: str\n        A description of what the graphs page shows\n    \"\"\"\n    # Parse hostname.\n    hostname = pathname.split('/')[1]\n    header = hostname + ' Graphs'\n\n    desc = ('This page features two histograms that will help you explore '\n            'and visualize image data from the {} device.'.format(hostname))\n\n    return header, desc",
        "sha1": "d59e3cab49d7afb2e33d0b9a371475b0da543147",
        "id": 64834
    },
    {
        "content": "def getUserListParameterName(paramNumber, epochCount=50):\n    \"\"\"\n    Given an epoch paramater number return a human readable description\n    of what it's supposed to modify in the epoch table.\n    Based on: ABFHEADR.H#L530-L549\n    \"\"\"\n    if paramNumber == 0:\n        return \"CONDITNUMPULSES\"\n    if paramNumber == 1:\n        return \"CONDITBASELINEDURATION\"\n    if paramNumber == 2:\n        return \"CONDITBASELINELEVEL\"\n    if paramNumber == 3:\n        return \"CONDITSTEPDURATION\"\n    if paramNumber == 4:\n        return \"CONDITSTEPLEVEL\"\n    if paramNumber == 5:\n        return \"CONDITPOSTTRAINDURATION\"\n    if paramNumber == 6:\n        return \"CONDITPOSTTRAINLEVEL\"\n    if paramNumber == 7:\n        return \"EPISODESTARTTOSTART\"\n    if paramNumber == 8:\n        return \"INACTIVEHOLDING\"\n    if paramNumber == 9:\n        return \"DIGITALHOLDING\"\n    if paramNumber == 10:\n        return \"PNNUMPULSES\"\n    if paramNumber == 11:\n        return \"PARALLELVALUE\"\n    paramRemainder = paramNumber - 11\n\n    if (paramRemainder < epochCount):\n        return f\"EPOCHINITLEVEL\"\n    paramRemainder -= epochCount\n\n    if (paramRemainder < epochCount):\n        return f\"EPOCHINITDURATION\"\n    paramRemainder -= epochCount\n\n    if (paramRemainder < epochCount):\n        return f\"EPOCHTRAINPERIOD\"\n    paramRemainder -= epochCount\n\n    if (paramRemainder < epochCount):\n        return f\"EPOCHTRAINPULSEWIDTH\"\n    paramRemainder -= epochCount\n\n    if (paramRemainder < epochCount):\n        return f\"EPOCHINITDURATION\"\n    paramRemainder -= epochCount\n\n    return f\"UNKNOWN {paramNumber}\"",
        "sha1": "6a4106f68b22755d4a551c5e1060429feda81d5f",
        "id": 295989
    },
    {
        "content": "def response_creator(text, card):\n    \"\"\"\n    Builds a response with speech part and Alexa appcard contents\n    :param text: text to be spoken\n    :param card: text for the app card\n    :return: JSON object to be returned\n    \"\"\"\n    text_item = {\"type\": \"PlainText\", \"text\": text}\n    card_item = {\"type\": \"Simple\", \"title\": \"Stop Info\", \"content\": card}\n    reprompt = {\n        \"outputSpeech\": {\"text\": \"Which stop do you want to know about?\",\n                         \"type\": \"PlainText\"}}\n    response = {\"version\": \"1.0\",\n                \"response\": {\"outputSpeech\": text_item, \"card\": card_item,\n                             \"reprompt\": reprompt,\n                             \"shouldEndSession\": True}}\n\n    return response",
        "sha1": "cc1ce310616fc7de60b636698e3d288403db8af6",
        "id": 59070
    },
    {
        "content": "import zlib\n\n\ndef dfu_crc(data, crc=0):  # type: (bytes, int) -> int\n    \"\"\" Calculate CRC32/JAMCRC of data, with an optional initial value \"\"\"\n    uint32_max = 0xFFFFFFFF\n    return uint32_max - (zlib.crc32(data, crc) & uint32_max)",
        "sha1": "0cfd2e674563b8760b2f7c55a020318e7e60e5ef",
        "id": 445019
    },
    {
        "content": "def token2sub_tokens(tokenizer, token):\n    \"\"\"\n    Take in a string value and use tokenizer to tokenize it into subtokens.\n    Return a list of sub tokens.\n    \"\"\"\n    res = []\n    for sub_token in tokenizer.tokenize(token):\n        # make sure it's not an empty string\n        if len(sub_token) > 0: \n            res.append(tokenizer.convert_tokens_to_ids(sub_token))\n    return res",
        "sha1": "aa3f4693bcd73b8317969dea1d58403512c5cd5d",
        "id": 243268
    },
    {
        "content": "def _is_positive(integer_string):\n    \"\"\"\n    Check if a string is a strictly positive integer.\n    \"\"\"\n    return int(integer_string) > 0",
        "sha1": "10d1f93b544031e75364d5fe74c48ab76c030a06",
        "id": 560293
    },
    {
        "content": "def dict_to_help(data_types):\n    \"\"\" Turns DATA_DICT into readable format for help text \"\"\"\n    output = \"\"\n    for item in data_types:\n        output = output + \" \" + item + \": \"\n        output = output + data_types[item] + \"\\n\"\n    return output",
        "sha1": "c419eb48f9a2f051a9ab0711c4e87de4a826e22f",
        "id": 304878
    },
    {
        "content": "def adtgrad(S, T, P=0):\n    \"\"\"Compute adiabatic temperature gradient\n\n    Usage: adtgrad(S, T, [P])\n\n    Input:\n        S = Salinity,     [PSS-78]\n        T = Temperature,  [\ufffdC]\n        P = Pressure,     [dbar]\n    P is optional, with a default value = zero\n\n    Output:\n        Adiabatic temperature gradient,  [K/dbar]\n\n    Algorithm: UNESCO 1983\n\n    \"\"\"\n\n    a0 =  3.5803e-5\n    a1 = +8.5258e-6\n    a2 = -6.836e-8\n    a3 =  6.6228e-10\n\n    b0 = +1.8932e-6\n    b1 = -4.2393e-8\n\n    c0 = +1.8741e-8\n    c1 = -6.7795e-10\n    c2 = +8.733e-12\n    c3 = -5.4481e-14\n\n    d0 = -1.1351e-10\n    d1 =  2.7759e-12\n\n    e0 = -4.6206e-13\n    e1 = +1.8676e-14\n    e2 = -2.1687e-16\n\n    return  a0 + (a1 + (a2 + a3*T)*T)*T  \\\n         + (b0 + b1*T)*(S-35)  \\\n\t + ( (c0 + (c1 + (c2 + c3*T)*T)*T) \\\n         +   (d0 + d1*T)*(S-35) )*P \\\n         + (e0 + (e1 + e2*T)*T )*P*P",
        "sha1": "656ed66f3dcd4286c69a3ecd92784b55f271ccc5",
        "id": 485558
    },
    {
        "content": "import collections\n\n\ndef align (crabs, cost):\n    \"\"\"Aligns the crab positions `crabs` (array) to the same position using\n    the least fuel, according to the given `cost` function.  The `cost`\n    function takes a single parameter, the number of steps to move,\n    i.e. `cost(steps)`.\n\n    Returns `(pos, fuel)`.\n    \"\"\"\n    fuel = collections.defaultdict(int)\n\n    for moveto in range( max(crabs) ):\n        for crab in crabs:\n            fuel[moveto] += cost( abs(crab - moveto) )\n\n    return min(fuel.items(), key=lambda pair: pair[1])",
        "sha1": "577bacf52cac36155a372fca139040ba1d96e1a7",
        "id": 72616
    },
    {
        "content": "from io import StringIO\n\n\ndef rel_pos(abs_pos, source):\n    \"\"\"Given absolute position, return relative (line, character) in source.\"\"\"\n    lines = StringIO(source).readlines()\n    nchars = len(source)\n    assert nchars >= abs_pos\n    while nchars > abs_pos:\n        assert nchars >= abs_pos\n        nchars -= len(lines[-1])\n        lines.pop()\n    return len(lines) + 1, abs_pos - len(''.join(lines))",
        "sha1": "7da2979cf84c80911d8317b19529bc125097484d",
        "id": 212441
    },
    {
        "content": "import math\n\n\ndef test1(x):\n    \"\"\"\n    >>> test1(3)\n    0.1411200080598672\n\n    >>> import dis\n    >>> dis.dis(test1)\n     23           0 LOAD_GLOBAL              0 (math)\n                  3 LOAD_ATTR                1 (sin)\n                  6 LOAD_FAST                0 (x)\n                  9 CALL_FUNCTION            1 (1 positional, 0 keyword pair)\n                 12 RETURN_VALUE\n    \"\"\"\n    return math.sin(x)",
        "sha1": "5fd40e6657c8fb91ef462e6be49caa17325f35bf",
        "id": 92823
    },
    {
        "content": "def dRackId(rackId):\n    \"\"\"Return rack id if valid, raise an exception in other case\"\"\"\n    if rackId >= 0:\n        return rackId\n    else:\n        raise ValueError(\n            '{} is not a valid Rack Id, Rack Id must be >= 0'.format(rackId))",
        "sha1": "33c485d870e0a99aeb6cc612d62818dc3261a0e2",
        "id": 223326
    },
    {
        "content": "import functools\nfrom warnings import warn\n\n\ndef obsolete_command(f):\n    \"\"\"Decorator to add a warning when a command is obsolete and will be removed.\"\"\"\n\n    @functools.wraps(f)\n    def wrapper(*args, **kwargs):\n        warn(\"This command is obsolete and will be removed in a feature release.\")\n        return f(*args, **kwargs)\n\n    return wrapper",
        "sha1": "cefb3e72387d7b61e59e0f7376cb7e4c0bf69d6a",
        "id": 211932
    },
    {
        "content": "from typing import List\n\n\ndef is_subcat(c: List[str], sub_c: List[str]) -> bool:\n    \"\"\"\n        Return True if sub_c is a subcategory of c.\n    \"\"\"\n    # if c is more precize category then sub_c can't be subcategory of c.\n    if len(c) > len(sub_c):\n        return False\n\n    for c_el, x_el in zip(c, sub_c):\n        if c_el != x_el:\n            return False\n    return True",
        "sha1": "0307399f695daf04f0d31a5c0b7d7def5fdcaa3a",
        "id": 449985
    },
    {
        "content": "def binary_search(a, value):\n    \"\"\"\n    Searches a value in a (sorted) list. Return the index if found, otherwise\n    returns <None>.\n    \"\"\"\n    start = 0\n    end = len(a) - 1\n\n    while (start <= end):\n\n        i = (start + end) // 2          # Middle point\n\n        # If found the value\n        if (a[i] == value):\n            return i\n\n        # If not found the value\n        else:\n            if (a[i] > value):\n                end = i - 1             # Check left side\n            else:\n                start = i + 1           # Check right side\n\n    return None",
        "sha1": "22c202840b961ed8527a9097e52f49e767f18597",
        "id": 255514
    },
    {
        "content": "def get_text(base, name):\n    \"\"\"Gets the text from an xml element.\"\"\"\n    try:\n        return base.find(name).text\n    except AttributeError:\n        return None",
        "sha1": "6e78d1af8aaf7d585adaae16207227f468e63e03",
        "id": 471470
    },
    {
        "content": "def lists_have_elements_in_common(a: list, b: list) -> bool:\n\t\"\"\" Checks if two lists have at least one element in common. \"\"\"\n\treturn len( set(a).intersection(set(b)) ) > 0",
        "sha1": "3ab4c3687b4f1b732320ad43e19d5f1c62796748",
        "id": 246970
    },
    {
        "content": "from typing import Mapping\nfrom typing import Any\n\n\ndef is_nested(item: Mapping[Any, Any]) -> bool:\n    \"\"\"Returns if 'item' is nested at least one-level.\"\"\"\n    return any(isinstance(v, Mapping) \n               for v in item.values())",
        "sha1": "4afa42acf01e49136eed37424bdc73049783274c",
        "id": 550608
    },
    {
        "content": "from bs4 import BeautifulSoup\n\n\ndef get_corpus_file_soup(corpus_filename):\n\n    \"\"\"\n    For a given corpus xml filename, return its BeautifulSoup soup.\n\n    \"\"\"\n\n    return BeautifulSoup(open(corpus_filename), 'xml')",
        "sha1": "6327cfc185e1ac1c7372b4879dcbcf62c0689a89",
        "id": 696649
    },
    {
        "content": "from typing import List\nimport pathlib\n\n\ndef find_all_coreref_files(directory: str) -> List[pathlib.Path]:\n    \"\"\"Searches for all the COREREF files in a directory and all its subdirectories.\n\n    Parameters\n    ----------\n    directory: str\n        A string containing the path to directory where we want to search the COREREF\n        files.\n\n    Returns\n    -------\n    List[pathlib.Path]\n        A list of pathlib.Path objects, pointing to the location of all the discovered\n        COREREF files.\n    \"\"\"\n    return list(pathlib.Path(directory).glob(\"**/COREREF*.txt.bz2\"))",
        "sha1": "6fabc4adc435d96ce4123ae7ac8cd122d8d975d3",
        "id": 637228
    },
    {
        "content": "def make_geoid_field(df):\n    \"\"\"Make correctly formatted geoid string column in census dataframe\n    Args: \n        df (DataFrame): census data\n    \"\"\"\n    df['geoid'] = df['state']+df['county']+df['tract']+df['block group']\n    return(df)",
        "sha1": "bc99bbb2fc08259c453c03f8157f1ae766fc1ace",
        "id": 463055
    },
    {
        "content": "import re\n\n\ndef rmspecialchars(value):\n    \"\"\"   \n    Remove any special characters except period (.) and negative (-) from numeric values\n    \n    Parameters\n    ----------\n    value : string\n        String value to remove any existing characters from\n        \n    Returns\n    -------\n    value : string\n        String value to without any special characters\n        \n    Examples\n    --------\n    >>> import helpers\n    >>> helpers.rmspecialchars(value = \"*6.5_\")\n    6.5\n    >>> helpers.rmspecialchars(value = \"ICE\")\n    ICE\n    >>> helpers.rmspecialchars(value = \"-4.2\")\n    -4.2\n    >>> helpers.rmspecialchars(value = \"\")\n    \n    >>> helpers.rmspecialchars(value = \"%&!@#8.32&#*;\")\n    8.32\n    \"\"\"\n    value = re.sub(\"[^A-Za-z0-9.-]+\", \"\", value)\n    \n    return value",
        "sha1": "26de451a5cfef33f9384ce13bda9e495ae81fc5d",
        "id": 34342
    },
    {
        "content": "import torch\n\n\ndef cosine_similarity(\n    x: torch.Tensor, y: torch.Tensor, eps: float = 1e-12\n) -> torch.Tensor:\n    \"\"\"\n    Computes cosine similarity between two tensors.\n    Value == 1 means the same vector\n    Value == 0 means perpendicular vectors\n    \"\"\"\n    x_n, y_n = x.norm(dim=1)[:, None], y.norm(dim=1)[:, None]\n    x_norm = x / torch.max(x_n, eps * torch.ones_like(x_n))\n    y_norm = y / torch.max(y_n, eps * torch.ones_like(y_n))\n    sim_mt = torch.mm(x_norm, y_norm.transpose(0, 1))\n    return sim_mt",
        "sha1": "650f97f093d61599c914d453c2847dc11cd327b0",
        "id": 239791
    },
    {
        "content": "def bytes2str(val):\n    \"\"\"\n    Converts a bytes into a hex string.\n    \"\"\"\n    return f'0x{val.hex()}'",
        "sha1": "5be3312c34691d3142c26d96f474dbd8d081cc6b",
        "id": 591256
    },
    {
        "content": "def format_phone(phone):\n    \"\"\"\n    Format a string as a phone number.\n    \"\"\"\n    if len(phone) == 10:\n        return \"(\" + phone[:3] + \") \" + phone[3:6] + \"-\" + phone[-4:]\n    else:\n        return phone",
        "sha1": "7506c7c5b3724b27435c54048a478a73edf16ece",
        "id": 38504
    },
    {
        "content": "from re import compile, findall\n\n\ndef filterSpilloverFilename(filename):\n    \"\"\" Remove any unwanted spill-over filename endings (i.e. _NNN or ._NNN) \"\"\"\n\n    # Create the search pattern\n    pattern = compile(r'(\\.?\\_\\d+)')\n    found = findall(pattern, filename)\n    if found:\n        # Make sure that the _NNN substring is at the end of the string\n        for f in found:\n            if filename.endswith(f):\n                # Do not use replace here since it might cut away something from inside the filename and not only at the end\n                filename = filename[:-len(f)]\n\n    return filename",
        "sha1": "c0053aed72cfe7c1f5a46f6224aaf8a47743d212",
        "id": 129958
    },
    {
        "content": "def euclidean_algorithm(f, g):\n    \"\"\"\n    Euclidean Algorithm.\n\n    Returns one gcd of two elements f, g belonging to an euclidean domain.\n\n    Parameters\n    ----------\n    f : the first element.\n    g : the second element.\n\n    Returns\n    -------\n    A gcd of f and g.\n    \"\"\"\n\n    if f.parent() is not g.parent():\n        raise ValueError(\"Arguments should belong to the same ring\")\n\n    domain = f.parent()\n\n    if not domain.is_euclidean_domain():\n        raise ValueError(\"Arguments should belong to an euclidean domain\")\n\n    a, b = f, g\n\n    while b != domain.zero():\n        _, rem = a.quo_rem(b)\n        a, b = b, rem\n\n    return a",
        "sha1": "c619cd9733c7144911e50806caf30aa052d3cfdd",
        "id": 214300
    },
    {
        "content": "def is_mark(codepoint):\n    \"\"\"Returns true for diacritical marks (combining codepoints).\"\"\"\n    return codepoint.general_category in (\"Mn\", \"Me\", \"Mc\")",
        "sha1": "8fb84f7a1b39fa1ce6504a31f6cc902b2d4ab7c8",
        "id": 602825
    },
    {
        "content": "from typing import Sequence\nfrom typing import Any\nfrom typing import Tuple\n\n\ndef _split_args(\n    args: Sequence[Any],\n    preds: Sequence[bool],\n) -> Tuple[Tuple[Tuple[int, Any]], ...]:\n  \"\"\"Split args into two lists based on some predicates.\"\"\"\n  assert len(args) == len(preds), (len(args), len(preds))\n  true_args = []\n  false_args = []\n  for idx, (arg, pred) in enumerate(zip(args, preds)):\n    if pred:\n      true_args.append((idx, arg))\n    else:\n      false_args.append((idx, arg))\n  return tuple(true_args), tuple(false_args)",
        "sha1": "54d96ef10d106cae9f2b4cc982ee4c9491105219",
        "id": 449163
    },
    {
        "content": "def choose(n,r):\n    \"\"\"\n    number of combinations of n things taken r at a time (order unimportant)\n    \"\"\"\n    if (n < r):\n        return 0\n    if (n == r):\n        return 1\n    s = min(r, (n - r))\n    t = n\n    a = n-1\n    b = 2\n    while b <= s:\n        t = (t*a)//b\n        a -= 1\n        b += 1\n    return t",
        "sha1": "5852054f1a6381278039b0ec2184d0887e2b1d2b",
        "id": 5083
    },
    {
        "content": "from typing import List\nfrom typing import Tuple\n\n\ndef connect_blp_lane_segments(start_lane_seg_idx: int, lane_seg_num: int) -> List[Tuple[int, int]]:\n    \"\"\"\n    Add connection info for neighboring segments in baseline path.\n    :param start_lane_seg_idx: Index for first lane segment in baseline path.\n    :param lane_seg_num: Number of lane segments.\n    :return obj_conns: Data recording lane-segment connection relations [from_lane_seg_idx, to_lane_seg_idx].\n    \"\"\"\n    obj_conns: List[Tuple[int, int]] = []\n\n    for lane_seg_idx in range(start_lane_seg_idx + 1, start_lane_seg_idx + lane_seg_num):\n        obj_conns.append((lane_seg_idx - 1, lane_seg_idx))\n\n    return obj_conns",
        "sha1": "d1a1e2bfd3d79dd430c81c4680c9d21cdbe37354",
        "id": 287836
    },
    {
        "content": "def move(\n    library,\n    session,\n    source_space,\n    source_offset,\n    source_width,\n    destination_space,\n    destination_offset,\n    destination_width,\n    length,\n):\n    \"\"\"Moves a block of data.\n\n    Corresponds to viMove function of the VISA library.\n\n    Parameters\n    ----------\n    library : ctypes.WinDLL or ctypes.CDLL\n        ctypes wrapped library.\n    session : VISASession\n        Unique logical identifier to a session.\n    source_space : constants.AddressSpace\n        Specifies the address space of the source.\n    source_offset : int\n        Offset of the starting address or register from which to read.\n    source_width : constants.DataWidth\n        Specifies the data width of the source.\n    destination_space : constants.AddressSpace\n        Specifies the address space of the destination.\n    destination_offset : int\n        Offset of the starting address or register to which to write.\n    destination_width : constants.DataWidth\n        Specifies the data width of the destination.\n    length: int\n        Number of elements to transfer, where the data width of the\n        elements to transfer is identical to the source data width.\n\n    Returns\n    -------\n    constants.StatusCode\n        Return value of the library call.\n\n    \"\"\"\n    return library.viMove(\n        session,\n        source_space,\n        source_offset,\n        source_width,\n        destination_space,\n        destination_offset,\n        destination_width,\n        length,\n    )",
        "sha1": "b8300789d82032ccff9b058304d20dbd4080d95a",
        "id": 430725
    },
    {
        "content": "def reducer(words, frequency):\n    \"\"\"\n    reduces words to final output frequency list\n    :param words:\n    :param frequency:\n    :return: dict of unique words and their frequencies\n    \"\"\"\n    for word in words:\n        frequency[word] += 1\n\n    return frequency",
        "sha1": "07e631ad152ba8551f7f152bdd5a7be343b10192",
        "id": 288443
    },
    {
        "content": "def read_sensor_package(bytes_serial):\n    \"\"\"\n    Read a sensor from serial bytes. Expected format is 5 bytes:\n        1, 2 : Package start 0x59 for YY\n        3 : unsigned integer for sensor index\n        4, 5 : unsigned integer for reading\n    :return:\n        sensor_index, reading\n    \"\"\"\n\n    if bytes_serial[0] == 0x59 and bytes_serial[1] == 0x59:  # check for 'YY'\n        # print(bytes_serial)\n        sensor_index = bytes_serial[2]  # sensor index\n        reading = bytes_serial[4] + bytes_serial[3] * 256  # 2 bytes for reading\n        return sensor_index, reading\n    else:\n        return -1, None",
        "sha1": "e2b3951b9d3eca4df9e5e18aa7ffe49dbf8b39c2",
        "id": 525799
    },
    {
        "content": "def clear_results(json_data: dict) -> dict:\n    \"\"\" Clears the results in the json file so that it can be analysed using\n    the COM interface.\n\n    Parameters\n    ----------\n    json_data : dict\n        A Python dictionary of the data held within the json model file.\n\n    Returns\n    -------\n    json_data : dict\n        A Python dictionary of the data held within the json model file without\n        the results.\n\n    \"\"\"\n    if json_data.get(\"Frew Results\", False):\n        del json_data[\"Frew Results\"]\n    return json_data",
        "sha1": "84caeb318dc613f5ed4ccd66873a1f2ef7a4d4e5",
        "id": 591068
    },
    {
        "content": "def decay_every_scheduler(step, steps_per_decay, decay_factor):\n  \"\"\"Gives a scaling factor based on scheduling with a decay every n-steps.\n\n  Args:\n    step: int; Current step.\n    steps_per_decay: int; How often to decay.\n    decay_factor: float; The amount to decay.\n\n  Returns:\n    Scaling factor applied to the learning rate on the given step.\n  \"\"\"\n  return decay_factor**(step // steps_per_decay)",
        "sha1": "1a7a0f333cebfbc6851111f9cd83156b72fecc3d",
        "id": 15727
    },
    {
        "content": "def invert_dict(d):\n    \"\"\"Invert the given dict.\"\"\"\n    return {v: k for k, v in d.items()}",
        "sha1": "5642f85bfd97f4303af43e56bbfb3730909963bf",
        "id": 69765
    },
    {
        "content": "def requirements(filename='requirements.txt'):\n    \"\"\"Return a list of requirements.\"\"\"\n    with open(filename) as fp:\n        return fp.read().splitlines()",
        "sha1": "b404263b34c6830b862fcc1d5caeaad05ab9ff36",
        "id": 77135
    },
    {
        "content": "def pypi_link(pkg_filename):\n    \"\"\"\n    Given the filename, including md5 fragment, construct the\n    dependency link for PyPI.\n    \"\"\"\n    root = 'https://files.pythonhosted.org/packages/source'\n    name, sep, rest = pkg_filename.partition('-')\n    parts = root, name[0], name, pkg_filename\n    return '/'.join(parts)",
        "sha1": "1f71b2c6c34b52a60c2ead14b40e98ef0c89a8cf",
        "id": 16844
    },
    {
        "content": "def NullWrap(x):\n  \"\"\"Convert Nones to null, needed for compatibility.\"\"\"\n  if x == None:\n    return 'null'\n  return x",
        "sha1": "9ec54b8de31f4ebad3c7fd1498ebc8b2440c3bc8",
        "id": 552171
    },
    {
        "content": "def get_path(backlinks, target):\n    \"\"\"\n    Given a dict of backlinks and target node,\n    follow backlinks to root node (whose backlink is None)\n    \"\"\"\n    path = []\n    path_node = target\n    while path_node is not None:\n        path.append(path_node)\n        path_node = backlinks.get(path_node, None)\n    return list(reversed(path))",
        "sha1": "c9d0d295399a7a649ceea1dbdd6c8cfba8c26731",
        "id": 147138
    },
    {
        "content": "import struct\n\n\ndef uint24(data):\n    \"\"\"convert three byte data that represents an unsigned int\"\"\"\n    u = int(struct.unpack(\">B\", data[0:1])[0]) << 16\n    u += int(struct.unpack(\">B\", data[1:2])[0]) << 8\n    u += int(struct.unpack(\">B\", data[2:3])[0])\n    return u",
        "sha1": "a65b08ac624fe60070b5144d65919a0753d5ec1f",
        "id": 561754
    },
    {
        "content": "from typing import List\n\n\ndef unordered_list_html(list_items: List[str]) -> str:\n    \"\"\"Embed list of html elements into an unordered list tag.\"\"\"\n    return \"<ul>{}</ul>\".format(\"\".join(list_items))",
        "sha1": "a364b6e7a808effe3bef737fc6f159edde92be01",
        "id": 414623
    },
    {
        "content": "import threading\n\n\ndef background(f):\n    \"\"\"Threading decorator.\n    \n    Use @background above the function you want to thread (run in the\n    background).\"\"\"\n    def bg_f(*a, **kw):\n        thread = threading.Thread(name=f.__name__, target=f, args=a, kwargs=kw)\n        thread.start()\n        return thread\n    return bg_f",
        "sha1": "7e8d2b1c8ab42cbe318914acb9dc3c8367b45323",
        "id": 620696
    },
    {
        "content": "import math\n\n\ndef score_item(recencies, time_constant):\n    \"\"\"Calculates the absolute score of a given item.\n\n    :param recencies: an iterable containing int time offsets from the current time\n    :param time_constant: numeric constant, should be > 0 in most cases\n    \"\"\"\n    return sum(math.exp(-r / time_constant) for r in recencies)",
        "sha1": "c8eca209e1920684817f3cb11694213ff4c58abf",
        "id": 449045
    },
    {
        "content": "from typing import Any\n\n\ndef deepset(data: dict, keys: list[str], value: Any):\n    \"\"\"\n    deepset nested keys to a value in a dictionary, filling in nested dictionaries\n    when needed.\n    \"\"\"\n\n    if keys:\n        k = keys.pop(0)\n        subdata = data[k] if k in data else {}\n        data[k] = deepset(subdata, keys, value)\n        return data\n\n    else:\n        return value",
        "sha1": "c24d4a18322047b1e847c2bb5471fe006e08ae49",
        "id": 257710
    },
    {
        "content": "def user_input_coords(string):\n    \"\"\"Converts user coordinate input from string to list\"\"\"\n    while True:  # makes sure the input has only two integer coordinates\n        inp = input(string)\n        if len([i for i in inp.split() if i.isdigit()]) == 2 and len(inp.split()) == 2:\n            return list(map(int, inp.split()))",
        "sha1": "c4f4e617777c7076e14fcba3fd84251b8f46edf9",
        "id": 43622
    },
    {
        "content": "def configure_chromatogram_builder(new_d,min_num_scans,group_intensity_threshold,min_peak_height,mz_tolerance):\n    \"\"\"\n    new_d = configure_chromatogram_builder(new_d,task.min_num_scans,task.group_intensity_threshold,task.min_peak_height,task.mz_tolerance)\n\n#     <batchstep method=\"net.sf.mzmine.modules.masslistmethods.ADAPchromatogrambuilder.ADAPChromatogramBuilderModule\">\n#         <parameter name=\"Raw data files\" type=\"ALL_FILES\"/>\n#         <parameter name=\"Scans\">\n#             <ms_level>1</ms_level>\n#         </parameter>\n#         <parameter name=\"Mass list\">masses</parameter>\n#         <parameter name=\"Min group size in # of scans\">5</parameter>\n#         <parameter name=\"Group intensity threshold\">1000000.0</parameter>\n#         <parameter name=\"Min highest intensity\">80000.0</parameter>\n#         <parameter name=\"m/z tolerance\">\n#             <absolutetolerance>0.002</absolutetolerance>\n#             <ppmtolerance>7.0</ppmtolerance>\n#         </parameter>\n#         <parameter name=\"Suffix\">chromatograms</parameter>\n#     </batchstep>\n\n\n    \"\"\"\n    idx = [i for i,d in enumerate(new_d['batch']['batchstep']) if 'ADAPChromatogramBuilderModule' in d['@method']][0]\n    idx2 = [i for i,d in enumerate(new_d['batch']['batchstep'][idx]['parameter']) if 'Min group size' in d['@name']][0]\n    new_d['batch']['batchstep'][idx]['parameter'][idx2]['#text'] = '%.3f'%(min_num_scans)\n\n    idx2 = [i for i,d in enumerate(new_d['batch']['batchstep'][idx]['parameter']) if 'Group intensity threshold' in d['@name']][0]\n    new_d['batch']['batchstep'][idx]['parameter'][idx2]['#text'] = '%.3f'%(group_intensity_threshold)\n\n    idx2 = [i for i,d in enumerate(new_d['batch']['batchstep'][idx]['parameter']) if 'Min highest intensity' in d['@name']][0]\n    new_d['batch']['batchstep'][idx]['parameter'][idx2]['#text'] = '%.3f'%(min_peak_height)\n\n    idx2 = [i for i,d in enumerate(new_d['batch']['batchstep'][idx]['parameter']) if 'm/z tolerance' in d['@name']][0]\n    new_d['batch']['batchstep'][idx]['parameter'][idx2]['ppmtolerance'] = '%.3f'%(mz_tolerance)\n\n    return new_d",
        "sha1": "673a1664e7b33a474f2ac32d91ee4f3e67b8cb30",
        "id": 123001
    },
    {
        "content": "import torch\n\n\ndef generate_fake_data(w, b, samples_num, pdf=\"normal\"):\n    \"\"\"Generate y = w*x + b + noises\"\"\"\n    m, n = w.size()\n    # print(m, n)\n    # mean_tensor = torch.arange(0.0, m + 1)\n    # std_tensor = torch.arange(0.0, m + 1)\n    # x = torch.normal(mean=mean_tensor, std=std_tensor)\n    x = torch.normal(mean=0, std=1, size=(n, samples_num))\n    # noise = torch.normal(0.0, 0.01, b.size())\n    y = torch.matmul(w, x) + b\n    # print(y)\n    return x, y",
        "sha1": "cd6dc0c8f3baaab475f081f87f7314a84089ce81",
        "id": 291089
    },
    {
        "content": "def fromTM35FINToMap(x, y):\n    \"\"\"\n    Function for moving from TM35FIN coordinates into the coordinates of the\n    picture\n    \"\"\"\n    pic_h = 2480\n    pic_w = 1748\n\n    x_min = -40442\n    y_min = 6555699\n    x_max = 837057\n    y_max = 7800801\n\n    y_max -= y_min\n    x_max -= x_min\n\n    x_return = pic_w * (x - x_min) / x_max\n    y_return = pic_h * (y_max - (y - y_min)) / y_max\n\n    return x_return, y_return",
        "sha1": "6519634fece8b1acc308f2262dc29f5ba45a00dc",
        "id": 210238
    },
    {
        "content": "def create_img_tag(oid, widths, summary):\n    \"\"\"\n    Creates an HTML <img> tag for an image post. Uses the OID, widths, and\n    optional summary for the different components of the tag.\n\n    Parameters\n    ----------\n    oid: A number representing the OID of the <img>'s associated post.\n    widths: A list of numbers representing each width of the image.\n    summary: A summary image that, if truthy, will cause an \"alt\" attribute to\n    be added to the tag.\n\n    Returns\n    -------\n    A string <img> tag.\n    \"\"\"\n\n    assets_url = '{{ site.assets_url }}'\n\n    # Use the second-to-smallest file (widths[1]) as the default.\n    src = '%s/%d-%d.jpg' % (assets_url, oid, widths[1])\n    srcset = [ '%s/%d-%d.jpg %dw' % (assets_url, oid, w, w) for w in widths ]\n    img_tag = '<img src=\"{0}\" '.format(src)\n    img_tag += 'srcset=\"{0}, {1}, {2}, {3}\" '.format(*srcset)\n    img_tag += 'sizes=\"(min-width: 700px) 50vw, calc(100vw - 2rem)\" '\n    img_tag += 'alt=\"{{ page.summary }}\" ' if summary else ''\n    img_tag += '/>'\n\n    return img_tag",
        "sha1": "4ee98a24d72b13a4dd1a4db4c37cd43d52fbdd26",
        "id": 295701
    },
    {
        "content": "import math\n\n\ndef _readString(data):\n\t\"\"\"Reads the next (null-terminated) block of data\n\t\"\"\"\n\tlength   = data.find(b'\\0')\n\tnextData = int(math.ceil((length+1) / 4.0) * 4)\n\treturn (data[0:length].decode('latin1'), data[nextData:])",
        "sha1": "e470dbb48250c46a578ab22a8179c8efb4dda9ca",
        "id": 272741
    },
    {
        "content": "def _check_not_in(obj, string):\n    \"\"\"\n    The 'i' substring of the list 'obj' must not be in 'string\n    :param obj: list of substring\n    :param string: element that needs to be checked\n    :return: False if a 'obj' substring is contained in 'string', True otherwise\n    \"\"\"\n    for i in obj:\n        if i in string:\n            return False\n    return True",
        "sha1": "5e0958a63dc089abb7809044e3d68ecb5216e013",
        "id": 645572
    },
    {
        "content": "def compute_etan(sch1, sch2, cos2phi, sin2phi):\n    \"\"\"Compute tangential ellipticity.\"\"\"\n    return - (sch1 * cos2phi + sch2 * sin2phi)",
        "sha1": "87c3a2831847604b88110b6c4c58aac2c73cce4d",
        "id": 622753
    },
    {
        "content": "def abfSort(IDs):\n    \"\"\"\n    given a list of goofy ABF names, return it sorted intelligently.\n    This places things like 16o01001 after 16901001.\n    \"\"\"\n    IDs=list(IDs)\n    monO=[]\n    monN=[]\n    monD=[]\n    good=[]\n    for ID in IDs:\n        if ID is None:\n            continue\n        if 'o' in ID:\n            monO.append(ID)\n        elif 'n' in ID:\n            monN.append(ID)\n        elif 'd' in ID:\n            monD.append(ID)\n        else:\n            good.append(ID)\n    return sorted(good)+sorted(monO)+sorted(monN)+sorted(monD)",
        "sha1": "6dce889d3d4aacf440745f13e55b60ef0e008cc8",
        "id": 559787
    },
    {
        "content": "def get_log_extra_item(log, item):\n    \"\"\"Returns an extra item from the logging object.\"\"\"\n    return log.extra.get(item, 'unknown')",
        "sha1": "6e60d685eb28c7a37984043e1bb28d546303aab7",
        "id": 285275
    },
    {
        "content": "def line_count(file_path, include_blank_line=True) -> int:\n    \"\"\"\n        \u8ba1\u7b97\u5355\u4e2a\u6587\u4ef6\u4e2d\u6709\u591a\u5c11\u884c\u5185\u5bb9\n        :param file_path:           \u6587\u4ef6\u8def\u5f84\n        :param include_blank_line:  \u8ba1\u7b97\u4e2d\u662f\u5426\u5305\u542b\u7a7a\u884c\uff0c\u9ed8\u8ba4\u5305\u542b\n    \"\"\"\n\n    with open(file_path, 'r', encoding='utf8') as f:\n        content = f.readlines()\n\n        if include_blank_line:\n            return len(content)\n\n        content2 = [x for x in content if len(x.strip()) != 0]\n        return len(content2)",
        "sha1": "6306639b1d97f788bd0bf62b8f0e4bcf5b373656",
        "id": 125780
    },
    {
        "content": "def isFloat(string):\n  \"\"\" Return a true is string is convertable to a float, otherwise false.\n  \"\"\"\n  try:\n    float(string)\n    return True\n  except ValueError:\n    return False",
        "sha1": "f37156b167a20aa80c600be56c65ed2945c99648",
        "id": 605609
    },
    {
        "content": "def test_if_duplicated_first_value_in_line_items_contents(lst):\n    \"\"\"Takes components and amounts as a list of strings and floats, and returns true if there is at least one repeated component name.\"\"\"\n    first_value = lst[0::2]\n    duplicated_first_value = [x for x in first_value if first_value.count(x) > 1]\n    return bool(len(duplicated_first_value) > 0)",
        "sha1": "fd308ac8452785a3c768f981fb6090b5747c5cfd",
        "id": 129612
    },
    {
        "content": "def idfs(corpus):\n    \"\"\" Compute IDF\n    Args:\n        corpus (RDD): input corpus\n    Returns:\n        RDD: a RDD of (token, IDF value)\n    \"\"\"\n\n    N = corpus.count()\n\n    # The result of the next line will be a list with distinct tokens...\n    unique_tokens = corpus.flatMap(lambda x: list(set(x)))  # No more records! FLATMAP --> unique_tokens is ONE SINGLE LIST\n    token_count_pair_tuple = unique_tokens.map(lambda x: (x, 1))  # every element in the list will become a pair!\n    token_sum_pair_tuple = token_count_pair_tuple.reduceByKey(lambda a, b: a + b)  # same elements in lists are aggregated\n    return token_sum_pair_tuple.map(lambda x: (x[0], float(N) / x[1]))",
        "sha1": "86fcc609c01049f3a5e61b44e1108c1084508f19",
        "id": 545792
    },
    {
        "content": "def has_marker(stacktrace, marker_list):\n  \"\"\"Return true if the stacktrace has atleast one marker in the marker list.\"\"\"\n  for marker in marker_list:\n    if marker in stacktrace:\n      return True\n\n  return False",
        "sha1": "15748b2fe47d07bfbed4e0b6b45c9bc5486ed0c4",
        "id": 277803
    },
    {
        "content": "def is_negative(b):\n    \"\"\"Returns True if the given byte represents a negative number.\n    \"\"\"\n    return ((b >> 7) & 0x1) == 1",
        "sha1": "9d897bb42a30ee0e50a6c2c52982f486e0a241f9",
        "id": 315731
    },
    {
        "content": "def load_bands(src, bands, masked=False):\n    \"\"\"\n    Load selected bands of a raster as array\n\n    *********\n\n    params:\n    --------\n\n        src : rasterio.DatasetReader object\n\n        bands : list\n                  list of bands to load, e.g. [1,2,3]\n\n        masked : bool (default:False)\n                 if True exclude nodata values\n\n\n    return:\n        tuple: array, metadata\n    \"\"\"\n    arr = src.read(bands, masked=masked)\n    metadata = src.profile\n    metadata.update({\n        'driver': 'GTiff',\n        'count': len(bands)})\n    return arr, metadata",
        "sha1": "e0c131ca93066387ae12f9bc6206626d4313cdf7",
        "id": 697527
    },
    {
        "content": "def gen_coords(clv, strand, window=50):\n    \"\"\"\n    generate the coordinates to be used for cutoff of searching range\n    \"\"\"\n    if strand == '+':\n        beg = clv - window + 1\n        # as clv is the last based of 3'UTR and 0-based, so it should be\n        # included in search\n        end = clv + 1\n    elif strand == '-':\n        beg = clv\n        end = clv + window\n    else:\n        raise ValueError('unknown strand: {0}'.format(strand))\n    return beg, end",
        "sha1": "579fb57393dcd0bf69c7f173af91259c21066d11",
        "id": 239088
    },
    {
        "content": "def is_placement_possible(y, x, n, board):\n\t\"\"\"\n\tReturns whether or not a number can be placed at a provided coordinate\n\tThis function uses the following format: board[y][x] == is_placement_possible(n)?\n\n\t:param x: the provided x coordinate (the column number from left)\n\t:param y: the provided y coordinate (the row number from the top)\n\t:param n: the integer for which to test placement in that particular location\n\t:param board: the current sudoku board for which to test on\n\t\"\"\"\n\n\t# Iterate over all possible indices for the rows  \n\tfor i in range(0, 9):\n\n\t\t# First check each cell in the provided row to see if the provided number is already present\n\t\tif board[y][i] == n:\n\n\t\t\t# If the provided number is found in the row, then it is not possible to place\n\t\t\treturn False\n\n\t\t# Then check each cell in the provided column to see if the provided number is already present \n\t\tif board[i][x] == n:\n\n\t\t\t# If the provided number is found in the column, then it is not possible to place\n\t\t\treturn False\n\n\t# Check within the current subsquare\n\t# First, determine which third of the rows the provided coordinates fall into\n\tsubsquare_x = (x//3)*3 # For example, if x=8, (x//3)*3 = 6, which is the first index of the 3rd horizontal subsquare of the board\n\tsubsquare_y = (y//3)*3 # For example, if y=2, (y//3)*3 = 0, which is the first index of the 1st vertical subsquare of the board\n\n\t# Once we know which subsquare we are placing into, we can iterate over each cell in that subsquare.\n\t# This is done by adding 0, 1 or 2 to the first index of both the vertical and horizontal coordinates of that subsquare  \n\tfor i in range(3):\n\t\tfor j in range(3):\n\t\t\tif board[subsquare_y+i][subsquare_x+j] == n:\n\n\t\t\t\t# If a cell within the subsquare contains the provided number, then it is not possible to place\n\t\t\t\treturn False\n\n\t# If we have reached here, then the provided number n is not present in the row, column, or subsquare. \n\t# Return True, since it is possible to place the number at these coordinates\n\treturn True",
        "sha1": "ae212ac9aae142a385035bfbc9cd49a9aacb2322",
        "id": 644840
    },
    {
        "content": "def application(environ, start_response):\n    \"\"\"\n    \u7b26\u5408WSGI\u6807\u51c6\u7684\u4e00\u4e2aHTTP\u5904\u7406\u51fd\u6570\uff0cweb\u7a0b\u5e8f\u5165\u53e3\n    :param environ: \u8bf7\u6c42\u7684\u4fe1\u606f\n    :param start_response: \u8fd4\u56de\u54cd\u5e94\u7684\u56de\u8c03\u51fd\u6570\n    :return:\n    \"\"\"\n    # \u54cd\u5e94\u4fe1\u606f\n    start_response(\"200 OK\", [(\"Content-Type\", \"text/html\")])\n\n    text = \"<h1>hello world</h1>\"\n    for i in environ.items():\n        print(i)\n        text += \"<div>{}</div>\".format(i)\n\n    # \u54cd\u5e94\u7684\u5185\u5bb9\u548c\u6570\u636e\n    return [text.encode(\"utf-8\")]",
        "sha1": "7b2a8fd72192d3c4791018a09598598bfe3c1b77",
        "id": 654374
    },
    {
        "content": "import math\n\n\ndef rotate_point(px, py, cx, cy, theta):\n    \"\"\"\n    Rotate a 2D point around a center\n    \"\"\"\n\n    dx = px - cx\n    dy = py - cy\n\n    new_dx = dx * math.cos(theta) + dy * math.sin(theta)\n    new_dy = dy * math.cos(theta) - dx * math.sin(theta)\n\n    return cx + new_dx, cy + new_dy",
        "sha1": "dd2d3bf963d2607a6bb9487e82db71844ee38044",
        "id": 232378
    },
    {
        "content": "def is_capitalized(text, pos, caps):\n    \"\"\"Checks if the current text position matches that start of a capitalized word\"\"\"\n    for cap in caps:\n        if text[pos:pos+len(cap)] == cap:\n            return True\n    return False",
        "sha1": "a4fe4d9c38cb3566c0f3558201b573487e264662",
        "id": 199359
    },
    {
        "content": "def is_module(node):\n  \"\"\"Returns whether node is a module node.\"\"\"\n  return node.__class__.__name__ == 'Module'",
        "sha1": "d7f26ec6ac97ca72b05dcf1893a08caf0888cc31",
        "id": 582590
    },
    {
        "content": "def resource_from_path(path):\n    \"\"\"Get resource name from path (first value before '.')\n\n    :param path: dot-separated path\n    :return: resource name\n    \"\"\"\n    index = path.find('.')\n    if index == -1:\n        return path\n    return path[:index]",
        "sha1": "ae5e00cec7be8dd94e4e7083a2db6a8878dabd2f",
        "id": 638522
    },
    {
        "content": "from typing import Dict\n\n\ndef is_transcript_segment_add(item: Dict) -> bool:\n    \"\"\"Checks if a dynamoDB Stream item is a transcript update event\"\"\"\n    return item.get(\"EventType\") == \"ADD_TRANSCRIPT_SEGMENT\"",
        "sha1": "baa9d6a07da7e1e5cc2d55c5a13c9e091dcf7d6f",
        "id": 410318
    },
    {
        "content": "def check_piece_collision(board, piece, i, j):\n    \"\"\"\n    Checks if a piece will collide with the board\n    \"\"\"\n    for x in range(len(piece)):\n        for y in range(len(piece[0])):\n            if piece[x][y] and board[i+x][j+y]:\n                return True\n    return False",
        "sha1": "f9ee5145338ddab4fe64338eab5f1f497b269623",
        "id": 535928
    },
    {
        "content": "from typing import Callable\nimport importlib\n\n\ndef get_method(pkg_path: str) -> Callable:\n    \"\"\"\n    Returns a method given the full package path of the method, e.g.\n    given the string ``oedtools.schema.get_schema`` it will return the\n    actual method ``get_schema`` from the ``oedtools.schema`` module.\n\n    :param pkg_path: Full package path of the method\n    :type pkg_path: str\n\n    :return: The actual method\n    :rtype: function\n    \"\"\"\n    path_tokens = pkg_path.split('.')\n    return getattr(importlib.import_module('.'.join(path_tokens[:-1])), path_tokens[-1])",
        "sha1": "324baedaed4cb7f5d217afb7645891a6f6eef16a",
        "id": 641921
    },
    {
        "content": "import torch\n\n\ndef val_epoch(model, val_loader, criterion, device):\n    \"\"\"Validate the model for 1 epoch\n    Args:\n        model: nn.Module\n        val_loader: val DataLoader\n        criterion: callable loss function\n        device: torch.device\n\n    Returns\n    -------\n    Tuple[Float, Float]\n        average val loss and average val accuracy for current epoch\n    \"\"\"\n\n    val_losses = []\n    val_corrects = []\n    model.eval()\n\n    # Iterate over data\n    with torch.no_grad():\n        for inputs, labels in val_loader:\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n\n            # prediction\n            outputs = model(inputs)\n\n            # calculate loss\n            _, preds = torch.max(outputs, 1)\n            loss = criterion(outputs, labels)\n\n            # statistics\n            val_losses.append(loss.item())\n            val_corrects.append(torch.sum(preds == labels.data).item())\n\n    return sum(val_losses)/len(val_losses), sum(val_corrects)/len(val_loader.dataset)",
        "sha1": "80576b4181f08a2a35276a78a143bbf59233dd9c",
        "id": 691832
    },
    {
        "content": "def _all_in_set_(cands, s):\n    \"\"\"\n    Test if all elements of 'cands' are in set 's'\n    \"\"\"\n    if len(cands) == 0:\n        return False\n    for c in cands:\n        if not c in s:\n            return False\n    return True",
        "sha1": "07ffa7d6d10fe60313c6308374b55802ea9c5ea5",
        "id": 630168
    },
    {
        "content": "def get_listing_panel(tool, ghidra):\n    \"\"\" Get the code listing UI element, so we can get up-to-date location/highlight/selection \"\"\"\n    cvs = tool.getService(ghidra.app.services.CodeViewerService)\n    return cvs.getListingPanel()",
        "sha1": "f14477cf13cb7eb4e7ede82b0c2068ca53a30723",
        "id": 4488
    },
    {
        "content": "def parse_max_acc(xmldoc):\n    \"\"\"parses the vehicle acceleration distributions from the VISSIM data\n    :param xmldoc:  input VISSIM xml\n    :type xmldoc:   xml.dom.minidom.Document\n    :return:    map of 1st acceleration function data point value by str(numeric id)\n    :rtype:     dict\n    \"\"\"\n    acc_d = dict()\n    for max_acc in xmldoc.getElementsByTagName('maxAccelerationFunction'):\n        acc_d[max_acc.getAttribute('no')] = max_acc.getElementsByTagName(\n            'accelerationFunctionDataPoint')[0].getAttribute('y')\n    return acc_d",
        "sha1": "030d0548c37fcf14c204a5680112c3bec18a3418",
        "id": 333340
    },
    {
        "content": "def can_preview(file):\n    \"\"\"Determine if the given file can be previewed.\"\"\"\n    supported_extensions = ('.jpg', '.jpeg', '.png', '.tif', '.tiff')\n    return file.has_extensions(*supported_extensions)",
        "sha1": "bbe1d23bcabe5666b950b961dbd554ccbf13f700",
        "id": 468838
    },
    {
        "content": "def match_i(instance1,instance2):\n    \"\"\"Return True if given instances match, i.e., contain the same vertex and edge object instances.\"\"\"\n    return (instance1.vs == instance2.vs) and (instance1.es == instance2.es)",
        "sha1": "62000889756b79d07932e93b7877792c23f4b89c",
        "id": 278570
    },
    {
        "content": "def parse_core_web_vitals(report: dict):\n    \"\"\"Return a dictionary containing the Core Web Vitals from the report.\n\n    Args:\n        report (dict): JSON dictionary containing report data.\n\n    Return:\n        data (dict): Dictionary containing the key data.\n\n    \"\"\"\n\n    final_url = report['lighthouseResult']['finalUrl']\n    fetch_time = report['lighthouseResult']['fetchTime']\n    form_factor = report['lighthouseResult']['configSettings']['formFactor']\n    overall_score = report[\"lighthouseResult\"][\"categories\"][\"performance\"][\"score\"] * 100\n    speed_index = report[\"lighthouseResult\"][\"audits\"][\"speed-index\"][\"score\"] * 100\n    first_meaningful_paint = report[\"lighthouseResult\"][\"audits\"][\"first-meaningful-paint\"][\"score\"] * 100\n    first_contentful_paint = report[\"lighthouseResult\"][\"audits\"][\"first-contentful-paint\"][\"score\"] * 100\n    time_to_interactive = report[\"lighthouseResult\"][\"audits\"][\"interactive\"][\"score\"] * 100\n    total_blocking_time = report[\"lighthouseResult\"][\"audits\"][\"total-blocking-time\"][\"score\"] * 100\n    cumulative_layout_shift = report[\"lighthouseResult\"][\"audits\"][\"cumulative-layout-shift\"][\"score\"] * 100\n\n    data = {\n        'final_url': final_url,\n        'fetch_time': fetch_time,\n        'form_factor': form_factor,\n        'overall_score': overall_score,\n        'speed_index': speed_index,\n        'first_meaningful_paint': first_meaningful_paint,\n        'first_contentful_paint': first_contentful_paint,\n        'time_to_interactive': time_to_interactive,\n        'total_blocking_time': total_blocking_time,\n        'cumulative_layout_shift': cumulative_layout_shift,\n    }\n\n    return data",
        "sha1": "d077636a20fb448b6427a751b39f799b0e946240",
        "id": 582622
    },
    {
        "content": "import copy\n\n\ndef get_midpoint_radius(pos):\n    \"\"\"Return the midpoint and radius of the hex maze as a tuple (x,y), radius.\n\n    Params\n    ======\n    pos: PositionArray\n        nelpy PositionArray containing the trajectory data.\n\n    Returns\n    =======\n    midpoint: (x0, y0)\n    radius: float\n    \"\"\"\n    # make a local copy of the trajectory data\n    local_pos = copy.copy(pos)\n\n    # merge the underlyng support to make computations easier\n    local_pos._support = pos.support.merge(gap=10)\n\n    # apply smoothing to tame some outliers:\n    local_pos = local_pos.smooth(sigma=0.02)\n\n    midpoint = local_pos.min() + (local_pos.max() - local_pos.min())/2\n    radius = ((local_pos.max() - local_pos.min())/2).mean()\n\n    return midpoint, radius",
        "sha1": "76290309f12e00b6a487f71b2393fabd8f3944ac",
        "id": 41570
    },
    {
        "content": "def sanitize_cr(clientrep):\n    \"\"\" Removes probably sensitive details from a client representation\n    :param clientrep: the clientrep dict to be sanitized\n    :return: sanitized clientrep dict\n    \"\"\"\n    result = clientrep.copy()\n    if 'secret' in result:\n        result['secret'] = 'no_log'\n    if 'attributes' in result:\n        if 'saml.signing.private.key' in result['attributes']:\n            result['attributes']['saml.signing.private.key'] = 'no_log'\n    return result",
        "sha1": "1020faf7dabce2aa33d43002d80c89a0853699c1",
        "id": 621400
    },
    {
        "content": "from typing import List\n\n\ndef tokenize(x) -> List[str]:\n    \"\"\"\n    :param x: string to tokenize\n    :return: x tokenized with whitespace tokenization\n    \"\"\"\n    return x.split()",
        "sha1": "6922dcae39b36bcbcda85c0a59a3cb45c4d65794",
        "id": 509216
    },
    {
        "content": "from platform import machine\n\n\ndef architecture() -> str:\n    \"\"\"Returns the machine type, (e.g.: 'x86_64' or 'i386')\"\"\"\n    return machine()",
        "sha1": "5ba7a2ef66dda359cd6910fd09a934eeb4af4eff",
        "id": 238541
    },
    {
        "content": "def get_slope(vector_one, vector_two):\n    \"\"\"Get the slope of a line specified by two vectors\"\"\"\n    return(vector_two[1] - vector_one[1])/(vector_two[0] - vector_one[0])",
        "sha1": "2d20b5fe0fe3734fb66b0b2fc7a3a7aee2811694",
        "id": 655605
    },
    {
        "content": "from typing import List\n\n\ndef batch_inference_result_split(results: List, keys: List):\n    \"\"\"\n    Split inference result into a dict with provided key and result dicts.\n    The length of results and keys must match\n    :param results:\n    :param keys:\n    :return:\n    \"\"\"\n\n    ret = {}\n    for result, key in zip(results, keys):\n        ret[key] = result\n\n    return ret",
        "sha1": "e7b977cf92fd8b5adad12b6aee839e4df3d7b0aa",
        "id": 676720
    },
    {
        "content": "def get_results_parameters(config: dict) -> tuple:\n    \"\"\"Returns recipe parameters after sanity check\n\n    :param dict config: parameters defined in the recipe settings\n    :raises: :class:`ValueError`: Missing parameters\n\n    :returns: Parameters\n    :rtype: tuple\n    \"\"\"\n    reference_column = config.get(\"user_reference\", None)\n    group_column = config.get(\"group_column\", None)\n    conversion_column = config.get(\"conversion_column\", None)\n\n    if not reference_column:\n        raise ValueError(\"User reference column is missing\")\n    if not group_column:\n        raise ValueError(\"Group column is missing\")\n    if not conversion_column:\n        raise ValueError(\"Conversion column is missing\")\n    return reference_column, group_column, conversion_column",
        "sha1": "5d0ab4c82517928e8998377283a8697ef1c27e3c",
        "id": 105613
    },
    {
        "content": "import json\n\n\ndef read_json(file_path: str) -> dict:\n    \"\"\"Reads a json file and returns the dict\n    \"\"\"\n    with open(file_path) as f:\n        return json.load(f)",
        "sha1": "93df73379300052dbecc6706050b0779ac568b98",
        "id": 668236
    },
    {
        "content": "def find_constant_features(data):  \n    \"\"\"\n    Get a list of the constant features in a dataframe. \n    \"\"\"\n    const_features = []\n    for column in list(data.columns):\n        if data[column].unique().size < 2:\n            const_features.append(column)\n    return const_features",
        "sha1": "58fb8f98e6da9026b26437fb93230258312714d6",
        "id": 643692
    },
    {
        "content": "def label_smooth(num_classes, epsilon, encoded_data_point):\n    \"\"\"\n    Simple label smoothing \n    Input: num_classes: number of classes\n           epsilon: epsilon value \n           encoded_data_point: one hot encoded data point\n    \"\"\"\n    smoothed_point = encoded_data_point * [1. - epsilon] + (1. - encoded_data_point) * (epsilon/float(num_classes - 1.))\n    return smoothed_point",
        "sha1": "60a36b73c31538b8946afc41d94984ef5df1589b",
        "id": 574372
    },
    {
        "content": "import re\n\n\ndef getidtype(identifier):\n    \"\"\" try and determine the type of identifier given \"\"\"\n\n    # is the string an inchikey?\n    m = re.search('^[A-Z]{14}-[A-Z]{10}-[A-Z]$', identifier)\n    if m:\n        return \"inchikey\"\n\n    # is the string an inchi?\n    n = re.search('^InChI=1S?/', identifier)\n    if n:\n        return \"inchi\"\n\n    # is the string a CASRN?\n    o = re.search('^[0-9]{2,7}-[0-9]{2}-[0-9]$', identifier)\n    if o:\n        return \"casrn\"\n\n    # is the string a ChEMBL ID?\n    p = re.search('^CHEMBL[0-9]+$', identifier)\n    if p:\n        return \"chemblid\"\n\n    # is the string a DSSTOX ID?\n    q = re.search('^DTXSID[0-9]+$', identifier)\n    if q:\n        return \"dsstox\"\n\n    return \"other\"",
        "sha1": "f853d512029b0034e9657a7260b233f3b9a9303e",
        "id": 70855
    },
    {
        "content": "def numberNs(orfSeq):\n\n        \"\"\"Function to count number of N's in a sequence\n\n        :param orfSeq: Str of orf sequence\n        :return:\n                numNs: Int number of N's\n        \"\"\"\n\n        numNs = 0\n\n        for i in  orfSeq:\n                if i == 'N':\n                        numNs+=1\n\n        return(numNs)",
        "sha1": "03ffc82970b13d1772ee48196817d8fc606e6153",
        "id": 185115
    },
    {
        "content": "def checkGameStatus(board, max_tile=2048):\n    \"\"\"\n    Update the game status by checking if the max. tile has been obtained.\n\n    Parameters:\n        board (list): game board\n        max_tile (int): tile number required to win, default = 2048\n    Returns:\n        (str): game status WIN/LOSE/PLAY\n    \"\"\"\n    flat_board = [cell for row in board for cell in row]\n    if max_tile in flat_board:\n        # game has been won if max_tile value is found\n        return \"WIN\"\n\n    for i in range(4):\n        for j in range(4):\n            # check if a merge is possible\n            if j != 3 and board[i][j] == board[i][j+1] or \\\n                    i != 3 and board[i][j] == board[i + 1][j]:\n                return \"PLAY\"\n\n    if 0 not in flat_board:\n        return \"LOSE\"\n    else:\n        return \"PLAY\"",
        "sha1": "6750ddbcd354366731119835da65457e1d0af778",
        "id": 149271
    },
    {
        "content": "def build_type_dict(converters):\n    \"\"\"\n    Builds type dictionary for user-defined type converters,\n    used by :mod:`parse` module.\n    This requires that each type converter has a \"name\" attribute.\n\n    :param converters: List of type converters (parse_types)\n    :return: Type converter dictionary\n    \"\"\"\n    more_types = {}\n    for converter in converters:\n        assert callable(converter)\n        more_types[converter.name] = converter\n    return more_types",
        "sha1": "b19af359448d06898948a9f7afa121c1c3095d24",
        "id": 127911
    },
    {
        "content": "def hamming_distance(seq1, seq2):\n    \"\"\"\n    Calculate the Hamming distance between two sequences \n    of the same length. \n    The Hamming distance corresponds to the number of characters\n    that differ between these two sequences.\n\n    Args:\n        seq1 (str), seq2 (str): Sequences to compare.\n\n    Returns:\n        Hamming distance (int) of seq1 and seq2.\n    \"\"\"\n    dist = sum([char1 != char2 for char1, char2 in zip(seq1, seq2)])\n    return dist",
        "sha1": "2e71d1276498a71cd6ca196d525560a7ba8e7046",
        "id": 76789
    },
    {
        "content": "from typing import Iterable\n\n\ndef all_unique(iterable: Iterable):\n    \"\"\"\n    Utility function for checking if all the elements are unique\n    \"\"\"\n    try:\n        seen = set()\n        return not any(i in seen or seen.add(i) for i in iterable)\n    except TypeError:\n        # Exception handling for unhashable types\n        seen = list()\n        return not any(i in seen or seen.append(i) for i in iterable)",
        "sha1": "afb243193fa232c2e44ccc37b37a443b6b048389",
        "id": 435371
    },
    {
        "content": "def bytes_to_hex(input_bytes:bytes) -> str:\n    \"\"\" Convert bytes to hex string\"\"\"\n    return ''.join('{:02x}'.format(x) for x in input_bytes)",
        "sha1": "6f61dccd3f22327dbfdd17a28bdb024484b49a78",
        "id": 423338
    },
    {
        "content": "def lerp(a,b,t):\n    \"\"\" Linear interpolation between from @a to @b as @t goes between 0 an 1. \"\"\"\n    return (1-t)*a + t*b",
        "sha1": "12cb8690ba5e5f2a4c08c1cd29d3497513b63438",
        "id": 394
    },
    {
        "content": "import string\nimport re\n\n\ndef remove_punctuation(word):\n    \"\"\"\n    Removes all punctuation marks from a word except for '\n    that is often a part of word: don't, it's, and so on\n    \"\"\"\n    all_punct_marks = string.punctuation.replace(\"'\", '')\n    return re.sub('[' + all_punct_marks + ']', '', word)",
        "sha1": "02667dbeada5fdd224f62852b2ec44590bb2b618",
        "id": 84550
    },
    {
        "content": "def transcript_names_from_patacsdb_csv(filename='patacsdb_all.csv'):\n    \"\"\"\n    Load stable ensembl transcript ids from given csv file.\n    The default file was exported from homo sapiens dataset\n    of patacsdb database (sysbio.ibb.waw.pl/patacsdb).\n\n    The identifiers are expected to be located in third column of each row.\n    \"\"\"\n    transcripts = set()\n    with open(filename) as f:\n        header = next(f)     # skip header\n        assert header == '\"Protein name\",\"Gene id\",\"Transcript id\",\"Location (%)\",Sequence'\n        for line in f:\n            data = line.strip().split(',')\n            transcripts.add(data[2])\n\n    return list(transcripts)",
        "sha1": "fe2106ee0f940babb4b40f0451f9c03caa3f82a4",
        "id": 156015
    },
    {
        "content": "def state_to_error_response(state):\n    \"\"\"Return None if the zone is OK, otherwise return an ErrorResponse.\"\"\"\n    if state['state'] == 'Stale':\n        return {\n            'type': 'ENDPOINT_UNREACHABLE',\n            'message': ''\n        }\n    return None",
        "sha1": "3a9e9f01dc7dc6677fcd5877c76259555bd29e01",
        "id": 429190
    },
    {
        "content": "def get_deriv_indices(grad):\n    \"\"\"\n    Returns the indices of the derivatives involved in the grid derivatives\n\n    Examples\n    --------\n    >>> get_deriv_indices(1)\n    [\"x\", \"y\", \"z\"]\n    \"\"\"\n    if grad == 0:\n        return []\n    elif grad == 1:\n        return [\"x\", \"y\", \"z\"]\n    elif grad == 2:\n        return [\"x\", \"y\", \"z\", \"xx\", \"xy\", \"xz\", \"yy\", \"yz\", \"zz\"]\n    else:\n        raise ValueError(\"Only grid derivatives up to Hessians is supported (grad=2).\")",
        "sha1": "90cb0868e6eb873d266c2a1973fb5dde830201c6",
        "id": 398770
    },
    {
        "content": "def color_rgb(red, green, blue):\n    \"\"\" Given three intensities red, green, blue, all from 0 to 255,\n        returns the corresponding CSS color string e.g. '#ff00cc' \"\"\"\n    return f\"#{red*256**2 + green*256 + blue:06x}\"",
        "sha1": "be02aea5a0fc04f7bbedce4b3fc18fb5cdfff862",
        "id": 646878
    },
    {
        "content": "def _andparam(params):\n    \"\"\"\n    string join parameter list with &\n    \"\"\"\n    return \"&\".join(params)",
        "sha1": "ca7cedf4dd1169dd83f83a97f707f2e43eeaa8f7",
        "id": 64408
    },
    {
        "content": "def read_with_check(buf, pos, nbytes):\n    \"\"\"\n    Reads and returns n bytes.\n    :param buf: The input buffer\n    :param pos: The current position\n    :param nbytes: number of bytes to open_topography in\n    :return: The bytes and the new position in the buffer\n    \"\"\"\n\n    if len(buf) < nbytes or len(buf) - nbytes < pos:\n        raise ValueError(\"Some sizes went wrong.\")\n\n    out = buf[pos:pos + nbytes]\n    pos += int(nbytes)\n\n    out = out[0] if nbytes == 1 else out\n    return out, pos",
        "sha1": "3b8efed1c908adc02538666b2e580f24ddbbf2be",
        "id": 178750
    },
    {
        "content": "def native_string_to_bytes(s, encoding=\"ascii\", errors=\"strict\"):\n    \"\"\"\n    Ensure that the native string ``s`` is converted to ``bytes``.\n    \"\"\"\n    if not isinstance(s, str):\n        raise TypeError(\"{} must be type str, not {}\".format(s, type(s)))\n    if str is bytes:\n        # Python 2\n        return s\n    else:\n        # Python 3\n        return s.encode(encoding=encoding, errors=errors)",
        "sha1": "54f33246a76079e7fa377031f8a3f1d2f4f18a86",
        "id": 532325
    },
    {
        "content": "def readattr(_obj, _name, _default=None, _error_on_none=None):\n    \"\"\"\n    Reads an attribute value from an object\n    :param _obj: The object\n    :param _name: The name of the attribute\n    :param _default: If set, the value to return if the attribute is missing or the value is None\n    :param _error_on_none: If set, the message of an exception that is raised if the attribute is missing or the value is None\n    :return: The value of the attribute\n    \"\"\"\n\n    try:\n        _value = getattr(_obj, _name)\n    except AttributeError:\n        if _error_on_none:\n            raise Exception(_error_on_none)\n        else:\n            return _default\n\n    if _value is None:\n        if _error_on_none:\n            raise Exception(_error_on_none)\n        else:\n            return _default\n    else:\n        return _value",
        "sha1": "357317473d238b63aa02c5cb7a720ad39a454237",
        "id": 624239
    },
    {
        "content": "from typing import Tuple\nfrom typing import List\n\n\ndef load_hurdat(filepath: str) -> Tuple[List[str], List[str]]:\n    \"\"\"\n    Extracts hurricane data from hurdat2.txt file\n\n    Parameters\n    ----------\n\n    filepath : The pathname to the hurdat2.txt file\n\n\n    Return\n    ------\n\n    tracks : List[str]\n                Contains all geographical data from hurdat2.txt\n    hurricanes : List[str]\n                Contains all hurricanes with ID, name and number of corresponding data points\n    \"\"\"\n\n    # Loading the text file:\n    tracks = []\n    hurricanes = []\n\n    # When reading the text file, we separate two possible line formats: whether the line contains an ID or not. This is\n    # due to the fact that ID's are followed by the corresponding data points without anymore mention of the ID.\n    # So it will be easier, later on to just add the ID's separately.\n    with open(filepath) as text:\n        for line in text:\n\n            if line.startswith('AL'):\n                hurricanes.append(line)\n            else:\n                tracks.append(line)\n\n    print(hurricanes[:5])\n    print('\\n')\n    print(tracks[:15])\n    print('\\n')\n    print('There are {} different hurricanes, for a total of {} measurements.'\n          .format(len(hurricanes), len(tracks)))\n    print('\\n')\n\n    return tracks, hurricanes",
        "sha1": "1ec7eeb71d25176dfac5b8848ab29004acd3563d",
        "id": 101728
    },
    {
        "content": "def get_model_verbose_name(instance):\n    \"\"\"\n    This tag returns the readable name of an instance's model.\n\n    :param instance: The model object instance\n    :type instance: ~django.db.models.Model\n\n    :return: The verbose name of the model\n    :rtype: str\n    \"\"\"\n    return instance._meta.verbose_name",
        "sha1": "dcc837f0e2b6f47cbe36a1a161c6c45118de7fa0",
        "id": 153003
    },
    {
        "content": "def get_atom_numbers(structure):\n    \"\"\"get a list of atom numbers composing a biopython structure\n\n    it gives you the atom numbers from the atoms composing the given\n    biopython structure\n\n    Parameters\n    ------------\n    structure : bioopython structure or list of atoms\n\n    Returns\n    -----------\n    list(int)\n        the list of all the atom numbers composing the structure\n\n    Notes\n    ------------\n    useful when definig an area of the structure (like an alchemical region)\n    with atom indexes (atom numbers)\n    \"\"\"\n\n    atom_numbers = []\n\n    if hasattr(structure, 'get_atoms'):\n\n        atoms = structure.get_atoms()\n\n    else:  # list of atoms\n\n        atoms = structure\n\n    for atom in atoms:\n\n        atom_numbers.append(atom.get_serial_number())\n\n    return atom_numbers",
        "sha1": "36e5164eb4acc1eac3e5bc8463b2da61e1289e39",
        "id": 85852
    },
    {
        "content": "import mimetypes\n\n\ndef get_mime_type(name):\n    \"\"\"\n    Get the MIME type of the given file based on it's file extension.\n\n    Args:\n        name (str): the name of the file including the extension\n\n    Returns:\n        The MIME type of the file and None if the MIME type cannot be found.\n    \"\"\"\n\n    return mimetypes.guess_type(name)[0]",
        "sha1": "437dc80ef669c4041e423313810a60d62793deb8",
        "id": 540827
    },
    {
        "content": "def _map_nodes_to_int(nodes):\n    \"\"\"\n    Return a dict mapping a list of nodes to their sorted indices. Nodes should\n    be a list of strings.\n\n    Returns:\n    --------\n    Dict[str, int]\n    \"\"\"\n    sorted_node_set = sorted(set(nodes))\n    name_to_id = {name: i for i, name in enumerate(sorted_node_set)}\n    return name_to_id",
        "sha1": "e878fe0c458cc516b6cbf612c076f85347c38bc0",
        "id": 252134
    },
    {
        "content": "def _change_galician_cc_token_before_subtree(sentence, token):\n\t\"\"\"Determine the token directly preceding a subtree in a sentence.\n\t\n\tArgs:\n\t\tsentence (`Sentence`): The sentence.\n\t\ttoken (`Token`): The root token of the subtree.\n\t\n\tReturns:\n\t\tstr: The ID of the token directly preceding the root token and all of its dependents.\n\n\t\"\"\"\n\ttokens = [token.id]\n\ta = True\n\twhile a:\n\t\ta = False\n\t\tfor t in sentence:\n\t\t\tif t.head in tokens and t.id not in tokens:\n\t\t\t\ttokens.append(t.id)\n\t\t\t\ta = True\n\ttokens = sorted(tokens)\n\treturn str(int(tokens[0])-1)",
        "sha1": "21eebf0ba9ec02af4a7c65e8631a95e83b68372e",
        "id": 697954
    },
    {
        "content": "import random\n\n\ndef roll_dice(probability):\n    \"\"\"given a probability, generate 1 or 0\"\"\"\n    randomResult = random.uniform(0, 1)\n    if probability >= randomResult:\n        return 1\n    else:\n        return 0",
        "sha1": "2272a5b02fbe1daa8a399cdb59f1ab7971febf40",
        "id": 102195
    },
    {
        "content": "def quote_xml_value(dom, value):\n    \"\"\"quote_xml_value(dom, value) -> value\n\n       quotes a value as an xml node so that\n       eval_xml_value(quote_xml_value(dom, value)) == value\n\n       <str value='foo'/> <- 'foo'\n       <int value='3'/> <- 3\n       <float value='3.141592'> <- 3.141592\n       <bool value='False'> <- False\n    \"\"\"\n\n    el = dom.createElement(type(value).__name__)\n    el.setAttribute('value', str(value))\n    return el",
        "sha1": "3495c7d29f92c9d3a9e66e776ff0183e6300b6c3",
        "id": 175271
    },
    {
        "content": "def prepend_chr(ser):\n    \"\"\"\n    prepend_chr(ser)\n    Add \"chr\" to the beginning of every element in the Series.\n    If the \"chr\" prefix has already exist, nothing will be appended but\n    the type of Series elements will be changed into str.\n\n    Parameters\n    ----------\n    ser: pd.Series\n        A Series of the chromosome number.\n    \n    Returns\n    -------\n    A Series of chromosome numbers in \"chr\" notation.\n    \"\"\"\n    dict_regex = {\"^([0-9]+|[XY]|MT)\":r\"chr\\1\", r\"^(M)\":r\"chrMT\"}\n    return ser.astype(str).replace(regex=dict_regex)",
        "sha1": "9059bb5dfd2ac4708d46aa096daa2000f991ef76",
        "id": 570851
    },
    {
        "content": "def calculate_next_closing_price(last_close, close):\n    \"\"\" Return closing price\n\n    :param last_close: closing price of last candle (not necessary)\n    :param close: closing price of candle to predict\n    :return: float\n    \"\"\"\n    return close",
        "sha1": "22e113a9329af228013e51b450e2cc71511c6ec3",
        "id": 166933
    },
    {
        "content": "def drag_force(c_d,A,rho,v):\n    \"\"\"\n    Calculate drag force given c_d,A,rho,v:\n        rho -> density of fluid\n        A   -> Frontal area\n        c_d -> Drag coefficient\n        v   -> Velocity of bicycle\n    \"\"\"\n    return 0.5*c_d*rho*A*v**2",
        "sha1": "f31caac5b43ad0387ededb225effa31b6f06b86c",
        "id": 666181
    },
    {
        "content": "def transport_degree_factor(\n    temperature,\n    deadband_lower=15,\n    deadband_upper=20,\n    lower_degree_factor=0.5,\n    upper_degree_factor=1.6):\n    \"\"\"\n    Work out how much energy demand in vehicles increases due to heating and cooling.\n    There is a deadband where there is no increase.\n    Degree factors are % increase in demand compared to no heating/cooling fuel consumption.\n    Returns per unit increase in demand for each place and time\n    \"\"\"\n\n    dd = temperature.copy()\n\n    dd[(temperature > deadband_lower) & (temperature < deadband_upper)] = 0.\n\n    dT_lower = deadband_lower - temperature[temperature < deadband_lower]\n    dd[temperature < deadband_lower] = lower_degree_factor / 100 * dT_lower\n\n    dT_upper = temperature[temperature > deadband_upper] - deadband_upper\n    dd[temperature > deadband_upper] = upper_degree_factor / 100 * dT_upper\n\n    return dd",
        "sha1": "ea5d5128fa985ea566aefe8b0f7b5875878a9838",
        "id": 600331
    },
    {
        "content": "def summary(task):\n    \"\"\"Given an ImportTask, produce a short string identifying the\n    object.\n    \"\"\"\n    if task.is_album:\n        return u'{0} - {1}'.format(task.cur_artist, task.cur_album)\n    else:\n        return u'{0} - {1}'.format(task.item.artist, task.item.title)",
        "sha1": "87387c47e90998c270f6f8f2f63ceacebd4cdc78",
        "id": 658
    },
    {
        "content": "async def hello_world():\n    \"\"\"A very simple Hello World example that simply returns a json response.\n    \n    This is where you would add additional information about the endpoint.\n    As you can see you can use standard docStrings for this section.\n    \"\"\"\n    return {\"Message\": \"Hello World!\"}",
        "sha1": "6ce9e5c582ded176e82dae2f3990771469058c35",
        "id": 67696
    },
    {
        "content": "def __create_python_code_block(message):\n    \"\"\"Create a python code block\"\"\"\n    return f\"```python\\n{message}```\"",
        "sha1": "8397187487af0780542e8a227118994e1fc8ced8",
        "id": 21364
    },
    {
        "content": "import re\n\n\ndef add_datepart(df, fieldname):\n    \"\"\"\n    Adds date related features to dataframe df inplace\n    df: dataframe\n    fieldname: name of the date field in df\n    \"\"\"\n    new_df = df.copy()\n    field = df[fieldname]\n    target_prefix = re.sub('[Dd]atetime$', '', fieldname)\n    \n    date_features = (\n         'hour',\n         'minute',\n         'Year', \n         'Month', \n         'Week', \n         'Day', \n         'Dayofweek', \n         'Dayofyear', \n    )\n    \n    for name in date_features:\n        new_df[target_prefix+name] = getattr(field.dt, name.lower())\n        \n    new_df[target_prefix+'Elapsed'] = (field - field.min()).dt.days\n    new_df[target_prefix+'MonthName'] = field.dt.month_name()\n    new_df[target_prefix+'DayName'] = field.dt.day_name()\n    new_df.drop(fieldname, axis=1, inplace=True)\n\n    return new_df",
        "sha1": "7774056dd856553501f288c9c97dbc04d359ae59",
        "id": 651549
    },
    {
        "content": "import string\nimport random\n\n\ndef generate_random_password(length=10):\n    \"\"\"Generate a random password with letters, numbers, and special characters\n    \"\"\"\n    password_characters = string.ascii_letters + string.digits\n    password = \"\".join(random.choice(password_characters) for i in range(length))\n    return password",
        "sha1": "98e1a54ed39256f7c90f78c9f8a1b4988e33bbb3",
        "id": 489342
    },
    {
        "content": "from typing import Tuple\nfrom typing import Dict\nimport re\n\n\ndef _handle_compatibility(doc) -> Tuple[str, Dict[str, str]]:\n  \"\"\"Parse and remove compatibility blocks from the main docstring.\n\n  Args:\n    doc: The docstring that contains compatibility notes.\n\n  Returns:\n    A tuple of the modified doc string and a hash that maps from compatibility\n    note type to the text of the note.\n  \"\"\"\n  compatibility_notes = {}\n  match_compatibility = re.compile(\n      r'[ \\t]*@compatibility\\(([^\\n]+?)\\)\\s*\\n'\n      r'(.*?)'\n      r'[ \\t]*@end_compatibility', re.DOTALL)\n  for f in match_compatibility.finditer(doc):\n    compatibility_notes[f.group(1)] = f.group(2)\n  return match_compatibility.subn(r'', doc)[0], compatibility_notes",
        "sha1": "24fe3a59e6df83dbb419eb3db199fc23341443ea",
        "id": 499809
    },
    {
        "content": "def monkeypatch(klass, methodname=None):\n    \"\"\"Decorator extending class with the decorated callable. This is basically\n    a syntactic sugar vs class assignment.\n\n    >>> class A:\n    ...     pass\n    >>> @monkeypatch(A)\n    ... def meth(self):\n    ...     return 12\n    ...\n    >>> a = A()\n    >>> a.meth()\n    12\n    >>> @monkeypatch(A, 'foo')\n    ... def meth(self):\n    ...     return 12\n    ...\n    >>> a.foo()\n    12\n    \"\"\"\n    def decorator(func):\n        try:\n            name = methodname or func.__name__\n        except AttributeError:\n            raise AttributeError('%s has no __name__ attribute: '\n                                 'you should provide an explicit `methodname`'\n                                 % func)\n        setattr(klass, name, func)\n        return func\n    return decorator",
        "sha1": "53fc23cba5287e39cf252dc6a3263768d3f227b3",
        "id": 370380
    },
    {
        "content": "def _ensure_iterable(x):\n    \"\"\"\n    If the object is iterable, return the object.\n    Else, return the object in a length 1 list.\n    \"\"\"\n    return x if hasattr(x, \"__iter__\") else [x]",
        "sha1": "f794a19b89b087b9454321a08ad1de3e4bbf0dd4",
        "id": 241794
    },
    {
        "content": "def cat_charsets(cs):\n  \"\"\"Combine a set into an alphabetically sorted list in written English,\n  using commas and 'and'. \"\"\"\n  d = sorted(cs)\n  if len(d) > 2:\n    d[-1] = \"and \" + d[-1]\n    d = \", \".join(d)\n  else:\n    d = \" and \".join(d)\n  return d",
        "sha1": "1d5992529b64a8427c353f330c2f47185701a49e",
        "id": 235656
    },
    {
        "content": "def verifyRasterNum(alg, parameters, context, rasters, mini, maxi=None):\n    \"\"\"Verify that we have at least n rasters in multipleInput\"\"\"\n    num = len(alg.parameterAsLayerList(parameters, rasters, context))\n    if num < mini:\n        return False, 'You need to set at least {} input rasters for this algorithm!'.format(mini)\n    if maxi and num > maxi:\n        return False, 'You need to set a maximum of {} input rasters for this algorithm!'.format(maxi)\n    return True, None",
        "sha1": "70b9125e79262453db66768fa17023d86cfeb571",
        "id": 618801
    },
    {
        "content": "def title_body(text):\n    \"\"\"Split text into its first line (the title) and the rest of the text.\"\"\"\n    newline = text.find(\"\\n\")\n    if newline < 0:\n        return text, \"\"\n    return text[:newline], text[newline:]",
        "sha1": "b8ecfc8fc91c2e8ae3180c0fdd9d4626a9d4ebc6",
        "id": 438303
    },
    {
        "content": "import random\n\n\ndef selector(input_list, check_list, return_list):\n    \"\"\" Repeat input several times.\n   \n    Parameters\n    ----------\n    input_list : list\n        List that contains a list of input.\n    \n    check_list : list\n        List that checks whether input contains certain items.\n    \n    return_list : list\n        List contains items that will be drawn randomly.\n        \n    Returns\n    -------\n    output : string\n        String to display the result of a random choice in a list given certain conditions met.\n   \"\"\"\n    \n    \n    output = None\n    for item in input_list:\n        if item in check_list:\n            output = random.choice(return_list)\n            break\n    return output",
        "sha1": "d5bd4ebc50d45c07282cae35b937a2fc171ed977",
        "id": 192352
    },
    {
        "content": "def loadingSign(state):\n    \"\"\"\n    Loading sign for long process...\n    :param state: current character state\n    :return: next character state\n    \"\"\"\n    if state == '|':\n        return '/'\n    elif state == '/':\n        return '\u2014\u2014'\n    elif state == '\u2014\u2014':\n        return \"\\\\\"\n    elif state == '\\\\':\n        return \"|\"",
        "sha1": "e7fb02264df6b0322395bee26a6a4be73eabdb0a",
        "id": 131433
    },
    {
        "content": "def get_md_links(link_list):\n    \"\"\"Return an unordered list of links in markdown syntax.\"\"\"\n    html_links = \"\\n\"\n    for link in link_list:\n        html_links += \"* [{title}]({url})\\n\".format(**link)\n    return html_links",
        "sha1": "dbb146c5f744f3d5cd764d7867e2666dea954a00",
        "id": 60847
    },
    {
        "content": "def get_label_addr(_lbls: list, label: str) -> int:\n    \"\"\"\n    Given a label, get the address for that label.\n\n    Parameters\n    ----------\n    _lbls : list, mandatory\n        A list of the known labels and their addresses\n\n    label: str, mandatory\n        The label whose address is required\n\n    Returns\n    -------\n    label_address\n        The integer address of the label\n\n    Raises\n    ------\n    N/A\n\n    Notes\n    -----\n    This will return -1 if the label is not found\n\n    \"\"\"\n    label_address = -1\n    for _i in _lbls:\n        if _i['label'] == label + ',':\n            label_address = _i['address']\n    return label_address",
        "sha1": "8078a9e0b612f0eba3ae25293f7880370122011c",
        "id": 551428
    },
    {
        "content": "from pathlib import Path\n\n\ndef create_data_dir(run_dir):\n    \"\"\"\n    Create initial files for HADDOCK3 run.\n\n    Returns\n    -------\n    pathlib.Path\n        A path referring only to 'data'.\n    \"\"\"\n    data_dir = Path(run_dir, 'data')\n    data_dir.mkdir(parents=True, exist_ok=True)\n    return data_dir",
        "sha1": "28945efcd3e50b91375d087e12178f855d0e3056",
        "id": 667459
    },
    {
        "content": "import torch\n\n\ndef all_pair_iou(boxes_a, boxes_b):\n    \"\"\"\n    Compute the IoU of all pairs.\n    :param boxes_a: (n, 4) minmax form boxes\n    :param boxes_b: (m, 4) minmax form boxes\n    :return: (n, m) iou of all pairs of two set\n    \"\"\"\n\n    N = boxes_a.size(0)\n    M = boxes_b.size(0)\n    max_xy = torch.min(boxes_a[:, 2:].unsqueeze(1).expand(N, M, 2), boxes_b[:, 2:].unsqueeze(0).expand(N, M, 2))\n    min_xy = torch.max(boxes_a[:, :2].unsqueeze(1).expand(N, M, 2), boxes_b[:, :2].unsqueeze(0).expand(N, M, 2))\n    inter_wh = torch.clamp((max_xy - min_xy + 1), min=0)\n    I = inter_wh[:, :, 0] * inter_wh[:, :, 1]\n    A = ((boxes_a[:, 2] - boxes_a[:, 0] + 1) * (boxes_a[:, 3] - boxes_a[:, 1] + 1)).unsqueeze(1).expand_as(I)\n    B = ((boxes_b[:, 2] - boxes_b[:, 0] + 1) * (boxes_b[:, 3] - boxes_b[:, 1] + 1)).unsqueeze(0).expand_as(I)\n    U = A + B - I\n\n    return I / U",
        "sha1": "1ca948e4a16016efa694d97c4829fcdfbc29e20d",
        "id": 702623
    },
    {
        "content": "def has_colab_badge(cell):\n    \"\"\"Return True if cell has a Colab badge as an HTML element.\"\"\"\n    return \"colab-badge.svg\" in cell[\"source\"]",
        "sha1": "86a6bafd7295a34f8304dd7c4a564b5e3a4febb6",
        "id": 443280
    },
    {
        "content": "import torch\n\n\ndef scale(x: torch.Tensor) -> torch.Tensor:\n    \"\"\" Scales all values of a tensor between 0 and 1 \"\"\"\n    x -= x.min()\n    x /= x.max()\n    return x",
        "sha1": "53a415b873778aaac7f0cd11f48f09a3fde717d0",
        "id": 535635
    },
    {
        "content": "def bin_to_hex(bin_str: str, width: int = 8) -> str:\n    \"\"\"Converts binary string to hex string\n\n    Parameters\n    ----------\n    bin_str : str\n        binary string to convert\n\n    width : int, optional\n        width of hex output (used for zero padding), default=8\n\n    Returns\n    -------\n    str\n        hexadecimal array as string\n\n    Raises\n    ------\n    ValueError\n        raises ValueError if supplied width is not wide enough for hex string\n    \"\"\"\n    if len(bin_str)/4.0 > width:\n        raise ValueError(\n            f\"Binary string of length {len(bin_str)} too large for hex array of width {width}\"\n        )\n\n    format_str = f\"{{0:0{width}X}}\"\n\n    return format_str.format(int(bin_str, 2))",
        "sha1": "658607596eb9a6c78714f5cca8cc0b0911260953",
        "id": 569953
    },
    {
        "content": "import struct\n\n\ndef read_u32(x):\n  \"\"\"read a 32-bit value from a byte buffer\"\"\"\n  return struct.unpack('<I', x)[0]",
        "sha1": "1b6df40b47c43ed520c7ff99410b021ee3e3b328",
        "id": 147152
    },
    {
        "content": "def validmove(loc):\n\t\"\"\"loc must be within 4x4 grid and cannot return to origin\"\"\"\n\treturn loc != (0, 0) and \\\n\tloc[0] >= 0 and \\\n\tloc[0] <= 3 and \\\n\tloc[1] >= 0 and \\\n\tloc[1] <= 3",
        "sha1": "d6aced49f923df2aef6e0a6ce4242f27d12e7122",
        "id": 99836
    },
    {
        "content": "def comb(n: int, k: int) -> int:\n    \"\"\"Defines `C(n, k)` as the number of ways to pick `k` items among `n`\n    \"\"\"\n    if k > n:\n        raise ValueError\n\n    result = 1\n    for i in range(n - k + 1, n + 1):\n        result *= i\n    for i in range(2, k + 1):\n        result /= i\n\n    return int(result)",
        "sha1": "79478e49da3a81010c3af503ff9d7a6a979e3f7f",
        "id": 161802
    },
    {
        "content": "from typing import Dict\nfrom typing import Set\nfrom pathlib import Path\n\n\ndef _expected_failing_examples(gallery_conf: Dict, mkdocs_conf: Dict) -> Set[Path]:\n    \"\"\"The set of expected failing examples\"\"\"\n    return set((Path(mkdocs_conf['docs_dir']) / path)\n               for path in gallery_conf['expected_failing_examples'])",
        "sha1": "01fe629efb9acef0494e56ee7587a923bd121d3c",
        "id": 518335
    },
    {
        "content": "def or_(x, y):\n    \"\"\"Implementation of `or` (`|`).\"\"\"\n    return x.__or__(y)",
        "sha1": "c81c1ab63b45a790e13d9b706832a1acf92240d4",
        "id": 368964
    },
    {
        "content": "def date_to_days(date_str: str) -> int:\n    \"\"\"Converts a date given in-game (\"2200.03.01\") to an integer counting the days passed since\n    2200.01.01.\n\n    :param date_str: Date in YYYY.MM.DD format\n    :return: Days passed since 2200.01.01\n    \"\"\"\n    y, m, d = map(int, date_str.split(\".\"))\n    return (y - 2200) * 360 + (m - 1) * 30 + d - 1",
        "sha1": "a2a0f16ad5128f043c780b0761e76068c7e28415",
        "id": 196183
    },
    {
        "content": "def is_list(context, value):\n  \"\"\"\n  Allow us to check for lists in our J2 \n  templates.\n  \"\"\"\n\n  return isinstance(value, list)",
        "sha1": "88a2a7f7d7b58ff19ba2c1ba943788132541bc94",
        "id": 164420
    },
    {
        "content": "def int_to_bin_string(x, bits_for_element):\n    \"\"\"\n    Convert an integer to a binary string and put initial padding\n      to make it long bits_for_element\n        x: integer\n        bit_for_element: bit length of machine words\n    Returns:\n        string\n    \"\"\"\n    encoded_text = \"{0:b}\".format(x)\n    len_bin = len(encoded_text)\n    if len_bin < bits_for_element: #aggiungo gli 0 iniziali che perdo trasformando in int\n        encoded_text = \"0\"*(bits_for_element-len_bin)+encoded_text\n    return encoded_text",
        "sha1": "e8590223d0581985f7dcd256fa0290878ba95f68",
        "id": 668157
    },
    {
        "content": "def get_first_child(node, type):\n  \"\"\"Return the first child of |node| that has type |type|.\"\"\"\n\n  index = node.first_child_matching_class(type)\n  return node[index]",
        "sha1": "090711b92a3a62ad9ee3ed3d2910652c9849d87f",
        "id": 464729
    },
    {
        "content": "import re\n\n\ndef check_id_type(row_dict):\n    \"\"\"Determine which type of id this record holds.\n    Varying on specific field values and lack thereof.\n    Results: 'specimen', 'extraction', 'library', 'pool', 'Invalid'\n    \"\"\"\n    parent = row_dict['parent_jaxid']\n    sample = row_dict['sample_type']\n    nucleic = row_dict['nucleic_acid_type']\n    seqtype = row_dict['sequencing_type']\n\n    ic = re.IGNORECASE\n    pool = re.compile('pool', flags=ic)\n    zero = re.compile('^Z$')\n\n    if pool.match(parent):\n        return 'pool'\n    elif zero.match(sample) and zero.match(nucleic) and zero.match(seqtype):\n        return 'Invalid!'\n    elif zero.match(nucleic) and zero.match(seqtype):\n        return 'specimen'\n    elif zero.match(seqtype):\n        return 'extraction'\n    else: # none == 'Z'\n        return 'library'",
        "sha1": "5fc383dfea4053f7cac9a568c9118b7098dc5ff9",
        "id": 79774
    },
    {
        "content": "def returnTrue(layer):\n    \"\"\"Returns True for any object passed to it.\"\"\"\n    return True",
        "sha1": "4a4fac0cb57e4f092571e395ae435c4069d26496",
        "id": 650652
    },
    {
        "content": "import base64\n\n\ndef decode_dcc_upload_contents(contents, encoding='utf-8'):\n    \"\"\" decodes the content returned by a dcc.Upload component's callback\n    :param contents: a string with a comma separating the content type and the\n                     encoded content\n    :param encoding: the encoding convention of the encoded content\n    :return: decoded string\n    \"\"\"\n    decoded = \"\"\n    if contents is not None:\n        content_type, content_string = contents.split(',')\n        decoded = base64.b64decode(content_string)\n        decoded = decoded.decode(encoding)\n    return decoded",
        "sha1": "222e392003417e8f7c846dbbcba77eb90f6845eb",
        "id": 606696
    },
    {
        "content": "from typing import Optional\n\n\ndef run_duration_string_to_seconds(s: str) -> Optional[int]:\n    \"\"\"\n    Parse a string that represents a timespan, and returns it converted into seconds. The string is expected to be\n    floating point number with a single character suffix s, m, h, d for seconds, minutes, hours, day.\n    Examples: '3.5h', '2d'. If the argument is an empty string, None is returned.\n    :param s: The string to parse.\n    :return: The timespan represented in the string converted to seconds.\n    \"\"\"\n    s = s.strip()\n    if not s:\n        return None\n    suffix = s[-1]\n    if suffix == \"s\":\n        multiplier = 1\n    elif suffix == \"m\":\n        multiplier = 60\n    elif suffix == \"h\":\n        multiplier = 60 * 60\n    elif suffix == \"d\":\n        multiplier = 24 * 60 * 60\n    else:\n        raise ArgumentError(\"s\", f\"Invalid suffix: Must be one of 's', 'm', 'h', 'd', but got: {s}\")  # type: ignore\n    return int(float(s[:-1]) * multiplier)",
        "sha1": "8aa3bbaf59749554baa200e8ae53eeb57e470650",
        "id": 420794
    },
    {
        "content": "def is_imported_module(in_module, m, qualified_name = None):\n    \"\"\"\n    Returns whether 'm' is imported from 'in_module'\n    If 'qualified_name' specified, 'm' must be 'qualified_name' or imported as 'qualified_name'\n    \"\"\"\n    if qualified_name:\n        if m.name in in_module.imports:\n            cur_import = in_module.imports[m.name]\n            return cur_import.module == qualified_name or cur_import.import_as == qualified_name\n        return False\n    else:\n        if m.name in in_module.imports:\n            return (not in_module.imports[m.name].is_qualified)\n        # Return True also on Prelude\n        return m.name == 'Prelude'",
        "sha1": "36b00edca74934460d80aff4338a400682b66109",
        "id": 298015
    },
    {
        "content": "from typing import List\n\n\ndef get_interval_in_s(interval: List[str]) -> List[int]:\n    \"\"\"Converts an interval in string form (e.g. [1:10, 2:30] in seconds, e.g. [70, 150] seconds\"\"\"\n    return [int(sec.split(\":\")[0]) * 60 + int(sec.split(\":\")[1]) for sec in interval]",
        "sha1": "18e21da1cdf3bd13f91550283040f9bcfbd14cb5",
        "id": 667952
    },
    {
        "content": "def get_header_files_size_data(sources_dir, header_files_by_size, rows_count):\n    \"\"\"\n    Builds table rows list containing data about header files sizes (largest/smallest).\n\n    :param sources_dir: configured sources directory\n    :param header_files_by_size: list of header files ordered by size (smallest to largest)\n    :param rows_count: number of rows to build\n    :return: the requested table rows\n    \"\"\"\n    data = [\n        (\"-------------------\", \"----\", \"--------------------\", \"----\"),\n        (\"Largest Header File\", \"Size\", \"Smallest Header File\", \"Size\"),\n        (\"-------------------\", \"----\", \"--------------------\", \"----\")\n    ]\n\n    largest_headers = header_files_by_size[-rows_count:]\n    largest_headers.reverse()\n    smallest_headers = header_files_by_size[:rows_count]\n    for n in range(0, rows_count):\n        top = largest_headers[n] if len(largest_headers) > n else None\n        bottom = smallest_headers[n] if len(smallest_headers) > n else None\n\n        data.append(\n            (\n                top.file_path.replace(sources_dir, '~') if top is not None else \"-\",\n                \"{0:.2f} KB\".format(top.size / 1024) if top is not None else \"-\",\n                bottom.file_path.replace(sources_dir, '~') if bottom is not None else \"-\",\n                \"{0:.2f} KB\".format(bottom.size / 1024) if bottom is not None else \"-\"\n            )\n        )\n\n    return data",
        "sha1": "20a17490b453197784433d9efb8fea89deb9661e",
        "id": 606092
    },
    {
        "content": "def create_urls(years):\n    \"\"\"Generates urls that lead to billboard top 100 chart for provided year.\n\n    :param years: List of years\n    :type years: list\n    :return: List of urls for provided years\n    :rtype: list\n    \"\"\"\n    urls = []\n    for year in years:\n        url = f\"http://billboardtop100of.com/{year}-2/\"\n        urls.append(url)\n    return urls",
        "sha1": "c16fc7b07176376ee14917e6b0c6e4dc2af4d219",
        "id": 121939
    },
    {
        "content": "import re\nimport json\n\n\ndef fix_hunspell_json(badjson_path='en_us.json', goodjson_path='en_us_fixed.json'):\n    \"\"\"Fix the invalid hunspellToJSON.py json format by inserting double-quotes in list of affix strings\n\n    Args:\n      badjson_path (str): path to input json file that doesn't properly quote\n      goodjson_path (str): path to output json file with properly quoted strings in list of affixes\n\n    Returns:\n      list of all words with all possible affixes in *.txt format (simplified .dic format)\n\n    References:\n      Syed Faisal Ali 's Hunspell dic parser: https://github.com/SyedFaisalAli/HunspellToJSON\n    \"\"\"\n    with open(badjson_path, 'r') as fin:\n        with open(goodjson_path, 'w') as fout:\n            for i, line in enumerate(fin):\n                line2 = re.sub(r'\\[(\\w)', r'[\"\\1', line)\n                line2 = re.sub(r'(\\w)\\]', r'\\1\"]', line2)\n                line2 = re.sub(r'(\\w),(\\w)', r'\\1\",\"\\2', line2)\n                fout.write(line2)\n\n    with open(goodjson_path, 'r') as fin:\n        words = []\n        with open(goodjson_path + '.txt', 'w') as fout:\n            hunspell = json.load(fin)\n            for word, affixes in hunspell['words'].items():\n                words += [word]\n                fout.write(word + '\\n')\n                for affix in affixes:\n                    words += [affix]\n                    fout.write(affix + '\\n')\n\n    return words",
        "sha1": "1339c0f900f1c6162866983f2a575dc30b4820cd",
        "id": 672377
    },
    {
        "content": "def users_to_names(users):\n    \"\"\"Convert a list of Users to a list of user names (str).\n    \"\"\"\n    return [u.display_name if u is not None else '' for u in users]",
        "sha1": "881b6717e11d88971ef307fd6b128f9d83d0868c",
        "id": 40424
    },
    {
        "content": "def calculate_tf(matrix, movie_lengths):\n\t\"\"\"Method that calculates TF values for each character over each movie\"\"\"\n\t# Copy the input char_movie_float_time matrix, as output TF matrix would be in the same format\n\ttf = matrix.copy()\n\tfor character in tf.index:\n\t\tfor movie in tf.columns:\n\t\t\t# Filling out the cells with TF values for those cells having time values only\n\t\t\tif str(tf.loc[character][movie]) != 'nan':\n\t\t\t\ttf.loc[character][movie] = tf.loc[character][movie] / float(movie_lengths[movie])\n\treturn tf",
        "sha1": "5ab0fce654c01453cefb393d280311d16ca610b0",
        "id": 391916
    },
    {
        "content": "def scurve(start,end,n,s):\n\t\"\"\"Return an S-Curve with a constant velocity section from start to end with\n\tn points. The start and end S-curves each have s points\"\"\"\n\tys,ye = start,end\n\ts1 = [ (ye-ys)*(x*x-s*s)/(2*s*float(n-1))+ys for x in range(0,s) ]\n\tcv = [ (ye-ys)*(x-s)/float(n-1)+ys for x in range(s,s+n) ]\n\ts2 = [ (ye-ys)*((s+n)*(s+n)+(4*s+2*n)*(x-s-n)-x*x)/(2*s*float(n-1))+ye for x in range(s+n+1,s+n+s+1) ]\n\treturn s1+cv+s2",
        "sha1": "a16ba1143cdeff9a8392a2c993b88918dff82350",
        "id": 646470
    },
    {
        "content": "def twos_complement(value: int) -> int:\n    \"\"\"\n    This method returns the 16-bit two's complement of a positive number\n    (used for sending negative values to the controller). It raises an error\n    if the argument is too large or is negative.\n    \"\"\"\n    assert value.bit_length() <= 16, \"Value too large!\"\n    assert value > 0, \"Value is negative!\"\n    return (2 ** 16) - value",
        "sha1": "58a48d6fa436c79158ba1e8de4a036acc712832e",
        "id": 218066
    },
    {
        "content": "def mount_for_local_dir(local_dir, container_dir):\n    \"\"\"\n    Returns a dict that can be passed as the `volumes` value when running a\n    docker container.  It will specify that the local_dir should be mounted in\n    the container at container_dir.\n    \"\"\"\n    return {local_dir: {\"bind\": container_dir, \"mode\": \"ro\"}}",
        "sha1": "2f426f9089143b43110c6a3a0c5ea84efef49b40",
        "id": 221982
    },
    {
        "content": "def field(name):\n    \"\"\"Return a string for accessing the field called 'name'.  This can\n       be either a field in a context, or a field in the target object. \"\"\"\n    if '.' in name:\n        return name.replace(\".\", \"_ctx->\")\n    else:\n        return \"obj->\" + name",
        "sha1": "b821ae2adcc9e524a3f6e2b2994589dbf8dec9a0",
        "id": 240817
    },
    {
        "content": "from typing import Callable\n\n\ndef filter(f: Callable, collection):\n    \"\"\"Returns a new collection containing elements where `f` returns ``True``.\n\n    Examples\n    --------\n    .. doctest::\n\n        >>> a = [1, 2, 3, 4]\n        >>> s = {'Alice', 'Bob', 'Charlie'}\n\n        >>> hl.eval_expr(hl.filter(lambda x: x % 2 == 0, a))\n        [2, 4]\n\n        >>> hl.eval_expr(hl.filter(lambda x: ~(x[-1] == 'e'), s))\n        {'Bob'}\n\n    Notes\n    -----\n    Returns a same-type expression; evaluated on a :class:`.SetExpression`, returns a\n    :class:`.SetExpression`. Evaluated on an :class:`.ArrayExpression`,\n    returns an :class:`.ArrayExpression`.\n\n    Parameters\n    ----------\n    f : function ( (arg) -> :class:`.BooleanExpression`)\n        Function to evaluate for each element of the collection. Must return a\n        :class:`.BooleanExpression`.\n    collection : :class:`.ArrayExpression` or :class:`.SetExpression`.\n        Array or set expression to filter.\n\n    Returns\n    -------\n    :class:`.ArrayExpression` or :class:`.SetExpression`\n        Expression of the same type as `collection`.\n    \"\"\"\n    return collection._bin_lambda_method(\"filter\", f, collection.dtype.element_type, lambda _: collection.dtype)",
        "sha1": "00ac360fc7cec83ce55cf12e6eb6e0b6fb21c66a",
        "id": 484630
    },
    {
        "content": "def fillna(df, fill_value=\"\"):\n    \"\"\"\n    Replace null values with `fill_value`. Also replaces in categorical columns.\n    \"\"\"\n    for col in df.dtypes[df.dtypes == \"category\"].index:\n        if fill_value not in df[col].cat.categories:\n            df[col].cat.add_categories([fill_value], inplace=True)\n    return df.fillna(fill_value)",
        "sha1": "71566a48b5e411a6cf4db0a6ee1dc127fad53a17",
        "id": 482609
    },
    {
        "content": "import mpmath\n\n\ndef invsf(p, loc=0, scale=1):\n    \"\"\"\n    Inverse survival function of the logistic distribution.\n    \"\"\"\n    with mpmath.extradps(5):\n        p = mpmath.mpf(p)\n        loc = mpmath.mpf(loc)\n        scale = mpmath.mpf(scale)\n        x = loc + scale*(mpmath.log1p(-p) - mpmath.log(p))\n    return x",
        "sha1": "a50fd557527d5a1c65892bb901f4fbe2851dc43f",
        "id": 498032
    },
    {
        "content": "def srwl_und_find_cen_len(_mag3d, relThrB=0.8):\n    \"\"\"\n    Finds longitudinal center position and length of undulator (dominating field component) by analyzing its tabulated field.\n    :param _mag3d: tabulated 3D magnetic field of undulator (object of SRWLMagFld3D type)\n    :param relThrB: relative threshold to be used with respect to peak field for determining the center\n    \"\"\"\n\n    if((_mag3d is None) or ((_mag3d.arBx is None) and (_mag3d.arBy is None))):\n        raise Exception(\"Incorrect definition of the input tabulated undulator magnetic field\")\n\n    if((relThrB <= 0.) or (relThrB >= 1.)):\n        raise Exception(\"relative threshold to be used with respect to peak field should be larger than 0 and less than 1\")\n    \n    nx = _mag3d.nx\n    ny = _mag3d.ny\n    nz = _mag3d.nz\n    if(nz <= 1): raise Exception(\"1D magnetic field with more than one grid point vs longitudinal position is expected\")\n    if((nx > 1) or (ny > 1)): raise Exception(\"1D magnetic field with more than one grid point vs longitudinal position and one point vs horizontal and vertical position is expected\")\n\n    BxIsDefined = False\n    if(_mag3d.arBx is not None):\n        if(len(_mag3d.arBx) == nz): BxIsDefined = True\n\n    ByIsDefined = False\n    if(_mag3d.arBy is not None):\n        if(len(_mag3d.arBy) == nz): ByIsDefined = True\n\n    if((BxIsDefined == False) and (ByIsDefined == False)):\n        raise Exception(\"1D magnetic field data (vertical or horizontal component) are not defined\")\n\n    absBxMax = 0.\n    absByMax = 0.\n    for iz in range(nz):\n        if(BxIsDefined):\n            curAbsB = abs(_mag3d.arBx[iz])\n            if(absBxMax < curAbsB): absBxMax = curAbsB\n        if(ByIsDefined):\n            curAbsB = abs(_mag3d.arBy[iz])\n            if(absByMax < curAbsB): absByMax = curAbsB\n\n    if((absBxMax <= 0.) and (absByMax <= 0.)):\n        raise Exception(\"Non-zero 1D magnetic field data (vertical or horizontal component) are not defined\")\n\n    arB = None\n    absBmax = 0.\n    if(absByMax >= absBxMax):\n        arB = _mag3d.arBy\n        absBmax = absByMax\n    else:\n        arB = _mag3d.arBx\n        absBmax = absBxMax\n\n    absThreshB = relThrB*absBmax\n    zHalfRange = 0.5*_mag3d.rz\n    zThreshLeft = -zHalfRange\n    zThreshRight = zHalfRange\n    zStep = _mag3d.rz/(nz - 1)\n\n    z = zThreshLeft\n    for iz in range(nz):\n        curAbsB = abs(arB[iz])\n        if(curAbsB >= absThreshB):\n            zThreshLeft = z\n            break\n        z += zStep\n\n    nz_mi_1 = nz - 1\n    z = zThreshRight\n    for iz in range(nz):\n        curAbsB = abs(arB[nz_mi_1 - iz])\n        if(curAbsB >= absThreshB):\n            zThreshRight = z\n            break\n        z -= zStep\n\n    #print(zThreshLeft, zThreshRight)\n    if(zThreshRight <= zThreshLeft):\n        return 0., _mag3d.rz\n    else:\n        return 0.5*(zThreshRight + zThreshLeft), zThreshRight - zThreshLeft",
        "sha1": "e51a6155b3a4d647cc00631ba35bda6f24188966",
        "id": 340074
    },
    {
        "content": "import pytz\n\n\ndef from_utc(dt, tz = 'America/Los_Angeles'):\n\t\"\"\"Take a UTC time and convert it to local timezone.\"\"\"\n\tdt_utc = pytz.utc.localize(dt)\n\tlocal = pytz.timezone(tz)\n\treturn dt_utc.astimezone(local)",
        "sha1": "7eb0978d1c44d2728690f8ac4ba60ce20196e1d9",
        "id": 488186
    },
    {
        "content": "def compress_name(champion_name):\n    \"\"\"To ensure champion names can be searched for and compared,\n    the names need to be reduced.\n\n    The process is to remove any characters not in the alphabet\n    (apostrophe, space, etc) and then convert everything to lowercase.\n\n    Note that reversing this is non-trivial, there are inconsistencies\n    in the naming scheme used.\n\n    Examples:\n        Jhin -> jhin\n        GALIO -> galio\n        Aurelion Sol -> aurelionsol\n        Dr. Mundo -> drmundo\n        kha'zix -> khazix\n    \"\"\"\n    compressed_name = \"\".join(c for c in champion_name if c.isalpha())\n    return compressed_name.lower()",
        "sha1": "de76dfd48436ae1ec66dc7e42357e6c52f15719a",
        "id": 688787
    },
    {
        "content": "def encode(operation, *args, **labels):\n    \"\"\"Encode a message so that it can be sent over the wire.\n\n    Parameters:\n      operation(str)\n      \\*args(tuple[str])\n      \\**labels(dict)\n\n    Returns:\n      bytes\n    \"\"\"\n    if labels:\n        args += (\",\".join(f'{name}=\"{labels[name]}\"' for name in sorted(labels)),)\n\n    message = operation\n    for arg in args:\n        message += b\"\\0\"\n        if arg:\n            message += arg if isinstance(arg, bytes) else arg.encode(\"utf-8\")\n\n    message_len = str(len(message)).encode(\"ascii\")\n    return b\"$\" + message_len + b\"\\0\" + message",
        "sha1": "be71ff2b64c95844f97e35943b7ebe52f6aed01c",
        "id": 130833
    },
    {
        "content": "def series(*resistors):\n    \"\"\"Calculate an equivalent resistory value for series resistors.\n    \n    Given resistor values as input, assume they are put in series in a circuit, and\n    return the equivalent resistor value as if they were a single resistor.\n    \"\"\"\n    return sum(resistors)",
        "sha1": "55223cbc69845c2b2fce366d1747cb77a1405e16",
        "id": 657108
    },
    {
        "content": "def dataCleaning(df):\n    \"\"\"  \n    This function takes the dataframe as an arugment.\n    \n    1) Filtering missing and meaningless values (e.g. Don't know, refused, etc.)\n       by using the loc technique.\n    2) Replacing Feature values with their actual meaning by using dictionaries.\n        \n    :param [df]: [dataframe that will be used to carry out the data cleaning]\n    :return: [cleaned (without NAs, meaningless values, renamed values) dataframe]\n    \n    \"\"\"\n    \n    df = df.loc[(df['Overweight_Obese'] != 9) & (df['Education_Level'] != 9)\n                & (df['Income_Categories'] != 9) & (df['Exercise'] != 7) \n                & (df['Exercise'] != 9) & (df['Age_Categories'] != 14) \n                & (df['Gender'] != 7) & (df['Gender'] != 9) \n                & (df['Alcohol_Consumption'] != 7) \n                & (df['Alcohol_Consumption'] != 9)]\n    \n    # =========================================================================\n    # Creating a copy of the BRFSS dataframe while removing all the missing \n    # values (NaNs) to ensure that the modifications to the data will not be\n    # reflected in the original dataframe\n    # =========================================================================\n    df = df.dropna().copy()\n    \n    gender_dict = {1: 'Male', 2:'Female'}\n\n    for k1, v1 in gender_dict.items():\n        df.Gender.replace(k1, v1, inplace=True)\n    \n    \n    overweight_dict = {1: 'No', 2: 'Yes'}\n    \n    for k2, v2 in overweight_dict.items():\n        df.Overweight_Obese.replace(k2, v2, inplace =True)\n    \n    \n    education_dict = {1: 'No_HighSchool_Graduate', 2: 'HighSchool_Graduate', \n                      3: 'Attended_College', 4: 'College_Graduate'}\n    \n    for k3, v3 in education_dict.items():\n        df.Education_Level.replace(k3, v3, inplace=True)\n    \n    \n    income_dict = {1: '<$15,000', 2: '$15,000_to_<$25,000',\n                   3: '$25,000_to_<$35,000', 4: '$35,000_to_<$50,000',\n                   5: '$50,000>='}\n    \n    for k4, v4 in income_dict.items():\n        df.Income_Categories.replace(k4, v4, inplace=True)\n    \n    \n    exercise_dict = {1: 'Yes', 2: 'No'}\n    \n    for k5, v5 in exercise_dict.items():\n        df.Exercise.replace(k5, v5, inplace=True)\n    \n    \n    age_dict = {1: '18-24', 2: '25-29', 3: '30-34', 4: '35-39', 5: '40-44',\n                6: '45-49', 7: '50-54', 8: '55-59', 9: '60-64', 10: '65-69',\n                11: '70-74', 12: '75-79', 13: '80>='}\n    \n    for k6, v6 in age_dict.items():\n        df.Age_Categories.replace(k6, v6, inplace=True)\n    \n    \n    alcohol_dict = {1: 'Yes', 2: 'No'}\n    \n    for k7, v7 in alcohol_dict.items():\n        df.Alcohol_Consumption.replace(k7, v7, inplace=True)\n    \n    \n    arthritis_dict = {1: 'Diagnosed', 2: 'Not_Diagnosed'}\n    \n    for k8, v8 in arthritis_dict.items():\n        df.Arthritis.replace(k8, v8, inplace=True)\n    \n    \n    return df",
        "sha1": "ae00f1f55445d743fa5ef28fad8d8f1dd88cfa35",
        "id": 83512
    },
    {
        "content": "import time\n\n\ndef utc_offset(time_struct=None):\n    \"\"\"\n    Returns the time offset from UTC accounting for DST\n\n    Keyword Arguments:\n        time_struct {time.struct_time} -- the struct time for which to\n                                          return the UTC offset.\n                                          If None, use current local time.\n    \"\"\"\n    if time_struct:\n        ts = time_struct\n    else:\n        ts = time.localtime()\n\n    if ts[-1]:\n        offset = time.altzone\n    else:\n        offset = time.timezone\n    return offset",
        "sha1": "f7b52f3fd292ac773d107f8143ffc7ab8be57499",
        "id": 663944
    },
    {
        "content": "from typing import List\nfrom typing import Optional\nfrom typing import Dict\n\n\ndef weighted_hamming_distance(\n    s1: List[int],\n    s2: List[int],\n    missing_state_indicator=-1,\n    weights: Optional[Dict[int, Dict[int, float]]] = None,\n) -> float:\n    \"\"\"Computes the weighted hamming distance between samples.\n\n    Evaluates the dissimilarity of two phylogenetic samples on the basis of\n    their shared indel states and the probability of these indel states\n    occurring. Specifically, for a given character, if two states are identical\n    we decrement the dissimilarity by the probability of these two occurring\n    independently; if the two states disagree, we increment the dissimilarity by\n    the probability of these states occurring. We normalize the dissimilarity\n    by the number of non-missing characters shared by the two samples.\n\n    If weights are not given, then we increment dissimilarity by +2 if the states\n    are different, +1 if one state is uncut and the other is an indel, and +0 if\n    the two states are identical.\n\n    Args:\n        s1: Character states of the first sample\n        s2: Character states of the second sample\n        missing_state_indicator: The character representing missing values\n        weights: A dictionary storing the state weights for each character, derived\n            from the state priors. This should be a nested dictionary where each\n            key corresponds to character that then indexes another dictionary\n            storing the weight of each observed state.\n            (Character -> State -> Weight)\n\n    Returns:\n        A dissimilarity score.\n\n    \"\"\"\n    d = 0\n    num_present = 0\n    for i in range(len(s1)):\n\n        if s1[i] == missing_state_indicator or s2[i] == missing_state_indicator:\n            continue\n\n        num_present += 1\n\n        if s1[i] != s2[i]:\n            if s1[i] == 0 or s2[i] == 0:\n                if weights:\n                    if s1[i] != 0:\n                        d += weights[i][s1[i]]\n                    else:\n                        d += weights[i][s2[i]]\n                else:\n                    d += 1\n            else:\n                if weights:\n                    d += weights[i][s1[i]] + weights[i][s2[i]]\n                else:\n                    d += 2\n\n    if num_present == 0:\n        return 0\n\n    return d / num_present",
        "sha1": "74cdedd499007a481f76fb4c054cd5465707843a",
        "id": 550088
    },
    {
        "content": "def process_group(grp):\n    \"\"\"\n    Given a list of list of ints,\n    where two ints share a directed edge\n    u-v with v > u if v = u + i for some\n    i in (1, 2, 3), compute the total number\n    of branches (or equivalently leaves) in\n    this directed tree.\n\n    :param grp: The list of list of ints.\n    :return: The count of the number of leaves.\n\n    \"\"\"\n\n    st = list(sorted(map(int, grp)))\n    st = [0] + st + [max(st) + 3]\n    exists = set(st)\n\n    def count_leaves(memo, curr_val):\n        \"\"\"\n        Given a tree structure with root 0\n        count the number of leaves present in it.\n\n        Notes\n        _____\n\n        Recursive Step:\n\n        Given a curr_val, we store in memo[curr_val]:\n\n        'The number of leaves in the subtree rooted at curr_val.'\n\n        \"\"\"\n        if curr_val == st[-1]:\n            # Reached a leaf.\n\n            # Leaves have exactly one leaf in the subtree\n            # rooted at them.\n\n            memo[curr_val] = 1\n            return 1\n\n        elif curr_val in memo:\n            # If memoized, don't recompute, save time.\n            return memo[curr_val]\n\n        else:\n            # Subdivide the problem amongst\n            # the current nodes children.\n            for i in range(1, 4):\n                if curr_val + i in exists:\n                    count_leaves(memo, curr_val + i)\n\n            # Assume it is solved for children.\n\n            # Then how to use children's solution\n            # to produce current node's?\n\n            # The number of leaves in the subtree rooted\n            # at curr_val is:\n\n            # The sum of the number of leaves in the\n            # subtrees rooted at its children.\n\n            memo[curr_val] = 0\n\n            for i in range(1, 4):\n                if curr_val + i in memo:\n                    memo[curr_val] += memo[curr_val + i]\n\n            # Populate memo[curr_val] with the result.\n            # and trace back to the next node.\n\n            return memo[curr_val]\n\n    mm = dict()\n\n    count_leaves(mm, 0)\n\n    return mm[0]",
        "sha1": "30192a6773a6d008e3c6b7f47b136ce80680df24",
        "id": 51406
    },
    {
        "content": "def filelist_latestcycle(filelist, cycle_list):\n    \"\"\"\n    Only get filelist for lastest cycle if multiple cycles are requested\n    \"\"\"\n    if len(cycle_list) >1:\n    # for multiple repeat cycles, only request data for lastest cycle\n        latest_cycle = max(cycle_list)\n        new_filelist = [f for f in filelist if int(f[-14:-12])==latest_cycle]\n        return new_filelist\n\n    else: # only one cycle\n        return filelist",
        "sha1": "6c3f7d23ab3dad8a98e049db1255cf9da10283e4",
        "id": 361152
    },
    {
        "content": "def binary_classification_metrics(prediction, ground_truth):\n    \"\"\"\n    Computes metrics for binary classification\n\n    Arguments:\n    prediction, np array of bool (num_samples) - model predictions\n    ground_truth, np array of bool (num_samples) - true labels\n\n    Returns:\n    precision, recall, f1, accuracy - classification metrics\n    \"\"\"\n    # Some helpful links:\n    # https://en.wikipedia.org/wiki/Precision_and_recall\n    # https://en.wikipedia.org/wiki/F1_score\n    num_of_samples = prediction.shape[0]\n    comparison = [(prediction[i] == ground_truth[i], ground_truth[i])\n                  for i in range(num_of_samples)]\n    true_positive = len([c for c in comparison if c[0] and c[1]])\n    false_positive = len([c for c in comparison if not c[0] and not c[1]])\n    false_negative = len([c for c in comparison if not c[0] and c[1]])\n\n    if true_positive == 0:\n        precision = 0\n        recall = 0\n        accuracy = 0\n        f1 = 0\n    else:\n        precision = true_positive / (true_positive + false_positive)\n        recall = true_positive / (true_positive + false_negative)\n        accuracy = (prediction == ground_truth).sum()/num_of_samples\n        f1 = 2 * (precision * recall) / (precision + recall)\n\n    return precision, recall, f1, accuracy",
        "sha1": "d8be85c8b3c3c01de0e2c81d04abab9f3102edd3",
        "id": 271645
    },
    {
        "content": "def get_ones_mask(mask):\n    \"\"\"\n    Calculates the mask used to set 1 values\n\n    >>> get_ones_mask(\"XXXXXXXXXXXXXXXXXXXXXXXXXXXXX1XXXX0X\")\n    64\n\n    >>> f\"{get_ones_mask('XXXXXXXXXXXXXXXXXXXXXXXXXXXXX1XXXX0X'):b}\"\n    '1000000'\n\n    :param mask:\n    :return:\n    \"\"\"\n    return int(mask.replace(\"X\", \"0\"), 2)",
        "sha1": "058571010e4bae2601e63f92718729b2c6934fa5",
        "id": 289657
    },
    {
        "content": "import torch\n\n\ndef logsubexp(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n    \"\"\"Calculate log(exp(x) - exp(y))\n    \"\"\"\n    return x + torch.log1p(input=-torch.exp(y - x))",
        "sha1": "307659d58b26654871eafbd07b058f5e69d99ca5",
        "id": 582294
    },
    {
        "content": "import asyncio\n\n\ndef create_future(loop: asyncio.AbstractEventLoop) -> asyncio.Future:\n    \"\"\"\n    Return a Future, using the loop if possible.\n\n    loop.create_future() is better, but was only added in Python 3.5.2, see:\n\n    https://docs.python.org/3/library/asyncio-eventloop.html#asyncio.AbstractEventLoop.create_future\n    \"\"\"\n    if hasattr(loop, 'create_future'):\n        return loop.create_future()\n    return asyncio.Future(loop=loop)",
        "sha1": "f75646fa191163b3151a90288f8b0853eaffc37b",
        "id": 372386
    },
    {
        "content": "import inspect\n\n\ndef _build_transform_name(lag, tfm, *args) -> str:\n    \"\"\"Creates a name for a transformation based on `lag`, the name of the function and its arguments.\"\"\"\n    tfm_name = f'{tfm.__name__}_lag-{lag}'\n    func_params = inspect.signature(tfm).parameters\n    func_args = list(func_params.items())[1:]  # remove input array argument\n    changed_params = [\n        f'{name}-{value}'\n        for value, (name, arg) in zip(args, func_args)\n        if arg.default != value\n    ]\n    if changed_params:\n        tfm_name += '_' + '_'.join(changed_params)\n    return tfm_name",
        "sha1": "38efdf4a815a33a1d6620f16b475068b77c9d475",
        "id": 634816
    },
    {
        "content": "import queue\n\n\ndef drain_queue(collection, given_queue):\n    \"\"\"\n    Get all available elements from the given queue without waiting.\n\n    :param collection: List.\n    :param given_queue: multiprocessing.Queue\n    :return: False - no elements will come, True - otherwise\n    \"\"\"\n    try:\n        while True:\n            element = given_queue.get_nowait()\n            if element is None:\n                given_queue.close()\n                return False\n            collection.append(element)\n    except queue.Empty:\n        return True",
        "sha1": "01f0417b7ebf67f9c76045220038eb921a2d9594",
        "id": 301485
    },
    {
        "content": "def _time_to_number(time_string):\n    \"\"\"Change time string to float for easier comparison\n    example -> '08:30:00' becomes 8.5\n    example -> '08:00:00' becomes 8.0\n    example -> '11:30:00' becomes 11.5\n    example -> '11:00:00' becomes 11.0\n    \"\"\"\n\n    time_list = time_string.split(':')\n\n    # Check time format\n    if len(time_list) != 3:\n        raise Exception('Times must be formatted as hh:mm:ss')\n\n    time = float(time_list[0])\n    minute = float(time_list[1])\n\n    if minute != 0:\n        time = time + 0.5\n\n    return time",
        "sha1": "511606a18ea52fbf3514fe3957fceb055a49e290",
        "id": 532378
    },
    {
        "content": "def getFinalFrequency(data: list) -> int:\n    \"\"\"Adds everything in puzzleArray and returns\n    it as frequency for submission\n\n    Args:\n        data: puzzleArray from AOC\n\n    Returns:\n        int to submit to puzzleInput\n\n    \"\"\"\n    frequency = 0\n    for eachFrequency in data:\n        frequency += eachFrequency\n    return frequency",
        "sha1": "15478055f700b01516ee6a83e36b9eabb6ee56e4",
        "id": 131887
    },
    {
        "content": "def is_cased(s):\n    \"\"\"Return True if string s contains some cased characters; otherwise false.\"\"\"\n    return s.lower() != s.upper()",
        "sha1": "8bffe4a6af122f376e2cb2e67f8ec4270ecc01cf",
        "id": 196309
    },
    {
        "content": "def control_key_name(code):\n    \"\"\"Prefix the name of a control key with '^'\"\"\"\n    name = chr(code - 1 + ord(\"A\"))\n    return f\"^{name}\"",
        "sha1": "2f03c47fb1c0f2374ba892dbd2752a76abf28e75",
        "id": 623736
    },
    {
        "content": "def floor_log2(number: int) -> int:\n    \"\"\"\n    Returns infimum of powers-of-two which are not greater than the number,\n    equivalent of ``floor(log2(number))``.\n    >>> floor_log2(1)\n    0\n    >>> floor_log2(2)\n    1\n    >>> floor_log2(3)\n    1\n    >>> floor_log2(4)\n    2\n    \"\"\"\n    return number.bit_length() - 1",
        "sha1": "aaaa4849b5233afcfc73f2c651175aecd71278c0",
        "id": 206155
    },
    {
        "content": "def dumb_filter(line: str) -> bool:\n    \"\"\"Filters a Wikidata line that is a dictionary in string format.\n\n    Applies a simple check that tests if the currenty entity line has a English\n    Wikipedia article before loading. The reason is that loading a JSON object is slow\n    before running it, that speeds up code. Removing this function call should not\n    change the resulting file.\n\n    Arguments:\n        line: ``str`` A line in the Wikidata dump.\n    Returns:\n        ``bool`` Whether the current line is for an entity with an English article\n    \"\"\"\n    return '\"enwiki\"' not in line and '\"type\":\"item\"' in line",
        "sha1": "06fcf2be2062d5741f933cd4cc0fa35a9990c8c5",
        "id": 502449
    },
    {
        "content": "def to_cluster(git_objects, label):\n    \"\"\"Return a string with a graphviz cluster of the provided git objects.\"\"\"\n    content = \"\\n\".join([obj.to_graphviz() for obj in git_objects])\n    return f\"\"\"subgraph cluster_{label} {{\nlabel=\"{label}\";\nstyle=\"rounded\";\nbgcolor=beige;\n{content}\n}}\n\"\"\"",
        "sha1": "b8df2b01c81c5518f58a892b0c75be7ff8783f8b",
        "id": 252237
    },
    {
        "content": "def pt2mm(value):\n    \"\"\"Converts given value in points to millimeters.\n\n    Args:\n        value (int): value in points.\n\n    Returns:\n        (float): value, in millimeters.\n    \"\"\"\n    return value * 0.352777778",
        "sha1": "530078d1cae9bda7fc3cb2ddde3411346dae37d7",
        "id": 561875
    },
    {
        "content": "def _canonicalize_to_list(value):\n    \"\"\"Canonicalize a value to a list.\n\n    If value is a list, return it.  If it is None or an empty string,\n    return an empty list.  Else, return value.\n    \"\"\"\n    if isinstance(value, list):\n        return value\n    if value == '' or value is None:\n        return []\n    return [value]",
        "sha1": "198dfb38f75a43783a3d9edb0d2f0b110f71bc0d",
        "id": 412971
    },
    {
        "content": "def split_uri(uri):\n    \"\"\"\n    Get the slash-delimited pieces of a URI.\n\n    >>> split_uri('/c/en/cat/n/animal')\n    ['c', 'en', 'cat', 'n', 'animal']\n    >>> split_uri('/')\n    []\n    \"\"\"\n    if not uri.startswith('/'):\n        return [uri]\n    uri2 = uri.lstrip('/')\n    if not uri2:\n        return []\n    return uri2.split('/')",
        "sha1": "818f23e738a9ea8575d1e139dc4f270661b28b0b",
        "id": 116458
    },
    {
        "content": "def elem2dict(elem, strip=True):\n    \"\"\"\n    Convert an ElementTree() object into a Python dictionary.\n\n    Arguments:\n        elem (obj): a valid ElementTree() object\n        strip (bool): a boolean value for striping whitespace (optional)\n\n    Credit: Hay Kranen (https://github.com/hay/xml2json)\n\n    \"\"\"\n    d = {}\n    for key, value in elem.attrib.items():\n        d['@'+key] = value\n\n    # loop over subelements to merge them\n    for subelem in elem:\n        v = elem2dict(subelem, strip=strip)\n        tag = subelem.tag\n        value = v[tag]\n        try:\n            # add to existing list for this tag\n            d[tag].append(value)\n        except AttributeError:\n            # turn existing entry into a list\n            d[tag] = [d[tag], value]\n        except KeyError:\n            # add a new non-list entry\n            d[tag] = value\n    text = elem.text\n    tail = elem.tail\n    if strip:\n        # ignore leading and trailing whitespace\n        if text:\n            text = text.strip()\n        if tail:  # pragma: no cover\n            tail = tail.strip()\n\n    if tail:  # pragma: no cover\n        d['#tail'] = tail\n\n    if d:\n        # use #text element if other attributes exist\n        if text:  # pragma: no cover\n            d[\"#text\"] = text\n    else:\n        # text is the value if no attributes\n        d = text or None\n\n    return {elem.tag: d}",
        "sha1": "a969867a6a10d9ddeaa061f626be9dd9dc2c9632",
        "id": 438295
    },
    {
        "content": "import json\n\n\ndef read_json(path: str, encoding: str) -> dict:\n    \"\"\"Read JSON file.\"\"\"\n    with open(path, encoding=encoding) as file:\n        return json.load(file)",
        "sha1": "476b072bbb441744a740703df0a583acbfe73de7",
        "id": 71659
    },
    {
        "content": "def euclid(a, b):\n    \"\"\"Solve x*a + y*b = ggt(a, b) and return (x, y, ggt(a, b))\"\"\"\n    # Non-recursive approach hence suitable for large numbers\n    x = yy = 0\n    y = xx = 1\n    while b:\n        q = a // b\n        a, b = b, a % b\n        x, xx = xx - q * x, x\n        y, yy = yy - q * y, y\n    return xx, yy, a",
        "sha1": "4998030919fef4db3246e20c2f71055988e342c7",
        "id": 613251
    },
    {
        "content": "async def runall(corogen):\n    \"\"\"\n    Run an array of coroutines\n\n    :param corogen: a generator that generates coroutines\n    :return: list or returns of the coroutines\n    \"\"\"\n    results = []\n    for c in corogen:\n        result = await c\n        results.append(result)\n    return results",
        "sha1": "aa3c6ea01f4430d071d7542969b7878e519bdd75",
        "id": 435559
    },
    {
        "content": "def graph_directionality(graph, has_data_reference=True):\n    \"\"\"\n    Return a graph overall directionality as 'directional', 'undirectional'\n    or 'mixed' based on node adjacency.\n\n    * undirected:  forward and reverse edges exists between every node\n    * directional: edges between nodes are one directional only\n    * mixed:       a mix of undirected and directed edges\n\n    Graphit represent edges in an undirected graph as linked pairs of\n    forward and reversed edges. That means that strictly considering the\n    existence of a pair of bidirectional edges between nodes or their\n    adjacency equivalent is not a unique identifier of directionality in\n    graphit.\n    This is resolved by checking for linked edges to identify a graphit graph\n    as truly undirectional. This check can be disabled using the\n    `has_data_reference` attribute.\n\n    :param graph:               Graph to asses directionality of\n    :type graph:                :graphit:Graph\n    :param has_data_reference:  Check for linked edges\n    :type has_data_reference:   :py:bool\n\n    :return:      'directional', 'undirectional' or 'mixed'\n    :rtype:       :py:str\n    \"\"\"\n\n    edge_directionality = []\n    for node, adj in graph.adjacency.items():\n        edge_directionality.extend([node in graph.adjacency[n] for n in adj])\n\n    if all(edge_directionality):\n\n        # check for linked edges\n        if has_data_reference:\n            pairs = []\n            for edge in graph.edges:\n                linked_edge = graph.edges.get_data_reference(edge)\n                if linked_edge:\n                    pairs.append(edge)\n                    pairs.append(linked_edge)\n\n            if len(pairs) == len(graph.edges):\n                return 'undirectional'\n            elif not len(pairs):\n                return 'directional'\n            else:\n                return 'mixed'\n\n        return 'undirectional'\n    elif any(edge_directionality):\n        return 'mixed'\n    else:\n        return 'directional'",
        "sha1": "3ec439cbc6b53c923503065e9176a61effb782da",
        "id": 129831
    },
    {
        "content": "def incremental_mean(mu_i, n, x):\n    \"\"\"\n    Calculates the mean after adding x to a vector with given mean and size.\n\n    :param mu_i: Mean before adding x.\n    :param n: Number of elements before adding x.\n    :param x: Element to be added.\n    :return: New mean.\n    \"\"\"\n\n    delta = (x - mu_i) / float(n + 1)\n    mu_f = mu_i + delta\n\n    return mu_f",
        "sha1": "8f1db96c12856f5bcdbf3cb154b93be944a6b289",
        "id": 678330
    },
    {
        "content": "def are_in_same_column(cell, free_cell):\n    \"\"\" Check if the given cell is in the same column than the free_cell.\n    :arg cell, the coordinate of the cell in the table.\n    :arg free_cell, the coordinate of the free cell in the table.\n    \"\"\"\n    xcell = list(cell)[0][0]\n    xfreecell = list(free_cell)[0][0]\n\n    return xcell == xfreecell",
        "sha1": "5a39cf706f99c22cfb5b3d396a15230ef77b5c6f",
        "id": 124262
    },
    {
        "content": "import math\n\n\ndef point_in_ellipse(origin, point, a, b, pa_rad, verbose=False):\n    \"\"\"\n    Identify if the point is inside the ellipse.\n\n    :param origin A SkyCoord defining the centre of the ellipse.\n    :param point A SkyCoord defining the point to be checked.\n    :param a The semi-major axis in arcsec of the ellipse\n    :param b The semi-minor axis in arcsec of the ellipse\n    :param pa_rad The position angle of the ellipse. This is the angle of the major axis measured in radians East of \n                   North (or CCW from the y axis).\n    \"\"\"\n    # Convert point to be in plane of the ellipse, accounting for distortions at high declinations\n    p_ra_dist = (point.icrs.ra.degree - origin.icrs.ra.degree)* math.cos(origin.icrs.dec.rad)\n    p_dec_dist = point.icrs.dec.degree - origin.icrs.dec.degree\n\n    # Calculate the angle and radius of the test opoint relative to the centre of the ellipse\n    # Note that we reverse the ra direction to reflect the CCW direction\n    radius = math.sqrt(p_ra_dist**2 + p_dec_dist**2)\n    diff_angle = (math.pi/2 + pa_rad) if p_dec_dist == 0 else math.atan(p_ra_dist / p_dec_dist) - pa_rad\n\n    # Obtain the point position in terms of the ellipse major and minor axes\n    minor = radius * math.sin(diff_angle)\n    major = radius * math.cos(diff_angle)\n    if verbose:\n        print ('point relative to ellipse centre angle:{} deg radius:{:.4f}\" maj:{:.2f}\" min:{:.2f}\"'.format(math.degrees(diff_angle), radius*3600, \n                major*3600, minor*3600))\n    \n    a_deg = a / 3600.0\n    b_deg = b / 3600.0\n\n    # Calc distance from origin relative to a and b\n    dist = math.sqrt((major / a_deg) ** 2 + (minor / b_deg) ** 2)\n    if verbose:\n        print(\"Point %s is %f from ellipse %f, %f, %f at %s.\" % (point, dist, a, b, math.degrees(pa_rad), origin))\n    return round(dist,3) <= 1.0",
        "sha1": "9c4b056c205b8d25e80211adb0eeb1cdfaf4c11c",
        "id": 706885
    },
    {
        "content": "def frequency_distributions(text, words_num=75):\n    \"\"\"\n    Creates frequency distribution form text and returns words_num most commom words.\n    \"\"\"\n    all_words = list(set(text))\n    res = list()\n    for word in all_words:\n        res.append((text.count(word), word))\n\n    res.sort(reverse=True)\n    return res[:words_num]",
        "sha1": "6742f58d8e6ebe5a4a3b5e5abb6137b252ffa264",
        "id": 191445
    },
    {
        "content": "def count_user(data_list):\n    \"\"\"\n    Function count the users.\n    Args:\n          data_list: The data list that is doing to be iterable.\n    Returns:\n           A list with the count value of each user type.\n    \"\"\"\n\n    customer = 0\n    subscriber = 0\n\n    for i in range(len(data_list)):\n        if data_list[i][5] == \"Customer\":\n            customer += 1\n\n        elif data_list[i][5] == \"Subscriber\":\n            subscriber += 1\n\n    return [customer, subscriber]",
        "sha1": "ccd4218efe017c4bebe332c9a258de61f03ec097",
        "id": 660631
    },
    {
        "content": "import string\n\n\ndef _get_placeholders(template):\n    \"\"\"Get all placeholders from a template string.\n    Parameters\n    ----------\n    template : str\n        The template string to get the placeholders for.\n    Returns\n    -------\n    placeholders : list of str\n        The list of placeholder names that were found in the template string.\n\n    Author: Marijn van Vliet <w.m.vanvliet@gmail.com>\n    \"\"\"\n    return [p[1] for p in string.Formatter().parse(template)\n            if p[1] is not None and len(p[1]) > 0]",
        "sha1": "f378486328afebf86f643cf8beaf1f883ffccd9c",
        "id": 40668
    },
    {
        "content": "import socket\nimport binascii\n\n\ndef subnetwork_to_ip_range(subnetwork):\n    \"\"\"\n    Returns a tuple (ip_lower, ip_upper, version) containing the\n    integer values of the lower and upper IP addresses respectively\n    in a subnetwork expressed in CIDR notation (as a string), with\n    version being the subnetwork IP version (either 4 or 6).\n\n    Both IPv4 subnetworks (e.g. \"192.168.1.0/24\") and IPv6\n    subnetworks (e.g. \"2a02:a448:ddb0::/44\") are accepted.\n    \"\"\"\n\n    try:\n        fragments = subnetwork.split('/')\n        network_prefix = fragments[0]\n        netmask_len = int(fragments[1])\n\n        # try parsing the subnetwork first as IPv4, then as IPv6\n        for version in (socket.AF_INET, socket.AF_INET6):\n\n            ip_len = 32 if version == socket.AF_INET else 128\n\n            try:\n                suffix_mask = (1 << (ip_len - netmask_len)) - 1\n                netmask = ((1 << ip_len) - 1) - suffix_mask\n                ip_hex = socket.inet_pton(version, network_prefix)\n                ip_lower = int(binascii.hexlify(ip_hex), 16) & netmask\n                ip_upper = ip_lower + suffix_mask\n\n                return (ip_lower,\n                        ip_upper,\n                        4 if version == socket.AF_INET else 6)\n            except:\n                pass\n    except:\n        pass\n\n    raise ValueError(\"invalid subnetwork\")",
        "sha1": "155d4a16cfc7bc99579b2637db4cd5d2b1d99e3c",
        "id": 369491
    },
    {
        "content": "from datetime import datetime\n\n\ndef parseCommentURL(url):\n    \"\"\"Parse a comment URL and extract the importer, username and timestamp.\n\n    @param url: The URL to parse, matching the following format::\n\n          https://loveme.do/comment/importer/name/2012-08-03T22:04:13.698896\n\n    @raise ValueError: Raised if the URL is malformed.\n    @return: A C{(importer, username, timestamp)} 3-tuple.  The timestamp is a\n        C{datetime} instance.\n    \"\"\"\n    _, path, importer, username, isoTimestamp = url.rsplit('/', 4)\n    if path != 'comment':\n        raise ValueError('Invalid root path, expected \"comment\".')\n    try:\n        timestamp = datetime.strptime(isoTimestamp, '%Y-%m-%dT%H:%M:%S.%f')\n    except ValueError:\n        # Maybe the ISO timestamp doesn't contain microseconds.\n        timestamp = datetime.strptime(isoTimestamp, '%Y-%m-%dT%H:%M:%S')\n    return importer, username, timestamp",
        "sha1": "82886e03647b6aac1d394afb7475d9857890cbdd",
        "id": 223866
    },
    {
        "content": "def apply_mask(xy):\n    \"\"\"\n    Apply bitmask to value.\n\n    Args:\n    xy: A tuple with the bit of the mask\n    and the bit to mask (mask, value).\n\n    Returns:\n    The bit after being masked.\n    \"\"\"\n    if xy[0] == 'X':\n        return xy[1]\n    else:\n        return xy[0]",
        "sha1": "2bcca53bdff924770c06bd489a3bed4d80126266",
        "id": 305840
    },
    {
        "content": "def remove_lowest(grade_list, dropped):\n    \"\"\"\n    :param grade_list: a list of grades\n    :param dropped: an int representing the amount of grades to drop\n    :return: a list with the lowest x amount of graded dropped\n    \"\"\"\n\n    grade_list.sort(reverse=True)\n    count = 0\n    while count < dropped:\n        grade_list.pop()\n        count += 1\n\n    return grade_list",
        "sha1": "fdeebe18468e7e652a4ba1e5e4fdbaf6c009514b",
        "id": 345336
    },
    {
        "content": "def get_max_path_len(distribution_artefacts):\n    \"\"\"\n    Get the max length of the paths to the distribution artefacts.\n\n    :param distribution_artefacts: Nested list containing distribution artefacts mapped to media packages and tenants\n    :type distribution_artefacts: dict\n    :return: Max path length\n    :rtype: int\n    \"\"\"\n\n    return max(max(max([[[len(dist_list) for dist_list in distribution_artefacts[tenant][media_package]]\n                        for media_package in distribution_artefacts[tenant]]\n                   for tenant in distribution_artefacts.keys()])))",
        "sha1": "9e53be85ae2099987fe868ff957b1b64284ad504",
        "id": 255564
    },
    {
        "content": "def connected_components(adjacency_matrix):\n    \"\"\"Connected components of an undirected graph.\n\n    Parameters\n    ----------\n    adjacency_matrix : numpy.ndarray\n        squared array representing the adjacency\n        matrix of an undirected graph.\n\n    Returns\n    -------\n    list of integer sets\n        connected components of the graph.\n    \"\"\"\n    node_idxs = set(range(adjacency_matrix.shape[0]))\n    components = []\n    while node_idxs:\n        idx = node_idxs.pop()\n        component = {idx}\n        component_follow = [idx]\n        while component_follow:\n            idx = component_follow.pop()\n            idxs = set(adjacency_matrix[idx].nonzero()[0]) - component\n            component |= idxs\n            component_follow += idxs\n        components.append(component)\n        node_idxs -= component\n    return components",
        "sha1": "6193d56847571dcb27027fac0e641fffcf4b2618",
        "id": 533855
    },
    {
        "content": "def pretty_exception(err: Exception, message: str = \"\"):\n    \"\"\"Return a pretty error message with the full path of the Exception.\"\"\"\n    return f\"{message} ({err.__module__}.{err.__class__.__name__}: {str(err)})\"",
        "sha1": "b5fdce86753ff70dd8b247b56e0e21ecf8d0f7d1",
        "id": 121896
    },
    {
        "content": "def parse_txt(file):\n    \"\"\" Parse text files\"\"\"\n    with open(file) as file:\n        contents = file.read()\n        return contents",
        "sha1": "109949cb817c6d04646c38e0439b199e58a41cea",
        "id": 107346
    },
    {
        "content": "def upper_bound(arr, value):\n    \"\"\"\n    find the index of the last element in arr <= value.\n    \"\"\"\n    low = 0\n    high = len(arr) - 1\n    found = -1\n    while(low <= high):\n        mid = (low + high) // 2\n        if arr[mid] > value:\n            high = mid - 1\n        else:\n            found = mid\n            low = mid + 1\n    return found",
        "sha1": "dac05243d5561575d8bd72319334893f565f0b7a",
        "id": 511382
    },
    {
        "content": "from pathlib import Path\n\n\ndef museum_packages_dir(tmpdir, museum_package):\n    \"\"\"\n    Museum package directory that already has a complete package with\n    some log files inside it\n    \"\"\"\n    path = Path(tmpdir) / \"museum_packages\"\n    path.mkdir(exist_ok=True)\n\n    museum_package.path.rename(path / \"1234567\")\n\n    (path / \"1234567\" / \"logs\" / \"ingest-report.html\").write_text(\n        \"<html><body><p>Success</p></body></html>\"\n    )\n    (path / \"1234567\" / \"logs\" / \"create-sip.log\").write_text(\n        \"SIP was created\"\n    )\n\n    return path",
        "sha1": "1b07d20c440a47670314d82d48cffafccbaa5045",
        "id": 304082
    },
    {
        "content": "def parse_http_protocol(s):\n    \"\"\"\n        Parse an HTTP protocol declaration. Returns a (major, minor) tuple, or\n        None.\n    \"\"\"\n    if not s.startswith(b\"HTTP/\"):\n        return None\n    _, version = s.split(b'/', 1)\n    if b\".\" not in version:\n        return None\n    major, minor = version.split(b'.', 1)\n    try:\n        major = int(major)\n        minor = int(minor)\n    except ValueError:\n        return None\n    return major, minor",
        "sha1": "2c629571a92b692256c17edfa071e1900fc1ec2c",
        "id": 312061
    },
    {
        "content": "def get_lock_filepath(filepath):\n    \"\"\"Get a lock file path for filepath.\n\n    Args:\n        filepath (str): Path.\n\n    Returns:\n        str: Lock file path.\n    \"\"\"\n    return f'{filepath}.lock'",
        "sha1": "1a31e119a3e0abddeb6363a0bbd5b62ea4dba7f6",
        "id": 228147
    },
    {
        "content": "def input_data_job_id(conf):\n    # type: (dict) -> str\n    \"\"\"Retrieve input data job id\n    :param dict conf: configuration object\n    :rtype: str\n    :return: job id\n    \"\"\"\n    return conf['job_id']",
        "sha1": "b0244719b55dfc9d36f1ecd8a58474d19f982f76",
        "id": 670328
    },
    {
        "content": "def letters_to_exclude(word_list: list, exclude_list: list) -> list:\n    \"\"\"Remove words from word_list if they are in exclude_list of characters and return.\"\"\"\n    return [word for word in word_list if all(char not in exclude_list for char in word)]",
        "sha1": "7a9b57bd12f28ee3afe5101deda77300011da17b",
        "id": 456438
    },
    {
        "content": "def get_id(page):\n  \"\"\"Extract the id from a page.\n\n  Args:\n    page: a string\n  Returns:\n    an integer\n  \"\"\"\n  start_pos = page.find(\"<id>\")\n  end_pos = page.find(\"</id>\")\n  assert start_pos != -1\n  assert end_pos != -1\n  start_pos += len(\"<id>\")\n  return int(page[start_pos:end_pos])",
        "sha1": "8e41f0082da81a3cd4c65276179cbcae589ad753",
        "id": 231923
    },
    {
        "content": "def get_cik_path(cik):\n    \"\"\"\n    Get path on EDGAR or S3 for a given CIK.\n    :param cik: company CIK\n    :return:\n    \"\"\"\n    return \"edgar/data/{0}/\".format(cik)",
        "sha1": "93fe3e4015cec97e9360d94a2f8020fe7bc5b2bd",
        "id": 623174
    },
    {
        "content": "def mixColor( originalColor, overColor):\n\t\"\"\"\n\tUse overColor's alpha to overlay it on top of original Color\n\tColor parameters in form [r, g, b, a]\n\tReturns [r, g, b, 1]\n\t\"\"\"\n\talpha = overColor[3]\n\tif alpha == 1:\n\t\treturn overColor[:4]\n\telse:\n\t\tout = [0, 0, 0, 1]\n\t\tfor i in range(3):\n\t\t\tout[i] = originalColor[i] * (1 - alpha) + overColor[i] * alpha\n\t\treturn out",
        "sha1": "0356386cf4742b0b41c149685f059707c09f2ed8",
        "id": 384725
    },
    {
        "content": "from pathlib import Path\n\n\ndef style_pth() -> Path:\n    \"\"\" Returns the Path to the ampycloud plotting styles. \"\"\"\n\n    return Path(__file__).parent / 'mpl_styles'",
        "sha1": "d45a9d5b1a001fbf1f99cdc0c9cf8fb66cb42e81",
        "id": 492818
    },
    {
        "content": "def drop_column(df, columns_to_drop):\n    \"\"\" Removes columns from a DataFrame\n\n    del df[name]\n\n    Args:\n        df (`pandas.DataFrame`): The dataframe to drop columns on\n        columns_to_drop (:type:`list` of :type:`str`): A list of the columns\n            to remove\n\n    Returns:\n        `pandas.DataFrame`: `df` with the provided columns removed\n    \"\"\"\n\n    for ctd in columns_to_drop:\n        del df[ctd]\n\n    return df",
        "sha1": "1eadbf301aff80752c93ca4393910dfa19a76b3a",
        "id": 38203
    },
    {
        "content": "def pystr(s):\n  \"\"\"Output a string as an eval'able representation of a Python string.\"\"\"\n  return s.__repr__()",
        "sha1": "19d379871ab1f11fd8d338dfbc07756fcfc9062d",
        "id": 679978
    },
    {
        "content": "import re\n\n\ndef camel2snake(txt: str) -> str:\n    \"\"\"Convert a CamelCasedString to a snake_cased_string\"\"\"\n    txt = re.sub(\"(.)([A-Z][a-z]+)\", r\"\\1_\\2\", txt)\n    txt = re.sub(\"__([A-Z])\", r\"_\\1\", txt)\n    txt = re.sub(\"([a-z0-9])([A-Z])\", r\"\\1_\\2\", txt)\n    return txt.lower()",
        "sha1": "7af8a13fb5543473d179d4104339888bca8b62dd",
        "id": 569642
    },
    {
        "content": "def str2list(src_str):\n    \"\"\"convert the str to shape list\n    Args:\n        src_str: for example '1,2,3,4'\n    Returns:\n        ret: list, for example [1,2,3,4]\n    \"\"\"\n    ret = []\n    s_list = src_str.split(',')\n    ret = [int(i) for i in s_list]\n    return ret",
        "sha1": "d1a98d65b4a838f168806f6b173149029b215fb9",
        "id": 539677
    },
    {
        "content": "def contains(text: str, pattern: str, start=0) -> bool:\n    \"\"\"Return a boolean indicating whether pattern occurs in text.\"\"\"\n    assert isinstance(text, str), 'text is not a string: {}'.format(text)\n    assert isinstance(pattern, str), 'pattern is not a string: {}'.format(text)\n    # Iterative\n\n    # ! Runtime = O(n), n is len of text. Text must be iterated over. \n\n    # case: pattern length is greater than text\n    if len(pattern) > len(text):\n        return False\n    # case: pattern is empty\n    if pattern == '' or \"\":\n        return True\n\n    # no need to check the rest of text if pattern length exceeds remainder\n    for index in range(start, len(text) - len(pattern) + 1):\n        # case: check if first letter of pattern exists in text\n        if text[index] == pattern[0]:\n            # A slice of text that is the length of pattern starting from text[i] to check if the remaining pattern follows after text[i]\n            rest = text[index: index + len(pattern)]\n            if rest == pattern:\n                return True\n    # case: pattern not in text\n    return False",
        "sha1": "0239ab7755dab39128ad0e50d41d49a2fd59b840",
        "id": 454289
    },
    {
        "content": "def isWeekend(dt):\n    \"\"\"Finds if a date lies on a weekend or not. Returns a boolean\"\"\"\n    if 0 < dt.weekday() < 6:\n        return False\n    else:\n        return True",
        "sha1": "61c1f8df1f8791147c794b60c0f6452b4b540510",
        "id": 637393
    },
    {
        "content": "from typing import Tuple\n\n\ndef get_product_section_names() -> Tuple[str, ...]:\n    \"\"\"Get the list of product section names, in their correct order.\"\"\"\n\n    return (\n        'manufacturer',\n        'product_name',\n        'part_number',\n        'product_version',\n        'serial_number',\n        'asset_tag',\n        'fru_file_id',\n    )",
        "sha1": "f69d536134f212e95c18236670885fb07aef5242",
        "id": 74429
    },
    {
        "content": "def format_question(question):\n    \"\"\"\n    Pretty format a question document\n    \"\"\"\n    return \"\"\"category: {q[category]}\nshow no:  {q[show_number]}\ndate:     {q[air_date]}\nround:    {q[round]}\nvalue:    {q[value]}\nquestion: {q[question]}\nanswer:   {q[answer]}\n\"\"\".format(q=question)",
        "sha1": "85e8936c648dd160c9adc35b31d07ca7464d40eb",
        "id": 232565
    },
    {
        "content": "def dict_factory(cursor, row):\n    \"\"\"\n    Turns rows into dictionaries (and corrects boolean values) for easier JSON\n    conversion. Plugs into Connection.row_factory.\n    \"\"\"\n\n    BOOLEAN_COLUMNS = {\"open\"}\n\n    d = {}\n    for idx, col in enumerate(cursor.description):\n        key = col[0]\n        value = bool(row[idx]) if key in BOOLEAN_COLUMNS else row[idx]\n        d[key] = value\n\n    return d",
        "sha1": "972c6f247e2fbab026c0049de74729a700c9372b",
        "id": 299733
    },
    {
        "content": "def decrypt(sk, c):\n    \"\"\"Decrypt a cyphertext based on the provided key.\"\"\"\n    return (c % sk) % 2",
        "sha1": "d3a96f66e1b449ffbd5b6ca1ba0827455b83f9e2",
        "id": 701346
    },
    {
        "content": "def prob(counts, variables=None, condition=(), transform=False):\n    \"\"\"Calculate (conditional) probabilites from counts.\n\n    The levels of the result series, will allways be sorted to ensure proper behavior\n    under multiplication and addition.\n\n    :param counts:\n        a series of counts with the variables as index. Typically,\n        it is the result of an operation similar to\n        ``df.groupby(variables).size()``.\n    :param variables:\n        the variables for which to calculate the probability. If\n        not given all variables are used.\n    :param conditon:\n        the variables to condition on.\n    :param transform:\n        if ``True`` return the result with the original index, similar\n        to how ``.groupby().transform()`` operates.\n    \"\"\"\n    condition = list(condition)\n\n    if variables is None:\n        variables = [*counts.index.names]\n\n    else:\n        variables = list(variables)\n\n    if transform:\n        complement = [name for name in counts.index.names if name not in {*variables}]\n\n    else:\n        complement = condition\n\n    grouped = counts.groupby(level=condition + variables)\n\n    counts = grouped.agg(\"sum\") if not transform else grouped.transform(\"sum\")\n    if not complement:\n        result = counts / counts.sum()\n\n    else:\n        result = counts / counts.groupby(level=complement).transform(\"sum\")\n\n    if len(result.index.names) > 1:\n        return result.reorder_levels(sorted(result.index.names))\n\n    else:\n        return result",
        "sha1": "a36e7cc30a07eb1fef5b4458c870ca4887eb0850",
        "id": 73438
    },
    {
        "content": "def length(p):\n    \"\"\"Calculate length of linked list\"\"\"\n    count = 1\n    i = p\n    while i.successor is not None:\n        count += 1\n        i = i.successor\n\n    return count",
        "sha1": "2db8d7ef9b5e5948e07142ed2ce081eb4b1dc767",
        "id": 157398
    },
    {
        "content": "def calc_iou(box1, box2):\n    \"\"\"\n    Param: box1, box2\n    Return: Intersection over Union of two boxes\n    \n    Each boxes should be like [x1, y1, x2, y2],\n    and x1 <= x2, y1 <= y2\n    \"\"\"\n    \n    (ax1, ay1, ax2, ay2) = box1\n    (bx1, by1, bx2, by2) = box2\n    \n    assert (ax1 <= ax2) & (ay1 <= ay2)\n    assert (bx1 <= bx2) & (by1 <= by2)\n    \n    cx1 = max(ax1, bx1)\n    cy1 = max(ay1, by1)\n    cx2 = min(ax2, bx2)\n    cy2 = min(ay2, by2)\n    \n    assert (cx1 <= cx2) & (cy1 <= cy2)\n        \n    a_area = (ax2 - ax1) * (ay2 - ay1)\n    b_area = (bx2 - bx1) * (by2 - by1)\n    c_area = (cx2 - cx1) * (cy2 - cy1)\n        \n    union_area = a_area + b_area - c_area\n    intersecion_area = c_area\n    \n    smooth = 1e-6\n    return (intersecion_area + smooth) / (union_area + smooth)",
        "sha1": "f69bcb182db4d464ae5ca60890ad527d2f146057",
        "id": 197302
    },
    {
        "content": "def display_body(body_list):\n    \"\"\"\n    Sets the body for the message to be displayed with display()\\n\n    The body needs to be a list with each element of the list being a line.\n    \"\"\"\n    global body\n    if type(body_list) == type(['hey', 'hey']):\n        body = body_list\n        return body\n    elif type(body_list) == type('Hey hey'):\n        body = [body_list]\n        return body\n    else:\n        return f'Error: non supported argument type {type(body_list)}'",
        "sha1": "a213245e60c4c3fb4c69392e45686329e440c076",
        "id": 346896
    },
    {
        "content": "def sphere_func(x):\n    \"\"\"Sphere objective function.\n\n    Has a global minimum at :code:`0` and with a search domain of\n        :code:`[-inf, inf]`\n\n    Parameters\n    ----------\n    x : numpy.ndarray\n        set of inputs of shape :code:`(n_particles, dimensions)`\n\n    Returns\n    -------\n    numpy.ndarray\n        computed cost of size :code:`(n_particles, )`\n    \"\"\"\n    j = (x ** 2.0).sum(axis=1)\n\n    return j",
        "sha1": "3514e9372688a409551243524444ea7388ce7e78",
        "id": 591746
    },
    {
        "content": "import yaml\n\n\ndef read_yaml(path: str) -> dict:\n    \"\"\"Returns YAML file as dict\"\"\"\n    with open(path, 'r') as file_in:\n        config = yaml.safe_load(file_in)\n    return config",
        "sha1": "f0175a15a2d8c2dcd3d1ecb7b3b206be0e392d59",
        "id": 293124
    },
    {
        "content": "def substitute(message, substitutions=[[], {}], depth=1):\n    \"\"\"\n    Substitute `{%x%}` items values provided by substitutions\n    :param message: message to be substituted\n    :param substitutions: list of list and dictionary. \n        List is used for {%number%} substitutions and dictionary for\n        {%name%} substitutions\n    :param depth: defines number of pass\n    :return: substituted message\n    \"\"\"\n    if not isinstance(message, str):\n        raise Exception('StringExpected!')\n\n    if depth <1 :\n        return message\n\n    new_message = message\n\n    # substitute numbered substitutions\n    i = 0\n    for value in substitutions[0]:\n        new_message = new_message.replace(\"{%%%d%%}\" % i, value)\n        i += 1\n    # processing named substitutions\n    for (k, value) in substitutions[1].items():\n        new_message = new_message.replace(\"{%%%s%%}\" % k, value)\n\n    return substitute(new_message, substitutions, depth-1)",
        "sha1": "4a46c46faf1848f1c7e437fed956e21cfdd3a813",
        "id": 652579
    },
    {
        "content": "def poseFromROSTransformMsg(msg):\n    \"\"\"\n    :param msg: A populated ROS Transform message.\n    :return: (pos, quat), where pos is a 3-element list of positions [x, y, z],\n             and quat is a 4-element list of quaternion elems [w, x, y, z]\n    \"\"\"\n    pos = [msg.translation.x, msg.translation.y, msg.translation.z]\n    quat = [msg.rotation.w, msg.rotation.x, msg.rotation.y, msg.rotation.z]\n    return pos, quat",
        "sha1": "23792434f72cd798690f74e4b049a02907d9ca48",
        "id": 68555
    },
    {
        "content": "def remove_duplicates_from_image_name_data(data):\n    \"\"\"remove duplicates from this data.\n\n    Parameters\n    ==========\n    data: pd.DataFrame\n        data filtered for one image_name\n\n    Returns\n    =======\n    For each `user_name` and `image_id` found in `data` return only the data\n    for the first found classification_id. There *should* only be one\n    classification_id per user_name and image_id, but sometimes the queue\n    presented the same image_id more than once to the same users. This removes\n    any later in time classification_ids per user_name and image_id.\n    \"\"\"\n\n    c_ids = []\n\n    def process_user_group(g):\n        c_ids.append(g[g.created_at == g.created_at.min()\n                       ].classification_id.min())\n\n    data.groupby(['image_id', 'user_name'],\n                 sort=False).apply(process_user_group)\n    return data.set_index('classification_id').loc[set(c_ids)].reset_index()",
        "sha1": "00a5045ad7c36620fcb2e913bf7a6809f83a3927",
        "id": 462480
    },
    {
        "content": "import functools\nimport warnings\n\n\ndef ignore_warnings(category):\n    \"\"\"Returns a decorator to ignore all warnings\n    in the specified category.\n\n    Parameters\n    ----------\n    category : class\n        Any warning that is a subclass of this category is ignored.\n\n    Returns\n    -------\n    decorator_ignore : function\n        A decorator that ignores all warnings in the category.\n    \"\"\"\n    def decorator_ignore(fn):\n        @functools.wraps(fn)\n        def fn_ignore(*args, **kwargs):\n            with warnings.catch_warnings():\n                warnings.simplefilter(\"ignore\", category)\n                return fn(*args, **kwargs)\n        return fn_ignore\n    return decorator_ignore",
        "sha1": "4ddfc3b0aa78851600d651f51154d08252020c86",
        "id": 430438
    },
    {
        "content": "def decode_http_header(raw):\n    \"\"\"\n    Decode a raw HTTP header into a unicode string. RFC 2616 specifies that\n    they should be latin1-encoded (a.k.a. iso-8859-1). If the passed-in value\n    is None, return an empty unicode string.\n\n    :param raw:\n        Raw HTTP header string.\n    :type raw:\n        string (non-unicode)\n    :returns:\n        Decoded HTTP header.\n    :rtype:\n        unicode string\n    \"\"\"\n    if raw:\n        return raw.decode('iso-8859-1', 'replace')\n    else:\n        return u''",
        "sha1": "fdb322e1fc6b8d9ff5cfdc02001015e2ebeaf606",
        "id": 47961
    },
    {
        "content": "def get_nested_attr(obj, attr, default=None):\n    \"\"\"Get nested attributes of an object.\"\"\"\n    attrs = attr.split(\".\")\n    for a in attrs:\n        try:\n            obj = getattr(obj, a)\n        except AttributeError:\n            if default:\n                return default\n            else:\n                raise\n    return obj",
        "sha1": "7f8dc82f6d08a763a0a083967ef524acbba8e983",
        "id": 680775
    },
    {
        "content": "import re\n\n\ndef ismultibyte(value):\n    \"\"\"\n    Return whether or not given value contains one or more multibyte chars.\n    If the value contains one or more multibyte chars, this function returns ``True``, otherwise ``False``.\n\n    Examples::\n\n        >>> ismultibyte('\u3042\u3044\u3046\u3048\u304a foobar')\n        True\n\n        >>> ismultibyte('abc')\n        False\n\n    :param value: string to validate one or more multibyte chars\n    \"\"\"\n    multi_byte = re.compile(r\"[^\\x00-\\x7F]\")\n    return bool(multi_byte.match(value))",
        "sha1": "866c6c0cf1b1b09d66f50b141ba0cd1b8badbc9d",
        "id": 82679
    },
    {
        "content": "def dict_of_load_relays(relays, buses_with_loads):\n    \"\"\"\n    Create dictionaries of the relay keys from the\n    load elements\n    \"\"\"\n    load_relays = {k: list() for k in buses_with_loads}\n\n    for relay_name, relay in relays.items():\n        load_relay_mappings = relay['load']\n        if load_relay_mappings:\n            for b in load_relay_mappings:\n                load_relays[b].append(relay_name)\n\n    return load_relays",
        "sha1": "59c9744422373a869c5bd005be29aac41f3366e1",
        "id": 608936
    },
    {
        "content": "def _remove_parens(atom):\n    \"\"\"\n    Returns the inner part of an expression like `(foo)`. Also removes nested\n    parens.\n    \"\"\"\n    try:\n        children = atom.children\n    except AttributeError:\n        pass\n    else:\n        if len(children) == 3 and children[0] == '(':\n            return _remove_parens(atom.children[1])\n    return atom",
        "sha1": "fbb180a99a92cedd966f907463e9acaa03ace57f",
        "id": 369439
    },
    {
        "content": "def bitscatter(bits, mask):\n    \"\"\"\n    Scatters the contents of bitvector `bits` onto the raised bits in `mask`.\n    \"\"\"\n    value = 0\n    mask_walker = enumerate(reversed(bin(mask)[2:]))\n    for bit_index, mask_index in enumerate([x for x, y in mask_walker if y == '1']):\n        value |= (bits & (1 << bit_index)) << (mask_index - bit_index)\n    return value",
        "sha1": "ec0abf3b33e8e68a08b18d8ec31b17772460f075",
        "id": 522435
    },
    {
        "content": "def convert_composition_string_to_dict( X ):\n    \"\"\"\n    Converts a composition string of style \"CH4:0.02, N2:0.01, O2:0.45\"\n    into a dict, a la composition['CH4'] = 0.02\n    \"\"\"\n    results = {}\n    for sp in X.split(\",\"):\n        st = sp.strip()\n        try:\n            results[ st.split(\":\")[0].strip() ] = float( st.split(\":\")[1].strip() )\n        except IndexError:\n            # X is probably not a list\n            # (why would we run split on a list?)\n            # or is empty\n            err = \"ERROR: CanteraGasUtils: your X is probably specified incorrectly. Expected type list, got type \"+type(X)\n            raise Exception(err)\n\n    # normalize\n    results_sum = sum( [results[k] for k in results.keys()] )\n    for k in results.keys():\n        results[k] = results[k]/results_sum\n\n    return results",
        "sha1": "06ebb35d5539e466367b60e7c20b07f26c82cc13",
        "id": 246948
    },
    {
        "content": "import random\n\n\ndef get_random_comment(arr):\n    \"\"\"Takes in array, returns 1 random item in the array\"\"\"\n    try:\n        randnum = random.randint(0, len(arr) - 1)\n        return arr[randnum]\n    except IndexError:\n        randnum = random.randint(0, 50)\n        return arr[randnum]",
        "sha1": "8bdec1aba852f5b9c63727d68046d0dd43e8a526",
        "id": 534430
    },
    {
        "content": "def find_in_dict(data, keys):\n    \"\"\"\n    Finds the value in a potentially nested dictionary by a key or set of\n    nested keys.\n\n    Parameters:\n    ----------\n    data: :obj:`dict`\n        The dictionary for which we want to find the value indexed by the\n        potentially nested keys.\n    keys: :obj:`list`, :obj:`tuple`, :obj:`str`\n        Either an iterable of nested keys or a single key for which we want\n        to locate the associated value in the dictionary for.\n\n    Example:\n    -------\n    >>> data = {'foo': {'bar': 'banana'}}\n    >>> find_in_dict(data, 'foo')\n    >>> {'bar': 'banana'}\n    >>> find_in_dict(data, ['foo', 'bar'])\n    >>> 'banana'\n    \"\"\"\n    if hasattr(keys, '__iter__') and not isinstance(keys, str):\n        if len(keys) == 1:\n            return data[keys[0]]\n        current = data[keys[0]]\n        for key in keys[1:]:\n            current = current[key]\n        return current\n    return data[keys]",
        "sha1": "c5da3ee5b37d6f84b85a39f75d15bffee0b9dfe8",
        "id": 97286
    },
    {
        "content": "import json\n\n\ndef _loadData(filePath):\n    \"\"\"\n    load data from a .json file.\n\n    :param filePath: Path of the .json file.\n    :type  filePath: Str.\n\n    \"\"\"\n    with open(filePath) as json_file:\n        j_data = json.load(json_file)\n\n    json_file.close()\n\n    print(\"Data successfully loaded !\")\n    return j_data",
        "sha1": "d02230a41b44e296da9b5d4b99570e623f3513cd",
        "id": 161311
    },
    {
        "content": "def get_numeric_trace_attribute_rep(trace_attribute: str) -> str:\n    \"\"\"\n    Get the feature name associated to a numeric trace attribute\n\n    Parameters\n    ------------\n    trace_attribute\n        Name of the trace attribute\n\n    Returns\n    ------------\n    feature_name\n        Name of the feature\n    \"\"\"\n    return \"trace:\" + trace_attribute",
        "sha1": "3d4944cf3e7b38db0e2c0e7b74111d010173cb77",
        "id": 247275
    },
    {
        "content": "def gettext_noop(message):\n    \"\"\"\n    Marks strings for translation but doesn't translate them now. This can be\n    used to store strings in global variables that should stay in the base\n    language (because they might be used externally) and will be translated\n    later.\n    \"\"\"\n    return message",
        "sha1": "0ee276b32468292d61ee81166a5fe183819e52d1",
        "id": 557424
    },
    {
        "content": "import unittest\n\n\ndef run_tests(path):\n    \"\"\"\n    Runs the tests located in the rovided path\n    :param path: str\n    \"\"\"\n    loader = unittest.TestLoader()\n    suite = loader.discover(path)\n\n    runner = unittest.TextTestRunner()\n    return not runner.run(suite).wasSuccessful()",
        "sha1": "27165dfe826cd0a765b28a3f986dfd5142f86894",
        "id": 387975
    },
    {
        "content": "def retreive_details(input_dict,keys_to_extract):\n    \"\"\"\n    Retreive specific keys from a dictionary object\n    \n    This function searches through a dictionary, and\n    retreives keys that match keys from a list. This function \n    will find matching keys in every level of the dictionary heirarchy.\n    \n    Inputs:\n    input_dict      -  A dictionary (expected to be from json.loads() \n                       in the context of BetterReads, but the function\n                       will work on any dictionary).\n    keys_to_extract -  A list of keys that you want to extract from the \n                       json_dict object.\n                       \n    Output:\n    new_dict        -  A new \"flattened\"dictionary with all the matching keys. \n                       All the keys are in the top level.\n    \"\"\"\n    \n    new_dict={}\n    for item in input_dict.keys():\n        if type(input_dict[item]) is dict:\n            temp_dict = retreive_details(input_dict[item],keys_to_extract)\n            new_dict.update(temp_dict)\n        if item in keys_to_extract:\n            new_dict[item] = input_dict[item] \n    return new_dict",
        "sha1": "f80c4bac321227a09a9cdf8bac3b1b4e0b16ff5d",
        "id": 637039
    },
    {
        "content": "import re\n\n\ndef find_identity_in_list(elements, identities):\n    \"\"\"Matches a list of identities to a list of elements.\n\n    Args:\n      elements: iterable of strings, arbitrary strings to match on.\n      identities: iterable of (string, string), with first string\n        being a regular expression, the second string being an identity.\n\n    Returns:\n      The identity specified in identities for the first regular expression\n      matching the first element in elements.\n    \"\"\"\n    for element in elements:\n        for regex, identity in identities:\n            if re.search(regex, element):\n                return identity\n    return None",
        "sha1": "597b0e89547046a5ff7746344175ef6c0494b5ad",
        "id": 623582
    },
    {
        "content": "import binascii\n\n\ndef hex2b(hex_str):\n    \"\"\"Produce bytes from the given hex string representation.\n\n    :param hex: hex string\n    :type hex: str\n    :rtype: bytes\n    \"\"\"\n    return binascii.unhexlify(hex_str.encode(\"ascii\"))",
        "sha1": "dfe526070b534e46c7737834f3f24da1560baf3f",
        "id": 213086
    },
    {
        "content": "import torch\n\n\ndef masked_min_pooling(data_tensor, mask, dim):\n    \"\"\"\n    Performs masked min-pooling across the specified dimension of a Tensor.\n\n    :param data_tensor: ND Tensor.\n    :param mask: Tensor containing a binary mask that can be broad-casted to the shape of data_tensor.\n    :param dim: Int that corresponds to the dimension.\n    :return: (N-1)D Tensor containing the result of the min-pooling operation.\n    \"\"\"\n\n    if dim < 0:\n        dim = len(data_tensor.shape) + dim\n\n    mask = mask.view(list(mask.shape) + [1] * (len(data_tensor.shape) - len(mask.shape)))\n    data_tensor = data_tensor.masked_fill(mask == 0, 1e9)\n\n    min_vals, min_ids = torch.min(data_tensor, dim=dim)\n\n    return min_vals",
        "sha1": "7d2fa435e495d84d9099ca2f23c4803960bd8710",
        "id": 342129
    },
    {
        "content": "def fmt_call_method(vm, argc: int, repr=repr) -> str:\n    \"\"\"\n    formats function name (without enclosing object), and positional args.\n    \"\"\"\n    pos_args = [vm.peek(i + 1) for i in range(argc)]\n\n    fn_name = vm.peek(argc + 2).__name__\n    return f\"\"\" {fn_name}({\", \".join((repr(a) for a in pos_args))})\"\"\"",
        "sha1": "7e1bb6898c3140d03af4c06d0f81e6447cb6c41e",
        "id": 134882
    },
    {
        "content": "def sparse(x0, rho, gamma):\n    \"\"\"\n    Proximal operator for the l1 norm (induces sparsity)\n\n    Parameters\n    ----------\n    x0 : array_like\n        The starting or initial point used in the proximal update step\n\n    rho : float\n        Momentum parameter for the proximal step (larger value -> stays closer to x0)\n\n    gamma : float\n        A constant that weights how strongly to enforce the constraint\n\n    Returns\n    -------\n    theta : array_like\n        The parameter vector found after running the proximal update step\n    \"\"\"\n\n    lmbda = float(gamma) / rho\n\n    return (x0 - lmbda) * (x0 >= lmbda) + (x0 + lmbda) * (x0 <= -lmbda)",
        "sha1": "bac67eca84cc666c93c4db06aa2dd39c95b053fe",
        "id": 378359
    },
    {
        "content": "import itertools\n\n\ndef flatten(ll):\n    \"\"\"Flatten an iterable of iterable to a single list.\n    \"\"\"\n    return list(itertools.chain(*ll))",
        "sha1": "4b3243ba517597569e4f805e916fe154ea7aab88",
        "id": 271198
    },
    {
        "content": "def pt_to_tup(pt):\n    \"\"\"\n    Convenience method to generate a pair of two ints from a tuple or list.\n\n    Parameters\n    ----------\n    pt : list OR tuple\n        Can be a list or a tuple of >=2 elements as floats or ints.\n\n    Returns\n    -------\n    pt : tuple of int\n        A pair of two ints.\n    \"\"\"\n    return (int(pt[0]),int(pt[1]));",
        "sha1": "7013b2477959f528b98d364e4cc44ac8700fb366",
        "id": 3760
    },
    {
        "content": "import socket\n\n\ndef send_all(s, text, stutdown_after_sending=True):\n    \"\"\"send_all(socket, text, stutdown_after_sending=True)\n\n    Send all text to the socket. Used during handshaking and in\n    the clientserver module.\n\n    If stutdown_after_sending, the socket is shut down. Some protocols\n    rely on this.\n\n    It is made sure that the text ends with a CRLF double-newline code.\n\n    \"\"\"\n\n    # Ensure closing chars\n    if not text.endswith(\"\\r\\n\"):\n        text += \"\\r\\n\"\n\n    # Make bytes\n    bb = text.encode(\"utf-8\")\n\n    # Send all bytes\n    try:\n        s.sendall(bb)  # -> n\n    except socket.error:\n        return -1  # Socket closed down badly\n\n    # Shutdown connection nicely from here\n    if stutdown_after_sending:\n        try:\n            s.shutdown(socket.SHUT_WR)\n        except socket.error:\n            pass",
        "sha1": "5f24df23db227f021746d3cf37b84f361259294a",
        "id": 629266
    },
    {
        "content": "import six\n\n\ndef byte_adaptor(fbuffer):\n    \"\"\" provides py3 compatibility by converting byte based\n    file stream to string based file stream\n\n    Arguments:\n        fbuffer: file like objects containing bytes\n\n    Returns:\n        string buffer\n    \"\"\"\n    if six.PY3:\n        strings = fbuffer.read().decode('utf-8')\n        fbuffer = six.StringIO(strings)\n        return fbuffer\n    else:\n        return fbuffer",
        "sha1": "3b1f5c5d96538d737d58076b99cea451e065919a",
        "id": 388337
    },
    {
        "content": "def fetch_samples(prj, selector_attribute=None, selector_include=None,\n                  selector_exclude=None):\n    \"\"\"\n    Collect samples of particular protocol(s).\n\n    Protocols can't be both positively selected for and negatively\n    selected against. That is, it makes no sense and is not allowed to\n    specify both selector_include and selector_exclude protocols. On the\n    other hand, if\n    neither is provided, all of the Project's Samples are returned.\n    If selector_include is specified, Samples without a protocol will be\n    excluded,\n    but if selector_exclude is specified, protocol-less Samples will be\n    included.\n\n    :param Project prj: the Project with Samples to fetch\n    :param str selector_attribute: name of attribute on which to base the\n    fetch\n    :param Iterable[str] | str selector_include: protocol(s) of interest;\n        if specified, a Sample must\n    :param Iterable[str] | str selector_exclude: protocol(s) to include\n    :return list[Sample]: Collection of this Project's samples with\n        protocol that either matches one of those in selector_include,\n        or either\n        lacks a protocol or does not match one of those in selector_exclude\n    :raise TypeError: if both selector_include and selector_exclude\n    protocols are\n        specified; TypeError since it's basically providing two arguments\n        when only one is accepted, so remain consistent with vanilla\n        Python2;\n        also possible if name of attribute for selection isn't a string\n    \"\"\"\n    if selector_attribute is None or \\\n            (not selector_include and not selector_exclude):\n        # Simple; keep all samples.  In this case, this function simply\n        # offers a list rather than an iterator.\n        return list(prj.samples)\n\n    if not isinstance(selector_attribute, str):\n        raise TypeError(\n            \"Name for attribute on which to base selection isn't string: \"\n            \"{} \"\n            \"({})\".format(selector_attribute, type(selector_attribute)))\n\n    # At least one of the samples has to have the specified attribute\n    if prj.samples and not any(\n            [hasattr(s, selector_attribute) for s in prj.samples]):\n        raise AttributeError(\n            \"The Project samples do not have the attribute '{attr}'\".\n                format(attr=selector_attribute))\n\n    # Intersection between selector_include and selector_exclude is\n    # nonsense user error.\n    if selector_include and selector_exclude:\n        raise TypeError(\n            \"Specify only selector_include or selector_exclude parameter, \"\n            \"not both.\")\n\n    # Ensure that we're working with sets.\n    def make_set(items):\n        if isinstance(items, str):\n            items = [items]\n        return items\n\n    # Use the attr check here rather than exception block in case the\n    # hypothetical AttributeError would occur; we want such\n    # an exception to arise, not to catch it as if the Sample lacks\n    # \"protocol\"\n    if not selector_include:\n        # Loose; keep all samples not in the selector_exclude.\n        def keep(s):\n            return not hasattr(s, selector_attribute) \\\n                   or getattr(s, selector_attribute) \\\n                   not in make_set(selector_exclude)\n    else:\n        # Strict; keep only samples in the selector_include.\n        def keep(s):\n            return hasattr(s, selector_attribute) \\\n                   and getattr(s, selector_attribute) \\\n                   in make_set(selector_include)\n\n    return list(filter(keep, prj.samples))",
        "sha1": "03b1e006cdf4dc83075930b110bf2dac0d84d695",
        "id": 529548
    },
    {
        "content": "import ast\nfrom typing import Any\n\n\ndef try_literal_eval(node_or_string: str) -> Any:\n    \"\"\"\n    Try to parse node_or_string as a Python value;\n    return node_or_string unchanged if ast.literal_eval raises an Error.\n    \"\"\"\n\n    try:\n        return ast.literal_eval(node_or_string)\n    except (SyntaxError, ValueError):\n        return node_or_string",
        "sha1": "93e6e5a4fb24b33151cb3d7dc8163b4542c3db7a",
        "id": 198366
    },
    {
        "content": "import requests\nfrom bs4 import BeautifulSoup\n\n\ndef get_article(url):\n    \"\"\"\n    Gets article content from URL\n    Input : URL of article\n    Output : Content in BeautifulSoup format\n    \"\"\"\n        \n    r = requests.get(url) \n    html_soup = BeautifulSoup(r.content, 'lxml')\n    return html_soup",
        "sha1": "4b1246d8a1e3f60b29a0b4c0bf9540d345c3a936",
        "id": 686195
    },
    {
        "content": "def parse_encryption_algorithms_list_from_file(file_path):\n    \"\"\"Parses a list of ciphersuites from a file.\n\n    Format of file:\n    CIPHERSUITE_ID[:id] CIPHERSUITE_NAME[:str] TAG[:str]\n    \"\"\"\n    with open(file_path, 'r') as sc_file:\n        ciphersuites = [line.strip().split(' ') for line in sc_file.readlines()]\n\n    ciphersuites = [ciphersuite[1] for ciphersuite in ciphersuites if len(ciphersuite) > 1]\n\n    print(f'Total Ciphersuites: {len(ciphersuites)}')\n\n    return ciphersuites",
        "sha1": "10be5b70acbebd0b905834541efc8b7fcddee57a",
        "id": 454226
    },
    {
        "content": "def namify_member(name):\n    \"\"\"Convert data formatted name into human readable form.\"\"\"\n    [last_name, first_name] = map(lambda s: s.capitalize(), name.split(\".\"))\n    return \" \".join([first_name, last_name])",
        "sha1": "f128d862b92af24adf9d363179ca25649b5de250",
        "id": 214740
    },
    {
        "content": "def risingfactorial(n, m):\n    \"\"\"\n    Return the rising factorial; n to the m rising, i.e. n(n+1)..(n+m-1).\n\n    For example:\n    >>> risingfactorial(7, 3)\n    504\n    \"\"\"\n    r = 1\n    for i in range(n, n+m):\n        r *= i\n    return r",
        "sha1": "7fef55ede604d8b5f576608c505edae56822857d",
        "id": 682369
    },
    {
        "content": "def _add_tag(tags, label: str) -> bool:\n  \"\"\"Adds the tag to the repeated field of tags.\n\n  Args:\n    tags: Repeated field of Tags.\n    label: Label of the tag to add.\n\n  Returns:\n    True if the tag is added.\n  \"\"\"\n  for tag in tags:\n    if tag.label == label:\n      # Episode already has the tag.\n      return False\n  tags.add().label = label\n  return True",
        "sha1": "932399e97ae823ef0922929dc5123a587c06b211",
        "id": 41680
    },
    {
        "content": "def convert_string_to_bool(string_value):\n    \"\"\"\n    simple method used to convert a tring to a bool\n\n    :param string_value: True or False string value\n\n    :type string_value: string - required \n\n    :return: bool True or False\n    :rtype bool\n    \"\"\"\n    if string_value == 'True':\n        return True\n    else:\n        return False",
        "sha1": "3e4113721df399408719ae7737136691f904ae78",
        "id": 42762
    },
    {
        "content": "def build_array_encoding(length: int, element_type_encoding: bytes) -> bytes:\n\t\"\"\"Build an array type encoding from a length and an element type encoding.\n\t\n\t.. note::\n\t\n\t\tThis function currently doesn't perform any checking on ``element_type_encoding``,\n\t\tbut such checks may be added in the future.\n\t\t``element_type_encoding`` should always be a valid type encoding string.\n\t\"\"\"\n\t\n\tif length < 0:\n\t\traise ValueError(f\"Array length cannot be negative: {length}\")\n\t\n\tlength_string = str(length).encode(\"ascii\")\n\treturn b\"[\" + length_string + element_type_encoding + b\"]\"",
        "sha1": "5515f9126e2b9d75020126032f6698579cd614c5",
        "id": 389125
    },
    {
        "content": "from typing import List\nfrom typing import Dict\nfrom typing import Optional\n\n\ndef _payload(projects: List[str], identities: List[Dict], priority: int,\n             id: Optional[str], resource_types: Optional[List[str]]) -> Dict:\n    \"\"\"Create a cross-project resolver payload.\n\n    :param projects: List of target projects, given with format ``organization/project``.\n    :param identities: List of identities the creator of the resolver has and\n        which have the permission ``resources/read`` on the target projects.\n    :param priority: Resolution priority.\n    :param id: ID of the resolver, given as an IRI which is not URL encoded.\n    :param resource_types: List of types of the resources to resolve,\n        given as IRIs.\n    :return: Payload of the cross-project resolver.\n    \"\"\"\n    payload = {\n        \"@type\": \"CrossProject\",\n        \"projects\": projects,\n        \"identities\": identities,\n        \"priority\": priority,\n    }\n    if id is not None:\n        payload[\"@id\"] = id\n    if resource_types is not None:\n        payload[\"resourceTypes\"] = resource_types\n    return payload",
        "sha1": "0f9450bb7627b2cbb3a105f3e09718eff7fadce9",
        "id": 569727
    },
    {
        "content": "def skycoord_to_string(skycoord):\n    \"\"\"Convert a one-dimenstional list of SkyCoord to string for Gaia's query format.\"\"\"\n    corners_list_str = skycoord.to_string()\n    corners_single_str = ' '.join(corners_list_str)\n    return corners_single_str.replace(' ', ', ')",
        "sha1": "cc02f3d157a45b3dd2e29a5204d0963d8d5d3ac8",
        "id": 289231
    },
    {
        "content": "def _unique_list(df_col):\n    \"\"\"Return a list of unique values without NA.\"\"\"\n    return list(df_col.unique().dropna())",
        "sha1": "fac06777d1b7e6abbe7aa8daf491e4171b5ef713",
        "id": 687486
    },
    {
        "content": "import itertools\n\n\ndef combo_space(ks, items):\n    \"\"\"\n    Create the Cartesian product of values sets, each bound to a key.\n\n    :param Iterable[str] ks: subset of keys from the mapping items\n    :param Mapping[str, Iterable[object]] items: bindings between key and\n        collection of values\n    :return itertools.product: Cartesian product of the values sets bound by\n        the given subset of keys\n    \"\"\"\n    return itertools.product(*[items[k] for k in ks])",
        "sha1": "1d2f73c11a97ef5dda978275afb608bd8c67b1fe",
        "id": 505876
    },
    {
        "content": "def next_decorator(event, message, decorates):\n    \"\"\"\n    Helper method for IAnnouncerEmailDecorators.  Call the next decorator\n    or return.\n    \"\"\"\n    if decorates and len(decorates) > 0:\n        next = decorates.pop()\n        return next.decorate_message(event, message, decorates)",
        "sha1": "2dbec5b53e532a2187be5fd3d97078f1db088d9d",
        "id": 82236
    },
    {
        "content": "def days_since(t1, t2):\n    \"\"\"\n    Returns the number of days between two timestamps.\n\n    :param t1: Time one.\n    :param t2: Time two.\n    \"\"\"\n    timedelta = t1 - t2\n    return timedelta.days",
        "sha1": "9ed73a98fcb2dcde9f3317b7785bfc1bd074a23f",
        "id": 663564
    },
    {
        "content": "def clean_response(result):\n    \"\"\"\n    Clean a response by removing unnecessary fields\n    \"\"\"\n\n    result = result[\"PET\"]\n    try:\n        del result[\"BLOQ\"]\n    except (KeyError, TypeError):\n        pass\n\n    return result",
        "sha1": "3c658d4fea334f3a843de6c3af3e06bd669bfde9",
        "id": 592279
    },
    {
        "content": "def find_keyword(ascii_file, keyword, max_lines = None):\n    \"\"\"Looks for line starting with given keyword; file pointer is left at start of that line.\"\"\"\n    start_pos = ascii_file.tell()\n    while True:\n        if max_lines is not None:\n            if max_lines <= 0:\n                ascii_file.seek(start_pos)\n                return False\n            max_lines -= 1\n        file_pos = ascii_file.tell()\n        line = ascii_file.readline()\n        if len(line) == 0:\n            ascii_file.seek(start_pos)\n            return False  # end of file\n        words = line.split()\n        if len(words) > 0 and words[0].upper() == keyword.upper():\n            ascii_file.seek(file_pos)\n            return True",
        "sha1": "a0ca95790e4cb803565e65a027d0d838e6d28b19",
        "id": 459974
    },
    {
        "content": "import sqlite3\n\n\ndef get_summoners(guild):\n    \"\"\"Get all summoners being tracked by a guild\n\n    Args:\n        guild (int): ID of guild\n\n    Returns:\n        tuple: All summoners being tracked by specified guild\n    \"\"\"\n    \n    conn = sqlite3.connect('jit.db')\n    c = conn.cursor()\n    \n    c.execute('''\n            SELECT * FROM summoner WHERE guild = ?\n            ''', (guild,))\n    \n    summoners = c.fetchall()\n    return summoners",
        "sha1": "47bde5d877a6f2b3875ffa250340a269dfbb51b0",
        "id": 208327
    },
    {
        "content": "def fib_recursive_mathy_cached(n):\n    \"\"\"Same function as \"fib_recursive_mathy\", but this is cached using a generic memoizer as decorator.\n    \n    The decorator is implemented in \"./util/cache.py\".\n    \"\"\"\n    if n < 2:\n        return n\n\n    return fib_recursive_mathy_cached(n - 1) + fib_recursive_mathy_cached(n - 2)",
        "sha1": "5560d857b235bb4a451c880f35ee58de9537df88",
        "id": 301051
    },
    {
        "content": "def add_extension_if_missing(test_file):\n    \"\"\"\n    Add .txt extension if not present\n    >>> add_extension_if_missing('this')\n    'this.txt'\n    >>> add_extension_if_missing('that.txt')\n    'that.txt'\n    \"\"\"\n    if not test_file.endswith(\".txt\"):\n        test_file += \".txt\"\n    return test_file",
        "sha1": "04d96c728c0e6fd72506d98c35cd4bbb4bab112f",
        "id": 333605
    },
    {
        "content": "def kBtu_h2Btu_h(x):\n    \"\"\"kBtu/h -> Btu/h\"\"\"\n    return 1000.*x",
        "sha1": "bdb895ba5874db1a56365561dd25eea6caf1d750",
        "id": 453570
    },
    {
        "content": "from typing import Optional\nfrom typing import Dict\nimport torch\n\n\ndef sample_linear_trajectories(\n    num_cell_types: int,\n    seed: Optional[int] = None,\n    a_min: float = 0.0,\n    a_max: float = 10.0,\n    b_min: float = -10.0,\n    b_max: float = 10.0,\n) -> Dict:\n    \"\"\"\n    Generate a sample of linear trajectory coefficients\n    \n    :param num_cell_types: number of cell types in trajectories\n    :param seed: Random seed (for reproducibility)\n    :param a_min: minimum value for a coefficient\n    :param a_max: maximum values for a coefficient\n    :param b_min: minimum value for b coefficient\n    :param b_max: maximum value for b coefficient\n    \n    :return: Dictionary coefficient and their values\n    \"\"\"\n    if seed is not None:\n        torch.manual_seed(seed)\n\n    # y = ax+b\n    a = torch.rand(num_cell_types) * (a_max - a_min) + a_min\n    b = torch.rand(num_cell_types) * (b_max - b_min) + b_min\n\n    return {\"a\": a, \"b\": b}",
        "sha1": "26266c844565b57bdf98eb9cfa1af233fd1c7581",
        "id": 502629
    },
    {
        "content": "def get_operator_priority(char: str) -> int:\n    \"\"\"\n    Used to get operator priority, or to check if char is correct operator\n    :param char: Operator\n    :return: Operator priority (positive), or -1 if char is not correct operator\n    \"\"\"\n    if char == '!' or char == '#':\n        # '#' - unary minus\n        return 12\n    elif char == '*' or char == '/':\n        return 10\n    elif char == '+' or char == '-':\n        return 8\n    elif char == '>' or char == '<' \\\n            or char == '>=' or char == '<=' \\\n            or char == '==' or char == '!=':\n        return 6\n    elif char == '&' or char == '|':\n        return 5\n    elif char == '=':\n        return 4\n    elif char == '(' or char == ')':\n        return 2\n    else:\n        return -1",
        "sha1": "77e04dd590a5079f8f7c4bbc66f4e96dfc38e844",
        "id": 602312
    },
    {
        "content": "def xLP2DP(lpX, lptLT, lPix = 1.0):\n    \"\"\"Convert logical coordinates into device coordinates\n        lpX   - x logical coordinate\n        lptLT - logical coordinates of left top screen corner\n        lPix  - zoom value, number of logical points inside one device point (aka pixel)\n        return coordinate in device coordinates\n    \"\"\"\n    return (lpX - lptLT.x) / lPix",
        "sha1": "71ebc2f3207711500deb5a06a8f0f9cca348f617",
        "id": 280981
    },
    {
        "content": "import json\n\n\ndef _load_notebook(f):\n    \"\"\" Load the ipython notebook as a dict.\n\n    Parameters\n    ----------\n    f: str\n        Path to the schema file.\n\n    Returns\n    -------\n    dict\n        json string representing the notebook file.\n    \"\"\"\n    with open(f, 'r') as json_file:\n        notebook = json.load(json_file)\n\n    return notebook",
        "sha1": "3f7f23de647c813a9464525b198c0598c0960a2c",
        "id": 505495
    },
    {
        "content": "def deployment_trigger(test_acc: float) -> bool:\n    \"\"\"Only deploy if the test accuracy > 90%.\"\"\"\n    return test_acc > 0.9",
        "sha1": "941fc0763b149f1d5f660b1e877dda1b4a460970",
        "id": 513458
    },
    {
        "content": "import toml\n\n\ndef read_config_file(filename):\n\n    \"\"\"\n    Reads TOML-formatted configuration file.\n\n    :param filename:    path to file to be read.\n    :type  filename:    str\n\n    :return:    Variables found in config file.\n    :rtype:     dict\n    \"\"\"\n\n    toml_data = open(filename).read()\n    data = toml.loads(toml_data)\n\n    return data",
        "sha1": "8a79861c054908278ff5456b70312082276d7def",
        "id": 344200
    },
    {
        "content": "def replace_headers(request, replacements):\n    \"\"\"\n    Replace headers in request according to replacements. The replacements\n    should be a list of (key, value) pairs where the value can be any of:\n      1. A simple replacement string value.\n      2. None to remove the given header.\n      3. A callable which accepts (key, value, request) and returns a string\n         value or None.\n    \"\"\"\n    new_headers = request.headers.copy()\n    for k, rv in replacements:\n        if k in new_headers:\n            ov = new_headers.pop(k)\n            if callable(rv):\n                rv = rv(key=k, value=ov, request=request)\n            if rv is not None:\n                new_headers[k] = rv\n    request.headers = new_headers\n    return request",
        "sha1": "4e7776081cc7b66b04ea31a55b8a62d38d13e990",
        "id": 512285
    },
    {
        "content": "import math\n\n\ndef block_to_coords(block, sequence):\n    \"\"\"Receives block number 1 through 9 and sequence 1 through 9, both read left to right, top to bottom, and\n    returns the y, x coordinates on the puzzle\"\"\"\n    #     0 1 2 3 4 5 6 7 8\n    #     - - - - - - - - -\n    # 0 - 0 1 2 0 1 2 0 1 2\n    # 1 - 3 4 5 3 4 5 3 4 5\n    # 2 - 6 7 8 6 7 8 6 7 8\n    # 3 - 0 1 2 0 1 2 0 1 2\n    # 4 - 3 4 5 3 4 5 3 4 5\n    # 5 - 6 7 8 6 7 8 6 7 8\n    # 6 - 0 1 2 0 1 2 0 1 2\n    # 7 - 3 4 5 3 4 5 3 4 5\n    # 8 - 6 7 8 6 7 8 6 7 8\n    y = (math.floor(block / 3) * 3) + (math.floor(sequence / 3))\n    x = ((block % 3) * 3) + (sequence % 3)\n    return y, x",
        "sha1": "1934b6467bdf2635f96ad1d10cfead4cb52fdd25",
        "id": 601787
    },
    {
        "content": "import unicodedata\n\n\ndef NormalizeString(string):\n    \"\"\" Normalize given string (remove accents etc.) \"\"\"\n\n    normalized = unicodedata.normalize('NFD', string)\n    normalized = normalized.encode('ASCII', 'ignore')  # ignore non ascii chars\n    normalized = normalized.decode('UTF-8')\n    return normalized",
        "sha1": "52b2b3e8d0f3e54ba556e9c7afe33335071752bf",
        "id": 547250
    },
    {
        "content": "def render_output(out: list):\n    \"\"\"Pretty's `create_seating_chart` output\n\n    Args:\n        out (list): Output of `create_seating_chart`\n\n    Returns:\n        str: Prettified output for presentation to user\n    \"\"\"\n    return \"\\n\\r\".join([\", \".join(i) for i in out])",
        "sha1": "20fc9ed04d656c90298ae76041a94ec3f1dab8b5",
        "id": 211927
    },
    {
        "content": "def group_cpu_metrics(metrics):\n    \"\"\"Group together each instances metrics per app/space\"\"\"\n    grouped_metrics = {}\n    for metric in metrics:\n        grouped_metrics.setdefault(metric['space'], {}).setdefault(metric['app'], []).append(metric['value'])\n\n    return [(app, space, metric_values,)\n            for space, apps in grouped_metrics.items() for app, metric_values in apps.items()]",
        "sha1": "91b91601c359e2b80c31b80fcb80bd5915d5972d",
        "id": 41915
    },
    {
        "content": "def parse_squad(dataset):\n    \"\"\"\n    Parses SQUAD database into more readable format. In this case I only care\n    about question/answers pairs in order to make a seq2seq model that would\n    generate questions out of a paragraph.\n\n    Inputs:\n        dataset: squad dataset in json format\n    Returns:\n        squad_json: parsed squad dataset in json format\n    \"\"\"\n    total_topics = 0\n    total_questions = 0\n    squad_json = []\n\n    # Iterate through every topic in the dataset\n    for topic in dataset:\n        total_topics += 1\n        # Iterate through every text passage in the topic\n        for passage in topic['paragraphs']:\n            # Iterate through every question/answer pairs in the passage\n            for qas in passage['qas']:\n                total_questions += 1\n                text_question_pair = {}\n                # Append the title\n                text_question_pair['topic'] = topic['title']\n                # Append the text paragraph\n                text_question_pair['paragraph'] = passage['context']\n                # Append the question\n                text_question_pair['question'] = qas['question']\n                # Iterate through available answers\n                answers = []\n                for answer in qas['answers']:\n                    answers.append(answer['text'])\n                # And append them all together\n                text_question_pair['answers'] = answers\n\n                # Append found dictionary to the full parsed dataset array\n                squad_json.append(text_question_pair)\n\n    print('Found ' + str(total_topics) + ' topics in total.')\n    print('Found ' + str(total_questions) + ' questions in total.')\n    return squad_json",
        "sha1": "15971b1bd8dd241af5e458fafe363b8859303e4f",
        "id": 25767
    },
    {
        "content": "def get_source_tweet(tweet):\n    \"\"\"\n    If the tweet is a retweet or a quote tweet, then retrieve the original tweet.\n    :param tweet: a JSON tweet object\n    :return: original tweet as a JSON object\n    :return: the author's username of the original tweet\n    \"\"\"\n    if hasattr(tweet, \"quoted_status\"):\n        quote_tweet = tweet.quoted_status\n        if hasattr(quote_tweet, 'user') and quote_tweet.user is not None:\n            if hasattr(quote_tweet.user, \"screen_name\") and quote_tweet.user.screen_name is not None:\n                return quote_tweet, quote_tweet.user.screen_name\n    elif hasattr(tweet, \"retweeted_status\"):\n        retweet = tweet.retweeted_status\n        if hasattr(retweet, 'user') and hasattr(retweet.user, \"screen_name\") and retweet.user is not None:\n            if retweet.user.screen_name is not None:\n                return retweet, retweet.user.screen_name\n    else:\n        return tweet, tweet.user.screen_name",
        "sha1": "fa2224d1b484d7721800df927808702daaf780e5",
        "id": 70620
    },
    {
        "content": "def identity(*args):\n    \"\"\"\n    Identity function\n\n    :param args: Any values\n    :type args: ```Tuple[Any]```\n\n    :returns: the input value\n    :rtype: ```Any```\n    \"\"\"\n    return args[0] if len(args) == 1 else args",
        "sha1": "e9c61b4d8bc2825cc25f0a24bfdce8dd305cd96c",
        "id": 487941
    },
    {
        "content": "def property_type(value):\n    \"\"\"\n    Return the type for given value in Neo4j land.\n\n    \"\"\"\n    if isinstance(value, str):\n        return \"string\"\n    elif isinstance(value, int):\n        return \"int\"\n    elif isinstance(value, float):\n        # Nb. Python floats are double precision\n        return \"double\"\n    elif isinstance(value, bool):\n        return \"boolean\"\n\n    # Default fallback\n    return \"string\"",
        "sha1": "7c037c2ad310b8c0a30823b10eda10a5c3f5f37f",
        "id": 628388
    },
    {
        "content": "def reloc_exit_patch(patch_data, relocs, start_address):\n    \"\"\"Relocate a piece of 6502 code to a given starting address.\"\"\"\n    patch_data = bytearray(patch_data)\n    for i in relocs:\n        address = patch_data[i] + (patch_data[i + 1] << 8) + start_address\n        patch_data[i] = address & 0xFF\n        patch_data[i + 1] = address >> 8\n    return patch_data",
        "sha1": "24ef61eeb5db23c41e4cea99e47826462775551c",
        "id": 688713
    },
    {
        "content": "def punct_space(token):\n    \"\"\"Removes punctuation and whitespace.\"\"\"\n    return token.is_punct or token.is_space",
        "sha1": "67b3fa1bd28216ba7f64e220db899dcf04be310b",
        "id": 544419
    },
    {
        "content": "def delete_session_mock(mocker):\n    \"\"\"Mock for patching DELETE request\"\"\"\n    return mocker.patch(\"hydra_agent.agent.Session.delete\")",
        "sha1": "26c3073e2d51dec1f3dcd9bfee3db311fdcf6a66",
        "id": 76284
    },
    {
        "content": "def get_index(string, list):\n    \"\"\"\n    Return index of string in list.\n    \"\"\"\n    return [i for i, s in enumerate(list) if string in s][0]",
        "sha1": "e3e491e06eb66c3433a31d95cb6d851e8eb79dc6",
        "id": 417232
    },
    {
        "content": "def intervals2positions(intervals, strand, hstart, hstop):\n    \"\"\" Turns intervals to positions depending on strand\"\"\"\n    positions = []\n    if strand == \"+\":\n        for i in intervals:\n            positions.append((hstart + i[0],hstart + i[1]))\n    elif strand == \"-\":\n        for i in intervals:\n            positions.append((hstop - i[1], hstop - i[0]))\n    return(positions)",
        "sha1": "b20e02c6f13485d424d3b4c3d35dc1e5732c7986",
        "id": 308241
    },
    {
        "content": "def read_input(input_path: str) -> str:\n    \"\"\"take input file path and return a str with the file's content\"\"\"\n    with open(input_path, 'r') as input_file:\n        input_data = input_file.read()\n        return input_data",
        "sha1": "07a6dbd0083e22aabdac596cfa8e62edf0885260",
        "id": 594471
    },
    {
        "content": "def get_channel_names(stream):\n    \"\"\"\n        extract channel name from xdf stream\n    :param stream: dictionnary\n        xdf stream to parse\n    :return: list\n        list of channels names\n    \"\"\"\n\n    try:\n        return [\n            channel_info[\"label\"][0]\n            for channel_info in stream[\"info\"][\"desc\"][0][\"channels\"][0][\"channel\"]\n        ]\n    except TypeError:\n        print(\"Warning : Channel description is empty\")\n        return None\n    except IndexError:\n        print(\"Warning : No channels names found\")\n        return None",
        "sha1": "da8038cc6f676616518f64e83c4f8889473b046e",
        "id": 72546
    },
    {
        "content": "from typing import Dict\nfrom typing import List\nimport collections\n\n\ndef load_run(path: str) -> Dict[str, List[str]]:\n    \"\"\"Loads run into a dict of key: query_id, value: list of candidate doc ids.\"\"\"\n    run = collections.OrderedDict()\n    with open(path) as f:\n        for line in f:\n            query_id, _, doc_title, rank, _, _ = line.split()\n            if query_id not in run:\n                run[query_id] = []\n            run[query_id].append((doc_title, int(rank)))\n\n    # Sort candidate docs by rank.\n    sorted_run = collections.OrderedDict()\n    for query_id, doc_titles_ranks in run.items():\n        doc_titles_ranks.sort(key=lambda x: x[1])\n        doc_titles = [doc_titles for doc_titles, _ in doc_titles_ranks]\n        sorted_run[query_id] = doc_titles\n\n    return sorted_run",
        "sha1": "c5ce6148398dca7aaeba93df797a6bd4d86cf631",
        "id": 422848
    },
    {
        "content": "def _fmt_rank(val):\n    \"\"\"Returns value (between 0 and 1) formatted as a percentage.\"\"\"\n    return '%.5f' % (100 * val)",
        "sha1": "cc9fe6ce15371fe0540112d7f24b82fdf8e29a2c",
        "id": 365980
    },
    {
        "content": "def sign(x):\n    \"\"\"\n    Sign function\n    \"\"\"\n    return 1 if x >= 0 else -1",
        "sha1": "20d85cf36d183c96e75fa3b795bf7f05f558e3b8",
        "id": 15844
    },
    {
        "content": "import re\n\n\ndef has_invalid_characters(string):\n    \"\"\"Check if the string has unpermitted characters: whitespace, digits, and hyphens.\n\n    :param string: the string to check for invalid characters in\n    \"\"\"\n    return bool(re.search(r\"[\\s\\d\\-\\']\", string))",
        "sha1": "8b445c802d85901d868dff9644c3a1a0ac9c527e",
        "id": 436981
    },
    {
        "content": "def lame_parameters(plane_strain: bool = False):\n    \"\"\"\n    Returns the Lame parameters for plane stress or plane strain.\n    Return type is lambda functions\n    \"\"\"\n    def mu(E, nu):\n        return E / (2 * (1 + nu))\n\n    if plane_strain:\n        def lmbda(E, nu):\n            return E * nu / ((1 + nu) * (1 - 2 * nu))\n        return mu, lmbda\n    else:\n        def lmbda(E, nu):\n            return E * nu / ((1 + nu) * (1 - nu))\n        return mu, lmbda",
        "sha1": "128ad6a26c1e4e5416e6f8d9075a6b5f4941f859",
        "id": 169707
    },
    {
        "content": "import torch\n\n\ndef rankdata_pt(b, tie_method='ordinal', dim=0):\n    \"\"\"\n    pytorch equivalent of scipy.stats.rankdata, GPU compatible.\n\n    :param b: torch.Tensor\n            The 1-D or 2-D tensor of values to be ranked. The tensor is first flattened\n            if tie_method is not 'ordinal'.\n    :param tie_method: str, optional\n            The method used to assign ranks to tied elements.\n                The options are 'average', 'min', 'max', 'dense' and 'ordinal'.\n                'average':\n                    The average of the ranks that would have been assigned to\n                    all the tied values is assigned to each value.\n                    Supports 1-D tensors only.\n                'min':\n                    The minimum of the ranks that would have been assigned to all\n                    the tied values is assigned to each value.  (This is also\n                    referred to as \"competition\" ranking.)\n                    Supports 1-D tensors only.\n                'max':\n                    The maximum of the ranks that would have been assigned to all\n                    the tied values is assigned to each value.\n                    Supports 1-D tensors only.\n                'dense':\n                    Like 'min', but the rank of the next highest element is assigned\n                    the rank immediately after those assigned to the tied elements.\n                    Supports 1-D tensors only.\n                'ordinal':\n                    All values are given a distinct rank, corresponding to the order\n                    that the values occur in `a`.\n                The default is 'ordinal' to match argsort.\n    :param dim: int, optional\n            The axis of the observation in the data if the input is 2-D.\n            The default is 0.\n    :return: torch.Tensor\n            An array of length equal to the size of `b`, containing rank scores.\n    \"\"\"\n    # b = torch.flatten(b)\n\n    if b.dim() > 2:\n        raise ValueError('input has more than 2 dimensions')\n    if b.dim() < 1:\n        raise ValueError('input has less than 1 dimension')\n\n    order = torch.argsort(b, dim=dim)\n\n    if tie_method == 'ordinal':\n        ranks = order + 1\n    else:\n        if b.dim() != 1:\n            raise NotImplementedError('tie_method {} not supported for 2-D tensors'.format(tie_method))\n        else:\n            n = b.size(0)\n            ranks = torch.empty(n).to(b.device)\n\n            dupcount = 0\n            total_tie_count = 0\n            for i in range(n):\n                inext = i + 1\n                if i == n - 1 or b[order[i]] != b[order[inext]]:\n                    if tie_method == 'average':\n                        tie_rank = inext - 0.5 * dupcount\n                    elif tie_method == 'min':\n                        tie_rank = inext - dupcount\n                    elif tie_method == 'max':\n                        tie_rank = inext\n                    elif tie_method == 'dense':\n                        tie_rank = inext - dupcount - total_tie_count\n                        total_tie_count += dupcount\n                    else:\n                        raise ValueError('not a valid tie_method: {}'.format(tie_method))\n                    for j in range(i - dupcount, inext):\n                        ranks[order[j]] = tie_rank\n                    dupcount = 0\n                else:\n                    dupcount += 1\n    return ranks",
        "sha1": "88bd901c945d6ca9b3d2e785c6b0ea627116a03f",
        "id": 689557
    },
    {
        "content": "from typing import Dict\n\n\ndef _filter_out_none_values_recursively(dictionary: Dict) -> Dict:\n    \"\"\"Return copy of the dictionary, recursively omitting all keys for which values are None.\n\n    >>> _filter_out_none_values_recursively({\"k1\": \"v1\", \"k2\": None, \"k3\": {\"k4\": \"v4\", \"k5\": None}})\n    {'k1': 'v1', 'k3': {'k4': 'v4'}}\n    \"\"\"\n    return {\n        k: v if not isinstance(v, Dict) else _filter_out_none_values_recursively(v)\n        for k, v in dictionary.items()\n        if v is not None\n    }",
        "sha1": "0a5f9670d8b6971d4f9076abf4443bbb0fe79257",
        "id": 82738
    },
    {
        "content": "import re\n\n\ndef meta_graph_filename(checkpoint_filename, meta_graph_suffix=\"meta\"):\n  \"\"\"Returns the meta graph filename.\n\n  Args:\n    checkpoint_filename: Name of the checkpoint file.\n    meta_graph_suffix: Suffix for `MetaGraphDef` file. Defaults to 'meta'.\n\n  Returns:\n    MetaGraph file name.\n  \"\"\"\n  # If the checkpoint_filename is sharded, the checkpoint_filename could\n  # be of format model.ckpt-step#-?????-of-shard#. For example,\n  # model.ckpt-123456-?????-of-00005, or model.ckpt-123456-00001-of-00002.\n  basename = re.sub(r\"-[\\d\\?]+-of-\\d+$\", \"\", checkpoint_filename)\n  suffixed_filename = \".\".join([basename, meta_graph_suffix])\n  return suffixed_filename",
        "sha1": "eb07edadb778526625d679b507bac82eb6ba39e8",
        "id": 97589
    },
    {
        "content": "def avarage(num1, num2):\n    \"\"\"  (number, number) -> number\n    Return the avarage of num1 and num2.\u200b\n    >>> avarage(10,20)\n    15.0\n    >>> avarage(2.5, 3.0)\n    2.75\n    \"\"\"\n    return (num1 + num2) / 2",
        "sha1": "275f7808a650f2c139a0f121d23e8044c59cf69b",
        "id": 32865
    },
    {
        "content": "from typing import List\nfrom typing import Optional\n\n\ndef remove_duplicate_flags(\n    flags: List[str], to_ignore: Optional[List[str]] = None\n) -> List[str]:\n    \"\"\"Remove any duplicate flag items even if the values differ.\n\n    >>> remove_duplicate_flags([\"foo\", \"--bar=123\", \"--baz=456\", \"--bar=789\"])\n    ['foo', '--bar=123', '--baz=456']\n    >>> remove_duplicate_flags([\"foo\", \"--bar=123\", \"--baz=456\", \"--bar=789\"], to_ignore=[\"--bar\"])\n    ['foo', '--bar=123', '--baz=456', '--bar=789']\n\n    :param flags: A list of flags to check.\n    :param to_ignore: An optional list of flag names to ignore duplicate of.\n    :return: The filtered list of flags.\n\n    \"\"\"\n    if to_ignore is None:\n        to_ignore = []\n\n    new_flags = []\n\n    seen = []\n\n    for flag in flags:\n        if \"=\" in flag:\n            name = flag.split(\"=\")[0]\n\n            # If we've seen this flag already, and we're not ignoring\n            # duplicates of it specifically then skip it.\n            if name in seen and name not in to_ignore:\n                continue\n\n            seen.append(name)\n\n        new_flags.append(flag)\n\n    return new_flags",
        "sha1": "65fae68db350b9b9d59f95d0a576c8a482e529cc",
        "id": 347168
    },
    {
        "content": "def get_ground_truth(reader_study, display_set, question):\n    \"\"\"Get the ground truth value for the display_set/question combination in reader_study.\"\"\"\n    ground_truths = reader_study.statistics[\"ground_truths\"]\n    return ground_truths[display_set][question]",
        "sha1": "e541eedfab9c46d325ffb328c573961053ef15c5",
        "id": 652098
    },
    {
        "content": "def is_observed_custom_module(module):\n    \"\"\" Check if a module is marked as observed custom module\n    or not\n    \"\"\"\n    return hasattr(module, '_is_observed_custom_module') and \\\n        module._is_observed_custom_module",
        "sha1": "b5455ba014d397849bbae637e9367dda1d53c94a",
        "id": 43555
    },
    {
        "content": "def fix_segment_table(segment_table):\n    \"\"\"Given a list of dictionaries in the form [{\"start\":start_frequency,\n    \"stop\":stop_frequency,\"number_points\":number_points,\"step\":frequency_step}...] returns a table that is ordered by start\n    frequency and has no overlapping points\"\"\"\n    segment_table = sorted(segment_table, key=lambda x: x[\"start\"])\n    i = 0\n    while (i + 1 < len(segment_table)):\n        if segment_table[i][\"stop\"] == segment_table[i + 1][\"start\"]:\n            segment_table[i + 1][\"start\"] = segment_table[i + 1][\"start\"] + segment_table[i + 1][\"step\"]\n            segment_table[i + 1][\"number_points\"] -= 1\n        i += 1\n    return segment_table",
        "sha1": "b057adde7450f24d082d1ce3cdca214b3088ca9d",
        "id": 413694
    },
    {
        "content": "import re\n\n\ndef get_word_count(text: str):\n    \"\"\"\n    Get the word count of the parsed string.\n    :return: Rough indication of the number of words in the string provided for analysis.\n    \"\"\"\n    return len(re.findall(r'\\S+', text))",
        "sha1": "e2692b08f0e1cdc77b6b79792d2b026a707ce033",
        "id": 421304
    },
    {
        "content": "def str_to_float(source: str) -> float:\n    \"\"\"Converts a str to a float.\"\"\"\n    return float(source)",
        "sha1": "05081e2155ffe33e03f60e0fb945cd110c1fc4ab",
        "id": 648741
    },
    {
        "content": "def get_parameter(dbutils, parameter_name: str, default_value='') -> str:\n    \"\"\"Creates a text widget and gets parameter value. If ran from ADF, the value is taken from there.\"\"\"\n    dbutils.widgets.text(parameter_name, default_value)\n    return dbutils.widgets.get(parameter_name)",
        "sha1": "cf8359e6acea68ea26e24cc656847e5560019bd1",
        "id": 707039
    },
    {
        "content": "def any_key_from_list_in_dict(test_list, test_dict):\n\t\"\"\"\n\tTakes a list and a dictionary and checks if any of the keys from the list are\n\tpresent in the dict. Raises a KeyError with the key if found, returns False\n\totherwise.\n\t\"\"\"\n\tfor key in test_list:\n\t\tif key in test_dict:\n\t\t\traise KeyError(key)\n\treturn False",
        "sha1": "4c9f9a52e4dc14e2bc77ad8354e93ca514a02d96",
        "id": 156404
    },
    {
        "content": "import networkx\n\n\ndef find_outdag(IGraph):\n    \"\"\"\n    Finds the maximal directed acyclic subgraph that is closed under the successors operation.\n    Essentially, these components are the \"output cascades\" which can be exploited by various algorithms, e.g.\n    the computation of basins of attraction.\n\n    **arguments**:\n        * *IGraph*: interaction graph\n\n    **returns**:\n        * *Names* (list): the outdag\n\n    **example**::\n\n        >>> find_outdag(igraph)\n        ['v7', 'v8', 'v9']\n    \"\"\"\n\n    graph = IGraph.copy()\n\n    sccs = networkx.strongly_connected_components(graph)\n    sccs = [list(x) for x in sccs]\n    candidates = [scc[0] for scc in sccs if len(scc)==1]\n    candidates = [x for x in candidates if not graph.has_edge(x,x)]\n    sccs = [scc for scc in sccs if len(scc)>1 or graph.has_edge(scc[0],scc[0])]\n\n    graph.add_node(\"!\")\n    for scc in sccs:\n        graph.add_edge(scc[0],\"!\")\n\n    outdags = [x for x in candidates if not networkx.has_path(graph,x,\"!\")]\n\n    return outdags",
        "sha1": "b9538104e26543683bb52bec09af9ca41d76e5df",
        "id": 430664
    },
    {
        "content": "def contains(value, arg):\n    \"\"\"\n    Test whether a value contains any of a given set of strings.\n    `arg` should be a comma-separated list of strings.\n    \"\"\"\n    return any(s in value for s in arg.split(\",\"))",
        "sha1": "727e1bd8ce69303091c150d42c165ea69f59e297",
        "id": 473521
    },
    {
        "content": "def readable_timedelta_reworked(days):\n    \"\"\" Display days in a human readable way\n\n    reworked function by Pierre Bouillon\n\n    Arguments:\n        - days : (int) amount of day to translate\n\n    Returns:\n        - translation : (str)  human readable string with date\n    \"\"\"\n    translation = '{} week(s) and {} day(s)'\n    return translation.format(days // 7, days % 7)",
        "sha1": "51d570dc48c2352b655ab9bb5a2c2906ae476366",
        "id": 421361
    },
    {
        "content": "import itertools\n\n\ndef nwise(iter, n):\n    \"\"\"\n    Like pairwise, except returns n-tuples of adjacent items.\n    s -> (s0,s1,...,sn), (s1,s2,...,s(n+1)), ...\n    \"\"\"\n    iterset = [iter]\n    while len(iterset) < n:\n        iterset[-1:] = itertools.tee(iterset[-1])\n        next(iterset[-1], None)\n    return zip(*iterset)",
        "sha1": "30d781c3c07e68de5949d977f1e967b46096ed74",
        "id": 327453
    },
    {
        "content": "def process_funql(funql):\n  \"\"\"Remove quotes and unnecessary spaces.\"\"\"\n  funql = funql.replace(\"'\", \"\")\n  funql = funql.replace(\",  \", \",\")\n  funql = funql.replace(\", \", \",\")\n  funql = funql.replace(\" ,\", \",\")\n  return funql",
        "sha1": "eaaeceda9c7f7963457752eea4f20542fb64e470",
        "id": 421703
    },
    {
        "content": "def convert_to_value(item, target_values, value, matching=True):\n    \"\"\"Convert target strings to NaN.\n    \n    Converts target strings listed in target_values to value so they can be\n    processed within a DataFrame, such as for removing specific rows. If\n    matching=False, strings that do not those listed in values are converted to\n    value. Note that it cannot take None as a value in values.\n    \n    Args:\n        item (str): String to be checked. item is converted to a string if\n        necessary.\n        target_values (list): Values to be checked. If found, value is returned\n        (if matching=True) or item is returned. If matching=False, this is\n        reversed.\n        value (str): Value that is to be returned if taget_values check is\n        positive (respective to matching)\n        matching (bool): If True, matching values are returned as value and \n        non-matching are returned as passed (item). If False, non-matching\n        values are returned as NaN and matching values are returned as passed\n        (item).\n        \n    Returns:\n        item or value (str): Depending on status of matching and if found or\n        not.\n    \"\"\"\n    # Convert to value if item is found in taget_values\n    if matching:\n        if str(item) in map(str, target_values):\n            return value\n        else:\n            return item\n        # Convert item to value if not found in values\n    else:\n        if str(item) in map(str, target_values):\n            return item\n        else:\n            return value",
        "sha1": "4dde98eacc2fb5824d4858749141551b0e0d4108",
        "id": 143643
    },
    {
        "content": "def number_to_cells(num, num_cells):\n    \"\"\"Convert an integer into 32-bit cells\"\"\"\n    cells = []\n    for i in range(num_cells):\n        cells.insert(0, (0xFFFFFFFF & (num >> (32 * i))))\n    return \" \".join([\"0x%x\" % x for x in cells])",
        "sha1": "2ef24e053feb6578d08c2c048ed7d3babb6b8928",
        "id": 140589
    },
    {
        "content": "def make_samplers(distances, sampler_factory):\n    \"\"\"Construct samplers for each informative subset of the summary statistic.\n\n    Parameters\n    ----------\n    distances : dict\n      A dictionary with discrepancy nodes corresponding to each subset of the summary statistic.\n    sampler_factory\n      A function which takes a discrepancy node as an argument\n      and returns an ELFI ABC sampler (e.g. elfi.Rejection).\n\n    Returns\n    -------\n    samplers : dict\n      A mapping from the marginals of the parameter to the corresponding sampler.\n    \"\"\"\n    return {k: sampler_factory(dist) for (k, dist) in distances.items()}",
        "sha1": "1e3ce6763e591b8342ed81b427b6b60524402feb",
        "id": 159102
    },
    {
        "content": "def _est_regularized_naive(mod, pnum, partitions, fit_kwds=None):\n    \"\"\"estimates the regularized fitted parameters.\n\n    Parameters\n    ----------\n    mod : statsmodels model class instance\n        The model for the current partition.\n    pnum : scalar\n        Index of current partition\n    partitions : scalar\n        Total number of partitions\n    fit_kwds : dict-like or None\n        Keyword arguments to be given to fit_regularized\n\n    Returns\n    -------\n    An array of the parameters for the regularized fit\n    \"\"\"\n\n    if fit_kwds is None:\n        raise ValueError(\"_est_regularized_naive currently \" +\n                         \"requires that fit_kwds not be None.\")\n\n    return mod.fit_regularized(**fit_kwds).params",
        "sha1": "7729ec7ad07fc5e88456882b37982ef5398d6e5b",
        "id": 540753
    },
    {
        "content": "def remove_hyphens(isbn):\n    \"\"\"\n    Remove hyphens from the given string.\n    \"\"\"\n    result = \"\"\n    for letter in isbn:\n        if letter == \"-\":\n            continue\n        else:\n            result += letter\n\n    return result",
        "sha1": "108664d94a8c2b6385036e11ff13d70fc97d570b",
        "id": 437293
    },
    {
        "content": "def shell_sort(li):\n    \"\"\" [list of int] => [list of int]\n    Shell sort: arranges the list of elements so that,\n    starting anywhere, considering every hth element\n    gives a sorted list. Such a list is said to be h-sorted.\n    \n    Beginning with large values of h, this rearrangement allows\n    elements to move long distances in the original list,\n    reducing large amounts of disorder quickly, and leaving\n    less work for smaller h-sort steps to do.\n\n    Determining which values of h we should use is a continuing problem\n    in computer science. I'll be using the simple sequence of powers of two,\n    which seem to work best for me.\n\n    See https://en.wikipedia.org/wiki/Shellsort for more information.\n    \"\"\"\n    \n    # calculating values of h (gaps), 1 is always the last gap (obviously)\n    gaps = []\n    k = 1\n    while 2 ** k < len(li):\n        gaps.append(2 ** k)\n        k += 1\n    \n    # sorting by gaps, starting with the largest (last) gap\n    for gap in reversed(gaps):\n        \n        # iterate through unsorted values as li[0:gap-1] is considered sorted\n        # by virtue of being the only value in our sorted list\n        for i in range(gap, len(li)):\n            # select value to be inserted\n            value = li[i]\n            \n            # new counter variable that can be changed, so i\n            # variable stays unchanged\n            j = i\n\n            # check value against the element h spaces before\n            # if the element h spaces before is larger, we need to swap\n            while j >= gap and li[j - gap] >= value:\n                li[j] = li[j - gap]\n                # go back gap spaces to check value against element h\n                # space before, in case another swap is needed\n                j -= gap\n\n            # replace the original value\n            li[j] = value\n    \n    return li",
        "sha1": "30d9b30fa924ae40322f085236c364a4c60eae76",
        "id": 128828
    },
    {
        "content": "def get_firing_time(pwm_period=0.0015, sample_size=7):\n    \"\"\" Returns laser firing time. \"\"\"\n    return sample_size * pwm_period",
        "sha1": "5055acda872918843ab8e19f56e3b80a5ed926de",
        "id": 479860
    },
    {
        "content": "def is_tail(span):\n    \"\"\"Returns if the sentence is in the last paragraph.\"\"\"\n    mpar = max(span.doc._.paragraphs)\n    if span._.paragraph == mpar:\n        return 1\n    else:\n        return 0",
        "sha1": "b90cdd6f9765bfa3158279261128925482c228ce",
        "id": 149503
    },
    {
        "content": "def pu_score(y_true, y_pred):\n   \"\"\"\n   Take truth vs predicted labels and calculate the pu-score, similar to f1-score.\n\n   The formula for the PU score is::\n\n       pu_score = recall ^ 2 / p(y_pred == 1)\n\n   Assumption, label -1 == unlabeled, 0 == negative, 1 == positive\n   \"\"\"\n\n   tp = sum([t == 1 and p == 1 for t, p in zip(y_true, y_pred)])\n   n_pos = (y_true == 1).sum() if tp > 0 else 1\n   recall = tp / n_pos\n\n   if recall == 0.0:\n       return 0.0\n\n   pr_true = (y_pred == 1).sum() / len(y_pred)\n\n   return recall * recall / pr_true",
        "sha1": "6954f0d3e6c8233f2daae4d58f8e9c77be721609",
        "id": 331322
    },
    {
        "content": "def read_kwargs_from_dict(input_dict, type_dict):\n    \"\"\" Reads, from an input dictionary, a set of keyword arguments.\n    This function is useful to gather optional arguments for a function call\n    from a greater, more complete, input dictionary. For this purpose,\n    keys that are not found at input_dict are simply ignored.\n\n    The desired keywords mus be the keys of type_dict, whose values\n    must be the type cast callable. None can be passed as a type cast\n    for no conversion.\n\n    Parameters\n    ------\n    input_dict : dict\n        Dictionary with the input keywords. Can contain more than the\n        necessary keywords.\n    type_dict : dict\n        Dictionary with the desired names as keys and their type casts\n        as values. Type can be None for no conversion.\n    \"\"\"\n    kwargs = dict()\n\n    for key, cast in type_dict.items():\n        try:\n            if cast is None:\n                kwargs[key] = input_dict[key]\n            else:\n                kwargs[key] = cast(input_dict[key])\n        except KeyError:\n            pass\n\n    return kwargs",
        "sha1": "5456c31d3359fd4fad2f8979917fc5f67e3c374b",
        "id": 542623
    },
    {
        "content": "from typing import List\n\n\ndef unpack_params(*args) -> List[tuple]:\n    \"\"\"\n    Returns a list containing tuples build from the arguments. Arguments that are lists are \"unpacked\" by combining the other elements\n    of the tuples with all elements in this list. Useful for constructing datasets for test parametrization.\n\n    Examples:\n\n    ```\n    # No unpacking because none of the arguments is a list:\n    unpack_params(1, 2, 3) == [\n        (1, 2, 3)\n    ]\n\n    # Unpacking a single list of values\n    unpack_params(1, 2, ['a', 'b', 'c']) == [\n        (1, 2, 'a'),\n        (1, 2, 'b'),\n        (1, 2, 'c'),\n    ]\n\n    # Unpacking multiple lists\n    unpack_params('foo', [1, 2, 3], ['a', 'b']) == [\n        ('foo', 1, 'a'),\n        ('foo', 1, 'b'),\n        ('foo', 2, 'a'),\n        ('foo', 2, 'b'),\n        ('foo', 3, 'a'),\n        ('foo', 3, 'b'),\n    ]\n\n    # Unpacking a list of tuples\n    unpack_params('foo', [(1, 'a'), (2, 'b'), (3, 'c')]) == [\n        ('foo', 1, 'a'),\n        ('foo', 2, 'b'),\n        ('foo', 3, 'c'),\n    ]\n\n    # Combine multiple unpacked lists, e.g. to use in pytest parametrization\n    [\n        *unpack_params('some_parameter', [('some', 1), ('test', 2), ('data', 3)]),\n        *unpack_params('other_parameter', [('other', 42), ('values', 1337)]),\n    ] == [\n        ('some_parameter', 'some', 1),\n        ('some_parameter', 'test', 2),\n        ('some_parameter', 'data', 3),\n        ('other_parameter', 'other', 42),\n        ('other_parameter', 'values', 1337)\n    ]\n    ```\n    \"\"\"\n    unpacked = [tuple()]\n    for arg in args:\n        if type(arg) is list:\n            arg_tuples = [item if type(item) is tuple else (item,) for item in arg]\n            unpacked = [(*current_params, *next_param) for current_params in unpacked for next_param in arg_tuples]\n        else:\n            unpacked = [(*current_params, arg) for current_params in unpacked]\n    return unpacked",
        "sha1": "a996a943b751238cf41dc7e2db02b2ba1082388f",
        "id": 279871
    },
    {
        "content": "def find_target_system(data, target_system):\n    \"\"\"\n    Finds target system ``aeroelastic``, ``aerodynamic`` or ``structural``.\n\n    Args:\n        data (sharpy.PreSharpy): Object containing problem data\n        target_system (str): Desired target system.\n\n    Returns:\n        sharpy.linear.src.libss.ss: State-space object of target system\n    \"\"\"\n\n    if target_system == 'aeroelastic':\n        ss = data.linear.ss\n\n    elif target_system == 'structural':\n        ss = data.linear.linear_system.beam.ss\n\n    elif target_system == 'aerodynamic':\n        ss = data.linear.linear_system.uvlm.ss  # this could be a ROM\n\n    else:\n        raise NameError('Unrecognised system')\n\n    return ss",
        "sha1": "5ac360acf1135eaf48e901137add6ee1ff83284c",
        "id": 600956
    },
    {
        "content": "def getenv(envarray, key, keyname=\"name\", valname=\"value\"):\n    \"\"\"get a value from a k8s \"env\" object (array of {\"name\":x,\"value\":y}); return None if not found\"\"\"\n    for e in envarray:\n        if e[keyname] == key:\n            return e[valname]\n    return None",
        "sha1": "92ab0696ee8cc4843c0861c5c74a62b790ec1eb8",
        "id": 417910
    },
    {
        "content": "def normalization(data, dmin=0, dmax=1, save_centering=False):\n\t\"\"\"\n\tNormalization in [a, b] interval or with saving centering\n\tx` = (b - a) * (xi - min(x)) / (max(x) - min(x)) + a\n\tArgs:\n\t\tdata (np.ndarray): data for normalization\n\t\tdmin (float): left interval\n\t\tdmax (float): right interval\n\t\tsave_centering (bool): if True -- will save data centering and just normalize by lowest data\n\tReturns:\n\t\tnp.ndarray: normalized data\n\t\"\"\"\n\t# checking on errors\n\tif dmin >= dmax:\n\t\traise Exception(\"Left interval 'dmin' must be fewer than right interval 'dmax'\")\n\tif save_centering:\n\t\treturn data / abs(min(data))\n\telse:\n\t\tmin_x = min(data)\n\t\tmax_x = max(data)\n\t\treturn (data - min_x) * (dmax - dmin) / (max_x - min_x) + dmin",
        "sha1": "acfa7aaae1bb7eb5752751f5c929ddb7868ccf49",
        "id": 703816
    },
    {
        "content": "def add_modal(form, error, name, url):\n    \"\"\"Render the given template to provide a modal dialog\n    that provides a popup form.\n\n    Args:\n        form (Forms) : Django form to render\n        error (String) : String to add to the form's class attribute\n        name (String) : Name of the buttons / modal\n        url (String) : URL to post the form to\n    \"\"\"\n    return {\n        'form': form,\n        'error': error,\n        'name': name,\n        'id': name.replace(' ', ''),\n        'url': url\n    }",
        "sha1": "f8000e6cb99637ee2d32caf8294a9fcd40e0edfa",
        "id": 337730
    },
    {
        "content": "def chop(n, xs):\n    \"\"\"\n    Create a list of lists sized with n from list elements in xs.\n    \"\"\"\n    if len(xs) == 0:\n        return []\n    return [xs[:n]] + chop(n, xs[n:])",
        "sha1": "57b2a79aef38d1cf1bed3c7f2aac3d4eedf1d0ea",
        "id": 540134
    },
    {
        "content": "def int_to_uint32(value_in):\n    \"\"\"\n    Convert integer to unsigned 32-bit (little endian)\n\n    :param value_in:\n    :return:\n    \"\"\"\n    return list(value_in.to_bytes(4, byteorder='little', signed=False))",
        "sha1": "906998350a19e1bf756ad8e87c35f4f3b5b2933c",
        "id": 666689
    },
    {
        "content": "def cut_dataframe(df_entire, start_date, end_date, date_column=\"date\"):\n    \"\"\"Cuts a dataframe to a desired date range\n\n    :param df_entire: The dataframe to cut\n    :type df_entire: pandas.core.frame.DataFrame\n    :param start_date: Start date of the dataframe entries\n    :type start_date: str\n    :param end_date: End date of the dataframe entries\n    :type end_date: str\n    :param date_column: Column name of the timestamp/date column, defaults to \"date\"\n    :type date_column: str, optional\n    :return: Returns the dataframe with entries only between inclusive boundaries of the mentioned dates\n    :rtype: pandas.core.frame.DataFrame\n    \"\"\"\n    mask = (df_entire[date_column] > start_date) & (df_entire[date_column] <= end_date)\n    return df_entire.loc[mask]",
        "sha1": "677c6a2a50e7fc268782ce7eff2437b5b7a069b6",
        "id": 173809
    },
    {
        "content": "import re\n\n\ndef extract_variables(sFormula):\n    \"\"\" Extract variables in expression, e.g.  {a}*x + {b} -> ['a','b']\n    The variables are replaced with p[0],..,p[n] in order of appearance\n    \"\"\"\n    regex = r\"\\{(.*?)\\}\"\n    matches = re.finditer(regex, sFormula, re.DOTALL)\n    formula_eval=sFormula\n    variables=[]\n    ivar=0\n    for i, match in enumerate(matches):\n        for groupNum in range(0, len(match.groups())):\n            var = match.group(1)\n            if var not in variables: \n                variables.append(var)\n                formula_eval = formula_eval.replace('{'+match.group(1)+'}','p[{:d}]'.format(ivar))\n                ivar+=1\n    return variables, formula_eval",
        "sha1": "7ae5b836504876c815b15c87bad774334fd4dd80",
        "id": 17493
    },
    {
        "content": "def _draw_outlines(ax, outlines):\n    \"\"\"Draw the outlines for a topomap.\"\"\"\n    outlines_ = {k: v for k, v in outlines.items()\n                 if k not in ['patch']}\n    for key, (x_coord, y_coord) in outlines_.items():\n        if 'mask' in key or key in ('clip_radius', 'clip_origin'):\n            continue\n        ax.plot(x_coord, y_coord, color='k', linewidth=1, clip_on=False)\n    return outlines_",
        "sha1": "0ded4e84408fb7987740601d73a33b5a33425c16",
        "id": 447546
    },
    {
        "content": "def update_portal_registration_config(\n    self,\n    host: str = \"\",\n    port: int = 0,\n    account_id: str = \"\",\n    account_name: str = \"\",\n    account_key: str = \"\",\n    old_account_key: str = \"\",\n    group: str = \"\",\n    site: str = \"\",\n) -> dict:\n    \"\"\"Update Cloud Portal registration information\n\n    .. list-table::\n        :header-rows: 1\n\n        * - Swagger Section\n          - Method\n          - Endpoint\n        * - spPortal\n          - POST\n          - /spPortal/config\n\n    :param host: Hostname for cloud portal,\n        e.g. ``portal.silverpeak.cloud``, defaults to \"\"\n    :type host: str, optional\n    :param port: Port to connect to cloud portal on, e.g. ``443``,\n        defaults to 0\n    :type port: int, optional\n    :param account_id: Account ID string,\n        e.g. ``5f347e4637f3de001a2a6e77``, defaults to \"\"\n    :type account_id: str, optional\n    :param account_name: Account name, defaults to \"\"\n    :type account_name: str, optional\n    :param account_key: Account key, defaults to \"\"\n    :type account_key: str, optional\n    :param old_account_key: Old account key, defaults to \"\"\n    :type old_account_key: str, optional\n    :param group: Group, can be ``unassigned``, defaults to \"\"\n    :type group: str, optional\n    :param site: Site, can be ``unassigned``, defaults to \"\"\n    :type site: str, optional\n    :return: Returns True/False based on successful call\n    :rtype: bool\n    \"\"\"\n    data = {\n        \"host\": host,\n        \"port\": port,\n        \"registration\": {\n            \"accountId\": account_id,\n            \"account\": account_name,\n            \"key\": account_key,\n            \"oldKey\": old_account_key,\n            \"group\": group,\n            \"site\": site,\n        },\n    }\n\n    return self._post(\n        \"/spPortal/config\",\n        data=data,\n        expected_status=[204],\n        return_type=\"bool\",\n    )",
        "sha1": "6271740d45949cc77a1bb1b90137132f57a9fee8",
        "id": 219959
    },
    {
        "content": "def stellar_radius(M, logg):\n    \"\"\"Calculate stellar radius given mass and logg\"\"\"\n    if not isinstance(M, (int, float)):\n        raise TypeError('Mass must be int or float. {} type given'.format(type(M)))\n    if not isinstance(logg, (int, float)):\n        raise TypeError('logg must be int or float. {} type given'.format(type(logg)))\n    if M < 0:\n        raise ValueError('Only positive stellar masses allowed.')\n\n    M = float(M)\n    return M/(10**(logg-4.44))",
        "sha1": "2afbd991c7461d7861370f18d90df840569da857",
        "id": 3166
    },
    {
        "content": "import torch\nfrom typing import Optional\n\n\ndef bernoulli(\n    datum: torch.Tensor,\n    time: Optional[int] = None,\n    dt: float = 1.0,\n    device=\"cpu\",\n    **kwargs\n) -> torch.Tensor:\n    # language=rst\n    \"\"\"\n    Generates Bernoulli-distributed spike trains based on input intensity. Inputs must\n    be non-negative. Spikes correspond to successful Bernoulli trials, with success\n    probability equal to (normalized in [0, 1]) input value.\n\n    :param datum: Tensor of shape ``[n_1, ..., n_k]``.\n    :param time: Length of Bernoulli spike train per input variable.\n    :param dt: Simulation time step.\n    :return: Tensor of shape ``[time, n_1, ..., n_k]`` of Bernoulli-distributed spikes.\n\n    Keyword arguments:\n\n    :param float max_prob: Maximum probability of spike per Bernoulli trial.\n    \"\"\"\n    # Setting kwargs.\n    max_prob = kwargs.get(\"max_prob\", 1.0)\n\n    assert 0 <= max_prob <= 1, \"Maximum firing probability must be in range [0, 1]\"\n    assert (datum >= 0).all(), \"Inputs must be non-negative\"\n\n    shape, size = datum.shape, datum.numel()\n    datum = datum.flatten()\n\n    if time is not None:\n        time = int(time / dt)\n\n    # Normalize inputs and rescale (spike probability proportional to input intensity).\n    if datum.max() > 1.0:\n        datum /= datum.max()\n\n    # Make spike data from Bernoulli sampling.\n    if time is None:\n        spikes = torch.bernoulli(max_prob * datum).to(device)\n        spikes = spikes.view(*shape)\n    else:\n        spikes = torch.bernoulli(max_prob * datum.repeat([time, 1]))\n        spikes = spikes.view(time, *shape)\n\n    return spikes.byte()",
        "sha1": "b50b15bf0a3ec1e0dea49d124c7dab8b234bf750",
        "id": 452133
    },
    {
        "content": "def http_basic_auth(auth_string):\n    \"\"\"\n    Convert a HTTP Basic Authentication string to a tuple.\n\n    :param auth_string: the basic auth string of the form ``login:passwd``.\n    :type auth_string: str, unicode\n    :returns: Basic auth as a tuple ``(login, passwd)``.\n    :rtype: tuple\n\n    **Example**::\n\n     >>> http_basic_auth(\"cheng:8jj_767hhgy\")\n     ('cheng', '8jj_767hhgy')\n    \"\"\"\n    return tuple(auth_string.split(\":\"))",
        "sha1": "7b5092d29b3e25d4635facf81f858034235e03bd",
        "id": 457932
    },
    {
        "content": "def remove_comment_lines(stream, comment_char=\"#\"):\n    \"\"\"Removes commented out lines from stream, e.g., those starting with '#'.\n\n    Parameters\n    ----------\n    stream : iterable of str\n        Lines from which comments should be excluded.\n    comment_char : str\n        Lines beginning with this character will be excluded.\n\n    Returns\n    -------\n    list of str\n        Lines that do not begin with comment_char.\n\n    \"\"\"\n    lines = list(stream)\n    lines = filter(lambda x: not x.startswith(comment_char), lines)\n    rules = \"\".join(lines)\n    return rules",
        "sha1": "24ab698e68a7a6cf0237189f195917c7c9ba629e",
        "id": 580844
    },
    {
        "content": "def generate_fibonacci_sequence(number_of_terms=2, first_term=0, second_term=1):\n    \"\"\"\n    Generates a Fibonacci sequence\n    :param int number_of_terms: the number of terms to be generated including the first and second\n    :param int first_term: first number in the sequence, must be >= 0\n    :param int second_term: second number in the sequence, must be >= first_term\n    :return [int]: Fibonacci sequence\n    \"\"\"\n\n    try:\n        if number_of_terms < 2:\n            raise ValueError(\"Number of terms must be >= 2\")\n        if first_term < 0:\n            raise ValueError(\"First term must be >= 0\")\n        if second_term < first_term:\n            raise ValueError(\"Second term must be >= first term\")\n\n        sequence = [first_term, second_term]\n        while len(sequence) != number_of_terms:\n            next_number = sequence[-1] + sequence[-2]\n            sequence.append(next_number)\n\n        return sequence\n    except TypeError:\n        raise TypeError(\"Input parameters must be positive integers\")",
        "sha1": "12f0b4571fbb3ea1d69ee7d97d11f8fac2192fb9",
        "id": 449284
    },
    {
        "content": "def convert(nested_dict):\n    \"\"\"\n    Convert nested dict with bytes to str.\n    This is needed because bytes are not JSON serializable.\n\n    :param nested_dict: nested dict with bytes\n    \"\"\"\n    if isinstance(nested_dict, dict):\n        return {convert(k): convert(v) for k, v in nested_dict.items()}\n    elif isinstance(nested_dict, list):\n        return [convert(v) for v in nested_dict]\n    elif isinstance(nested_dict, tuple):\n        return tuple(convert(v) for v in nested_dict)\n    elif (\n        isinstance(nested_dict, str)\n        or isinstance(nested_dict, int)\n        or isinstance(nested_dict, float)\n    ):\n        return nested_dict\n    else:\n        return str(nested_dict)",
        "sha1": "d267259c6113411d0db757d59248db18beb29e31",
        "id": 165349
    },
    {
        "content": "import marshal\nimport types\n\n\ndef reconstruct_function(func_bytecode, func_defaults):\n    \"\"\"reconstruct function from bytecode.\n\n    Args:\n        func_bytecode (str) : the byte code of a function\n        func_default  (dict): the default parameters of a function\n\n    Returns: Function\n    \"\"\"\n    func_code = marshal.loads(func_bytecode)\n    return types.FunctionType(func_code, globals(), func_code.co_name,\n                              func_defaults)",
        "sha1": "3572de781d2b87b605d18468a3e1cf67ee8f5267",
        "id": 331480
    },
    {
        "content": "def get_comfort_temp(condition, method, ambTemp=20, hdd=5467, cdd=850):\n    \"\"\" Get the mean and standard deviation of equivalent temperature of other heat gains for the given state\n    Args:\n    - condition: str, 'heating', 'cooling'\n    - method: str, the method to determine the comfort temperature, currently four approaches are available\n        -- 'Wang2020': Bayesian Inference from ASHRAE Database ,details refer to https://doi.org/10.1016/j.rser.2019.109593\n        -- 'ASHRAE PMV': PMV-PPD model of ASHRAE Standard 55\u20132017\n        -- 'ASHRAE adaptive': 90% acceptability based on Adapative Comfort Model of ASHRAE Standard 55\u20132017\n            Linearly depends on \n        -- 'ResStock': Regressed from Residential Energy Consumption Survey, Page 41 https://www.nrel.gov/docs/fy18osti/68670.pdf\n            Linearly depends on HDD18 and CDD18\n    - ambTemp: float, Prevailing Mean Outdoor Temperature, which is arithmetic average of the mean daily outdoor temperatures over \n        no fewer than 7 and no more than 30 sequential days prior to the day in question, required by the adaptive comfort model,\n        default 20 degC\n    - hdd: Heating Degree Day (base temperature of 18), required by the ResStock method, default 5467 (medium level of US cities)\n    - cdd: Cooling Degree Day (base temperature of 18), required by the ResStock method, default 850 (medium level of US cities)\n    Return:\n    - two tuples: (tsp_mean, tsp_sd), (trange_mean, trange_sd)\n        -- tsp: temperature set point\n        -- trange: acceptable comfort temperature range, temperature difference between the lower and upper bound\n    \"\"\"    \n    assert condition in ['heating', 'cooling'], 'Condition of {} is not supported'.format(condition)\n    assert method in ['Wang2020', 'ASHRAE PMV', 'ASHRAE adaptive', 'ResStock'], 'Method of {} is not supported'.format(method)\n\n    if method == 'Wang2020':\n        if condition == 'heating':\n            return (23.6, 1.4), (1.2, 0.1)\n        elif condition == 'cooling':\n            return (22.7, 1.1), (1.2, 0.1)\n    elif method == 'ASHRAE PMV':\n        if condition == 'heating':\n            return (22.3,   0), (3.7,   0)\n        elif condition == 'cooling':\n            return (25.4,   0), (2.9,   0)\n    elif method == 'ASHRAE adaptive':\n        heatSP = 0.325*ambTemp + 15.35\n        coolSP = 0.31*ambTemp + 20.2\n        return ((coolSP+heatSP)/2, 0), (coolSP-heatSP, 0)\n    elif method == 'ResStock':\n        heatSP = -0.0002*hdd + 20.97\n        coolSP = 0.0006*cdd + 22.065\n        return ((coolSP+heatSP)/2, 0), (coolSP-heatSP, 0)",
        "sha1": "194b965e7a70acc2e91c7cbba43975a49171bdc9",
        "id": 375817
    },
    {
        "content": "import math\n\n\ndef truncate_middle(content, max_length, middle=\"...\"):\n    \"\"\"\n    Truncates the middle part of the string if the total length if too long.\n\n    For example:\n    truncate_middle('testabcdecho', 8) == 'tes...ho'\n\n    :param content: The string that must be truncated.\n    :type: str\n    :param max_length: The maximum amount of characters the string can have.\n    :type max_length: int\n    :param middle: The part that must be added in the middle if the provided\n        content is too long.\n    :type middle str\n    :return: The truncated string.\n    :rtype: str\n    \"\"\"\n\n    if len(content) <= max_length:\n        return content\n\n    if max_length <= len(middle):\n        raise ValueError(\n            \"The max_length cannot be lower than the length if the \" \"middle string.\"\n        )\n\n    total = max_length - len(middle)\n    start = math.ceil(total / 2)\n    end = math.floor(total / 2)\n\n    left = content[:start]\n    right = content[-end:] if end else \"\"\n\n    return f\"{left}{middle}{right}\"",
        "sha1": "16f9ec8e6210ab2820a78328e2f21ec6d636b787",
        "id": 664347
    },
    {
        "content": "def strip_version(idstr):\n    \"\"\" identity function if arxiv id has no version, otherwise strips it. \"\"\"\n    parts = idstr.split('v')\n    return parts[0]",
        "sha1": "c3e583e00980e2002f2ee49b54e6648b016c5a10",
        "id": 513539
    },
    {
        "content": "from typing import Any\n\n\ndef str_capped(object: Any, max_len: int) -> str:\n    \"\"\"Return the string representation of `object` trimmed to `max_len`.\n\n    Trailing ellipsis is added to the returned string if the original had to be trimmed.\n    \"\"\"\n    s = str(object)\n    if len(s) <= max_len:\n        return s\n    return s[: max_len - 3] + \"...\" if max_len >= 3 else \"...\"",
        "sha1": "753ef207a56b8a72cf7e7bf983cd2996041a33f8",
        "id": 119820
    },
    {
        "content": "import re\n\n\ndef is_divider(item: str) -> bool:\n    \"\"\"Return true if the string contains a divider character.\"\"\"\n    return re.match(r'^(,|:)', item) is not None",
        "sha1": "3160a586b83dd3415026918936bee27ac0c7548f",
        "id": 42066
    },
    {
        "content": "def get_file_size(fp):\n    \"\"\"Get size of the file or length of a bytes object\"\"\"\n    fp.seek(0, 2)  # move the current position to the end of the file\n    size = fp.tell()\n    fp.seek(0, 0)  # move the current position to the beginning of the file\n    return size",
        "sha1": "98b3615244104db3fb0bf58ca586d88cfbf9e17d",
        "id": 328622
    },
    {
        "content": "def fit_params_txt(fit_params, bp_list, out_dir):\n    \"\"\"Generates a text file in the same folder as the detrending plots that lists applied linear fit equations\"\"\"\n\n    # Create .txt file and copy breakpoint list\n    text_dir = out_dir + '\\\\detrending_fit_eqs.txt'\n    text_file = open(text_dir, 'w+')\n    bps_form = [i for i in bp_list]\n\n    # Write to and save .txt file\n    for count, params in enumerate(fit_params):\n        if len(bp_list) != 0:\n            text_file.write('From %s to %s: %.4f * dist_downstream + %.4f\\n' % (bps_form[count], bps_form[count+1], params[0], params[1]))\n        else:\n            text_file.write('For full reach: %.4f * dist_downstream + %.4f\\n' % (params[0], params[1]))\n    text_file.close()\n\n    return text_dir",
        "sha1": "5b0075fd90f25d446bbb1060da23b62221be6260",
        "id": 562586
    },
    {
        "content": "def add_trip_duration_in_hours(df):\n    \"\"\"Adding new variable for trip duration in hours.\"\"\"\n\n    print(\"adding trip duration in hours\")\n    df['trip_duration_hours'] = df.trip_duration_minutes / 60\n    \n    return df",
        "sha1": "96777727edfbd4fae307c84d144436870d2b8e5b",
        "id": 426828
    },
    {
        "content": "def getWorst ( population, n ) :\n\t\"\"\"\n\tfitnesses\u306e\u5927\u304d\u3044\u65b9\u304b\u3089n\u500b\u306e\u500b\u4f53\u3092\u53d6\u5f97\n\t@param\tpopulation\tindividual\u306e\u30ea\u30b9\u30c8\n\t@param\tn\t\u53d6\u5f97\u3059\u308b\u500b\u4f53\u6570\n\t@return nth worst individuals\n\t\"\"\"\n\treturn sorted ( population, key=lambda x: x.fitness.values, reverse=True  )[ : n ]",
        "sha1": "b2f50790f1365469d17d2106c4fd591121c1f3ce",
        "id": 125359
    },
    {
        "content": "from pathlib import Path\n\n\ndef is_doorstop_item_file(file_name, *args):\n    \"\"\"\n    Returns whether the given file is most likely to\n    be a file that represents a doorstop item\n    \"\"\"\n    if not file_name:\n        return False\n    file_name = Path(file_name)\n    if file_name.suffix != \".yml\":\n        return False\n    if not (file_name.parent / \".doorstop.yml\").exists():\n        return False\n    if file_name.name.startswith(\".\"):\n        return False\n    if file_name.name == \".doorstop.yml\":\n        return False\n    return True",
        "sha1": "0df41af3186823ec2477f9b42c0eb41e3f3bc915",
        "id": 350789
    },
    {
        "content": "def assign_region_to_channels(channels, anat, parc_type='aparc', max_approx=3,\n                              exclude_regions=None):\n    \"\"\"Assign a brain region based on the channel location.\n\n    Parameters\n    ----------\n    channels : instance of wonambi.attr.chan.Channels\n        channels to assign regions to\n    anat : instance of wonambi.attr.anat.Freesurfer\n        anatomical information taken from freesurfer.\n    parc_type : str\n        'aparc', 'aparc.a2009s', 'BA', 'BA.thresh', or 'aparc.DKTatlas40'\n        'aparc.DKTatlas40' is only for recent freesurfer versions\n    max_approx : int, optional\n        approximation to define position of the electrode.\n    exclude_regions : list of str or empty list\n        do not report regions if they contain these substrings. None means\n        that it does not exclude any region. For example, to exclude white\n        matter regions and unknown regions you can use\n        exclude_regions=('White', 'WM', 'Unknown')\n\n    Returns\n    -------\n    instance of wonambi.attr.chan.Channels\n        same instance as before, now Chan have attr 'region'\n    \"\"\"\n    for one_chan in channels.chan:\n        one_region, approx = anat.find_brain_region(one_chan.xyz,\n                                                    parc_type,\n                                                    max_approx,\n                                                    exclude_regions)\n        one_chan.attr.update({'region': one_region, 'approx': approx})\n\n    return channels",
        "sha1": "45ba5558eba8a6f79b792296423bc49b5081a8e4",
        "id": 407019
    },
    {
        "content": "def generate_tap_stream_id(catalog_name, schema_name, table_name):\n    \"\"\"Generate tap stream id as appears in properties.json\"\"\"\n    return catalog_name + '-' + schema_name + '-' + table_name",
        "sha1": "65b2e024009372d327774f48470f092d31638e60",
        "id": 89795
    },
    {
        "content": "def is_ascii(string):\n    \"\"\"Indicates whether a string contains only ASCII characters.\n\n    :param string: The string to be evaluated.\n    :type string: str\n\n    :rtype: bool\n\n    .. note::\n        As of Python 3.7, strings provide the ``isascii()`` method. See the discussions at:\n        https://stackoverflow.com/q/196345/241720\n\n    \"\"\"\n    try:\n        string.encode('ascii')\n    except UnicodeEncodeError:\n        return False\n    else:\n        return True",
        "sha1": "66c4547d75490ff82419b36a4d75fcf4099eb944",
        "id": 497580
    },
    {
        "content": "def deep_del(data, fn):\n    \"\"\"Create dict copy with removed items.\n\n    Recursively remove items where fn(value) is True.\n\n    Returns:\n        dict: New dict with matching items removed.\n    \"\"\"\n    result = {}\n\n    for k, v in data.items():\n        if not fn(v):\n            if isinstance(v, dict):\n                result[k] = deep_del(v, fn)\n            else:\n                result[k] = v\n\n    return result",
        "sha1": "2912e8ea8e7c7c3a91b15a314b34daa6cf52a42a",
        "id": 160018
    },
    {
        "content": "import torch\n\n\ndef sm_sim(q, M):\n    \"\"\"\n    q : D\n    M : M x D\n    returns : M \n    Computes the dot product, followed by softmax between vector q, and all vectors in M.\n    I.e. the distribution we aim to model using Noise Contrastive Estimation or Negative Sampling.\n    \"\"\"\n    return torch.softmax(M @ q, 0)",
        "sha1": "b1506381892ef6f5ee58ea2a4f1249ede18a7e12",
        "id": 77731
    },
    {
        "content": "def prepare_actions(actions, enabled_analyzers):\n    \"\"\"\n    Set the analyzer type for each buildaction.\n    Multiple actions if multiple source analyzers are set.\n    \"\"\"\n    res = []\n\n    for ea in enabled_analyzers:\n        for action in actions:\n            res.append(action.with_attr('analyzer_type', ea))\n    return res",
        "sha1": "a8a8624c5921f9addaef9c9e532f333726e35618",
        "id": 30149
    },
    {
        "content": "def hex_to_string(data):\n    \"\"\"Convert an hex string to a string\"\"\"\n    return bytes.fromhex(data).decode('utf-8')",
        "sha1": "67643dd516098c69f4fafe7ea230df694f29c741",
        "id": 209211
    },
    {
        "content": "def convolution_math(in_features, filter_size, out_features):\n  \"\"\"\n  Convolution math: Implement how parameters scale as a function of feature maps\n  and filter size in convolution vs depthwise separable convolution.\n\n  Args:\n    in_features:  number of input features\n    filter_size:  size of the filter\n    out_features: number of output features\n  \"\"\"\n  # calculate the number of parameters for regular convolution\n  conv_parameters = in_features * filter_size * filter_size * out_features\n  # calculate the number of parameters for depthwise separable convolution\n  depthwise_conv_parameters = in_features * filter_size * filter_size + in_features * out_features\n\n  print('Depthwise separable: {} parameters'.format(depthwise_conv_parameters))\n  print('Regular convolution: {} parameters'.format(conv_parameters))\n\n  return None",
        "sha1": "0d3dfbaf203146604d4ed0a750e75751e54d1238",
        "id": 144903
    },
    {
        "content": "def get_key_by_value(dict_, value):\n    \"\"\"Return key by value.\"\"\"\n    for key, val in dict_.items():\n        if value == val:\n            return key\n    return None",
        "sha1": "5239268839759b567ba122ab5dacc5c7c9fdc7d0",
        "id": 578075
    },
    {
        "content": "def create_close_string(line: str) -> str:\n    \"\"\" Creates single .po file translation target sting. \"\"\"\n    return r\"msgstr \" + '\"' + line + '\"' + \"\\n\"",
        "sha1": "f8d64d9142f79b464fcea4b48e4fca76517f3d9f",
        "id": 354661
    },
    {
        "content": "def _unsur(s: str) -> str:\n    \"\"\"Merge surrogates.\"\"\"\n    return s.encode(\"utf-16\", \"surrogatepass\").decode(\"utf-16\", \"surrogatepass\")",
        "sha1": "d6bc230a77c735c9922ec66aa6a3537beea5b42f",
        "id": 701159
    },
    {
        "content": "def has_mapping(mapping, id_list, key):\n    \"\"\"\n    Check if a vrt id has a mapping\n\n    :param mapping: The mapping object, keyed by id\n    :param id_list: The vrt id, split into components, eg ['category', 'subcategory', 'variant']\n    :param key: The mapping key to look for, eg 'cvss_v3'\n    :return: True/False\n    \"\"\"\n    if key in mapping:\n        return True\n    elif 'children' in mapping:\n        return has_mapping(mapping['children'], id_list, key)\n    elif len(id_list) > 0 and id_list[0] in mapping:\n        return has_mapping(mapping[id_list[0]], id_list[1:], key)\n    else:\n        return False",
        "sha1": "0d0946c1936f0aadc2212b06e3858a9ecd18f168",
        "id": 143087
    },
    {
        "content": "import re\n\n\ndef get_numwords(text):\n    \"\"\"\n    Estimates the length of the plain text in number of words.\n    \"\"\"\n    text = re.split(\" \", text)\n    text = [word for word in text if word]\n    numwords = len(text)\n    return numwords",
        "sha1": "83db5a84d9af185f812ebf530fcc13899102dfb9",
        "id": 267170
    },
    {
        "content": "def all_subclasses(cls):\n    \"\"\"Get all subclasses for a class recursevely.\"\"\"\n    return cls.__subclasses__() + [subcls for s in cls.__subclasses__()\n                                   for subcls in all_subclasses(s)]",
        "sha1": "0ae3b2d5c5a5f36cfd974c98013518f54dc61b0b",
        "id": 263565
    },
    {
        "content": "def get_task_error_message(task_result):\n    \"\"\"\n    Parse error message from task result.\n    \"\"\"\n    try:\n        res = task_result['result']\n    except Exception:\n        res = task_result\n\n    for key in ('detail', 'message'):\n        try:\n            return res[key]\n        except Exception:\n            continue\n\n    return str(res)",
        "sha1": "e761e98c96446c15d8c2bf465f32c663febcfd8b",
        "id": 120741
    },
    {
        "content": "import struct\n\n\ndef _decode_byte(data):\n    \"\"\"Decodes one byte of network data into an unsigned char.\"\"\"\n    return struct.unpack('!B', data)[0]",
        "sha1": "55b03db13f8dbd2841cbd882641ed395581e6a03",
        "id": 464291
    },
    {
        "content": "import torch\nfrom typing import List\n\n\ndef masked_flip(padded_sequence: torch.Tensor, sequence_lengths: List[int]) -> torch.Tensor:\n    \"\"\"\n    Flips a padded tensor along the time dimension without affecting masked entries.\n\n    # Parameters\n\n    padded_sequence : `torch.Tensor`\n        The tensor to flip along the time dimension.\n        Assumed to be of dimensions (batch size, num timesteps, ...)\n    sequence_lengths : `torch.Tensor`\n        A list containing the lengths of each unpadded sequence in the batch.\n\n    # Returns\n\n    `torch.Tensor`\n        A `torch.Tensor` of the same shape as padded_sequence.\n    \"\"\"\n    assert padded_sequence.size(0) == len(\n        sequence_lengths\n    ), f\"sequence_lengths length ${len(sequence_lengths)} does not match batch size ${padded_sequence.size(0)}\"\n    num_timesteps = padded_sequence.size(1)\n    flipped_padded_sequence = torch.flip(padded_sequence, [1])\n    sequences = [\n        flipped_padded_sequence[i, num_timesteps - length :]\n        for i, length in enumerate(sequence_lengths)\n    ]\n    return torch.nn.utils.rnn.pad_sequence(sequences, batch_first=True)",
        "sha1": "01e5d06c1fc2be0d795ec4afd5c61984d16bc998",
        "id": 373912
    },
    {
        "content": "def get_qual_name(o, lower=False):\n    \"\"\"\n    Returns the qualified name of an object o.\n    \"\"\"\n    if not isinstance(o, type):\n        o = o.__class__\n    if lower:\n        return o.__qualname__.lower()\n    else:\n        return o.__qualname__",
        "sha1": "166672a8242a6d388d46e707155363d13f09b066",
        "id": 625228
    },
    {
        "content": "def cell_renderer(item):\n    \"\"\"Simple cell renderer\"\"\"\n    return \"%s item\" % item.title",
        "sha1": "0bc370d92c77f7d127e1052c732348558b8464ab",
        "id": 597801
    },
    {
        "content": "from typing import List\nimport glob\n\n\ndef get_html(module: str) -> List[str]:\n    \"\"\"Return all html files on the html/module folder\"\"\"\n    files = glob.glob(f\"./html/{module}/*.html\")\n    # remove ./html/module\n    return [f.replace(f\"./html/{module}/texthero.\", \"\") for f in files]",
        "sha1": "6cd95d0f0fea509e81902d0afd5e7f4414c19a1d",
        "id": 590423
    },
    {
        "content": "def boolean(string):\n    \"\"\"Recover the boolean value from a string and return it.\"\"\"\n    if string in [\"False\", \"false\", \"FALSE\"]:\n        return False\n    if string in [\"True\", \"true\", \"TRUE\"]:\n        return True\n    raise ValueError(\"Cannot be converted to a boolean type\")",
        "sha1": "3f006632609688a7e9f0d1294a04f2c05ed5dc43",
        "id": 564766
    },
    {
        "content": "def capitalize_names(ds):\n    \"\"\"\n    Capitalize the 'long_name' attribute for plotting purposes\n    \"\"\"\n    for var in ds.variables.values():\n        if \"long_name\" in var.attrs:\n            var.attrs[\"long_name\"] = var.attrs[\"long_name\"].capitalize()\n    return ds",
        "sha1": "7d511e5d29e95f2bf2af4b3357f1b296cb538dc6",
        "id": 302251
    },
    {
        "content": "def calculate_expected_wins(win_rate, num_games):\n    \"\"\"Calculate current expected wins\"\"\"\n\n    expected_wins = win_rate * float(num_games)\n    result = int(round(expected_wins, 0))\n\n    return result",
        "sha1": "5f979a861a0fae1e0fe654c9cc4c9ccf6583b35e",
        "id": 678484
    },
    {
        "content": "def tcp_opts_tuple_list_to_dict(opts_list: list) -> dict:\n    \"\"\"Convert tuple of TCP options to a dictionary\n    :param opts_list: list of TCP options tuple\n    :return: diction of TCP options\n    \"\"\"\n    opts = {}\n    if None in opts_list:\n        opts_list.remove(None)\n    for opt, value in opts_list:\n        # here TCP_OPT_NOP is saved only once, even though multiple TCP_OPT_NOP might be present\n        # since it doesn't affect our operations, or at least I couldn't think of one, so it's okay to overwrite it\n        opts[opt] = value\n    return opts",
        "sha1": "751689e93e66f54251944343f0cac626bf5103c2",
        "id": 642815
    },
    {
        "content": "def is_indented(text):\n    \"\"\" Simple check to see if a line is indented\n    For now, a line that starts with ANY whitespace is indented\n    \"\"\"\n\n    return bool(len(text) - len(text.lstrip()))",
        "sha1": "1bf7b02595518878f2db5fc72a6b6b44f5a1f5d7",
        "id": 441088
    },
    {
        "content": "def kind(n, ranks):\n    \"\"\"Return the card for which there is n-of-a-kind, else return None\"\"\"\n    for card in ranks:\n        if ranks.count(card) == n: return card\n    return None",
        "sha1": "e27aafe5774d83cd84bde696fddb86eebdc6bec3",
        "id": 237832
    },
    {
        "content": "def is_tag_match(tag1, tag2):\n    \"\"\"Determines if two tags have the same span.\n\n    True if the start and end indices match.\n    This does not necessarily mean the tags have the same category.\n\n    Args:\n        tag1: An xml based tag as from ElementTree\n        tag2: An xml based tag as from ElementTree\n\n    Returns:\n        True if the tags' indices match, False otherwise.\n        \n    \"\"\"\n    if tag1.attrib['start'] == tag2.attrib['start'] and tag1.attrib['end'] == tag2.attrib['end']:\n            return True\n    return False",
        "sha1": "909d755ef2dc199452091bec871187d33cb96fc7",
        "id": 302281
    },
    {
        "content": "def max_val(t): \n    \"\"\" t, tuple or list\n    Each element of t is either an int, a tuple, or a list\n    No tuple or list is empty\n    Returns the maximum int in t or (recursively) in an element of t \"\"\" \n    maxVal = False\n    \n    def helper(obj):\n        nonlocal maxVal\n        for el in obj:\n            if isinstance(el, int):\n                if maxVal == False or maxVal < el:\n                    maxVal = el\n            else:\n                helper(el)\n                \n    helper(t)\n    return maxVal",
        "sha1": "159531e895bb1c5b51dd8e8d1c308b554f44050f",
        "id": 86989
    },
    {
        "content": "def get_object_name(file_name):\n    \"\"\" Get object name from file name and add it to file\n    YYYY/MM/dd/file\n    \"\"\"\n    return \"{}/{}/{}/{}\".format(file_name[4:8],file_name[8:10],file_name[10:12], file_name)",
        "sha1": "437c286b2c14712d80f68f8f7b3d36ccf279da81",
        "id": 682000
    },
    {
        "content": "def get_same_padding(kernel_size):\n    \"\"\"Calculate padding size for same padding,\n    assuming stride of 1 and square kernel\"\"\"\n\n    if type(kernel_size) is tuple:\n        kernel_size = kernel_size[0]\n    pad_size = (kernel_size-1)//2\n    if kernel_size%2 == 0:\n        padding = (pad_size, pad_size+1)\n    else:\n        padding = pad_size\n\n    return padding",
        "sha1": "ca61b5538c41166a6fcf605dd7b32018ba9ff1d2",
        "id": 285904
    },
    {
        "content": "def cantor_pairing(a, b, ):\n    \"\"\"Create an unique number from two natural numbers\n\n    For more information about the Cantor pairing function, have a look at:\n    https://en.wikipedia.org/wiki/Pairing_function\n\n    This create an unique number from two natural numbers according to\n    .. math::\n        \\pi (a,b):={\\frac{1}{2}}(a+b)(a+b+1)+b.\n\n    Args:\n        a: A numpy.array with natural numbers, i.e. unsigned integer.\n        b: A numpy.array with natural numbers, i.e. unsigned integer.\n\n    Returns:\n        A numpy.array with the unique values.\n    \"\"\"\n\n    a_b_sum = a + b\n    return (0.5 * a_b_sum * (a_b_sum+1) + b).astype(\"int\")",
        "sha1": "bfe7637723e7129242c655ce35d598d3fc9c4565",
        "id": 419981
    },
    {
        "content": "def get_current_application(request):\n    \"\"\"Get current application.\"\"\"\n    try:\n        app_name = request.resolver_match.namespace\n\n        if not app_name:\n            app_name = \"home\"\n    except Exception as e:\n        app_name = \"home\"\n\n    return app_name",
        "sha1": "310004714da3129cafb2bc635837920a7457fbe7",
        "id": 15683
    },
    {
        "content": "from typing import Tuple\n\n\ndef parse_emane_model_id(_id: int) -> Tuple[int, int]:\n    \"\"\"\n    Parses EMANE model id to get true node id and interface id.\n\n    :param _id: id to parse\n    :return: node id and interface id\n    \"\"\"\n    interface = -1\n    node_id = _id\n    if _id >= 1000:\n        interface = _id % 1000\n        node_id = int(_id / 1000)\n    return node_id, interface",
        "sha1": "3fc07cf102217c49f1df128a4c45f8edcdfc8c7a",
        "id": 483583
    },
    {
        "content": "def mktestFfile(tmpdir_factory):\n    \"\"\"Creates test file for unit testing in tempFdir.\n\n    :param tmpdir_factory:\n    :returns: path of file for pin 435 test\n    \"\"\"\n    tempFdir = tmpdir_factory.mktemp('tempF')\n    testFile = tempFdir.join('L435L0.BIN')\n    test435Path = testFile.strpath\n    with open('./testing/435L0.BIN', 'rb') as binaryFile:\n        hold = binaryFile.read()\n    with open(test435Path, 'wb') as binaryFile:\n        binaryFile.write(hold)\n    return test435Path",
        "sha1": "8a523fadeeb135a697c3d2f3b8ca189f14af6e2c",
        "id": 179226
    },
    {
        "content": "def db_get_loss(conn, experiment, fold):\n    \"\"\"\n    Get loss of a model in db.\n\n    Args:\n        conn: Sqlite3 connection object.\n        experiment: Experiment name.\n        fold: Fold number.\n\n    Returns:\n        The loss of the model.\n    \"\"\"\n    cur = conn.cursor()\n\n    loss = cur.execute(\"SELECT loss FROM experiments WHERE experiment=? and fold=?\", (experiment, fold)).fetchone()[0]\n\n    return loss",
        "sha1": "0bc0ffd7770edf1bcf123c2b4dbeba48ce6dcca8",
        "id": 497039
    },
    {
        "content": "def get_network_description(network):\n        \"\"\"Get the string and total parameters of the network\"\"\"\n        # pdb.set_trace()\n        # network = network.module\n        return str(network), sum(map(lambda x: x.numel(), network.parameters()))",
        "sha1": "fda242425035775fd5346184e552a4aed9aae28b",
        "id": 564873
    },
    {
        "content": "def has_dim_prop(fake_services, dim_name, dim_value, prop_name, prop_value=None):\n    \"\"\"\n    Tests if the given dimension has a property.  If prop_value is None, tests\n    for the presence of the property regardless of value.\n    \"\"\"\n    dim = fake_services.dims[dim_name].get(dim_value, {})\n    props = dim.get(\"customProperties\", {})\n    if props is not None:\n        return prop_name in props and props.get(prop_name) == prop_value\n    return False",
        "sha1": "0e1aa587324e54d25e953f1ed618faea6c44e597",
        "id": 419111
    },
    {
        "content": "def set_module(module):\n    \"\"\"\n    A decorator to update the __module__ variable as is done in numpy.\n\n    References\n    ----------\n    * https://numpy.org/devdocs/release/1.16.0-notes.html#module-attribute-now-points-to-public-modules\n    * https://github.com/numpy/numpy/blob/544094aed5fdca536b300d0820fe41f22729ec66/numpy/core/overrides.py#L94-L109\n    \"\"\"\n    def decorator(func):\n        if module is not None:\n            func.__module__ = module\n        return func\n    return decorator",
        "sha1": "271043a933f177cf84c8986d301ad882f8185445",
        "id": 225652
    },
    {
        "content": "def get_valid_users(user_base_file):\n    \"\"\"Read in users from the userbase file.\"\"\"\n    with open(user_base_file, 'r') as file:\n        return [user.strip() for user in file.readlines()]",
        "sha1": "d9119bbaeb3ec93940db6233e8f780ef2cbbecfb",
        "id": 441179
    },
    {
        "content": "import re\n\n\ndef strip_regex(tweet, regex):\n    \"\"\"Strip a tweet of any text matching the supplied regex. Returns the\n    text(s) removed, count of text(s) removed, and remaining tweet minus the\n    removed text(s).\"\"\"\n    # Extract all matches from the tweet:\n    matches = re.findall(regex, tweet)\n\n    # Count all matches:\n    match_count = len(matches)\n\n    # Remove matches from tweet:\n    stripped_tweet = tweet\n    for text in matches:\n        stripped_tweet = stripped_tweet.replace(text, \"\")\n\n    # Remove any double spaces left behind from text stripping:\n    stripped_tweet = re.sub(\" +\", \" \", stripped_tweet).strip()\n\n    return matches, match_count, stripped_tweet",
        "sha1": "0a04f1154f775509470e17bd75720a052614cd8f",
        "id": 204708
    },
    {
        "content": "def augment_data(meta, response_left, stimulus):\n    \"\"\"Augment meta data with fields for specific cases\n\n    Args:\n        meta: DataFrame\n        response_left: ndarray\n            1 if subject made a left_response / yes response\n        stimulus: ndarray\n            1 if a left_response is correct\n    \"\"\"\n    # add columns:\n    meta[\"all\"] = 1\n\n    meta[\"left\"] = response_left.astype(int)\n    meta[\"right\"] = (~response_left).astype(int)\n\n    meta[\"hit\"] = ((response_left == 1) & (stimulus == 1)).astype(int)\n    meta[\"fa\"] = ((response_left == 1) & (stimulus == 0)).astype(int)\n    meta[\"miss\"] = ((response_left == 0) & (stimulus == 1)).astype(int)\n    meta[\"cr\"] = ((response_left == 0) & (stimulus == 0)).astype(int)\n    return meta",
        "sha1": "2f6d1c91c7124947600be214decbf47fd0e935b9",
        "id": 171652
    },
    {
        "content": "import typing\n\n\ndef get_all(iterable: typing.Iterable, **attributes) -> typing.List[typing.Any]:\n    \"\"\"\n    A helper that returns all elements in the iterable that meet all\n    of the traits passed in ``attributes``.\n\n    .. note::\n\n        When multiple attributes are specified, they are checked using\n        logical AND, not logical OR. Meaning they have to meet every\n        attribute passed in and not one of them.\n\n    If nothing is found that matches the attributes passed, then an \n    empty list is returned.\n\n    Parameters\n    ----------\n    iterable\n        An iterable to search through.\n    \\\\*\\\\*attributes\n        Keyword arguments that denote attributes to search with.\n\n    Returns\n    -------\n    List[Any]\n        The elements that met all traits passed in ``attributes``.\n    \"\"\"\n\n    l = list()\n\n    for (i) in iterable:\n        try:\n            if all([getattr(i, k) == v for (k, v) in attributes.items()]):\n                l.append(i)\n        except (AttributeError) as e:\n            pass\n\n    return l",
        "sha1": "d23b99a98fc1f2079f6700b73bf3c2bee8de0e41",
        "id": 144488
    },
    {
        "content": "def MI_Tuple(value, Is): \n    \"\"\"\n    Define function for obtaining multiindex tuple from index value\n    value: flattened index position, Is: Number of values for each index dimension\n    Example: MI_Tuple(10, [3,4,2,6]) returns [0,0,1,4]\n    MI_Tuple is the inverse of Tuple_MI.    \n    \"\"\"\n    IsValuesRev = []\n    CurrentValue = value\n    for m in range(0,len(Is)):\n        IsValuesRev.append(CurrentValue % Is[len(Is)-m-1])\n        CurrentValue = CurrentValue // Is[len(Is)-m-1]\n    return IsValuesRev[::-1]",
        "sha1": "97a71e9049fec0e95fccfcebb2aea4032f980fbe",
        "id": 62844
    },
    {
        "content": "import re\n\n\ndef argv_to_pairs(argv):\n    \"\"\"Convert a argv list to key value pairs. For example,\n    -x 10 -y 20 -z=100\n    {x: 10, y: 20, z: 100}\n    \"\"\"\n\n    arg_dict = {}\n\n    i = 0\n    while i < len(argv):\n        if argv[i].startswith('-'):\n            entry = re.sub('-+', '', argv[i])\n\n            items = entry.split('=')  # handle the case '-z=100'\n            if len(items) == 2:\n                key = items[0]\n                value = items[1]\n                i += 1\n            else:  # handle the case  '-x 10'\n                key = entry\n                value = argv[i + 1]\n                i += 2\n\n            if key in arg_dict:\n                raise ValueError(\n                    'You cannot specify a key multiple'\n                    'times in commandline, {}'.format(key))\n\n            arg_dict[key] = value\n        else:\n            raise ValueError(\n                'argv in wrong format, the key must be started'\n                'with - or --, but found {}'.format(argv[i]))\n\n    return arg_dict",
        "sha1": "e375d2cfc835d33c44f3646e41668852fb8be6d7",
        "id": 687595
    },
    {
        "content": "def mangle_name(name, prefix='', postfix=''):\n    \"\"\"\n    \"Mangles\" *name* by putting a *prefix* and *postfix* around it.\n    :param name: name to mangle\n    :param prefix: *optional* - defaults to '' - prefix to put at the beginning\n    of the name to mangle it\n    :param postfix: *optional* - defaults to '' - postfix to put at the ending\n    of the name to mangle it\n    :return: prefix + name + postfix\n    \"\"\"\n    return prefix + name + postfix",
        "sha1": "c00aa345e5d8522846a31316eb18b0737e4573d2",
        "id": 256580
    },
    {
        "content": "def url_from_id(id):\n    \"\"\"Returns URL of article with given ID\"\"\"\n    return \"https://www.ncbi.nlm.nih.gov/pubmed/\" + id",
        "sha1": "3dff22b1c8f20f3379de682af01d768ef75412ca",
        "id": 225632
    },
    {
        "content": "def build_notification_text(text_parameters) -> str:\n    \"\"\"Create and format the contents of the notification.\"\"\"\n    nr_changed = len(text_parameters[\"metrics\"])\n    plural_s = \"s\" if nr_changed > 1 else \"\"\n    report_link = f'[{text_parameters[\"report_title\"]}]({text_parameters[\"url\"]})'\n\n    result = f'{report_link} has {nr_changed} metric{plural_s} that changed status:\\n\\n'\n    for metric in text_parameters[\"metrics\"]:\n        name = metric[\"metric_name\"]\n        unit = metric[\"metric_unit\"]\n        unit = unit if unit.startswith(\"%\") else f\" {unit}\"\n        result += f'* {name} status is {metric[\"new_metric_status\"]}, was {metric[\"old_metric_status\"]}. ' \\\n                  f'Value is {metric[\"new_metric_value\"]}{unit}, was {metric[\"old_metric_value\"]}{unit}.\\n'\n    return result",
        "sha1": "5ad5c80e4b9642d1c5d15e98c627ca35e03360f6",
        "id": 228289
    },
    {
        "content": "def should_include(dirname, files):\n    \"\"\"Returns true if the directory should be included in the source set.\"\"\"\n    for file in files:\n        if file.endswith('.flac'):\n            return True\n    return False",
        "sha1": "39047affce0ccfc6e2916fe912faf2e06ee12855",
        "id": 442370
    },
    {
        "content": "def convert_schedule_time(string: str) -> str:\n    \"\"\"\n    convert_schedule_time - Provides extra time types for schedule times\n\n    Args:\n        string (str): The string to convert\n\n    Returns:\n        str: The converted string\n    \"\"\"\n    # ticks, seconds, and days are included\n    if isinstance(string, int):\n        return f\"{string}t\"\n    elif string.endswith(\"m\"):\n        return f\"{int(string[:-1]) * 60}s\"\n    elif string.endswith(\"h\"):\n        return f\"{int(string[:-1]) * 3600}s\"\n    elif string.endswith(\"w\"):\n        return f\"{int(string[:-1]) * 7}d\"\n    else:\n        return string",
        "sha1": "8edfa67bd1c7422cdcb4bec7daeae6c90b9f7197",
        "id": 395408
    },
    {
        "content": "def validate_float(p: str) -> bool:\n    \"\"\"\n    Validate a float.\n\n    :param p: Value\n    :return: True if integer\n    \"\"\"\n    if p == '' or p == '-':\n        return True\n    try:\n        float(p)\n        return True\n    except ValueError:\n        pass\n    return False",
        "sha1": "14653e2b780e2fbbf297aadf16a5e031244a8d6a",
        "id": 254419
    },
    {
        "content": "def power_of_two(n):\n  \"\"\"Check if value is a power of two.\"\"\"\n  if n == 2:\n    return True\n  elif n%2 != 0:\n    return False\n  else:\n    return power_of_two(n/2.0)",
        "sha1": "8a435ac95f6d8b2b8006788400b7bd04615f8b5e",
        "id": 42085
    },
    {
        "content": "def trim_callstack(exception_msg: str, target_user_file: str):\n    \"\"\"\n    Shorten the call stack to the starting point of the user script\n    \"\"\"\n    exception_msg_list = exception_msg.splitlines(keepends = True)\n    # Store title\n    trimmed_msg = exception_msg_list[0]\n\n    # Find the starting point\n    i = 0\n    for i in range(2, len(exception_msg_list)):\n        if target_user_file in exception_msg_list[i]:\n            break\n\n    return trimmed_msg + \"\".join(exception_msg_list[i:])",
        "sha1": "872531483cd019a0f9d7c9a6368c883d3cda1521",
        "id": 378882
    },
    {
        "content": "def get_dotted_key(dictionary, key_str, default=None):\n    \"\"\"Fetch a value from a nested dictionary with keys of the form a.b.c\"\"\"\n    key_list = key_str.split('.')\n    key_val = key_list.pop()\n\n    for key in key_list:\n        try:\n            dictionary = dictionary[key]\n        except (KeyError, ValueError):\n            return default\n\n    return dictionary.get(key_val, default)",
        "sha1": "ce18e34fcacee6182a160154734a4b050e309dfd",
        "id": 141890
    },
    {
        "content": "def is_entropy(entropy: str) -> bool:\n    \"\"\"\n    Check entropy hex string.\n\n    :param entropy: Entropy hex string.\n    :type entropy: str\n\n    :returns: bool -- True/False.\n\n    >>> from pybytom.utils import is_entropy\n    >>> is_entropy(\"ee535b143b0d9d1f87546f9df0d06b1a\")\n    True\n    \"\"\"\n\n    return len(entropy) in [32, 40, 48, 56, 64]",
        "sha1": "5072399bc729b102c1ccf5c0632c481edf529be8",
        "id": 445963
    },
    {
        "content": "def count_occupied_seats(data):\n    \"\"\" Counts all occupied seats.\n\n    :param data: seat map (2d array)\n    :return: int\n    \"\"\"\n    return sum(row.count('#') for row in data)",
        "sha1": "885619c6b43c5b8d41eb2bddc8123305d8cbcbd8",
        "id": 355095
    },
    {
        "content": "def message_warning(msg, *a, **kwargs):\n    \"\"\"Ignore everything except the message.\"\"\"\n    return str(msg) + '\\n'",
        "sha1": "a39440007b6e07c4810e73a9c16edd06316962e4",
        "id": 638035
    },
    {
        "content": "def static_vars(**kwargs):\n    \"\"\"Python decorator to declare static variables on a method.\n\n    NOTE: Relies on the Python feature that allows attributes to be added to a\n    function.  See:\n\n        http://stackoverflow.com/questions/279561/what-is-the-python-equivalent-of-static-variables-inside-a-function\n\n    for source of decorator code and additional information.\n    \"\"\"\n    def decorate(func):\n        for k in kwargs:\n            setattr(func, k, kwargs[k])\n        return func\n    return decorate",
        "sha1": "0a60a8596c4a293c5f3a185fd07df890dff6a754",
        "id": 305377
    },
    {
        "content": "def find_empty(brd):\n    \"\"\"\n    Uses the Sudoku problem as a Matrix input and returns the row/column location of the next empty field.\n\n    :param brd: Matrix input of the board needed to solve for the Sudoku problem.\n    :return: The row/column of the next empty field as a tuple.\n    \"\"\"\n    for i in range(len(brd)):\n        for j in range(len(brd[0])):\n            if brd[i][j] == 0:\n                return i, j  # i: row, j: column\n    return None",
        "sha1": "82ab16530d4f7951f18a470996637c25bd848493",
        "id": 266125
    },
    {
        "content": "def create_s1_product_specs(product_type='*', polarisation='*', beam='*'):\n    \"\"\"Convert Sentinel-1's product metadata to scihub's product attributes\n\n    Default values for all product specifications is the wildcard\n    '*' in order to check for all\n\n    :param product_type: Sentinel-1 product type (RAW, SLC, GRD),\n                         defaults to '*'\n    :type product_type: str\n    :param polarisation: Sentinel-1 polarisation mode (VV; VV VH; HH; HH HV),\n                         defaults to '*'\n    :type polarisation: string\n    :param beam: Sentinel-1 beam mode (IW; SM, EW), defaults to '*'\n    :type beam: str\n    :return: Copernicus' scihub compliant product specifications query string\n    :rtype: str\n    \"\"\"\n\n    # bring product type, polarisation and beam to query format\n    return (\n        f'producttype:{product_type} AND '\n        f'polarisationMode:{polarisation} AND '\n        f'sensoroperationalmode:{beam}'\n    )",
        "sha1": "2e560cd3adb6dc6ad975e3e51a0e15c53c5c40e4",
        "id": 521224
    },
    {
        "content": "def format_offset_time(offset: int) -> str:\n    \"\"\"Format offset time to exiftool format: -04:00\"\"\"\n    sign = \"-\" if offset < 0 else \"+\"\n    hours, remainder = divmod(abs(offset), 3600)\n    minutes, _ = divmod(remainder, 60)\n    return f\"{sign}{hours:02d}:{minutes:02d}\"",
        "sha1": "abcc1c9a0129dc33a97f9f1318ce247884d4c9cf",
        "id": 458622
    },
    {
        "content": "def create_upload_form_attributes(prefix, input_type, name):\n    \"\"\"Creates attribute dicts for the switchable upload form\n\n    :type prefix: str\n    :param prefix: prefix (environment, template) of field\n    :type input_type: str\n    :param input_type: field type (file, raw, url)\n    :type name: str\n    :param name: translated text label to display to user\n    :rtype: dict\n    :return: an attribute set to pass to form build\n    \"\"\"\n    attributes = {'class': 'switched', 'data-switch-on': prefix + 'source'}\n    attributes['data-' + prefix + 'source-' + input_type] = name\n    return attributes",
        "sha1": "bade2cdd07c89ebdab14fda9875fb2ccf5f5fb67",
        "id": 599940
    },
    {
        "content": "def get_object_or_none(model_class, name):\n    \"\"\"\n    Returns the object or None.\n    \"\"\"\n    try:\n        return model_class.objects.get(name=name)\n    except model_class.DoesNotExist:\n        return None",
        "sha1": "72e6af4f8de00095b4f04427e5316add8c82b362",
        "id": 664506
    },
    {
        "content": "def timeshift(df, threshold, criterion=\"Infected\", time=\"time\"):\n    \"\"\"Add a series of adjusted times, zeroed on a threshold number\n    of infections.\n\n    Arguments:\n    df -- A Pandas dataframe in the format output by the SIR function\n    threshold -- an integer number of infections to be set to time zero\n\n    Keyword arguments:\n    criterion -- the column in df containing the critrion to which the\n    threshold applies (default \"Infected\")\n    time -- the column in df containing the time variable to adjust (default\n    \"time\")\n    \"\"\"\n    timeshift = df[df[criterion] >= threshold][time].min()\n    df[\"time_adj\"] = df[time] - timeshift\n    return df",
        "sha1": "967e73ac7284db5880c97c5146e0b4b5aecb306f",
        "id": 105575
    },
    {
        "content": "def UT2TOW(UT):\n\t\"\"\"Universal Time (UT) to Time Of Week (TOW)\n\t\n\tConverts time of the day in Universal Time to time in seconds measured since the start of the week.\n\t\n\tArgs:\n\t\tUT (str): Universal Time\n\t\t\n\tReturns:\n\t\tint: Time Of Week (TOW)\n\t\"\"\"\n\thrs, mins, sec = UT.split(':')\n\thrs = int(hrs)\n\tmins = int(mins)\n\tsec = int(sec)\n\tTOW = hrs*3600 + mins*60 + sec\n\treturn TOW",
        "sha1": "910ad39624d538ca35300044ae6d763e3998986b",
        "id": 167374
    },
    {
        "content": "def contained_items_once(str, start, end):\n    \"\"\"\n    Given a string that represents items as asterisks (*) and compartment walls \n    as pipes (|), a start index, and an end index, return the number of items in a closed compartment.\n\n    >>> str = '|**|*|*'\n    >>> contained_items_once(str, 0, 5)\n    2\n    >>> contained_items_once(str, 0, 6)\n    3\n    >>> contained_items_once(str, 1, 7)\n    1\n    \"\"\"\n    substr = str[start:end].strip('*')\n    splits = substr.split('|')\n    return sum(len(s) for s in splits)",
        "sha1": "8c56a58fd487db747e4e4b8a46081265d1a1486e",
        "id": 70125
    },
    {
        "content": "def get_dimensions_by_order(dims_in, dataset):\n    \"\"\"get dimension\n\n    Parameters\n    ----------\n    dims_in: int or list of int\n        the dimensions by numerical order\n    dataset: sidpy.Dataset\n\n    Returns\n    -------\n    dims_out: list of dimensions\n    \"\"\"\n\n    if isinstance(dims_in, int):\n        dims_in = [dims_in]\n    dims_out = []\n    for item in dims_in:\n        if isinstance(item, int):\n            if item in dataset._axes:\n                dims_out.append([item, dataset._axes[item]])\n    return dims_out",
        "sha1": "3430f045ed57e3d98aec15ffb7298d1c727bee27",
        "id": 16522
    },
    {
        "content": "import string\n\n\ndef digits(token):\n    \"\"\"\n    Whether a given string token contains digits or not.\n\n    :param token: input token\n    :type token: str\n\n    :return: description of the content\n    :rtype: str\n    \"\"\"\n    if token.isdigit():\n        return 'all_digits'\n    elif set(token) & set(string.digits):\n        return 'some_digits'\n    else:\n        return 'no_digits'",
        "sha1": "2546aeeddce9d8c3d014c809e383c277409448f6",
        "id": 544656
    },
    {
        "content": "import torch\n\n\ndef is_tensor_nan_or_inf(tensor: torch.Tensor) -> bool:\n    \"\"\"\n    Returns True if any of the tensor elements is Not a Number or Infinity.\n\n    :param tensor: The tensor to check.\n    :return: True if any of the tensor elements is Not a Number or Infinity, False if all entries are valid numbers.\n    \"\"\"\n    result = torch.isnan(tensor).any().item() or torch.isinf(tensor).any().item()\n    if isinstance(result, bool):\n        return result\n    raise ValueError(\"torch not returning bool as we expected\")",
        "sha1": "5a0da7c6016fe62f1e831103bdc0720b09399574",
        "id": 184285
    },
    {
        "content": "def is_colored_img(x):\n    \"\"\"Check if an image or batch of image is colored.\"\"\"\n    if x.shape[-3] not in [1, 3]:\n        raise ValueError(f\"x doesn't seem to be a (batch of) image as shape={x.shape}.\")\n    return x.shape[-3] == 3",
        "sha1": "6f9d4919a95189136c171656d4524096927c2b81",
        "id": 554298
    },
    {
        "content": "from pathlib import Path\n\n\ndef _expand_home(path):\n  \"\"\"\n  pathlib doesn't interpret ~/ as $HOME\n  This doesnt deal with other user's homes e.g. ~another/dir is not changed\n  \"\"\"\n  return Path(str(path).replace(\"~/\", str(Path.home()) + \"/\"))",
        "sha1": "42250a00f30c81678f742e1512990f0b859cd05b",
        "id": 324311
    },
    {
        "content": "def renormalization(norm_data, norm_parameters):\n    \"\"\"Renormalize data from [0, 1] range to the original range.\n\n    Args:\n      - norm_data: normalized data\n      - norm_parameters: min_val, max_val for each feature for renormalization\n\n    Returns:\n      - renorm_data: renormalized original data\n    \"\"\"\n\n    min_val = norm_parameters['min_val']\n    max_val = norm_parameters['max_val']\n\n    _, dim = norm_data.shape\n    renorm_data = norm_data.copy()\n\n    for i in range(dim):\n        renorm_data[:, i] = renorm_data[:, i] * (max_val[i] + 1e-6)\n        renorm_data[:, i] = renorm_data[:, i] + min_val[i]\n\n    return renorm_data",
        "sha1": "9e1a3e4fc78389a95cf9d891df1a75272193fb33",
        "id": 487731
    },
    {
        "content": "def filter_blackwhitelist(l, key):\n    \"\"\"\n    Filter black/whitelist for the keys that belong to the\n    subdirectory which is embedded into the nested dictionary\n    structure with the given key.\n\n    Three different cases:\n    - if l is None, then return none\n    - if key is None, then we are at the top-level dictionary, thus\n      include all scalar keys and the first element of tuples.\n    - if key is not None, then return only the keys that are tuples\n      where the first element of the tuple matches the given key\n\n    Parameters\n    ----------\n    l : list\n        Black- or whitelist to filter\n    key : scalar variable or None\n        Key to filter for. See above for the behavior if key is None\n    \"\"\"\n    if l is None:\n        return None\n    else:\n        fl = []\n        for k in l:\n            if isinstance(k, tuple):\n                if key is not None and k[0] == key:\n                    if len(k) == 2:\n                        fl.append(k[1])\n                    else:\n                        fl.append(k[1:])\n                elif key is None:\n                    fl.append(k[0])\n            elif key is None:\n                fl.append(k)\n        if len(fl) == 0:\n            return None\n        else:\n            return fl",
        "sha1": "d9edbf5b95d1baa5d741aa55173f032132c96ea0",
        "id": 572144
    },
    {
        "content": "def cli(ctx, dataset_id):\n    \"\"\"Get the permissions for a dataset.\n\nOutput:\n\n    dictionary with all applicable permissions' values\n    \"\"\"\n    return ctx.gi.libraries.get_dataset_permissions(dataset_id)",
        "sha1": "664bbb2d5cbd4ba9f341c2b14e45fb106b8d7d18",
        "id": 125472
    },
    {
        "content": "def cbmc_text_program(log_section):\n    \"\"\"Find program in cbmc text output\"\"\"\n\n    for line in log_section:\n        if line.startswith('CBMC version'):\n            return line\n    return None",
        "sha1": "5ea89d398dfa5812e0738baac98d10def7c0ccd8",
        "id": 446863
    },
    {
        "content": "def calculate_width(count, parent_count, parent_width):\n    \"\"\"calculate_width(count: int, parent_count: int, parent_width: int)\n    Calculate string width based on parent width and sample count.\n    \"\"\"\n    return int(count * parent_width // parent_count) if parent_count != 0 else 0",
        "sha1": "34dabcb8f0cf90866c7e4ab1b5834554f5341d5b",
        "id": 64687
    },
    {
        "content": "def on_off(image, w, h, threshold=128):\n    \"\"\"\n    Black and white (no greyscale) with a simple threshold.\n    If the color is dark enough, the laser is on!\n    \"\"\"\n    result = []\n\n    for row in image:\n        result_row = []\n        for pixel in row:\n            # We draw black, so 255 is for dark pixels\n            result_row.append(255 if pixel < threshold else 0)\n        result.append(result_row)\n    return result",
        "sha1": "c9e577bf851fa972c1bbe7f8a61afc09ffb37de5",
        "id": 689111
    },
    {
        "content": "def remove_from_end(string, text_to_remove):\n    \"\"\"\n    Remove a String from the end of a string if it exists\n    Args:\n        string (str): string to edit\n        text_to_remove (str): the text to remove\n\n    Returns: the string with the text removed\n\n    \"\"\"\n    if string is not None and string.endswith(text_to_remove):\n        return string[:-len(text_to_remove)]\n    return string",
        "sha1": "19cebd002fcf5aea5290a6998129427363342319",
        "id": 4570
    },
    {
        "content": "from typing import Tuple\nfrom typing import Optional\n\n\ndef bounding_box_intersection(origin: Tuple[int, int, int, int], intersect: Tuple[int, int, int, int]) \\\n        -> Optional[Tuple[int, int, int, int]]:\n    \"\"\"Returns the coordinates of the origin bounding box that\n    are intersected by the intersect bounding box.\n\n    >>> bounding_box = 10, 100, 30, 110\n    >>> other_bbox = 20, 100, 40, 105\n    >>> bounding_box_intersection(bounding_box, other_bbox)\n    (10, 0, 20, 5)\n    >>> bounding_box_intersection(other_bbox, bounding_box)\n    (0, 0, 10, 5)\n    >>> containing_bbox = 4, 55, 44, 115\n    >>> bounding_box_intersection(bounding_box, containing_bbox)\n    (0, 0, 20, 10)\n    >>> contained_bbox = 12, 102, 22, 108\n    >>> bounding_box_intersection(bounding_box, contained_bbox)\n    (2, 2, 12, 8)\n    >>> non_overlapping_bbox = 0, 0, 3, 3\n    >>> bounding_box_intersection(bounding_box, non_overlapping_bbox) is None\n    True\n\n    \"\"\"\n    o_t, o_l, o_b, o_r = origin\n    t, l, b, r = intersect\n\n    out_top = max(t, o_t)\n    out_left = max(l, o_l)\n    out_bottom = min(b, o_b)\n    out_right = min(r, o_r)\n\n    if (out_top < out_bottom) and (out_left < out_right):\n        return out_top - o_t, \\\n               out_left - o_l, \\\n               out_bottom - o_t, \\\n               out_right - o_l\n    else:\n        return None",
        "sha1": "0e7aa462fefd0a381c6ba6c6ec357870937c00a0",
        "id": 522176
    },
    {
        "content": "def get_dict_sensitive_vals(dict_sensitive_lists):\n    \"\"\"\n    Takes a dictionary mapping sensitive attributes to lists in the test data and returns a\n    dictionary mapping sensitive attributes to lists containing each sensitive value only once.\n    \"\"\"\n    newdict = {}\n    for sens in dict_sensitive_lists:\n         sensitive = dict_sensitive_lists[sens]\n         newdict[sens] = list(set(sensitive))\n    return newdict",
        "sha1": "9931b7477a38093bd3d3ef91d67fcfdf7e56c2a0",
        "id": 565255
    },
    {
        "content": "def satname(rocketsatname):\n    \"\"\"Takes in rocket and satellite name from SpaceflightNow website and returns only the satellite name\"\"\"\n    \n    # split the rocket and satellite name at the bullet\n    names = rocketsatname.split('\u2022')\n    \n    # remove spaces around satellite name\n    namefull = names[1].strip()\n    \n    # return the satellite's name\n    return namefull",
        "sha1": "a8c58cb09f847522b72018848a77d182cd394f8b",
        "id": 268548
    },
    {
        "content": "from typing import Any\n\n\ndef _strip_db_indexing_key(obj: dict[str, Any]):\n    \"\"\"\n    Strip MongoDB's ObjectId key `_id`.\n    \"\"\"\n    return {key: obj[key] for key in obj if key != \"_id\"}",
        "sha1": "5c63742bb4f8e2bd8a3844eb549468a33dd5df7c",
        "id": 31422
    },
    {
        "content": "from typing import SupportsIndex\n\n\ndef ireplace(text: str, old_: str, new_: str, count_: SupportsIndex = -1) -> str:\n    \"\"\"Case-insensitive :py:meth:`str.replace` alternative.\n\n    :param text: String to search through.\n    :param old_: Case-insensitive substring to look for.\n    :param new_: Case-sensitive substring to replace with.\n    :param count_: Maximum number of occurrences to replace. -1 (the default value) means replace all occurrences.\n\n    :return: A string with all substrings matching old_ replaced with new_.\n    \"\"\"\n    index: int = 0\n    if not old_:\n        return text.replace('', new_)\n    if text.lower() == old_.lower() and count_.__index__() != 0:\n        return new_\n    while index < len(text) and count_.__index__() < 0:\n        index_l = text.lower().find(old_.lower(), index)\n        if index_l == -1:\n            return text\n        text = text[:index_l] + new_ + text[index_l + len(old_):]\n        index = index_l + len(new_)\n    return text",
        "sha1": "8509314192458c867b6471a381dc6470a2546902",
        "id": 16192
    },
    {
        "content": "def _get_port(config):\n    \"\"\"Get the server's port from configuration.\"\"\"\n    if not config.has_option(\"server\", \"port\"):\n        return None\n\n    port = config.getint(\"server\", \"port\")\n    return port",
        "sha1": "bee579fcfc82ea80c593dc7bd93ff3d39e63ef7b",
        "id": 7033
    },
    {
        "content": "def get_2d(a):\n    \"\"\"\n    Reshape a 1- or 2-d numpy-array to be 2-dimensional\n    \"\"\"\n    if len(a.shape) <= 1:\n        a = a.reshape(-1, 1)\n    return a",
        "sha1": "309476c899fcc932b4a874169736c01f2b578a37",
        "id": 328961
    },
    {
        "content": "def email_is_string(email):\n    \"\"\"\n    Check if the email is a string.\n\n    :param email: The email to be tested.\n    :type email: str\n    :return: True if the email is a string, else false.\n    :rtype: bool\n    \"\"\"\n    return isinstance(email, str)",
        "sha1": "9262fa4fbfdbaeaa2d605f695b94fbba93de813c",
        "id": 32539
    },
    {
        "content": "def to_symbol(i):\n    \"\"\"Covert ids to text.\"\"\"\n    if i == 0: return \"\"\n    if i == 11: return \"+\"\n    if i == 12: return \"*\"\n    return str(i - 1)",
        "sha1": "f52b93415f76ae4d0767977b1b368981724237ca",
        "id": 576764
    },
    {
        "content": "def serialise_version(version):\n    \"\"\"\n    Convert a version tuple back to a string.\n\n    \"\"\"\n    return '.'.join(str(v) for v in version)",
        "sha1": "eadbce007e357a04f9c82bf43775dd0e5ff9cb69",
        "id": 280383
    },
    {
        "content": "import torch\n\n\ndef tensorlist_to_tensor(weights):\n    \"\"\" Concatnate a list of tensors into one tensor.\n\n        Args:\n            weights: a list of parameter tensors, e.g. net_plotter.get_weights(net).\n\n        Returns:\n            concatnated 1D tensor\n    \"\"\"\n    return torch.cat([w.view(w.numel()) if w.dim() > 1 else torch.FloatTensor(w) for w in weights])",
        "sha1": "544fa3577cfd51b18bf0364bf3d6634a80c9dc4d",
        "id": 148053
    },
    {
        "content": "def process_sampling_size(\n    n: int | None, frac: float | None, replace: bool\n) -> int | None:\n    \"\"\"\n    Process and validate the `n` and `frac` arguments to `NDFrame.sample` and\n    `.GroupBy.sample`.\n\n    Returns None if `frac` should be used (variable sampling sizes), otherwise returns\n    the constant sampling size.\n    \"\"\"\n    # If no frac or n, default to n=1.\n    if n is None and frac is None:\n        n = 1\n    elif n is not None and frac is not None:\n        raise ValueError(\"Please enter a value for `frac` OR `n`, not both\")\n    elif n is not None:\n        if n < 0:\n            raise ValueError(\n                \"A negative number of rows requested. Please provide `n` >= 0.\"\n            )\n        if n % 1 != 0:\n            raise ValueError(\"Only integers accepted as `n` values\")\n    else:\n        assert frac is not None  # for mypy\n        if frac > 1 and not replace:\n            raise ValueError(\n                \"Replace has to be set to `True` when \"\n                \"upsampling the population `frac` > 1.\"\n            )\n        if frac < 0:\n            raise ValueError(\n                \"A negative number of rows requested. Please provide `frac` >= 0.\"\n            )\n\n    return n",
        "sha1": "b2d9486e1a403822ba760ab71e8d18e9fa728081",
        "id": 204726
    },
    {
        "content": "import re\n\n\ndef _regex_search(regex, lines, first):\n    \"\"\"Remove all except 1 line that matches the regex.\n    Returns the unique lines.\n\n    regex: the regular expression to match\n    lines: the lines of the file to deduplicate\n    first: if the first match should be kept, if false, the last match is kept\n    \"\"\"\n    unique_lines = []\n    last_match = -1\n    for line in lines:\n        res = re.match(regex, line)\n        if res is not None:\n            if last_match < 0:\n                unique_lines.append(line)\n                last_match = len(unique_lines)-1\n            elif last_match >= 0 and not first:\n                unique_lines.pop(last_match)\n                unique_lines.append(line)\n                last_match = len(unique_lines)-1\n        else:\n            unique_lines.append(line)\n\n    return unique_lines",
        "sha1": "e6f7b4be0702907bfa9681d433254c0c0a6edb25",
        "id": 543919
    },
    {
        "content": "def findNotZeroIndex(a, fromIdx):\n    \"\"\" ([[int]], int) => int\n        Look up not zero item from a[fromIdx][fromIdx]\n        to a[len(a)][fromIdx].\n        if found return index\n        if did not find return -1\n    \"\"\"\n    n = len(a)\n    for i in range(fromIdx+1,n):\n        if a[i][fromIdx] != 0:\n            return i\n    return -1",
        "sha1": "a0d88d41c98faea16557fd2992d3d768c59a0fe6",
        "id": 378752
    },
    {
        "content": "def trapezoid_error_bound(a, b, N, M):\n    \"\"\"\n        Computes the theoretical error bound when calculating an integral using Trapezoid method.\n\n        Parameters\n        ----------\n        a,b : float\n            The min and max value of the integrating interval\n        N : int\n            Number of partitions.\n        M : float\n            The value of local max on [a,b] of the <b>absolute</b> 2nd derivative of function f used in Trapezoid\n            integration\n\n        Returns\n        -------\n        return_value : float\n            The value of theoretical error bound when calculating an integral using Trapezoid method\n    \"\"\"\n    interval_length = b - a\n    denominator = 12 * (N ** 2)\n    return ((interval_length**3)/denominator)*M",
        "sha1": "e27a4cacff609f52adbdf38310479c55b5a3627b",
        "id": 405779
    },
    {
        "content": "def average_sub_key(batch_list, key, sub_key):\n    \"\"\"\n    Average subkey in a dictionary in a list of batches\n\n    Parameters\n    ----------\n    batch_list : list of dict\n        List containing dictionaries with the same keys\n    key : str\n        Key to be averaged\n    sub_key :\n        Sub key to be averaged (belonging to key)\n\n    Returns\n    -------\n    average : float\n        Average of the value contained in the sub_key of key for all batches\n    \"\"\"\n    values = [batch[key][sub_key] for batch in batch_list]\n    return sum(values) / len(values)",
        "sha1": "188574c67ab14b92b53eeed306df714acebbba5a",
        "id": 537284
    },
    {
        "content": "import re\n\n\ndef check_args(varformat=None, level='l2', coord=None):\n    \"\"\"\n    Return varformat for fgm data\n\n    Parameters:\n        varformat: str, optional\n            If is not empty return as is\n        level: {'l1', 'l2'}, optional\n            Data level\n        coord: {'ssl', 'dsl', 'gse', 'gsm'}, optional\n             Coordinate system. If set to None all possible values are used. coord is applicable for level='l2'\n\n    Returns: str\n        varformat for themis.load\n    \"\"\"\n\n    # DEV COMMENTS:\n    # If varformat is set to None then assign default value\n    # We construct regular expression that covers the fgm variable format\n    # IDL thm_load_xxx function has the following input:\n    #   vsnames = 'a b c d e', $\n    #   type_sname = 'probe', $\n    #   vdatatypes = 'fgl fgh fge', $\n    #   vlevels = 'l1 l2', $ - This input is inherited\n    #   vL2datatypes = 'fgs fgl fgh fge fgs_btotal fgl_btotal fgh_btotal fge_btotal', $\n    #   vL2coord = 'ssl dsl gse gsm none', $\n    # Resulted \"varformat\" for 'l2' and default coodr ('dsl') is:\n    #  th?_fgs_dsl th?_fgl_dsl th?_fgh_dsl th?_fge_dsl\n    #  th?_fgs_btotal_dsl th?_fgl_btotal_dsl th?_fgh_btotal_dsl th?_fge_btotal_dsl\n    #  th?_fgs th?_fgl th?_fgh th?_fge\n    #  th?_fgs_btotal th?_fgl_btotal th?_fgh_btotal th?_fge_btotal\n    # Resulted \"varformat\" for 'l1' is:\n    #  *fgl* *fgh* *fge*\n\n\n    if varformat is None:\n        if level == 'l2':\n            # Prepare coordinates, we reuse the same regex to check the coord variable\n            coord_regexp = '(ssl|dsl|gse|gsm){1}'\n            coord_str = coord_regexp\n\n            # If coord is set and it matches the pattern, then include it into varformat\n            if coord is not None and re.search(coord_regexp, coord, re.IGNORECASE):\n                coord_str = '(' + coord.lower() + '){1}'\n\n            varformat = '^th[a-e]{1}_(fgs|fgl|fgh|fge){1}(_btotal)?(_{1}' + coord_str + ')?$'\n        else:  # This case must be 'l1'\n            varformat = '^th[a-e]{1}_(fgl|fgh|fge){1}*'  # keep right end open with '*' (wildcard)\n\n    return varformat",
        "sha1": "a50c032f8c19bf732e4f5680f67e7083a498f2f2",
        "id": 461907
    },
    {
        "content": "def is_end_word(word):\n    \"\"\"\n    Determines if a word is at the end of a sentence.\n    \"\"\"\n    if word == 'Mr.' or word == 'Mrs.':\n        return False\n    punctuation = \"!?.\"\n    return word[-1] in punctuation",
        "sha1": "9165fede070654b3d0b10b2bb02855307f9ab0c5",
        "id": 67187
    },
    {
        "content": "from typing import Sequence\nfrom typing import Dict\n\n\ndef get_masks(counterfactuals: Sequence[Sequence[str]],\n              vocab_to_indices: Dict[str, Sequence[int]]):\n  \"\"\"Returns masks indicating if counterfactual tokens are in original sentence.\n\n  Args:\n    counterfactuals: The tokens of the input's counterfactuals.\n    vocab_to_indices: A dictionary mapping unique tokens in the input sentence\n      to the indices of its occurrence in the original sentence.\n\n  Returns:\n    A list of masks, which are lists of booleans corresponding to tokens.\n  \"\"\"\n  masks = []\n  for counterfactual in counterfactuals:\n    counterfactual_tokens = set(counterfactual)\n    masks.append([\n        (token in counterfactual_tokens) for token in vocab_to_indices.keys()\n    ])\n  return masks",
        "sha1": "504d4f9e3fba844c4d2496c47003e34fd54b8752",
        "id": 628365
    },
    {
        "content": "import re\n\n\ndef replace_fullwidth_alpha_numeral_to_halfwidth(text: str):\n    \"\"\"\n    Replace full-width alpha-numeral characters to half-width characters.\n\n    Args:\n        text (str): Text to replace.\n\n    Returns:\n        str: Replaced text.\n    \"\"\"\n    return re.sub(r'[\uff21-\uff3a\uff41-\uff5a\uff10-\uff19]', lambda mathobj: chr(ord(mathobj.group(0)) - 0xFEE0), text)",
        "sha1": "acf0d45e63a6595955c3da1d9f70424ea95c8d95",
        "id": 670815
    },
    {
        "content": "import math\n\n\ndef log_loss_one_example(prob, outcome):\n  # type: (float, int) -> float\n  \"\"\"Returns log loss of given prediction for given 0/1 outcome.\"\"\"\n  return - outcome*math.log(prob) - (1-outcome)*math.log(1-prob)",
        "sha1": "c8fdb3528537ea352eae59d75cf663e349c454b2",
        "id": 235772
    },
    {
        "content": "from typing import Any\nimport unicodedata\n\n\ndef mangle_unicode_to_ascii(s: Any) -> str:\n    \"\"\"\n    Mangle unicode to ASCII, losing accents etc. in the process.\n    \"\"\"\n    # http://stackoverflow.com/questions/1207457\n    if s is None:\n        return \"\"\n    if not isinstance(s, str):\n        s = str(s)\n    return (\n        unicodedata.normalize('NFKD', s)\n                   .encode('ascii', 'ignore')  # gets rid of accents\n                   .decode('ascii')  # back to a string\n    )",
        "sha1": "64edd3db9ff9ea023a6f327a49a67478fdc30d5e",
        "id": 478954
    },
    {
        "content": "def _IsDuplicateInterestedUrl(bots_user_entity, url_key):\n  \"\"\"Checks if url_key exist in interested_urls.\n\n  Args:\n    bots_user_entity: bots_user entity (BotsUser).\n    url_key: BotsUrl Entity to check for (db.Key).\n\n  Returns:\n    True if url_key exist in interested_urls, else False.\n  \"\"\"\n  return str(url_key) in [str(x) for x in bots_user_entity.interested_urls]",
        "sha1": "3fd0698af92d7a7141bb922a5ec02d68309111d7",
        "id": 503980
    },
    {
        "content": "def get_emoji_by_name(emoji, name):\n    \"\"\"Lookup @name key in iterable @emoji and return value\"\"\"\n    try:\n        return list(filter(lambda x: x[0] == name, emoji))[0][1]\n    except IndexError:\n        return ''",
        "sha1": "52448c67ff1ca952f0858683c04896f38d824f2f",
        "id": 200133
    },
    {
        "content": "from typing import Callable\nfrom typing import List\nimport torch\n\n\ndef gen_control_data(system: Callable, ics: List, inputs: List):\n\t\"\"\"Generate trajectories given dynamical system with control inputs\n\n\tArgs:\n\t\tsystem: trajectory generator : (initial condition, input) -> trajectory\n\t\tics: list of initial conditions\n\t\tinputs: list of control inputs \n\n\tReturns:\n\t\tbatch_data: b (# initial conditions * # control inputs) x d (state dimension) x N (trajectory length)\n\t\tbatch_inputs b (# initial conditions * # control inputs) x d_u (control input dimension) x N (trajectory length)\n\t\"\"\"\n\tbatch_data = []\n\tbatch_inputs = []\n\tfor u in inputs:\n\t\tfor ic in ics:\n\t\t\tbatch_data.append(system(ic, u))\n\t\t\tbatch_inputs.append(u) \n\treturn torch.stack(batch_data), torch.stack(batch_inputs)",
        "sha1": "bb7e332526b07a696c4635b292e4b98de0d72116",
        "id": 437264
    },
    {
        "content": "def _get_update_parent_dict(parent_record: dict, nr_of_children: int, spouses: list):\n    \"\"\"\n    Increment the married counter, add the nr of children and replace the existing\n    spouses with the spouses list passed in.\n    :param parent_record: the parent record for the parent currently under consideratio n\n    :param nr_of_children: a scalar number of children in this current family iteration\n    :param spouses: currently just a pass through to build the dictionary\n    :return: a dictionary containing the updated fields *only*, i.e. 'married_count', 'children_count'\n    and 'spouses'\n    \"\"\"\n    married_count = 1\n    if 'married_count' in parent_record:\n        married_count += parent_record['married_count']\n    children_count = nr_of_children\n    if 'children_count' in parent_record:\n        children_count += parent_record['children_count']\n\n    update_dict = {'married_count': married_count,\n                   'children_count': children_count,\n                   'spouses': spouses}\n\n    return update_dict",
        "sha1": "241bdc4c3ec40ee0a03338da310b75c9cf7a3729",
        "id": 117386
    },
    {
        "content": "from typing import Dict\n\n\ndef _get_headers(token: str) -> Dict[str, str]:\n    \"\"\"Create headers for API calls.\"\"\"\n\n    return {\n        \"Accept\": \"application/vnd.github.v3+json\",\n        \"Authorization\": f\"token {token}\",\n    }",
        "sha1": "20c7c74fb8e0df0d9bf180c47ec04f41006740bc",
        "id": 481852
    },
    {
        "content": "def ode45_step(f, x, t, dt, *args):\n    \"\"\"\n    One step of 4th Order Runge-Kutta method\n    \"\"\"\n    k = dt\n    k1 = k * f(t, x, *args)\n    k2 = k * f(t + 0.5*k, x + 0.5*k1, *args)\n    k3 = k * f(t + 0.5*k, x + 0.5*k2, *args)\n    k4 = k * f(t + dt, x + k3, *args)\n    return x + 1/6. * (k1 + k2 + k3 + k4)",
        "sha1": "84cb95edd2e94cd442f7b767e5a33252b42bfb36",
        "id": 551173
    },
    {
        "content": "def convert_date(date):\n    \"\"\"\n    Convert date of form MM/DD/YYYY into YYYY-MM-DD. Assumes date is correct\n    form.\n    \"\"\"\n    \n    mdy = date.split(\"/\")\n    m = mdy[0]\n    d = mdy[1]\n    y = mdy[2]\n    newDate = \"%s-%s-%s\" % (y, m, d)\n    return newDate",
        "sha1": "7c63d628d7a1cf96a039555e69e733b9ba2593b2",
        "id": 325927
    },
    {
        "content": "def prettify_setting_name(value):\n    \"\"\" Convert `SETTING_NAME` to `Setting Name`\"\"\"\n    return value.replace('_', ' ').strip().title()",
        "sha1": "d7e62fc80f2dad02c019b7d4cbd93ff0311bae50",
        "id": 425272
    },
    {
        "content": "def quote_string(prop):\n    \"\"\"\n    RedisGraph strings must be quoted,\n    quote_string wraps given prop with quotes incase\n    prop is a string.\n    \"\"\"\n    if not isinstance(prop, str):\n        return prop\n\n    if prop[0] != '\"':\n        prop = '\"' + prop\n\n    if prop[-1] != '\"':\n        prop = prop + '\"'\n\n    return prop",
        "sha1": "19cd16de31568ebdf140196c5b6d0db41cc2f260",
        "id": 72274
    },
    {
        "content": "def is_some_keyword_in_text(text, keywords):\n    \"\"\"\n    >>> test_text = \"What is the current price of Ethereum\"\n    >>> is_some_keyword_in_text(test_text, keywords=[\"price\", \"buy\"])\n    True\n    >>> is_some_keyword_in_text(test_text, keywords=[\"potato\"])\n    False\n    \"\"\"\n    res = False\n    lowered_text = text.lower()\n    for keyword in keywords:\n        lowered_keyword = keyword.lower()\n        if lowered_keyword in lowered_text:\n            res = True\n            break\n    return res",
        "sha1": "98449ec3d2225dd2075c445efb502eabef3c14c5",
        "id": 94811
    },
    {
        "content": "def find_link_joints(model, link_name):\n    \"\"\"Find the joints attached to a given link\n\n    Parameters\n    ----------\n    model : <ModelSDF>\n        SDF model\n\n    link_name : <str>\n        Name of the link in the sdf\n\n    Returns\n    -------\n    out : <tuple>\n        Tuple of joint names attached to the link\n    \"\"\"\n    return tuple([\n        joint.name\n        for joint in model.joints\n        if joint.parent == link_name\n    ])",
        "sha1": "1d09cdf09889e19b7d8b911686ce90765d772a1c",
        "id": 37203
    },
    {
        "content": "def getNodeTypeCategory(node):\n\t\"\"\"Convenience for calling hou.Node.type().category()\n\n\tArgs:\n\t    node (hou.Node): The node to get the node type category\n\t    \tof\n\n\tReturns:\n\t    hou.NodeTypeCategory: The node type category of the given\n\t    \tnode\n\t\"\"\"\n\treturn node.type().category()",
        "sha1": "7fa14e1b72a4a017d604a561f6b2ae15584d1f98",
        "id": 146319
    },
    {
        "content": "def allowed_file(filename, extensions={'csv'}):\n    \"\"\"\n    Checks if a filename contains an allowable extension.\n\n    Parameters\n    ----------\n    filename : str\n        The filename to check.\n    extensions : set\n        The set of allowable file extensions.\n\n    Returns\n    -------\n    allowed : bool\n        True if allowable extension, False otherwise.\n    \"\"\"\n    return '.' in filename and \\\n           filename.rsplit('.', 1)[1].lower() in extensions",
        "sha1": "3d69412b59e84659d38276d7fbb2e71822e54a48",
        "id": 86723
    },
    {
        "content": "def temporal_degree(graph, labels=None, intervals=None, in_out=None, add_data=False):\n    \"\"\"\n        Returns the temporal degree centralities of nodes in a temporal graph.\n\n        Parameter(s):\n        -------------\n        graph : TemporalGraph\n            A temporal graph or its subclasses.\n        labels: list\n            A list of node labels to calculate centralities for. Default is all nodes in the temporal graph.\n            Example: [\"A\", \"B\", \"C\", ...]\n        intervals : tuple/List\n            A tuple of intervals (pairs of start and end times) for the temporal graph to be restricted to.\n            Example: ((0,3), (5,7))\n        in_out : string\n            What type of degree centrality to use. Can be \"in\" for in-degree, \"out\" for out-degree. Leave unspecified\n            for undirected graphs where normal degree centrality is default.\n        add_data : bool\n            Whether to add the centrality values to the data attributes of the nodes in the nodes collection.\n\n        Returns:\n        --------\n        temporal_degree : dict\n            The temporal degrees of the nodes.\n            For example: {A: 1.3, B:1.2, C:2.5, ...}\n\n        Example(s):\n        -----------\n            graph = TemporalGraph('test_network', data=CsvInput('./network.csv'))\n            degree_values = degree_centrality(graph, labels=[\"A\", \"B\", \"C\"], intervals=((1, 5), (8, 10)))\n\n        Notes:\n        ------\n        Here, temporal degree centrality is the average of a nodes' degree over the snapshots of the graph in a given\n        time interval. Degree may refer to in-degree, out-degree, or both.\n\n    \"\"\"\n    if not graph.directed and in_out == \"in\" or not graph.directed and in_out == \"out\":\n        raise TypeError(\"Graph must be directed for in- or out- degree.\")\n\n    # Restrict graph to specified time interval\n    if intervals:\n        graph = graph.get_temporal_subgraph(intervals)\n\n    # Only calculate for specified labels\n    if not labels:\n        labels = graph.nodes.labels()   # If labels not specified, set labels to all nodes in input graph\n\n    # Initialize\n    node_count = {node: 0 for node in labels}\n\n    # Calculate total degree for each node\n    for edge in graph.edges.aslist():       # Increment temporal degree every time node is seen as endpoint of edge\n\n        if not graph.directed:                          # Undirected graph - normal degree centrality\n            node_count[edge.node1.label] += 1\n            node_count[edge.node2.label] += 1\n\n        if graph.directed and in_out == \"in\":           # Directed graph - in-degree\n            node_count[edge.node2.label] += 1\n\n        if graph.directed and in_out == \"out\":          # Directed graph - out-degree\n            node_count[edge.node1.label] += 1\n\n    # Calculate average over snapshots\n    graph_age = graph.edges.end() - graph.edges.start()\n    temporal_degree_centrality = {label: value / graph_age for label, value in node_count.items()}\n\n    # Add data to nodes\n    if add_data:\n        for node in graph.nodes.set:\n            node.data[\"degree\"] = temporal_degree_centrality[node.label]\n\n    # Only return values for specified labels\n    if labels:\n        temporal_degree_centrality = {label: value for label, value in temporal_degree_centrality.items() if label in labels}\n\n    return temporal_degree_centrality",
        "sha1": "5d77deaec7de45ae52c5257ef4900f98fa360021",
        "id": 431896
    },
    {
        "content": "import struct\n\n\ndef uint32_unpack(buf):\n    \"\"\"Unpack 32 bit integer from a stream\"\"\"\n    return struct.unpack('<L', buf)[0]",
        "sha1": "eac54b24cd84d826d95386cf797f46ad0997b178",
        "id": 640798
    },
    {
        "content": "from typing import AnyStr\nimport locale\n\n\ndef to_string(path: AnyStr) -> str:\n    \"\"\"Return the string representation of a byte string using the preferred\n     encoding, or the string itself if path is a str.\"\"\"\n    if isinstance(path, bytes):\n        return path.decode(locale.getpreferredencoding(False))\n    return path",
        "sha1": "be14e43b9872f639d18c1de584f7e133f05b6f92",
        "id": 222292
    },
    {
        "content": "def hexStrToInt(inputstr):\n\t\"\"\"\n\tConverts a string with hex bytes to a numeric value\n\tArguments:\n\tinputstr - A string representing the bytes to convert. Example : 41414141\n\n\tReturn:\n\tthe numeric value\n\t\"\"\"\n\tvaltoreturn = 0\n\ttry:\n\t\tvaltoreturn = int(inputstr,16)\n\texcept:\n\t\tvaltoreturn = 0\n\treturn valtoreturn",
        "sha1": "33a54c1212b75caa961c667b4ca25c6d888517e5",
        "id": 621470
    },
    {
        "content": "def generate_model_masks(\n    depth,\n    mask = None,\n    masked_layer_indices = None):\n  \"\"\"Creates empty masks for this model, or initializes with existing mask.\n\n  Args:\n    depth: Number of layers in the model.\n    mask: Existing model mask for layers in this model, if not given, all\n      module masks are initialized to None.\n    masked_layer_indices: The layer indices of layers in model to be masked, or\n      all if None.\n\n  Returns:\n    A model mask, with None where no mask is given for a model layer, or that\n    specific layer is indicated as not to be masked by the masked_layer_indices\n    parameter.\n  \"\"\"\n  if depth <= 0:\n    raise ValueError(f'Invalid model depth: {depth}')\n\n  if mask is None:\n    mask = {f'MaskedModule_{i}': None for i in range(depth)}\n\n  # Have to explicitly check for None to differentiate from empty array.\n  if masked_layer_indices is not None:\n    # Check none of the indices are outside of model's layer bounds.\n    if any(i < 0 or i >= depth for i in masked_layer_indices):\n      raise ValueError(\n          f'Invalid indices for given depth ({depth}): {masked_layer_indices}')\n    mask = {\n        f'MaskedModule_{i}': mask[f'MaskedModule_{i}']\n        for i in masked_layer_indices\n    }\n\n  return mask",
        "sha1": "b7a56adacf77c1e7249ae53446032e9f877db65a",
        "id": 603024
    },
    {
        "content": "def _ValidateReplicationFlags(flag_dict):\n  \"\"\"Verifies correct usage of the bigtable replication flags.\"\"\"\n  return (not flag_dict['bigtable_replication_cluster'] or\n          flag_dict['bigtable_replication_cluster_zone'])",
        "sha1": "1462455171f631cf18dc2e09650df2322d451dd6",
        "id": 686151
    },
    {
        "content": "import re\n\n\ndef get_label_pattern(label):\n  \"\"\"Get the label pattern regex.\"\"\"\n  return re.compile('^' + re.sub(r'%.*?%', r'(.*)', label) + '$', re.IGNORECASE)",
        "sha1": "19a113acb7226e16677b342460feee70ef82ad15",
        "id": 61158
    },
    {
        "content": "def int_to_mac(i):\n    \"\"\"Converts integer representation of MAC address to hex string.\"\"\"\n    mac = format(i, 'x').zfill(12)\n    blocks = [mac[x:x + 2] for x in range(0, len(mac), 2)]\n    return ':'.join(blocks)",
        "sha1": "1be1b4fe955103b2af050f84cc4d03bd20b3ca6a",
        "id": 169146
    },
    {
        "content": "def blacklist(d, fields):\n    \"\"\"Blacklists a dictionary by keeping all EXCEPT the selected `fields`.\n    Non-destructive (creates and returns a new dict).\"\"\"\n    ret = type(d)()\n    fields = set(fields)\n    for k, v in d.iteritems():\n        if k not in fields:\n            ret[k] = v\n    return ret",
        "sha1": "9f168b7b6c782d8019c627b3dcce3895682bbcab",
        "id": 530606
    },
    {
        "content": "def retrieve_pendulum_data(filename = \"pendulum.dat\"):\n    \"\"\"\n    Gets pendulum data from filename and outputs a 2d list of\n    output[0] = L and output[1] = T\n    \"\"\"\n    L_list = []\n    T_list = []\n    with open(filename, 'r') as infile:\n        for line in infile:\n            data = line.split()\n            try:\n                L_list.append(float(data[0]))\n                T_list.append(float(data[1]))\n            except ValueError:\n                pass\n            \n    infile.close()\n    return [L_list, T_list]",
        "sha1": "8d8fdc0e7cb042881e6d1be5b2453985cf8a9481",
        "id": 671565
    },
    {
        "content": "def sort(seq):\n    \"\"\"\n    Takes a list of integers and sorts them in ascending order. This sorted\n    list is then returned.\n\n    :param seq: A list of integers\n    :rtype: A list of sorted integers\n    \"\"\"\n\n    gaps = [x for x in range(len(seq) // 2, 0, -1)]\n\n    for gap in gaps:\n        for i in range(gap, len(seq)):\n            temp = seq[i]\n            j = i\n            while j >= gap and seq[j - gap] > temp:\n                seq[j] = seq[j - gap]\n                j -= gap\n            seq[j] = temp\n\n    return seq",
        "sha1": "b2eeac775e68aa181a751354b3e5d46ee0f2f5a0",
        "id": 389746
    },
    {
        "content": "import torch\n\n\ndef _make_positions(tensor, padding_idx, left_pad):\n    \"\"\"Replace non-padding symbols with their position numbers.\n\n    Position numbers begin at padding_idx+1.\n\n    Padding symbols are ignored, but it is necessary to specify whether padding\n    is added on the left side (left_pad=True) or right side (left_pad=False).\n    \"\"\"\n    mask = tensor.ne(padding_idx).long()\n    return torch.cumsum(mask, dim=1) * mask + padding_idx",
        "sha1": "131411e8363d83ed5f18274d88a65cba3e7cafd3",
        "id": 249339
    },
    {
        "content": "import shutil\n\n\ndef binaries_check(app_configs, **kwargs):\n    \"\"\"\n    Papermerge requires the existence of a few binaries, so we do some checks\n    for those here.\n    \"\"\"\n\n    msg = {\n        \"tesseract\": \"Without it, OCR of the documents is impossible\",\n        \"pdfinfo\": \"Without it, Papermerge won't function properly\",\n        \"pdftk\": \"Without it, Papermerge won't be able to cut/paste PDF pages\"\n    }\n    error = \"Papermerge can't find {}. {}.\"\n    hint = \"Either it's not in your PATH or it's not installed.\"\n\n    check_messages = []\n    for binary in msg.keys():\n        if shutil.which(binary) is None:\n            check_messages.append(\n                Warning(\n                    error.format(binary, msg[binary]),\n                    hint\n                )\n            )\n\n    return check_messages",
        "sha1": "093599e5492cc8dfd1700d6585e3229743fbcb2f",
        "id": 451233
    },
    {
        "content": "def pyboilerplate() -> str:\n    \"\"\"Return a string.\n\n    Returns\n    -------\n    str\n        A string\n\n    Examples\n    --------\n    >>> pyboilerplate()\n    'pyboilerplate'\n    \"\"\"\n    return 'pyboilerplate'",
        "sha1": "8e9cdd1756b37f988fd8a4ed8c3e950f436c72ee",
        "id": 523287
    },
    {
        "content": "import requests\n\n\ndef _open_url(url):\n    \"\"\"Open a HTTP connection to the URL and return a file-like object.\"\"\"\n    response = requests.get(url, stream=True)\n    if response.status_code != 200:\n        raise IOError('Unable to download {}, HTTP {}'.format(url, response.status_code))\n    return response",
        "sha1": "67c8c4f05b7c26fb2ace01338bb8c10596474235",
        "id": 378036
    },
    {
        "content": "def convert_string_plus_value(values):\n    \"\"\"\n    Normalizes a list of string\n\n    :param values: A list of strings\n    :return: None, a single string or a list of strings\n    \"\"\"\n    if not values:\n        return None\n\n    if len(values) == 1:\n        return values[0]\n\n    return values",
        "sha1": "76b768b2adbf3d9a5aceb0ec1b844ed9cd61ee30",
        "id": 425924
    },
    {
        "content": "def is_increasing(arr):\n  \"\"\" Returns true if the sequence is increasing. \"\"\"\n  return all([x < y for x, y in zip(arr, arr[1:])])",
        "sha1": "76cb2f4b804456e64fab8b4ddaa060b7242dfca1",
        "id": 502056
    },
    {
        "content": "def is_valid_row(row):\n    \"\"\"check if a row is valid for transformation\"\"\"\n    if not row[\"PID\"]:\n        return False\n    if not row[\"document type\"]:\n        return False\n    if not row[\"URL\"]:\n        return False\n    return True",
        "sha1": "528836117b0561a318d427c42ef4eeb83e94bd08",
        "id": 235135
    },
    {
        "content": "def calculate_tensor_size_after_convs(input_size: int, sizes: list, strides: list):\n    \"\"\"helper method to calculate output size of an input into a conv net consisting of conv layers with filter `sizes`\n     and `strides`\"\"\"\n    t = input_size\n    for size, stride in zip(sizes, strides):\n        t = int((t - size) / stride + 1)\n    return t",
        "sha1": "48bb537b6bb48c434453ec3dec03c57106bf740a",
        "id": 155110
    },
    {
        "content": "def get_amazon_price(prod):\n    \"\"\"Get the price of a given Amazon product\"\"\"\n    # We will have to transform \",\" to \".\" if we use prices from France or Germany\n    # Moreover, we will have to remove the currency symbol\n    return float(\n        prod.find(\"span\", class_=\"a-price\")\n        .find(\"span\", class_=\"a-offscreen\")\n        .text.replace(\",\", \".\")\n        .strip()\n        .strip(\"\u00a3\u20ac\")\n    )",
        "sha1": "1060d7605e28d1eec8c29bb7c8dd05b951035766",
        "id": 360403
    },
    {
        "content": "def get_row_index_with_max_edges(d: int) -> int:\n    \"\"\"\n    Return the row index of any benchmark CSV file with given dimension.\n    The number of nodes considered \"inseresting\" are:\n    - 14\n    - 16\n    - 22\n    - 52\n    - 202\n    - 1000\n\n    :param d: graph dimension in term nodes\n    :return: index of line with given graph dimension\n    \"\"\"\n\n    # these values correspond to the (line - 2) of the CSV line\n    obj = {\n        14: 1,\n        16: 11,\n        22: 12,\n        52: 0,\n        202: 6,\n        1000: 4,\n    }\n\n    return obj[d]",
        "sha1": "b68524026fa8d95daefe36b77d030e161ffe7b06",
        "id": 561107
    },
    {
        "content": "def smooth_columns(input_frame):\n    \"\"\"\n    Returns an argued DataFrame with columns names made lowercase and with underscores removed.\n    \"\"\"\n    column_labels = list(input_frame.columns)\n    input_frame.columns = [c.lower().replace('_','') for c in column_labels]\n    return input_frame",
        "sha1": "abf43fe866b3e0aef8ef0b27cf4da34db89576a8",
        "id": 202136
    },
    {
        "content": "def list_contacts(client):\n    \"\"\"\n    List domain contacts.\n    \"\"\"\n    return client.domain.contacts.all()",
        "sha1": "f8dd19f389567e51299329e1a495a38f08ddbac4",
        "id": 274745
    },
    {
        "content": "def interpolation(x0, y0, x1, y1, x):\n    \"\"\"\n    Performs interpolation.\n\n    Parameters\n    ----------\n    x0 : float.\n        The coordinate of the first point on the x axis.\n    y0 : float.\n        The coordinate of the first point on the y axis.\n    x1 : float.\n        The coordinate of the second point on the x axis.\n    y1 : float.\n        The coordinate of the second point on the y axis.\n    x : float.\n        A value in the interval (x0, x1).\n\n    Returns\n    -------\n    float.\n        Is the interpolated  or extrapolated value.\n\n    Examples\n    --------\n    - interpolation 1: (30, 3, 40, 5, 37) -> 4.4\n    - interpolation 2: (30, 3, 40, 5, 35) -> 4.0\n\n    \"\"\"\n\n    return y0 + (y1 - y0) * ((x - x0) / (x1 - x0))",
        "sha1": "7e78c5f9cb697c58d6a7ef405cc8e37cff84641d",
        "id": 647789
    },
    {
        "content": "import struct\n\n\ndef read_vary(stream, data_size_type=\"B\"):\n    \"\"\"!\n    @brief Read variable length data from stream.\n\n    @param stream Data stream.\n    @param data_size_type Type of data size in Python's struct module representation.\n    @return Data in byte array.\n    \"\"\"\n    # Read data size\n    data_size_size = struct.calcsize(data_size_type)\n    data_size = struct.unpack(data_size_type, stream.read(data_size_size))[0]\n    # Read data\n    return bytearray(stream.read(data_size))",
        "sha1": "9aa8a29470dad880b3f9b718a34c80eb28e2110e",
        "id": 40931
    },
    {
        "content": "def dead_code_remark(dead_code):\n    \"\"\"Generate remark for dead code detection.\"\"\"\n    if dead_code[\"display_results\"]:\n        if dead_code[\"failed\"] != 0:\n            return \"<li>remove dead code</li>\"\n        else:\n            return \"\"\n    else:\n        return \"<li>setup dead code detection tool</li>\"",
        "sha1": "cc122cb5398d5e737eacf418f125c8a0a0311c9b",
        "id": 451693
    },
    {
        "content": "def buscar_vuelos_escala(vuelos: dict, origen: str, destino: str) -> list:\n    \"\"\" Buscar vuelos (con escala)\n    Par\u00e1metros:\n      vuelos (dict): Es un diccionario de diccionarios con la informaci\u00f3n de los vuelos.\n      origen (str): El c\u00f3digo del aeropuerto de origen\n      destino (str): El c\u00f3digo del aeropuerto de destino\n    Retorno:\n      list: Retorna una lista con los itinerarios posibles entre el origen y el destino. Cada itinerario debe\n            ser una lista con los c\u00f3digos de los vuelos que componen el itinerario. Si el itinerario no tiene\n            escalas, tendr\u00e1 un solo elemento (el c\u00f3digo del vuelo directo). Si el itinerario tiene una escala\n            tendr\u00e1 dos c\u00f3digos (el c\u00f3digo del primer vuelo seguido del c\u00f3digo del segundo vuelo).\n    \"\"\"\n    lista = []\n    if vuelos != {}:\n        # Vuelos directos.\n        for codigo_vuelo in vuelos:\n            if vuelos[codigo_vuelo]['origen'] == origen and vuelos[codigo_vuelo]['destino'] == destino:\n                lista.append([codigo_vuelo])\n        # Vuelos con una escala\n        codigos_origen = []\n        codigos_destino = []\n        for codigo_vuelo in vuelos:\n            diccionario = vuelos[codigo_vuelo]\n            if diccionario['origen'] == origen and diccionario['destino'] != destino:\n                codigos_origen.append(codigo_vuelo)\n            if diccionario['destino'] == destino and diccionario['origen'] != origen:\n                codigos_destino.append(codigo_vuelo)\n        for a in codigos_origen:\n            for b in codigos_destino:\n                llegada_a = (vuelos[a]['salida'] + vuelos[a]['duracion'] + vuelos[a]['retraso']) / 60\n                salida_b = (vuelos[b]['salida']) / 60\n                if llegada_a < salida_b:\n                    if vuelos[a]['destino'] == vuelos[b]['origen']:\n                        lista.append([a, b])\n    return lista",
        "sha1": "2231d18c60b1bf6d0553e75dc449ea27688ba21d",
        "id": 647012
    },
    {
        "content": "def add_one(number):\n    \"\"\"\n    Example of a simple function.\n\n    Parameters\n    ----------\n    number: int, float, str\n\n    Returns\n    -------\n    out: int, float, str\n        The input value plus one. Raises TypeError if the input is not\n        the expected type\n    \"\"\"\n\n    if isinstance(number, (float, int)):\n        return number + 1\n    elif isinstance(number, (str)):\n        return number + '1'\n    else:\n        raise TypeError('Expecting an int, float or string.')",
        "sha1": "d24a5d9e1a02098d1a6638bdc8b5493bc4d732e2",
        "id": 21016
    },
    {
        "content": "import requests\n\n\ndef query(data_type, uid):\n    \"\"\"\n    :param data_type: the Pharos data type (e.g. ligand, target ..)\n    :param uid: Pharos concept identifier\n    :return: JSON containing the query results\n    \"\"\"\n    url = \"https://pharos.nih.gov/idg/api/v1/{}({})?view=full\".format(data_type, uid)\n    try:\n        resp = requests.get(url).json()\n    except:\n        resp = None\n    return resp",
        "sha1": "3ef9bb23e4f330e4dc2c348ebfeb9d59e21d7005",
        "id": 183900
    },
    {
        "content": "def splitdrive(p):\n    \"\"\"Split a pathname into drive and path specifiers. Returns a 2-tuple\n    \"(drive,path)\";  either part may be empty\"\"\"\n    if p[1:2] == ':':\n        return (p[0:2], p[2:])\n    return ('', p)",
        "sha1": "ebe7bd94204459eb0451fa7cec9dc5c3f01344ec",
        "id": 531899
    },
    {
        "content": "def reverse_url(context, name, **parts):\n    \"\"\"\n    jinja2 filter for generating urls,\n    see http://aiohttp.readthedocs.io/en/stable/web.html#reverse-url-constructing-using-named-resources\n\n    Usage:\n\n      {{ 'the-view-name'|url }} might become \"/path/to/view\"\n\n    or with parts and a query\n\n      {{ 'item-details'|url(id=123, query={'active': 'true'}) }} might become \"/items/1?active=true\n\n    see app/templates.index.jinja for usage.\n\n    :param context: see http://jinja.pocoo.org/docs/dev/api/#jinja2.contextfilter\n    :param name: the name of the route\n    :param parts: url parts to be passed to route.url(), if parts includes \"query\" it's removed and passed seperately\n    :return: url as generated by app.route[<name>].url(parts=parts, query=query)\n    \"\"\"\n    app = context['app']\n\n    kwargs = {}\n    if 'query' in parts:\n        kwargs['query'] = parts.pop('query')\n    if parts:\n        kwargs['parts'] = parts\n    return app.router[name].url(**kwargs)",
        "sha1": "963737f6fe4ee2fb3a79bb419051e815295253ef",
        "id": 636421
    },
    {
        "content": "def is_true(s):\n    \"\"\"Case insensitive string parsing helper. Return True for true (case insensitive matching), False otherwise.\"\"\"\n    return s.lower() == 'true'",
        "sha1": "6123289ee2958b2bc4e96b7b75950f113173acf9",
        "id": 506226
    },
    {
        "content": "def get_values_matching_key(doc, key):\n    \"\"\"\n    Returns iterator of values in 'doc' with the matching 'key'.\n    \"\"\"\n\n    def _get_values(doc, key):\n        if doc is not None:\n            if key in doc:\n                yield doc[key]\n\n            for z in doc.items():\n                v = z[1]\n                if isinstance(v, dict):\n                    for item in _get_values(v, key):\n                        yield item\n                elif isinstance(v, list):\n                    for i in v:\n                        for j in _get_values(i, key):\n                            yield j\n\n    return _get_values(doc, key)",
        "sha1": "da7beea651f2e8ffa461c9f60d577628815d323d",
        "id": 666121
    },
    {
        "content": "def complement(sequence):\n\t\"\"\"Get complementary DNA sequence.\n\n\tArgs:\n\t\tsequence (str): DNA sequence:\n\n\tReturns:\n\t\tstr: Complementary sequence.\n\t\"\"\"\n\tcomplement_bases = {'A': 'T','T': 'A', 'C': 'G', 'G': 'C'}\n\treturn ''.join(complement_bases[base] for base in sequence.upper())",
        "sha1": "dc9c4fd6e8c140a9c41195055027bd8e55bb4223",
        "id": 342569
    },
    {
        "content": "def choose_poi_and_neighbors(gdf, placekey, neighbor_radius, projection = 'EPSG:3857'):\n    \"\"\"\n    Identifies the \"target\" POI and its nearby neighbors, based on the `neighbor_radius` parameter (by default expressed in meters).\n    \"\"\"\n\n    # classify Neighbors v. Target POI\n    gdf['POI'] = 'Neighbor'\n    gdf.loc[gdf['placekey'] == placekey, 'POI'] = 'Target'\n    \n    # transform to a projected coordinate reference system\n    gdf_proj = gdf.to_crs(projection)\n    \n    # get the buffer for filtering neighbors\n    target = gdf_proj.loc[gdf_proj['POI'] == 'Target']\n    target_buffer = target.geometry.buffer(neighbor_radius)\n    \n    # find the neighbors\n    output_proj = gdf_proj.loc[gdf_proj.intersects(target_buffer.unary_union)]\n    \n    # transform back to original coordinate reference system\n    output = output_proj.to_crs(gdf.crs)\n    \n    return output",
        "sha1": "1f6b945023ef480e7f6433979d6b6b7b723c34c3",
        "id": 105989
    },
    {
        "content": "import re\n\n\ndef is_idiom(phrase):\n    \"\"\"\n        Checks if a phrase meets certain criteria to make it an idiom\n    \"\"\"\n    if re.match('Category:English|Citation:|Appendix:', phrase):\n        return False\n    # One word phrases\n    if re.match(r\"^[\\w\\-\\']+$\", phrase):\n        return False\n    # Two-worded phrases\n    if re.match(r\"^[\\w\\-\\']+ [\\w\\-\\']+$\", phrase):\n        return False\n    # Similes\n    if 'as a' in phrase:\n        return False\n    return True",
        "sha1": "0c6efef4fdff5a9e5b0fd1250dfa569175110e50",
        "id": 297770
    },
    {
        "content": "import re\n\n\ndef word_replace(re_map, word):\n    \"\"\"Regex replace all occurrences of keys in re_map with their value.\"\"\"\n    for key, value in re_map.items():\n        word = re.sub(key, value, word, flags=re.UNICODE)\n    return word",
        "sha1": "5db4e3fbe347e6f56b008a74874432df3ce5654a",
        "id": 146345
    },
    {
        "content": "def lorenz(x: float, x0: float, fwhm: float, a: float, o: float) -> float:\n    \"\"\"\n    Lorenzian function\n    Args:\n        x: frequency(s) to evaluate function\n        x0: center frequency of resonance\n        fwhm: full-width-half-max of resonance\n        a: amplitude of function\n        o: offset of function from 0\n\n    Returns:\n        o + a /((x-x0)**2 + (fwhm)**2)\n    \"\"\"\n    return o + a / ((x-x0)**2 + (fwhm)**2)",
        "sha1": "34b072443a4b84923a246384ebcea15a7d2445c7",
        "id": 151288
    },
    {
        "content": "def check_length_of_shape_or_intercept_names(name_list,\n                                             num_alts,\n                                             constrained_param,\n                                             list_title):\n    \"\"\"\n    Ensures that the length of the parameter names matches the number of\n    parameters that will be estimated. Will raise a ValueError otherwise.\n\n    Parameters\n    ----------\n    name_list : list of strings.\n        Each element should be the name of a parameter that is to be estimated.\n    num_alts : int.\n        Should be the total number of alternatives in the universal choice set\n        for this dataset.\n    constrainted_param : {0, 1, True, False}\n        Indicates whether (1 or True) or not (0 or False) one of the type of\n        parameters being estimated will be constrained. For instance,\n        constraining one of the intercepts.\n    list_title : str.\n        Should specify the type of parameters whose names are being checked.\n        Examples include 'intercept_params' or 'shape_params'.\n\n    Returns\n    -------\n    None.\n    \"\"\"\n    if len(name_list) != (num_alts - constrained_param):\n        msg_1 = \"{} is of the wrong length:\".format(list_title)\n        msg_2 = \"len({}) == {}\".format(list_title, len(name_list))\n        correct_length = num_alts - constrained_param\n        msg_3 = \"The correct length is: {}\".format(correct_length)\n        total_msg = \"\\n\".join([msg_1, msg_2, msg_3])\n        raise ValueError(total_msg)\n\n    return None",
        "sha1": "d83ed7d6989c7e3ccdbbb256eaa72759a7f242d3",
        "id": 28167
    },
    {
        "content": "import re\n\n\ndef is_release_identifier(text):\n    \"\"\"\n    >>> is_release_identifier('utf-8')\n    False\n    >>> is_release_identifier('killbill-0.1.66')\n    True\n    >>> is_release_identifier('jargo-parent-0.1.1')\n    True\n    \"\"\"\n\n    return bool(re.match(r'''\n        [\\w\\-_]+\n        # Copy-pasted is_version_number() regex\n        \\d+              # Major number\n        (?: [.] \\d+)?    # Minor number\n            [.] \\d+      # Patch number\n        (?: [\\-.](?:\\w+|rc[.]?\\d+))*  # Tag\n    ''', text, re.VERBOSE))",
        "sha1": "5f0599d4c18a82aecff5be36059401f5f3c37a38",
        "id": 196120
    },
    {
        "content": "import re\n\n\ndef get_fortfloat(key, txt, be_case_sensitive=True):\n    \"\"\"\n    Matches a fortran compatible specification of a float behind a defined key in a string.\n    :param key: The key to look for\n    :param txt: The string where to search for the key\n    :param be_case_sensitive: An optional boolean whether to search case-sensitive, defaults to ``True``\n\n    If abc is a key, and f is a float, number, than this regex\n    will match t and return f in the following cases:\n\n    *   charsbefore, abc = f, charsafter\n    *   charsbefore\n        abc = f\n        charsafter\n    *   charsbefore, abc = f\n        charsafter\n\n    and vice-versa.\n    If no float is matched, returns None\n\n    Exampes of matchable floats are:\n\n    *   0.1d2\n    *   0.D-3\n    *   .2e1\n    *   -0.23\n    *   23.\n    *   232\n    \"\"\"\n    pattern = \"\"\"\n        [\\n,]                       # key - value pair can be prepended by comma or start\n        [ \\t]*                      # in a new line and some optional white space\n        {}                          # the key goes here\n        [ \\t]*                      # Optional white space between key and equal sign\n        =                           # Equals, you can put [=:,] if you want more specifiers\n        [ \\t]*                      # optional white space between specifier and float\n        (?P<float>                  # Universal float pattern\n            ( \\d*[\\.]\\d+  |  \\d+[\\.]?\\d* )\n            ([ E | D | e | d ] [+|-]? \\d+)?\n        )\n        [ \\t]*[,\\n,#]               # Can be followed by comma, end of line, or a comment\n        \"\"\".format(key)\n    REKEYS = re.X | re.M if be_case_sensitive else re.X | re.M | re.I\n    match = re.search(\n        pattern,\n        txt,\n        REKEYS)\n    if not match:\n        return None\n    else:\n        return float(match.group('float').replace('d', 'e').replace('D', 'e'))",
        "sha1": "c0171ab82196f9cffc0b66dfeeb7a766ff73dfe9",
        "id": 688764
    },
    {
        "content": "def calc_centroids(triangles):\n    \"\"\" Calculates the centroids of the of triangles of the convex hull.\n\n    Parameters\n    ----------\n    triangles : an array of shape (N, 3, 3)\n        consisting of N triangles defined by 3 points in 3D\n\n    Returns\n    ----------\n    centroids : an array of shape (N, 3)\n        consisting of the N centroids of the triangles in 3D\n\n    \"\"\"\n\n    return triangles.sum(axis=1)/3.0",
        "sha1": "feadb78d4308068989ac33137335175cfad0b352",
        "id": 362127
    },
    {
        "content": "def equivalent(list1, list2):\n    \"\"\"Ensures that two lists are equivalent, i.e., contain the same items.\"\"\"\n    if len(list1) != len(list2):\n        return False\n    set1 = set(list1)\n    for item in list2:\n        if item not in set1:\n            return False\n    return True",
        "sha1": "2a14eab905143bd7549e5e8edc6242ea78351c46",
        "id": 277878
    },
    {
        "content": "def getObjs(objs, attr, val=''):\n    \"\"\"\n    one liner to get list of all the objects whose attribute attr has value val\n    Example call: getObjs(objs, 'class', 'load')\n    \"\"\"\n    if val:\n        return [obj for obj in objs if attr in obj and obj[attr] == val]\n    else:\n        return [obj for obj in objs if attr in obj]",
        "sha1": "6e9dc8698fbf06ee5dc70452088d541c89b79c31",
        "id": 375037
    },
    {
        "content": "def insert_nulls(df, fill_val=-99999999.99):\n    \"\"\"replaces fill_val with null in all columns of df.\n\n    :df: Dataframe\n    :fill_val: fill value to be replaced with nulls. \n        default=-99999999.99\n    :returns: Dataframe with fill_val replaced with nulls\n\n    \"\"\"\n\n    for col in df.columns.tolist():\n        df.ix[df[col] == fill_val, col] = None\n\n    return df",
        "sha1": "71fdf29a16916ee1f119b5267043960c9a6d5227",
        "id": 56120
    },
    {
        "content": "def date_in_yymmdd(date_string):\n    \"\"\"\n    Convert date string YYYYMMDD into YY_MM_DD format to be used for\n    drive log file names.\n\n    Parameters\n    ----------\n    date_string: in format YYYYMMDD\n\n    Returns\n    -------\n    yy_mm_dd: date_string in format YY_MM_DD\n\n    \"\"\"\n    date = list(date_string)\n    year = \"\".join(date[2:4])\n    month = \"\".join(date[4:6])\n    day = \"\".join(date[6:8])\n    return f\"{year}_{month}_{day}\"",
        "sha1": "cb436a6aca1301104b05e8ac82017defb3c155cd",
        "id": 227777
    },
    {
        "content": "import pickle\n\n\ndef read_object(filename):\n    \"\"\"\n    Function to read/un-pickle python object\n\n    :param filename: path to pickle file\n\n    \"\"\"\n\n    with open(filename, 'rb') as input_stream:\n        obj = pickle.load(input_stream)\n    return obj",
        "sha1": "b0c0e510bb3029055036df6b38f7653c6affc0f0",
        "id": 106270
    },
    {
        "content": "import yaml\n\n\ndef read_yaml(fpath):\n    \"\"\"Reads YAML file from a path.\"\"\"\n    with open(fpath, 'r') as f:\n        obj = yaml.safe_load(f)\n    return obj",
        "sha1": "eda18b11774b324bff26dd0ba36d86488929821e",
        "id": 327511
    },
    {
        "content": "def sort_streams_on_start_point(stream_array):\n    \"\"\"\n    sorts an array of streams on start point = [0]\n    \"\"\"\n    return sorted(stream_array, key=lambda l: l[0])",
        "sha1": "f69214cae75dd86ed8610ca2531ef988b950b2a4",
        "id": 265458
    },
    {
        "content": "def binary_search(arr, target):\n    \"\"\"\n    ATTENTION: THE PROVIDED ARRAY MUST BE SORTED!\n    Searches for an item using binary search algorithm. Run time: O(log n)\n    \"\"\"\n    if arr == []:\n        return False\n    mid_ind = int(len(arr) / 2)\n    if (target < arr[mid_ind]):\n        return binary_search(arr[:mid_ind], target)\n    elif target > arr[mid_ind]:\n        return binary_search(arr[mid_ind + 1:], target)\n    return True",
        "sha1": "3062250caa67cbe2164713751af6eea23f0c9b60",
        "id": 129086
    },
    {
        "content": "def wrap_url(s, l):\n    \"\"\"Wrap a URL string\"\"\"\n    parts = s.split('/')\n\n    if len(parts) == 1:\n        return parts[0]\n    else:\n        i = 0\n        lines = []\n        for j in range(i, len(parts) + 1):\n            tv = '/'.join(parts[i:j])\n            nv = '/'.join(parts[i:j + 1])\n\n            if len(nv) > l or nv == tv:\n                i = j\n                lines.append(tv)\n\n        return '/\\n'.join(lines)",
        "sha1": "4da93ee941b26711bc21a0653a4307a3edc72a64",
        "id": 67626
    },
    {
        "content": "def unpack(seq):\n    \"\"\" Unpack sequence of length one\n\n    >>> unpack([1, 2, 3])\n    [1, 2, 3]\n\n    >>> unpack([1])\n    1\n    \"\"\"\n    seq = list(seq)\n    if len(seq) == 1:\n        seq = seq[0]\n    return seq",
        "sha1": "f14b74d17e0f63aa63e88b79be71611dfa6d051b",
        "id": 433712
    },
    {
        "content": "def get_min_key_for_max_value(d):\n    \"\"\"\n    For a dictionary d, get the key for the largest value.\n    If largest value is attained several times, get the \n    smallest respective key.\n    \"\"\"\n    \n    sorted_keys = sorted([key for key in d.keys()])\n    values = [d[key] for key in sorted_keys]\n    max_value = max(values)\n    for key, value in zip(sorted_keys, values):\n        if value == max_value:\n            return key",
        "sha1": "af5dfb5f9471e3b4757e129cc81eb0f89579c7fc",
        "id": 671634
    },
    {
        "content": "def function(ydata,x):\n    \n    \"\"\" Returns x index value of ydata array \"\"\"\n    \n    return ydata[x]",
        "sha1": "de77963f8626625336281a03b593e50bb8defdb2",
        "id": 378265
    },
    {
        "content": "def normalize_newlines(s):\n    \"\"\"Normalize line breaks in a string.\"\"\"\n    return s.replace('\\r\\n', '\\n').replace('\\r', '\\n') if s else s",
        "sha1": "5aba7b6d95f659735d3b8be27814dfcbc3cedc2b",
        "id": 125356
    },
    {
        "content": "def addChildNode(node, name, obj=None):\n    \"\"\"\n    Use this to build paths to your plugin's endpoints.\n\n    :param node: The parent node to add the child node to.\n    :param name: The name of the child node in the URL path.\n    :type name: str\n    :param obj: The object to place at this new node, or None if this child\n                should not be exposed as an endpoint, instead just used as\n                an intermediary hidden node.\n    :type obj: object or None\n    :returns: The node that was created.\n    \"\"\"\n    if obj:\n        setattr(node, name, obj)\n        return obj\n    else:\n        hiddenNode = type('', (), dict(exposed=False))()\n        setattr(node, name, hiddenNode)\n        return hiddenNode",
        "sha1": "7ac31e965dfa4f33c40040677ace07fb9439e3d1",
        "id": 108593
    },
    {
        "content": "def _direction_to_index(direction):\n    \"\"\"Map direction identifier to index.\n    \"\"\"\n    directions = {-1: 0, 0: slice(None), 1: 1, '<=': 0, '<=>': slice(None), '>=': 1}\n\n    if direction not in directions:\n        raise RuntimeError('Unknown direction \"{:d}\".'.format(direction))\n\n    return directions[direction]",
        "sha1": "1612c7107601a6166e478815b89725701f8dc3ff",
        "id": 675621
    },
    {
        "content": "def calculate_mm_volumes(num_rxns): \n    \"\"\" calculate_mm_volumes\n\n    Description: Calculates volumes of reagents needed to make master mix depending on number of reactions (num_rxns)\n\n    Parameters: \n        num_rxns: (int) number of rxns to perform (1-96) \n\n    Output: \n        mm_volumes_dict: dictionatry of master mix source wells to volumes\n            NOTE: reagent source rack contains 5 1.5mL tubes \n                A1 - RP Primer\n                A2 - 5x Multimodal RT Buffer\n                A3 - Nuclease-free Water\n                A4 - Rnase Inhibitor\n                A5 - EZ Reverse Transcriptase\n\n    \"\"\"\n    rp_primer_vol = (num_rxns * 1) * 1.1\n    multi_buff_5x_vol = (num_rxns * 4) * 1.1\n    nuc_free_water_volume = (num_rxns * 8) * 1.1\n    rnase_inhibitor_vol = (num_rxns * 1) * 1.1\n    ez_rev_trans_vol = (num_rxns * 1) * 1.1\n\n    mm_volumes_dict = {\n        'A1': rp_primer_vol,\n        'A2': multi_buff_5x_vol,\n        'A3': nuc_free_water_volume,\n        'A4': rnase_inhibitor_vol,\n        'A5': ez_rev_trans_vol,\n    }\n\n    return mm_volumes_dict",
        "sha1": "d6ca3d377eff2d4c61b8fb1fb560039d3de8d34d",
        "id": 174441
    },
    {
        "content": "def zoo(total_head, total_legs, animal_legs=[2, 4]):\n    \"\"\"\n    Find the number of kangaroo and tiger in a zoo.\n    For example:\n\n    zoo(6, 16) -> (4, 2)\n    zoo(8, 20, [4, 2, 2]) -> (2, 0, 6)\n\n    Parameters \n    ----------\n    total_head: int \n                Total number of animals\n    total_legs: int \n                Total number of animals'legs\n    animal_legs: list \n                 A list of animal legs. The length of the list is either 2 or 3.\n\n    Returns \n    -------\n    tuple\n        The number of animals in each animal type (based on the given \n        animal_legs). Return None if there is no solution.\n    \"\"\"\n    animal_num = len(animal_legs)\n    if animal_num == 2:\n        animal_0_leg = animal_legs[0]\n        animal_1_leg = animal_legs[1]\n\n        for animal_0_num in range(total_head + 1):\n            animal_1_num = total_head - animal_0_num\n            animal_total_legs = animal_0_leg * animal_0_num \\\n                                + animal_1_leg * animal_1_num\n            if animal_total_legs == total_legs:\n                return animal_0_num, animal_1_num \n        return None, None\n    elif animal_num == 3:\n        animal_0_leg = animal_legs[0]\n        animal_1_leg = animal_legs[1]        \n        animal_2_leg = animal_legs[2]\n\n        for animal_0_num in range(total_head + 1):\n            for animal_1_num in range(total_head + 1 - animal_0_num):\n                animal_2_num = total_head - animal_0_num - animal_1_num\n                animal_total_legs = animal_0_leg * animal_0_num \\\n                                    + animal_1_leg * animal_1_num \\\n                                    + animal_2_leg * animal_2_num\n                if animal_total_legs == total_legs:\n                    return animal_0_num, animal_1_num, animal_2_num \n        return None, None, None \n    else:\n        print(\"The allowed number of animal types is 2 or 3 only.\")",
        "sha1": "e24b678ebaea8ef78a93bbfb8abe1e98b560bb06",
        "id": 650425
    },
    {
        "content": "def GetLeadSpaces(title):\n    \"\"\"Return the leading spaces of the string\"\"\"\n    spaces = ''\n\n    for i in range(0, len(title)):\n        if not title[i].isspace():\n            break\n\n        spaces += title[i]\n\n    return spaces",
        "sha1": "1253f026e47cd2007b8219cf712f50334924f971",
        "id": 250507
    },
    {
        "content": "def str_to_bool(string, truelist=None):\n    \"\"\"Returns a boolean according to a string. True if the string\n    belongs to 'truelist', false otherwise.\n    By default, truelist has \"True\" only.\n    \"\"\"\n    if truelist is None:\n        truelist = [\"True\"]\n    return string in truelist",
        "sha1": "bd43763fc211e555b977953ef574d64a2d82eee1",
        "id": 678409
    },
    {
        "content": "def get_next_url(urls, path):\n    \"\"\"\n    Returns the URL of the next step in the voice cloning process.\n\n    Parameters\n    ----------\n    urls : dict\n        Frontend url paths and names\n    path : str\n        Current URL\n\n    Returns\n    -------\n    str\n        URL of next step or '' if not found\n    \"\"\"\n    urls = list(urls.keys())\n    next_url_index = urls.index(path) + 1\n    return urls[next_url_index] if next_url_index < len(urls) else \"\"",
        "sha1": "c6e7087b0ee8529e7a7de60129e169301363095d",
        "id": 464508
    },
    {
        "content": "import re\n\n\ndef divWithClass_has_styles(s,classname, styleString):\n    \"\"\"\n    Checks whether a div with some class attribute bears a set of other\n    attributes.\n    @param s string to be searched\n    @param classname the value of the class attribute\n    @param styleString a sequence of style stances separated by semicolons\n    @return True when the div opening tag bears all the requested\n    styles, independently of their order\n    \"\"\"\n    pattern=re.compile(r'<div [^>]*class=\"'+classname+r'\"[^>]*>', re.MULTILINE)\n    found=pattern.findall(s)\n    if not found:\n        return False\n    found=found[0]\n    foundStyle=re.match(r'''.*style ?= ?[\"']([^\"']*)[\"'].*''', found)\n    if not foundStyle:\n        return False\n    foundStyle=foundStyle.group(1)\n    foundStyle  = map(lambda x: x.replace(' ',''), foundStyle.split(';'))\n    styleString = map(lambda x: x.replace(' ',''), styleString.split(';'))\n    return set(foundStyle)==set(styleString)",
        "sha1": "595b04dd87cf3345d15fe84b62d9f0df158c5749",
        "id": 470550
    },
    {
        "content": "import re\n\n\ndef _retrieve_token(request):\n    \"\"\"Retrieve NEXUS token from the request header.\"\"\"\n    auth_string = request.headers.get('Authorization')\n    try:\n        match = re.match(\"Bearer (.+)\", auth_string)\n    except TypeError:\n        match = None\n    if match:\n        return match.groups()[0]",
        "sha1": "2efe02483642124d550d9354e7e0304b3e5cebeb",
        "id": 277062
    },
    {
        "content": "def bandwidth_converter(\n    number, *, from_unit, to_unit, from_time=\"seconds\", to_time=\"seconds\"\n):\n    \"\"\"\n    Bandwidth Calculator.\n\n    Convert data rate from one unit to another.\n\n    Arguments:\n        number     (int): number to be converted\n\n    Keyword arguments:\n        from_unit  (str): convert from this data unit. Example:\n                          (bps, Kbps, Mbps, Gbps... KB, KiB, MB, MiB...)\n        to_unit    (str): convert to this data unit. Example:\n                          (bps, Kbps, Mbps, Gbps... KB, KiB, MB, MiB...)\n\n    Keyword arguments (opt):\n        from_time  (str): Specify the time frame used in from_unit\n                          (seconds, minutes, hours, days, months)\n                          default: seconds\n        to_time    (str): Specify the time frame used in to_unit\n                          (seconds, minutes, hours, days, months)\n                          default: seconds\n\n    bps, Kbps, Mbps, Gbps... = decimal base = 1000^n\n    KB, MB, GB, TB...        = decimal base = 1000^n\n    KiB, MiB, GiB, TiB...    = binary base  = 1024^n\n\n    References:\n        - https://en.wikipedia.org/wiki/Units_of_information\n        - https://physics.nist.gov/cuu/Units/binary.html\n\n    Returns: tuple\n       (number_converted, to_unit/to_time)\n\n    Example:\n    >>> bandwidth_converter(100, from_unit=\"Mbps\", to_unit=\"MB\")\n    (12.5, 'MB/seconds')\n    >>> bandwidth_converter(100, from_unit=\"Mbps\", to_unit=\"GB\", to_time=\"hours\")\n    (45.0, 'GB/hours')\n    >>> bandwidth_converter(1, from_unit=\"Gbps\", to_unit=\"MB\")\n    (125.0, 'MB/seconds')\n    >>> bandwidth_converter(10, from_unit=\"Gbps\", to_unit=\"GB\")\n    (1.25, 'GB/seconds')\n    >>> bandwidth_converter(10, from_unit=\"Gbps\", to_unit=\"TB\", to_time=\"hours\")\n    (4.5, 'TB/hours')\n    >>> bandwidth_converter(10, from_unit=\"GB\", to_unit=\"Gbps\")\n    (80.0, 'Gbps/seconds')\n    >>> Convert 2.25 GB per hours to Mbps # doctest: +SKIP\n    >>> bandwidth_converter(2.25, from_unit=\"GB\", from_time=\"hours\", to_unit=\"Mbps\", to_time=\"seconds\") # noqa\n    (5.0, 'Mbps/seconds')\n    \"\"\"\n    unit_power = {\n        \"bps\": 1,\n        \"Kbps\": 1000,\n        \"Mbps\": 1000 ** 2,\n        \"Gbps\": 1000 ** 3,\n        \"Tbps\": 1000 ** 4,\n        \"Pbps\": 1000 ** 5,\n        \"Ebps\": 1000 ** 6,\n        \"Bytes\": 1,\n        \"KB\": 1000,\n        \"MB\": 1000 ** 2,\n        \"GB\": 1000 ** 3,\n        \"TB\": 1000 ** 4,\n        \"PB\": 1000 ** 5,\n        \"EB\": 1000 ** 6,\n        \"KiB\": 1024,\n        \"MiB\": 1024 ** 2,\n        \"GiB\": 1024 ** 3,\n        \"TiB\": 1024 ** 4,\n        \"PiB\": 1024 ** 5,\n        \"EiB\": 1024 ** 6,\n    }\n\n    time_in_sec = {\n        \"seconds\": 1,\n        \"minutes\": 60,\n        \"hours\": 3600,\n        \"days\": 3600 * 24,\n        \"months\": 3600 * 24 * 30,\n    }\n\n    if from_unit not in unit_power or to_unit not in unit_power:\n        raise ValueError(\n            \"invalid unit. It must be {}\".format(\", \".join(unit_power.keys()))\n        )\n\n    if from_time not in time_in_sec or to_time not in time_in_sec:\n        raise ValueError(\n            \"invalid time. It must be {}\".format(\", \".join(time_in_sec.keys()))\n        )\n\n    # Convert input number to bps\n    bps = (float(number) * int(unit_power[from_unit])) / time_in_sec[from_time]\n    if not from_unit.endswith(\"bps\"):\n        bps = bps * 8\n\n    # to_unit is bits or bytes\n    new_unit = bps if to_unit.endswith(\"bps\") else bps / 8\n    # Convert to new unit\n    new_unit = (new_unit / unit_power[to_unit]) * time_in_sec[to_time]\n\n    return new_unit, \"{}/{}\".format(to_unit, to_time)",
        "sha1": "93068fb52bc3cd04615749e61a6ecea54b2eda0f",
        "id": 645208
    },
    {
        "content": "def create_unbroadcast_axis(shape, broadcast_shape):\n  \"\"\"Creates the reduction axis for unbroadcasting.\n\n  Args:\n    shape: A list. The shape after the broadcast operation.\n    broadcast_shape: A list. The original shape the array being unbroadcast\n      had.\n  Returns:\n    A list. The axes along which the array needs to be reduced. These axes will\n    be distributed evenly into the original shape.\n  \"\"\"\n  return tuple(\n      -(1 + i)\n      for i in range(len(broadcast_shape))\n      if i >= len(shape) or broadcast_shape[-(1 + i)] > shape[-(1 + i)])",
        "sha1": "2758f1f1b993dfa7bdba10343cc9afde4cfcf38e",
        "id": 684249
    },
    {
        "content": "def intval(mystring):\n    \"\"\"Convert a string to an integer, representing errors by None\"\"\"\n    try:\n        retval = int(mystring)\n    except ValueError:\n        retval = None\n    return retval",
        "sha1": "23c0803c8d90787c5d6cac26598e031313fc1eff",
        "id": 512186
    },
    {
        "content": "def has_double_chars(entry):\n    \"\"\"Return True if there are double chars (aa, bb,...) in the given string.\"\"\"\n    for idx in range(1, len(entry)):\n        if entry[idx] == entry[idx - 1]:\n            return True\n    return False",
        "sha1": "dce735a78131a42e01acfeac159c03cebedbdb7b",
        "id": 231313
    },
    {
        "content": "def cut_length(cut):\n    \"\"\"\n    Convenience function for getting the length of a cut\n    :param cut: a Cut\n    :return: the length of the cut\n    \"\"\"\n    return cut.length",
        "sha1": "07557c36612e415835bbbb38b0821ce1b56b1da6",
        "id": 407483
    },
    {
        "content": "import re\n\n\ndef _clean_tags(text):  # pylint: disable=R0915\n    \"\"\" Clean known tags from the text.\n    \"\"\"\n    text = text.replace('[center]', '')\n    text = text.replace('[/center]', '')\n    text = text.replace('[right]', '')\n    text = text.replace('[/right]', '')\n    text = text.replace('[b]', '')\n    text = text.replace('[/b]', '')\n    text = text.replace('[i]', '')\n    text = text.replace('[/i]', '')\n    text = text.replace('[bi]', '')\n    text = text.replace('[/bi]', '')\n    text = text.replace('[u]', '')\n    text = text.replace('[/u]', '')\n    text = text.replace('[strike]', '')\n    text = text.replace('[/strike]', '')\n    text = text.replace('[red]', '')\n    text = text.replace('[/red]', '')\n    text = text.replace('[space]', '')\n    text = text.replace('[vspace]', '')\n    text = text.replace('[tab]', '')\n    text = text.replace('[nobr]', '')\n    text = text.replace('[inline]', '')\n    text = text.replace('[lsb]', '')\n    text = text.replace('[rsb]', '')\n    text = text.replace('[lfb]', '')\n    text = text.replace('[rfb]', '')\n\n    text = re.sub(r'\\[lotr [^\\]]+\\]', '', text)\n    text = re.sub(r'\\[lotrheader [^\\]]+\\]', '', text)\n    text = re.sub(r'\\[size [^\\]]+\\]', '', text)\n    text = re.sub(r'\\[defaultsize [^\\]]+\\]', '', text)\n    text = re.sub(r'\\[img [^\\]]+\\]', '', text)\n\n    text = text.replace('[/lotr]', '')\n    text = text.replace('[/lotrheader]', '')\n    text = text.replace('[/size]', '')\n\n    text = text.replace('[unique]', '')\n    text = text.replace('[threat]', '')\n    text = text.replace('[attack]', '')\n    text = text.replace('[defense]', '')\n    text = text.replace('[willpower]', '')\n    text = text.replace('[leadership]', '')\n    text = text.replace('[lore]', '')\n    text = text.replace('[spirit]', '')\n    text = text.replace('[tactics]', '')\n    text = text.replace('[baggins]', '')\n    text = text.replace('[fellowship]', '')\n    text = text.replace('[sunny]', '')\n    text = text.replace('[cloudy]', '')\n    text = text.replace('[rainy]', '')\n    text = text.replace('[stormy]', '')\n    text = text.replace('[sailing]', '')\n    text = text.replace('[eos]', '')\n    text = text.replace('[pp]', '')\n\n    text = text.replace('[unmatched quot]', '')\n    return text",
        "sha1": "eac4a140b516853019c2f6a6f4fe7ac8184cd9e0",
        "id": 269125
    },
    {
        "content": "def get_completions(text, options):\n    \"\"\"\n    Returns a list of options, which could be used to complete provided text.\n    \"\"\"\n    completions = []\n    l = len(text)\n    for x in options:\n        if len(x) < l:\n            continue\n        if x.startswith(text):\n            completions.append(x)\n    return completions",
        "sha1": "34489873d13900ae4795fb8f80d36d5eacaac8e0",
        "id": 468007
    },
    {
        "content": "def iterable(obj):\n    \"\"\"Returns ``True`` if *obj* can be iterated over and is *not* a  string.\"\"\"\n    if type(obj) is str:\n        return False    # avoid iterating over characters of a string\n\n    if hasattr(obj, 'next'):\n        return True    # any iterator will do\n    try:\n        len(obj)       # anything else that might work\n    except TypeError:\n        return False\n    return True",
        "sha1": "f868687cc939c1dc49f3ab65b5e2d1fc5174fdfb",
        "id": 130515
    },
    {
        "content": "import json\n\n\ndef example_route(data):\n    \"\"\" Example API route. \"\"\"\n    \n    return json.dumps(data)",
        "sha1": "ee522a7c8776acaf8116ba85841f8c0877f39a2e",
        "id": 169261
    },
    {
        "content": "import torch\n\n\ndef load_model(model_path=''):\n    \"\"\"\n    Load saved model from file\n    :param model_path: mlp.pth prepared using mlp.py\n    :return net: loaded model\n    \"\"\"\n    print(f'[INFO]: Loading saved model...')\n    net = torch.load(model_path)\n    net = net.to('cuda:0')\n    net.eval()\n    return net",
        "sha1": "c6c2c4427ae64846b05cb948683a1ddf6ff9bd6a",
        "id": 581426
    },
    {
        "content": "import psutil\n\n\ndef get_num_procs(njobs:int)->int:\n    \"\"\"\n    Small wrapper function used to get an integer number of cores, based on\n    the user selection via njobs:\n        \n        - njobs = 0, set it to 1 core (serial)\n        - njobs > 0, set it to njobs cores (parallel)\n        - njobs < 0, set it to njobs * the number of physical cores as obtained by psutil.cpu_count\n                    (This is useful for multi-socket setups, where psutil will only give the \n                    number of cores on a single socket)\n    \"\"\"\n    \n    if (njobs==0):\n        njobs=1\n    if (njobs<0):\n        procs=psutil.cpu_count(logical=False)*abs(njobs)\n    else:\n        procs=njobs\n    \n    return procs",
        "sha1": "f026e4a748e25175f335eceebb2e709cb310bf5e",
        "id": 317876
    },
    {
        "content": "from typing import Generic\nfrom re import T\n\n\ndef filter_by_organization(model: Generic[T], organization_id: str) -> QuerySet:  # type: ignore\n    \"\"\"\n    Filters active resources that belong to an organization.\n\n    Args:\n        model (Generic[T]): Model that is going to be queried.\n        organization_id (str): ID of the organization whose clients are to filtered.\n\n    Returns:\n        QuerySet of the active resources of an organization whose ID has been supplied.\n    \"\"\"\n    queryset = model.objects.filter(organization_id=organization_id, is_deleted=False, deleted_at__isnull=True)  # type: ignore\n    return queryset",
        "sha1": "a8c7f2144f451f3f0950c112744a853f099f1337",
        "id": 676215
    },
    {
        "content": "def area_retangulo(lado_maior: float, lado_menor: float):\n\t\"\"\"\n\tCalcula a \u00e1rea de um ret\u00e2ngulo a partir do par\u00e2metro \"lado\" e retorna o valor baseado na f\u00f3rmula:\n\n\t\u00c1rea = lado * lado\n\n\t:param lado_maior \u00e9 um valor real que define o lado de maior comprimento de um ret\u00e2ngulo\n\n\t:param lado_menor \u00e9 um valor real que define o lado de menor comprimento de um ret\u00e2ngulo\n\n\t:returns Retorna o valor da \u00e1rea do ret\u00e2ngulo baseado na f\u00f3rmula acima\n\t\"\"\"\n\treturn lado_maior * lado_menor",
        "sha1": "bfb377c5638773192c88149515a95c6948b279eb",
        "id": 532074
    },
    {
        "content": "def ishex(c):\n    \"\"\"Return true if the byte ordinal 'c' is a hexadecimal digit in ASCII.\"\"\"\n    assert isinstance(c, bytes)\n    return b'0' <= c <= b'9' or b'a' <= c <= b'f' or b'A' <= c <= b'F'",
        "sha1": "220e54afe26663eb6ae09446fd9ab95dfbb4719d",
        "id": 424378
    },
    {
        "content": "from typing import Callable\nfrom typing import Any\nimport time\n\n\ndef _pytimed(callback: Callable[..., None], *args: Any, **kwargs: Any):\n  \"\"\"Call the given callback and return time in nanoseconds as result.\"\"\"\n  start_time = time.monotonic_ns()\n  results = callback(*args, **kwargs)\n  end_time = time.monotonic_ns()\n  duration = (end_time - start_time)\n  return duration",
        "sha1": "1d7ece3d4da8fdb4305d96e1a7623bd2e3a9a89e",
        "id": 597742
    },
    {
        "content": "def is_data_remote(rgc):\n    \"\"\"\n    Determine if server genome config defines a 'remotes' key, 'http is one of them and\n     additionally assert the correct structure -- 'prefix' key defined.\n\n    :param refgenconf.RefGenConf rgc: server genome config object\n    :return bool: whether remote data source is configured\n    \"\"\"\n    return (\n        True\n        if \"remotes\" in rgc\n        and isinstance(rgc[\"remotes\"], dict)\n        and all(\n            [\n                \"prefix\" in r and isinstance(r[\"prefix\"], str)\n                for r in rgc[\"remotes\"].values()\n            ]\n        )\n        else False\n    )",
        "sha1": "199b94b5670fea3654908609e3444a906cee47b8",
        "id": 399550
    },
    {
        "content": "from typing import List\n\n\ndef hyphen_range(string: str) -> List[int]:\n    \"\"\"\n    Expands a string of numbers separated by commas and hyphens into a list of integers.\n    For example:  2-3,5-7,20-21,23,100-200\n    \"\"\"\n    list_numbers = list()\n    temporary_list = string.split(\",\")\n\n    for element in temporary_list:\n        sub_element = element.split(\"-\")\n\n        if len(sub_element) == 1:\n            list_numbers.append(int(sub_element[0]))\n        elif len(sub_element) == 2:\n            for number in range(int(sub_element[0]), int(sub_element[1]) + 1):\n                list_numbers.append(number)\n        else:\n            raise Exception(\n                \"Something went wrong expanding the range {}\".format(string)\n            )\n\n    return list_numbers",
        "sha1": "2a8ebd5a60c180e722db053e4e9d4c69808d8fc8",
        "id": 614500
    },
    {
        "content": "def depend_on_proj_props(target, source, env):\n    \"\"\" Emitter which adds a dependency for the project properties file \"\"\"\n    #sys.stderr.write(\"depend_on_proj_props called\\n\")\n    #sys.stderr.flush()\n\n    return (target, source + [env['XISE_PY_PROPFILE']])",
        "sha1": "f0ea4c5aa0a6958e71dd051a6aff08cf9318d136",
        "id": 687756
    },
    {
        "content": "from typing import List\n\n\ndef ckpts() -> List[int]:\n  \"\"\"Model checkpoints.\"\"\"\n  return [0, 1, 2]",
        "sha1": "c486e549baf224e0f5aca5afeb0934d5400a4727",
        "id": 250990
    },
    {
        "content": "def remove_group(api, assessment_id):\n    \"\"\"Remove all groups from an assessment.\"\"\"\n    allGroups = api.groups.get()\n\n    for group in allGroups:\n        if group.name.startswith(assessment_id):\n            api.groups.delete(group.id)\n\n    return True",
        "sha1": "0513ecd4fe9ebfcb6e70cbe69dd540b40401dd7e",
        "id": 442516
    },
    {
        "content": "import math\n\n\ndef round_half_up(n: float, decimals: float = 0) -> float:\n    \"\"\"This function rounds to the nearest integer number (e.g 2.4 becomes 2.0 and 2.6 becomes 3);\n     in case of tie, it rounds up (e.g. 1.5 becomes 2.0 and not 1.0)\n    Args:\n        n (float): number to round\n        decimals (int): number of decimal figures that we want to keep; defaults to zero\n    Returns:\n        rounded_number (float): input number rounded with the desired decimals\n    \"\"\"\n    multiplier = 10 ** decimals\n\n    rounded_number = math.floor(n * multiplier + 0.5) / multiplier\n\n    return rounded_number",
        "sha1": "e0aab5cba456b4ffe6fab11a21b97fe4e17b045a",
        "id": 704362
    },
    {
        "content": "import time\n\n\ndef CreatePastDate(secs):\n    \"\"\"\ncreate a date shifted of 'secs' \nseconds respect to 'now' compatible with mysql queries\nNote that using positive 'secs' you'll get a 'future' time!\n    \"\"\"\n    T=time.time()+time.timezone+secs\n    tup=time.gmtime(T)\n    #when=\"%d-%d-%d %d:%d:%d\"%(tup[0],tup[1],tup[2],tup[3],tup[4],tup[5])\n    when=\"%d-%d-%d %d:%d:%d\"%tup[0:6]\n    return when",
        "sha1": "bad798fb9e7588d1bbdbca9a814cf44fdbadb733",
        "id": 675470
    },
    {
        "content": "def dfdz_PReLU(z, alpha):\n    \"\"\"Derivative of the parametric rectified linear unit function...\n    Args:\n        z (np.array)\n\n    Returns:\n        df(z)/dz = 1 if x > 0 else alpha (np.array)\n    \"\"\"\n    return 1.0 * (z > 0) + alpha * (z <= 0)",
        "sha1": "649a0b2b00c0fc1cb2267d28e0ba06347e382dd1",
        "id": 421175
    },
    {
        "content": "from typing import Dict\nfrom typing import Any\nfrom typing import Set\n\n\ndef build_resources(\n    translation_strings: Dict[str, Dict[str, Any]],\n    components: Set[str],\n    category: str,\n) -> Dict[str, Dict[str, Any]]:\n    \"\"\"Build the resources response for the given components.\"\"\"\n    # Build response\n    resources: Dict[str, Dict[str, Any]] = {}\n    for component in components:\n        new_value = translation_strings[component].get(category)\n\n        if new_value is None:\n            continue\n\n        resources[component] = {category: new_value}\n\n    return {\"component\": resources}",
        "sha1": "71592355af3769611b4ba38083671cf5732434e9",
        "id": 493249
    },
    {
        "content": "def _GetHandlerType(handler):\n  \"\"\"Get handler type of mapping.\n\n  Args:\n    handler: Original handler.\n\n  Returns:\n    Handler type determined by which handler id attribute is set.\n\n  Raises:\n    ValueError: when none of the handler id attributes are set.\n  \"\"\"\n  if 'apiEndpoint' in handler:\n    return 'apiEndpoint'\n  elif 'staticDir' in handler:\n    return 'staticDirectory'\n  elif 'path' in handler:\n    return 'staticFiles'\n  elif 'scriptPath' in handler:\n    return 'script'\n\n  raise ValueError('Unrecognized handler type: %s' % handler)",
        "sha1": "04887329fc5dd8fd44bfc678095e49dfc1b190bf",
        "id": 133436
    },
    {
        "content": "def pre_slash(path):\n    \"\"\"\n    Connivence function to ensure prepended slash to a path\n    \"\"\"\n    if path == '':\n        path = \"/\"\n    elif path[0] != '/':\n        path = '/' + path\n    return path",
        "sha1": "9cd0191d1db8b73f775555537356dce68d5904f8",
        "id": 313111
    },
    {
        "content": "import six\nimport re\n\n\ndef to_bytes(value):\n    \"\"\"Convert numbers with a byte suffix to bytes.\n    \"\"\"\n    if isinstance(value, six.string_types):\n        pattern = re.compile('^(\\d+)([K,M,G]{1})$')\n        match = pattern.match(value)\n        if match:\n            value = match.group(1)\n            suffix = match.group(2)\n            factor = {\n                'K': 1024,\n                'M': 1024 ** 2,\n                'G': 1024 ** 3,\n            }[suffix]\n\n            return int(round(factor * float(value)))\n\n    return value",
        "sha1": "a2e686d56bd2bed9918ea4e8a165de36d54994e8",
        "id": 682810
    },
    {
        "content": "def add_old_flag(df):\n    \"\"\"\n    Mark all mechanisms which were originally evaluated in StatDP [1].\n\n    [1] Ding, Zeyu, Yuxin Wang, Guanhong Wang, Danfeng Zhang, and Daniel Kifer.\n        \"Detecting Violations of Differential Privacy.\" In Proceedings of the 2018\n        ACM SIGSAC Conference on Computer and Communications Security  - CCS \u201918.\n        https://doi.org/10.1145/3243734.3243818.\n    \"\"\"\n    old_mechanisms = \\\n        ['NoisyHist1', 'NoisyHist2'] + \\\n        [f'ReportNoisyMax{i}' for i in range(1, 5)] + \\\n        [f'SVT{i}' for i in range(1, 7) if i != 2]\n\n    df['old'] = False\n    df.loc[old_mechanisms, 'old'] = True\n    df = df.sort_values(by=['old', 'mechanism'])\n    return df",
        "sha1": "65170264e46d917aeb69d331054e7bdc23f3f774",
        "id": 589787
    },
    {
        "content": "def compute_missing_stats(dat):\n    \"\"\"Computes summary of missing values in the dataset\"\"\"\n    dat_missing = dat.isnull()\n\n    missing_total = dat_missing.sum().sum()\n\n    missing_dict = {\n        \"total\": {\n            \"num_missing\": int(missing_total),\n            \"num_missing_pct\": float(missing_total / (dat.shape[0] * dat.shape[1])),\n        }\n    }\n\n    if missing_total > 0:\n        missing_dict[\"rows\"] = dat_missing.T.sum().to_dict()\n        missing_dict[\"columns\"] = dat_missing.sum().to_dict()\n\n    return missing_dict",
        "sha1": "291812f7cc2c2b22728d895bb64d237a08f0f601",
        "id": 408715
    },
    {
        "content": "def remove_outliers(so):\n    \"\"\"Remove the top and bottom 5 percentile of text by char length\"\"\"\n    so['text_lengths'] = so.text.str.len()\n    so = so[so.text_lengths < so.text_lengths.quantile(0.95)]\n    so = so[so.text_lengths > so.text_lengths.quantile(0.05)]\n    return so.drop(columns='text_lengths')",
        "sha1": "78c0ebd6ba9c6049a800cdbfe2d0e122ff80c3bc",
        "id": 504801
    },
    {
        "content": "from typing import Tuple\n\n\ndef tile_slices(\n    index: Tuple[int, ...],\n    shape: Tuple[int, ...],\n) -> Tuple[slice, ...]:\n    \"\"\"Create a tuple of slices to read a tile region from an array.\n\n    Args:\n        location (Tuple[int, ...]):\n            The index of the tile e.g. the (ith, jth) tile in a 2d grid.\n        shape (Tuple[int, ...]):\n            The shape of the tiles in the grid.\n\n    Returns:\n        Tuple[slice, ...]:\n            The slices to read the tile region from an array-like.\n    \"\"\"\n    return tuple(slice(loc * s, (loc + 1) * s) for loc, s in zip(index, shape))",
        "sha1": "6340623f39692a00d1db7549e8c15be1be4cda8d",
        "id": 212253
    },
    {
        "content": "def _find_prime_factors(n):\n    \"\"\"\n    Find all the prime factors of `n`, sorted from largest to smallest\n\n    Parameters\n    ----------\n    n : int\n        Number to factorize\n\n    Returns\n    -------\n    factors : list[int]\n\n    Notes\n    -----\n    From http://stackoverflow.com/questions/15347174/python-finding-prime-factors\n\n    Examples\n    --------\n    >>> _find_prime_factors(8)\n    [2, 2, 2]\n\n    >>> _find_prime_factors(25)\n    [5, 5]\n\n    >>> _find_prime_factors(26)\n    [13, 2]\n\n    >>> _find_prime_factors(29)\n    [29]\n\n    >>> _find_prime_factors(14)\n    [7, 2]\n\n    >>> _find_prime_factors(12)\n    [3, 2, 2]\n    \"\"\"\n    i = 2\n    factors = []\n    while i * i <= n:\n        if n % i:\n            i += 1\n        else:\n            n //= i\n            factors.append(i)\n    if n > 1:\n        factors.append(n)\n    factors = sorted(factors, reverse=True)\n    return factors",
        "sha1": "fcdd8a0f13e889c012a181319606162db4f54ad7",
        "id": 222885
    },
    {
        "content": "def load_labels(path, encoding='utf-8'):\n  \"\"\"Loads labels from file (with or without index numbers).\n\n  Args:\n    path: path to label file.\n    encoding: label file encoding.\n  Returns:\n    Dictionary mapping indices to labels.\n  \"\"\"\n  with open(path, 'r', encoding=encoding) as f:\n    lines = f.readlines()\n    if not lines:\n      return {}\n\n    if lines[0].split(' ', maxsplit=1)[0].isdigit():\n      pairs = [line.split(' ', maxsplit=1) for line in lines]\n      return {int(index): label.strip() for index, label in pairs}\n    else:\n      return {index: line.strip() for index, line in enumerate(lines)}",
        "sha1": "ca8378e4c3e909ab05f73e923f7bc6c115f1e23c",
        "id": 297185
    },
    {
        "content": "def get_story_element_text_format(element_json):\n    \"\"\"Helper function to Stories Parser, for converting story element from from json format to text\"\"\"\n    if element_json['type'] == 'action':\n        return element_json['name']\n    elif element_json['type'] == 'slot':\n        return 'slot{{\"{slot_name}\" : \"{slot_value}\"}}'.format(slot_name=element_json['name'],\n                                                               slot_value=element_json['value'])\n    elif element_json['type'] == 'form':\n        return 'form{{\"name\": \"{form_name}\"}}'.format(form_name=element_json['name'])\n    else:\n        return False",
        "sha1": "474e2617e7af2e59bf5feeec37f23da3416baf57",
        "id": 481864
    },
    {
        "content": "def _get_jsonld_property(jsonld, property, default=None):\n    \"\"\"Return property value from expanded JSON-LD data.\"\"\"\n    value = jsonld.get(property)\n    if not value:\n        return default\n    if isinstance(value, list) and len(value) == 1 and isinstance(value[0], dict) and \"@value\" in value[0]:\n        value = value[0][\"@value\"]\n    return value",
        "sha1": "5cda221bc065b53411f460ab226d99200c68a148",
        "id": 626746
    },
    {
        "content": "def makeDFWithCommonColumns(df1, df2):\n  \"\"\"\n  Returns dataframes that have columns in common.\n  \"\"\"\n  columns = set(df1.columns).intersection(df2.columns)\n  df1_sub = df1.copy()\n  df2_sub = df2.copy()\n  df1_sub = df1_sub[columns]\n  df2_sub = df2_sub[columns]\n  return df1_sub, df2_sub",
        "sha1": "838467b332f35c5c1a733f21716d5c71815978e9",
        "id": 432118
    },
    {
        "content": "def coalesce(*args):\n    \"\"\":yaql:coalesce\n\n    Returns the first predicate which evaluates to non-null value. Returns null\n    if no arguments are provided or if all of them are null.\n\n    :signature: coalesce([args])\n    :arg [args]: input arguments\n    :argType [args]: chain of any types\n    :returnType: any\n\n    .. code::\n\n        yaql> coalesce(null)\n        null\n        yaql> coalesce(null, [1, 2, 3][0], \"abc\")\n        1\n        yaql> coalesce(null, false, 1)\n        false\n    \"\"\"\n    for f in args:\n        res = f()\n        if res is not None:\n            return res\n    return None",
        "sha1": "1cf53279add1991131148ac62da6edb1af128652",
        "id": 225643
    },
    {
        "content": "def name_handling(prompt=''):\n    \"\"\" Ask the user for input\n    :param prompt:\n    :return:\n        name_item: a category\n    \"\"\"\n    cF = True\n    while cF:\n        try:\n            name_item = input(prompt)\n            assert (name_item.isalpha()), 'Assume valid input'\n            return name_item\n        except AssertionError:\n            print('The input is not valid!')\n            continue",
        "sha1": "ed2396e896695b36978dd7f0e07e652b056ea223",
        "id": 310439
    },
    {
        "content": "def dimension(pot):\n    \"\"\" Find the dimension of the potential\n    \"\"\"\n    return len(list(pot.keys())[0])",
        "sha1": "2d6cd45b18ad223edfae0ad8b060b4a3500886c5",
        "id": 129604
    },
    {
        "content": "def get_cart_location(env, screen_width=600):\n    \"\"\"\n    Get the position of cart\n    :param env: environment, in this file means cartpole-v0\n    :param screen_width:screen width defined in gym\n    :return:middle position of cart\n    \"\"\"\n    world_width = env.x_threshold * 2\n    scale = screen_width / world_width\n    return int(env.state[0] * scale + screen_width / 2.0)",
        "sha1": "da4b7f710e0ecff94e65b48e40e71a108d239bac",
        "id": 377132
    },
    {
        "content": "def search_key_for_post(player):\n    \"\"\"Generate a string search key for a post\"\"\"\n    elements = [player['id'], player['title'], player['years']]\n    return u' '.join(elements)",
        "sha1": "f0ea7acd2f0e5548144d963de4c07c78df5aa6f4",
        "id": 171318
    },
    {
        "content": "def string_ellipse(string, maxlen):\n    \"\"\"Clamp the string to be no longer than the maximum length.  If the string\n    is too long, we write it as \"... []\" where \"[]\" is the final part of the\n    string.\n    \n    :param string: The input string.\n    :param maxlen: The maximum length of the string.\n    \"\"\"\n    if len(string) <= maxlen:\n        return string\n    \n    if maxlen <= 4:\n        raise ValueError(\"maxlen too small\")\n    \n\n    return \"... \" + string[4-maxlen:]",
        "sha1": "56ebe5aa7a54e3f5e9e4a49632768e6906aeade2",
        "id": 86776
    },
    {
        "content": "def lerp1D(u, v, t: float) -> float:\n    \"\"\"\n    lerp1D ( u, v, t ) = u + t * (v - u)\n    :param u: any type that supports vector addition and scalar multiplication\n    :param v: any type that supports vector addition and scalar multiplication\n    :param t: (float) parameter at which interpolation is sought\n    :return: (float) the interpolated value at the desired parameter\n    \"\"\"\n    w = v - u\n    return u + (w * t)",
        "sha1": "12684c8aa9eeb7cf6a2e9020d5aef788008a65ce",
        "id": 243500
    },
    {
        "content": "from typing import Generator\nimport re\n\n\ndef split_lines(string: str) -> Generator[str, None, None]:\n    \"\"\"\n    Splits string into lines, skipping empty; surrounding spaces are removed.\n    \"\"\"\n    return (\n        x.group(0).strip()\n        for x in re.finditer(r\".*(?:$|\\n)\", string)\n        if len(x.group(0).strip()) > 0\n    )",
        "sha1": "521882cb40826fe9eb6d84349aaba1decbbfa52b",
        "id": 108676
    },
    {
        "content": "def reduce_dataset(data, vars_to_keep, coords_to_keep):\n    \"\"\" Removes all variables and coordinates from input dataset that are not\n    listed in paramters.\n\n    Parameters\n    ----------\n    data : xarray.Dataset\n    vars_to_keep : sequence of strings\n    dims_to_keep : sequence of strings\n\n    Returns\n    -------\n    xarray.Dataset\n    \"\"\"\n    for v in data.data_vars:\n        if v not in vars_to_keep:\n            data = data.drop(v)\n    for c in data.coords:\n        if c not in coords_to_keep:\n            data = data.drop(c)\n    return data",
        "sha1": "94b4f471733757aab05591710d84dfc685764532",
        "id": 592199
    },
    {
        "content": "def _Backward1_T_Ps(P, s):\n    \"\"\"Backward equation for region 1, T=f(P,s)\n\n    Parameters\n    ----------\n    P : float\n        Pressure [MPa]\n    s : float\n        Specific entropy [kJ/kgK]\n\n    Returns\n    -------\n    T : float\n        Temperature [K]\n\n    References\n    ----------\n    IAPWS, Revised Release on the IAPWS Industrial Formulation 1997 for the\n    Thermodynamic Properties of Water and Steam August 2007,\n    http://www.iapws.org/relguide/IF97-Rev.html, Eq 13\n\n    Examples\n    --------\n    >>> _Backward1_T_Ps(3,0.5)\n    307.842258\n    >>> _Backward1_T_Ps(80,3)\n    565.899909\n    \"\"\"\n    I = [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 4]\n    J = [0, 1, 2, 3, 11, 31, 0, 1, 2, 3, 12, 31, 0, 1, 2, 9, 31, 10, 32, 32]\n    n = [0.17478268058307e3, 0.34806930892873e2, 0.65292584978455e1,\n         0.33039981775489, -0.19281382923196e-6, -0.24909197244573e-22,\n         -0.26107636489332, 0.22592965981586, -0.64256463395226e-1,\n         0.78876289270526e-2, 0.35672110607366e-9, 0.17332496994895e-23,\n         0.56608900654837e-3, -0.32635483139717e-3, 0.44778286690632e-4,\n         -0.51322156908507e-9, -0.42522657042207e-25, 0.26400441360689e-12,\n         0.78124600459723e-28, -0.30732199903668e-30]\n\n    Pr = P/1\n    sigma = s/1\n    T = 0\n    for i, j, ni in zip(I, J, n):\n        T += ni * Pr**i * (sigma+2)**j\n    return T",
        "sha1": "22382bb64594b8851f40a3398f29807483160b3e",
        "id": 269957
    },
    {
        "content": "def points_from_columns(xs):\n    \"\"\"\n    Takes a list of rows and converts it into\n    a list of columns. For example,\n        xs = [[1, 2, 3], [4, 5, 6], [1], [2, 4]]\n    will return\n        x = [[1, 4, 1, 2], [2, 5, 4], [3, 6]]\n    xs: list of rows (lists).\n    returns: list of columns (lists).\n    \"\"\"\n    x = []\n    for row in xs:\n        while len(x) < len(row): x.append([])\n        for i in range(len(row)): x[i].append(row[i])\n    return x",
        "sha1": "190a9e7d2d7f439a4a8c918bf5e7fa43d3960b95",
        "id": 502530
    },
    {
        "content": "def filter_sample(sample, filtr):\n    \"\"\"\n    Applies a given filter to a sample, returning the sample if the filter is passed, or None otherwise.\n\n    :param sample: Samples to be filtered.\n    :type sample: dict\n    :param filtr: Function to filter samples (returns a boolean value for a given sample)\n    :type filtr: function\n    :return: The filtered sample.\n    :rtype: dict or None\n    \"\"\"\n\n    filtered_sample = None\n    if filtr(sample):\n        filtered_sample = sample\n\n    return filtered_sample",
        "sha1": "e26159f5df5fbf7496c9bc08eb183c4b2b3d8637",
        "id": 361554
    },
    {
        "content": "import re\n\n\ndef expand_parameters(host, params):\n    \"\"\"Expand parameters in hostname.\n\n    Examples:\n    * \"target{N}\" => \"target1\"\n    * \"{host}.{domain} => \"host01.example.com\"\n\n    \"\"\"\n    pattern = r\"\\{(.*?)\\}\"\n\n    def repl(match):\n        param_name = match.group(1)\n        return params[param_name]\n\n    return re.sub(pattern, repl, host)",
        "sha1": "04f62924fdc77b02f3a393e5cc0c5382d1d4279a",
        "id": 2332
    },
    {
        "content": "def make_unique(qs, unique_var):\n    \"\"\"Make the queryset unique by unique_var, sort by unique_var\"\"\"\n    if hasattr(qs, \"distinct\"):  # Need to check so that this does not break in Preview mode in Wagtail\n        distinct_pks = qs.distinct(unique_var).order_by(unique_var).values_list('pk', flat=True)\n        return qs.filter(pk__in=distinct_pks)\n    else:\n        return qs",
        "sha1": "62b1bb9453d6eba885906ec3c67851a75a5f781d",
        "id": 127736
    },
    {
        "content": "def _choose_latest_version(*app_versions):\n    \"\"\"\n    Chooses the latest version from a list of AppVersion objects - choosing the first one passed\n    in with the highest version number.\n    \"\"\"\n    usable_versions = [_f for _f in app_versions if _f]\n    if usable_versions:\n        return sorted(usable_versions, key=lambda v: v.build_version)[-1]",
        "sha1": "cd6bc674757831d53fc1a6dd8f96bfbb78eb9a91",
        "id": 291832
    },
    {
        "content": "def induce_subgraph(G, node_subset):\n    \"\"\"\n    Induce a subgraph of G.\n\n    Parameters\n    ----------\n    G : networkx.MultiDiGraph\n        input graph\n    node_subset : list-like\n        the subset of nodes to induce a subgraph of G\n\n    Returns\n    -------\n    H : networkx.MultiDiGraph\n        the subgraph of G induced by node_subset\n    \"\"\"\n    node_subset = set(node_subset)\n\n    # copy nodes into new graph\n    H = G.__class__()\n    H.add_nodes_from((n, G.nodes[n]) for n in node_subset)\n\n    # copy edges to new graph, including parallel edges\n    if H.is_multigraph:\n        H.add_edges_from(\n            (n, nbr, key, d)\n            for n, nbrs in G.adj.items()\n            if n in node_subset\n            for nbr, keydict in nbrs.items()\n            if nbr in node_subset\n            for key, d in keydict.items()\n        )\n    else:\n        H.add_edges_from(\n            (n, nbr, d)\n            for n, nbrs in G.adj.items()\n            if n in node_subset\n            for nbr, d in nbrs.items()\n            if nbr in node_subset\n        )\n\n    # update graph attribute dict, and return graph\n    H.graph.update(G.graph)\n    return H",
        "sha1": "952dea186a93dacdbabda64d64484c6d4aebf889",
        "id": 608607
    },
    {
        "content": "import functools\n\n\ndef memoize(obj):\n    \"\"\"'Memoize' aka remember the output from a function and return that,\n    rather than recalculating\n\n    Stolen from:\n    https://wiki.python.org/moin/PythonDecoratorLibrary#CA-237e205c0d5bd1459c3663a3feb7f78236085e0a_1\n\n    do_not_memoize : bool\n        IF this is a keyword argument (kwarg) in the function, and it is true,\n        then just evaluate the function and don't memoize it.\n    \"\"\"\n    cache = obj.cache = {}\n\n    @functools.wraps(obj)\n    def memoizer(*args, **kwargs):\n        if 'do_not_memoize' in kwargs and kwargs['do_not_memoize']:\n            return obj(*args, **kwargs)\n        key = str(args) + str(kwargs)\n        if key not in cache:\n            cache[key] = obj(*args, **kwargs)\n        return cache[key]\n\n    return memoizer",
        "sha1": "cadb88fafe6d33a3a43d60b77a1c30c80bb2919f",
        "id": 255438
    },
    {
        "content": "from typing import List\nimport re\nimport yaml\n\n\ndef parse_multidicts(config: str) -> List[dict]:\n    \"\"\"Parse multiple YAML dictionaries from string, return list of them\"\"\"\n    data = \"\"\n    for line in config.split(\"\\n\"):\n        if line.startswith(\"#\") or re.fullmatch(r\"^\\s+$\", line):\n            print(\"IGNORE\", line)\n            continue\n        elif re.match(r\"^\\S\", line):\n            data += \"\\n--- \\n\"\n        data += line + \"\\n\"\n    res = list(yaml.safe_load_all(data))\n    return res",
        "sha1": "fbb5b8cee0346eb6257578665ebbbcb88539f645",
        "id": 574150
    },
    {
        "content": "def square_rect_deindex(index, rect_x, rect_y, width, height):\n    \"\"\"Performs the inverse of square_rect_index\n    Equivalent to list(square_rect(...))[index]\"\"\"\n    dx = index % width\n    dy = index // width\n    assert dx >= 0 and dy < height\n    return (rect_x + dx, rect_y + dy)",
        "sha1": "f8a71cb63607d0040fd4c4640d86affe1d37ac71",
        "id": 446337
    },
    {
        "content": "def zero_counter(number):\n    \"\"\"Counts the number of consecutive 0's at the end of the number\"\"\"\n    x = 0\n    while (number >> x) & 1 == 0:\n        x = x + 1\n    return x",
        "sha1": "598fdc5bfed992f6921151e831c3d71c6ea184cf",
        "id": 258911
    },
    {
        "content": "import math\n\n\ndef correl(x, y):\n    \"\"\"\n    Compute Pearson correlation coefficient (r)\n    \"\"\"\n    xbar = x.mean(0)\n    ybar = y.mean(0)\n    numerator = sum((x-xbar)*(y-ybar))\n    denominator = math.sqrt(sum((x-xbar)**2))*math.sqrt(sum((y-ybar)**2))\n\n    # Compute mean product (r)\n    return numerator/denominator",
        "sha1": "aeb3d2f4b93b3ff47582a0beef1959915a11d832",
        "id": 490563
    },
    {
        "content": "def process_range(max_process_id, process_number=None):\n    \"\"\"\n    Creates an iterable for all the process ids, if process number is set\n    then an iterable containing only that number is returned.\n\n    This allows for the loss generation to be ran in different processes\n    rather than accross multiple cores.\n\n    :param max_process_id: The largest process number\n    :param process_number: If set iterable only containing this number is returned\n    :return: iterable containing all the process numbers to process\n    \"\"\"\n    if process_number is not None:\n        return [process_number]\n    else:\n        return range(1, max_process_id + 1)",
        "sha1": "aec4639c9e00a9e6786462e787cc379464533989",
        "id": 301557
    },
    {
        "content": "def validate_management_key_id_against_chain_id(key_id, chain_id):\n    \"\"\"\n    Checks if the chain in the key_id matches the value supplied in chain_id.\n\n    Parameters\n    ----------\n    key_id: bytes or str\n        The partial or full key identifier\n    chain_id: str\n        The chain ID\n\n    Raises\n    ------\n    UnicodeDecodeError\n        If the key_id cannot be decoded to a Unicode string\n\n    Returns\n    -------\n    bool\n    \"\"\"\n    if type(key_id) is bytes:\n        key_id = key_id.decode()\n    # If the identifier is a full key id, extract the chain and compare it to the provided value\n    if \":\" in key_id:\n        key_id_chain = key_id.split(\":\")[-1].split(\"#\")[0]\n        return key_id_chain == chain_id\n    # Otherwise, just return True\n    else:\n        return True",
        "sha1": "da02214f46528298f1c3ca336ffef2df3529b638",
        "id": 624529
    },
    {
        "content": "def _find_exclude_idx(ch_names, exclude):\n    \"\"\"Find the index of all channels to exclude.\n\n    If there are several channels called \"A\" and we want to exclude \"A\",\n    then add (the index of) all \"A\" channels to the exclusion list.\n    \"\"\"\n    return [idx for idx, ch in enumerate(ch_names) if ch in exclude]",
        "sha1": "db754d5e92af59563d6ee2004e5470bfe08a0fc1",
        "id": 18629
    },
    {
        "content": "from typing import OrderedDict\nimport torch\nimport copy\n\n\ndef copy_state(model):\n    \"\"\"\n    Given PyTorch module `model`, makes a copy of the state onto CPU.\n    Args:\n        model: PyTorch module to copy state dict of\n\n    Returns:\n        A copy of state dict with all tensors allocated on the CPU\n    \"\"\"\n    copy_dict = OrderedDict()\n    state_dict = model.state_dict()\n    for k, v in state_dict.items():\n        if torch.is_tensor(v):\n            copy_dict[k] = v.cpu() if v.is_cuda else v.clone()\n        else:\n            copy_dict[k] = copy.deepcopy(v)\n\n    return copy_dict",
        "sha1": "fc5e7705c880f4d11a479aace875583437b6ad7a",
        "id": 548243
    },
    {
        "content": "from typing import List\n\n\ndef convertTo(data) -> List[dict]:\n    \"\"\"\n    Converst scores from new to old format\n    :param data: New format data\n    :return: Old format data\n    \"\"\"\n    return [\n        {\n            'Team': {\n                'Number': team['teamno'],\n                'DisplayName': team['teamname']\n            },\n            'Scores': team['scores'],\n            'TotalGuess': -1\n        } for team in data\n    ]",
        "sha1": "68d4686e93bb2fb4b754643168f3b181d9ec9438",
        "id": 207367
    },
    {
        "content": "def make_bare_labels(subsystem_count, *args):\n    \"\"\"\n    For two given subsystem states, return the full-system bare state label obtained by placing all remaining\n    subsys_list in their ground states.\n\n    Parameters\n    ----------\n    subsystem_count: int\n        number of subsys_list inside Hilbert space\n    *args: tuple(int, int)\n        each argument is a tuple of the form (subsys_index, label)\n\n    Returns\n    -------\n    tuple\n        Suppose there are 5 subsys_list in total. Let (subsys_index1=0, label1=3), (subsys_index2=2, label2=1). Then the\n        returned bare-state tuple is: (3,0,1,0,0)\n    \"\"\"\n    bare_labels = [0] * subsystem_count\n    for subsys_index, label in args:\n        bare_labels[subsys_index] = label\n    return tuple(bare_labels)",
        "sha1": "30fb36fc230f2fa4e9dde55e535996ba7549ed7b",
        "id": 692621
    },
    {
        "content": "def has_variable_scope(obj):\n  \"\"\"Determines whether the given object has a variable scope.\"\"\"\n  return \"variable_scope\" in dir(obj)",
        "sha1": "a8bd5317cc2c93a38996eb491bfe436ecac13272",
        "id": 149863
    },
    {
        "content": "import collections\n\n\ndef _merge_compiler_kwds(list_of_kwds):\n    \"\"\"\n    Merges a list of keyword dictionaries. Values in these dictionaries are\n    lists of values, the merged dictionaries will contain the concatenations\n    of lists specified for the same key.\n\n    Parameters\n    ----------\n    list_of_kwds : list of dict\n        A list of compiler keyword dictionaries that should be merged.\n\n    Returns\n    -------\n    merged_kwds : dict\n        The merged dictionary\n    \"\"\"\n    merged_kwds = collections.defaultdict(list)\n    for kwds in list_of_kwds:\n        for key, values in kwds.items():\n            if not isinstance(values, list):\n                raise TypeError(\n                    \"Compiler keyword argument '{}' requires a \"\n                    \"list of values.\".format(key)\n                )\n            merged_kwds[key].extend(values)\n    return merged_kwds",
        "sha1": "0538be77849a061d04a2678f15c77fff9b07c5dd",
        "id": 468710
    },
    {
        "content": "def remove_starting_backward_slash(value: str):\n    \"\"\"\n    Removes backward slash at the begin of string.\n    \"\"\"\n    if value.startswith('\\\\'):\n        return value[1:]\n    return value",
        "sha1": "8b05aa5aa89b0cdca3de2d55084eedc0dcb9d3f9",
        "id": 537600
    },
    {
        "content": "def length(v):\n    \"\"\"\n    Calculate the length of a vector.\n\n    Arguments:\n        v: tuple of numbers\n\n    Returns:\n        The length of the vector.\n    \"\"\"\n    return sum(j * j for j in v) ** 0.5",
        "sha1": "0494e313057ab9577ee3b0593760aed736dfce29",
        "id": 112109
    },
    {
        "content": "def is_authenticated(cookie_string, prefix='', rconn=None):\n    \"\"\"Check for a valid cookie, if exists, return True\n       If not, return False.\"\"\"\n\n    if rconn is None:\n        return False\n\n    if (not cookie_string) or (cookie_string == \"noaccess\"):\n        return False\n\n    cookiekey = prefix+cookie_string\n\n    try:\n        if rconn.exists(cookiekey):\n            # key exists, and update expire after ten minutes\n            rconn.expire(cookiekey, 600)\n        else:\n            return False\n    except:\n        return False\n    return True",
        "sha1": "0d90aff0e85a4e2523991ef035bce68ca7636966",
        "id": 230633
    },
    {
        "content": "def error_function(w1, w2, b, y, t):\n    \"\"\"\n        @w1, @w2 : Neuron weight associated with each input\n        @b : Neuron bias\n        @y : neuron prediction\n        @t : true target value\n        Error function\n    \"\"\"\n    return (1./2.) * (t - y)**2",
        "sha1": "eb9418828ee30ef6bef8f9f65034ca6b4d3c339b",
        "id": 375657
    },
    {
        "content": "def clear_json_quotes(json_data):\n    \"\"\"Remove quotes surrounding types from JSON response documentation\"\"\"\n    lines = []\n    for line in json_data.splitlines():\n        if ':' in line:\n            key, value = line.split(':', maxsplit=1)\n            value = value.strip()\n            lines.append(key + ': ' + value.strip('\",') + (',' if value.endswith(',') else ''))\n        else:\n            lines.append(' ' * (len(line) - len(line.lstrip(' '))) + line.strip('\" '))\n    return '\\n'.join(lines)",
        "sha1": "3e2ef0655911eabba1d2ff5ad6593464772bdbc2",
        "id": 281960
    },
    {
        "content": "import math\n\n\ndef f2(x):\n    \"\"\" Computes and returns f2(x). \"\"\"\n    return math.log(x + 2.2)",
        "sha1": "bcd9be4a3f68f8c69fdf815a68dad1e38c193522",
        "id": 86705
    },
    {
        "content": "def lookup_fits_header(remote_path):\n    \"\"\"Read the FITS header from storage.\n\n    FITS Header Units are stored in blocks of 2880 bytes consisting of 36 lines\n    that are 80 bytes long each. The Header Unit always ends with the single\n    word 'END' on a line (not necessarily line 36).\n\n    Here the header is streamed from Storage until the 'END' is found, with\n    each line given minimal parsing.\n\n    See https://fits.gsfc.nasa.gov/fits_primer.html for overview of FITS format.\n\n    Args:\n        remote_path (`google.cloud.storage.blob.Blob`): Blob or path to remote blob.\n            If just the blob name is given then the blob is looked up first.\n\n    Returns:\n        dict: FITS header as a dictonary.\n    \"\"\"\n    i = 1\n    if remote_path.name.endswith('.fz'):\n        i = 2  # We skip the compression header info\n\n    headers = dict()\n\n    streaming = True\n    while streaming:\n        # Get a header card\n        start_byte = 2880 * (i - 1)\n        end_byte = (2880 * i) - 1\n        b_string = remote_path.download_as_string(start=start_byte, end=end_byte)\n\n        # Loop over 80-char lines\n        for j in range(0, len(b_string), 80):\n            item_string = b_string[j: j + 80].decode()\n\n            # End of FITS Header, stop streaming\n            if item_string.startswith('END'):\n                streaming = False\n                break\n\n            # Get key=value pairs (skip COMMENTS and HISTORY)\n            if item_string.find('=') > 0:\n                k, v = item_string.split('=')\n\n                # Remove FITS comment\n                if ' / ' in v:\n                    v = v.split(' / ')[0]\n\n                v = v.strip()\n\n                # Cleanup and discover type in dumb fashion\n                if v.startswith(\"'\") and v.endswith(\"'\"):\n                    v = v.replace(\"'\", \"\").strip()\n                elif v.find('.') > 0:\n                    v = float(v)\n                elif v == 'T':\n                    v = True\n                elif v == 'F':\n                    v = False\n                else:\n                    v = int(v)\n\n                headers[k.strip()] = v\n\n        i += 1\n\n    return headers",
        "sha1": "c150e2e260060b5b6446170729d51cdfd154c9b0",
        "id": 180224
    },
    {
        "content": "import re\n\n\ndef cleanbibblo(text):\n    \"\"\"Remove bibliographic notation in []\"\"\"\n    return re.sub(r' \\[[^\\]]*\\]', '',text)",
        "sha1": "1d0b2a72ae9156ced0ae32a62ed22d8812b48224",
        "id": 561438
    },
    {
        "content": "def strategy_largest_first(G, colors):\n    \"\"\"Returns a list of the nodes of ``G`` in decreasing order by\n    degree.\n\n    ``G`` is a NetworkX graph. ``colors`` is ignored.\n\n    \"\"\"\n    return sorted(G, key=G.degree, reverse=True)",
        "sha1": "babb83d0a5f1c289ee5ffea39469a2c4de21c8bd",
        "id": 343583
    },
    {
        "content": "def _get_attr_as_list(attr, attribute):\n    \"\"\"Helper method to always get an attribute as a list.\"\"\"\n    value = getattr(attr, attribute)\n    if not value:\n        return []\n    if type(value) == type([]):\n        return value\n    return [value]",
        "sha1": "aed603b26f07405a92c9e4b1703f45c1dec20093",
        "id": 364755
    },
    {
        "content": "def filterList(all=True, extraSql=None, extraMetadata=None):\n    \"\"\"Return a list of filters, plot colors and orders.\n\n    Parameters\n    ----------\n    all : `bool`, optional\n        Include 'all' in the list of filters and as part of the colors/orders dictionaries.\n        Default True.\n    extraSql : str, optional\n        Additional sql constraint to add to sqlconstraints returned per filter.\n        Default None.\n    extraMetadata : str, optional\n        Substitute metadata to add to metadata strings composed per band.\n        Default None.\n\n    Returns\n    -------\n    list, dict, dict\n        List of filter names, dictionary of colors (for plots), dictionary of orders (for display)\n    \"\"\"\n    if all:\n        filterlist = ('all', 'u', 'g', 'r', 'i', 'z', 'y')\n    else:\n        filterlist = ('u', 'g', 'r', 'i', 'z', 'y')\n    colors = {'u': 'cyan', 'g': 'g', 'r': 'orange', 'i': 'r', 'z': 'm', 'y': 'b'}\n    orders = {'u': 1, 'g': 2, 'r': 3, 'i': 4, 'z': 5, 'y': 6}\n    if all:\n        colors['all'] = 'k'\n        orders['all'] = 0\n    sqls = {}\n    metadata = {}\n    if extraMetadata is None:\n        if extraSql is None or len(extraSql) == 0:\n            md = ''\n        else:\n            md = '%s ' % extraSql\n    else:\n        md = '%s ' % extraMetadata\n    for f in filterlist:\n        if f == 'all':\n            sqls[f] = ''\n            metadata[f] = md + 'all bands'\n        else:\n            sqls[f] = 'filter = \"%s\"' % f\n            metadata[f] = md + '%s band' % f\n    if extraSql is not None and len(extraSql) > 0:\n        for s in sqls:\n            if s == 'all':\n                sqls[s] = extraSql\n            else:\n                sqls[s] = '(%s) and (%s)' % (extraSql, sqls[s])\n    return filterlist, colors, orders, sqls, metadata",
        "sha1": "7a162addbcf7be95349906eaacdbc25b70425a03",
        "id": 328478
    },
    {
        "content": "import re\n\n\ndef get_circuit_name(circuit_filename):\n    \"\"\"\n    parse the circuit filename, and, assuming it follows a naming convention,\n    return the name of the circuit and the number of inputs.\n    If it doesn't follow the convention, just return the filename and 0.\n    \"\"\"\n    filename_without_path = circuit_filename.split(\"/\")[-1]\n    match = re.search(\"circuit-([-_\\w]+)-([\\d]+).sheep\",filename_without_path)\n    if match:\n        return match.groups()[0], int(match.groups()[1])\n### some circuits (e.g. PIR) follow a different convention:\n    match = re.search(\"circuit-([-_\\w]+).sheep\",filename_without_path)\n    if match:\n        return match.groups()[0], 0\n### if we got to here, just return the filename\n    return filename_without_path, 0",
        "sha1": "4ff40d454977a2416c57e03bfbd8ec4416fdf116",
        "id": 370964
    },
    {
        "content": "def get_total_line_for_file(path_to_file, ignore_blank_lines=False):\n    \"\"\"\n    Calculate the total number of line in the given file.\n    \"\"\"\n    i = 0\n    with open(path_to_file) as f:\n        for line in f:\n            if line == \"\\n\" and ignore_blank_lines:\n                pass\n            else:\n                i+=1\n\n    return i",
        "sha1": "08c94e270ed3aac7fa8c71445cd505c4ecc7b1b8",
        "id": 170038
    },
    {
        "content": "import torch\n\n\ndef bin_to_tensor(filename: str) -> torch.Tensor:\n    \"\"\"\n    Loads tensor from disk at filename in torch.save(...) format.\n    \"\"\"\n    return torch.load(filename)",
        "sha1": "39237c49e97225dd97fdef8f405f205421ae6c38",
        "id": 178920
    },
    {
        "content": "import re\n\n\ndef get_all_local_funcs(vba):\n    \"\"\"\n    Get the names of all locally defined functions.\n    \"\"\"\n    pat = r\"(?:Sub |Function )([^\\(]+)\"\n    r = []\n    for (_, _, _, vba_code) in vba.extract_macros():\n        for line in vba_code.split(\"\\n\"):\n            names = re.findall(pat, line)\n            r.extend(names)\n    return r",
        "sha1": "eeb9889d230410a3f0087e039681870066f512dc",
        "id": 597674
    },
    {
        "content": "def calculate_gamma(P,sigma):\n    \"\"\"\n    Calcualte the fraction of the information\n\n    Parameters\n    ----------\n    P: integer\n        The number of the basis function\n    sigma: array\n        The array obtained from the singular value decomposition\n    \n    Returns\n    ----------\n    res : float\n        Information carried by the first P basis function\n    \"\"\"\n\n    M = len(sigma)\n    sigma_sum = 0\n    for i in range(M):\n        sigma_sum += sigma[i]*sigma[i]\n    P_sum = 0\n    for i in range(P+1):\n        P_sum += sigma[i]*sigma[i]\n    res = P_sum / sigma_sum * 100  # 100 percent\n    return res",
        "sha1": "cf6784071d07108b42206c28be8315faa18bc675",
        "id": 302868
    },
    {
        "content": "def train_LeNet_model(model, trainX, trainY, testX, testY, n_epochs, batch_size):\n    \"\"\"\n    This function trains the LeNet model on the training data and validates it on the test data.\n    \"\"\"\n    # Train model\n    H = model.fit(trainX, trainY, \n                  validation_data=(testX, testY), \n                  batch_size=batch_size, \n                  epochs=n_epochs, verbose=1)\n    \n    return H",
        "sha1": "470177ff03ae8ba911f3bffdb7eb58cca0f1970e",
        "id": 551013
    },
    {
        "content": "def compute_overview_levels(band):\n    \"\"\"Return an appropriate list of overview levels.\"\"\"\n    max_dim = max(band.XSize, band.YSize)\n    overviews = []\n    level = 1\n    while max_dim > 256:\n        level *= 2\n        overviews.append(level)\n        max_dim /= 2\n    return overviews",
        "sha1": "96c2a73b902ac11dbedfde6e77511519a5f93619",
        "id": 95564
    },
    {
        "content": "def get_badge_date(badge, badges, dates):\n    \"\"\"\n    For a given badge name, return the date it was awarded.\n    :param badge: Instructor badge name.\n    :param badges: List of all badges awarded.\n    :param dates: List of dates the badges were awarded, order of dates corresponds to the order of badges in badges list.\n    :return: For a given badge name, return the date it was awarded or None is no such badge was awarded.\n    \"\"\"\n    if badges is None or badges == [] or dates is None or dates == []:\n        return None\n\n    # Find index of badge in badges list\n    try:\n        index = badges.index(badge)\n        return dates[index]\n    except ValueError:\n        return None",
        "sha1": "706d08a3df6f6b2050f47a06fc05d8c400a22d6b",
        "id": 357398
    },
    {
        "content": "def non_none(*args, raise_if_all_none=None):\n    \"\"\"\n    Return the first arg that is not none; optionally raise specified exception if all none\n    \"\"\"\n    for a in args:\n        if a is not None:\n            return a\n    if raise_if_all_none is not None:\n        raise raise_if_all_none\n    return None",
        "sha1": "f072f384bbdcf5b4bd79a5ba7ef98e15178a0b97",
        "id": 530891
    },
    {
        "content": "def key(resource):\n    \"\"\"\n    Returns key name based on resource\n    \"\"\"\n    return {\n        'token': 'address',\n        'pool': 'address',\n        'address': 'address',\n        'allowance': 'hash',\n        'balance': 'hash',\n        'claim': 'hash',\n        'wallet': 'hash',\n        'transaction': 'hash',\n        'transfer': 'hash',\n        'object': 'hash',\n        'candle': 'key',\n        'statistic': 'key',\n    }[resource]",
        "sha1": "8cd60e306d74ad6f12eaba07478a4bed86f1bc94",
        "id": 630990
    },
    {
        "content": "def factorial(n):\n    \"\"\"Return the factorial of a natural number.\n\n    >>> factorial(0)\n    1\n    >>> factorial(5)\n    120\n    \"\"\"\n    if n <= 1:\n        return 1\n    else:\n        return n * factorial(n-1)",
        "sha1": "31c936511bcb6c9ad2c31ce5302e6935f41e27a6",
        "id": 217566
    },
    {
        "content": "def sequenceCategoryLengths(read, categories, defaultCategory=None,\n                            suppressedCategory='...', minLength=1):\n    \"\"\"\n    Summarize the nucleotides or AAs found in a read by assigning each to a\n    category and reporting the lengths of the contiguous category classes\n    found along the sequence.\n\n    @param read: A C{Read} instance or one of its subclasses.\n    @param categories: A C{dict} mapping nucleotides or AAs to category.\n    @param defaultCategory: The category to use if a sequence base is not\n        in C{categories}.\n    @param suppressedCategory: The category to use to indicate suppressed\n        sequence regions (i.e., made up of stretches of bases that are less\n        than C{minLength} in length).\n    @param minLength: stretches of the read that are less than this C{int}\n        length will be summed and reported as being in the\n        C{suppressedCategory} category.\n    @raise ValueError: If minLength is less than one.\n    @return: A C{list} of 2-C{tuples}. Each tuple contains a (category, count).\n    \"\"\"\n    result = []\n    append = result.append\n    get = categories.get\n    first = True\n    currentCategory = None\n    currentCount = 0\n    suppressing = False\n    suppressedCount = 0\n\n    if minLength < 1:\n        raise ValueError('minLength must be at least 1')\n\n    for base in read.sequence:\n        thisCategory = get(base, defaultCategory)\n        if first:\n            first = False\n            currentCategory = thisCategory\n            currentCount += 1\n        else:\n            if thisCategory == currentCategory:\n                # This base is still in the same category as the last base.\n                # Keep counting.\n                currentCount += 1\n            else:\n                # This is a new category.\n                if currentCount < minLength:\n                    # The category region that was just seen will not be\n                    # emitted.\n                    if suppressing:\n                        # Already suppressing. Suppress the just-seen\n                        # region too.\n                        suppressedCount += currentCount\n                    else:\n                        # Start suppressing.\n                        suppressedCount = currentCount\n                        suppressing = True\n                else:\n                    if suppressing:\n                        append((suppressedCategory, suppressedCount))\n                        suppressedCount = 0\n                        suppressing = False\n                    append((currentCategory, currentCount))\n                currentCategory = thisCategory\n                currentCount = 1\n\n    if suppressing:\n        append((suppressedCategory, suppressedCount + currentCount))\n    elif currentCount >= minLength:\n        append((currentCategory, currentCount))\n    elif currentCount:\n        append((suppressedCategory, currentCount))\n\n    return result",
        "sha1": "a4f62792121dd9955fbbf2e333c768fc72bd3560",
        "id": 265240
    },
    {
        "content": "from typing import List\nfrom typing import Any\nfrom typing import OrderedDict\n\n\ndef unique(lst: List[Any]) -> List[Any]:\n    \"\"\"uniquify list while preserving order\"\"\"\n    return list(OrderedDict.fromkeys(lst))",
        "sha1": "2eda5f5d5ad9fc4bf927c11be3145b2df2fc2b4a",
        "id": 243908
    },
    {
        "content": "def get_consecutive_num(arr):\n    \"\"\"\n    Method to get indices of second number in a pair of consecutive numbers\n    Note: solve_90f3ed37 uses this function\n    \"\"\"\n    rows = []\n    for i in range(len(arr) - 1):\n        if (arr[i] + 1) == arr[i + 1]:\n            rows.append(arr[i + 1])\n    return rows",
        "sha1": "24417126d4133db62519dd044f7e0b9498d1ad0f",
        "id": 676356
    },
    {
        "content": "def vel_space(lam, lam0):\n    \"\"\"\n    Convert a maximum absorption wavelength to a velocity\n    using the relativistic Doppler formula\n\n    Args:\n        lam: maximum absorption wavelength\n        lam0: rest-frame absorption wavelength\n\n    Returns:\n        v: ejecta velocity\n    \"\"\"\n    z = lam / lam0\n    return 3e5 * (z ** 2 - 1) / (z ** 2 + 1)",
        "sha1": "7713faea281dfee4bfba4d6c35279100d64fc4f0",
        "id": 612097
    },
    {
        "content": "def _starts_with_space(line, return_on_blank=True):\n    \"\"\"\n    returns true if line starts with space\n\n    :line:                  the line to be examined\n    :return_on_blank:       what to return if line == \"\"\n    :returns:               True if starts with space, False else\n    \"\"\"\n    try: return line[0] == ' '\n    except IndexError: return return_on_blank",
        "sha1": "fe992032343f79e321705e1d4f7cca25acc46d9a",
        "id": 73087
    },
    {
        "content": "def add(value1, value2):\n    \"\"\"Calculate the sum of value1 and value2.\"\"\"\n    return value1 + value2",
        "sha1": "fb2c141c486c1c956825f6eb10500607137971d2",
        "id": 59623
    },
    {
        "content": "def choice_yes(msg: str) -> bool:\n    \"\"\"\n    Awaits user choice (Y/n).\n    \"\"\"\n    while True:\n        print(f\"{msg} (Y/n) \", end=\"\", flush=True)\n        choice = input().lower()\n        if choice in [\"\", \"y\", \"yes\"]:\n            return True\n        if choice in [\"n\", \"no\"]:\n            return False\n\n        print(f\"Response '{choice}' not understood.\")",
        "sha1": "45445e397946a55447e0ea0c550f8c8b063a2a88",
        "id": 221190
    },
    {
        "content": "def correct_jumps_between_arr(arr1, arr2, jump_quantum):\n    \"\"\"\n    Shift the start value of one array to align with the end value of another.\n\n    Parameters\n    ----------\n    arr1 : numpy.ndarray\n        Nx0 array of values in the trajectory of some variable.\n    arr2 : numpy.ndarray\n        Mx0 array of values in the trajectory of some variable. The function\n        will shift all values in this array such that its start aligns with the\n        end of arr1.\n\n    jump_quantum : float\n        The shift added to arr2 will be an integer multiple of this\n        jump_quantum size.\n\n    Returns\n    -------\n    np.ndarray\n        Mx0 array of shifted arr2 values.\n    \"\"\"\n    jump = arr2[0] - arr1[-1]\n    n = round(jump/jump_quantum)\n    # correct the elements following the jump\n    arr2 -= n * jump_quantum\n    return arr2",
        "sha1": "1733c313a70c72c6eeeec83f0e1b9527234abc09",
        "id": 57818
    },
    {
        "content": "def convert_bytes_psec_to_mbits_psec(value):\n    \"\"\"Convert bytes/sec to megabits/sec\n\n    Args:\n        value (float): bytes per sec\n\n    Returns:\n        float: megabits per sec\n    \"\"\"\n    return round(8 * value / pow(10, 6), 3)",
        "sha1": "f95e537944d0eb49b43db87c5722d68b567b3dca",
        "id": 212763
    },
    {
        "content": "def get_page(request, default=''):\n    \"\"\"Get a page name given a request.\n    Args:\n        request (request): The request object.\n        default (str): A default page to return.\n    Returns\n        page (str): The page's URL slug.\n    \"\"\"\n    page = '/'.join(request.path.split('/')[2:]).rstrip('/')\n    if not page:\n        page = default\n    return page",
        "sha1": "e098df8429d5defe09dd2a76a3bc5b49a39d5f25",
        "id": 549420
    },
    {
        "content": "import torch\n\n\ndef neighbor_elements(atomic_numbers, neighbors):\n    \"\"\"\n    Return the atomic numbers associated with the neighboring atoms. Can also be used to gather other properties by\n    neighbors if different atom-wise Tensor is passed instead of atomic_numbers.\n\n    Args:\n        atomic_numbers (torch.Tensor): Atomic numbers (Nbatch x Nat x 1)\n        neighbors (torch.Tensor): Neighbor indices (Nbatch x Nat x Nneigh)\n\n    Returns:\n        torch.Tensor: Atomic numbers of neighbors (Nbatch x Nat x Nneigh)\n\n    \"\"\"\n    # Get molecules in batch\n    n_batch = atomic_numbers.size()[0]\n    # Construct auxiliary index\n    idx_m = torch.arange(n_batch, device=atomic_numbers.device,\n                         dtype=torch.long)[:, None, None]\n    # Get neighbors via advanced indexing\n    neighbor_numbers = atomic_numbers[idx_m, neighbors[:, :, :]]\n    return neighbor_numbers",
        "sha1": "1a76135478162dc1a602a701a45e6a46b5c533ca",
        "id": 576947
    },
    {
        "content": "def UniqueFrozenset(seq):\n  \"\"\"Makes C{frozenset} from sequence after checking for duplicate elements.\n\n  @raise ValueError: When there are duplicate elements\n\n  \"\"\"\n  if isinstance(seq, (list, tuple)):\n    items = seq\n  else:\n    items = list(seq)\n\n  result = frozenset(items)\n\n  if len(items) != len(result):\n    raise ValueError(\"Duplicate values found\")\n\n  return result",
        "sha1": "5858b5a3e8ea76014be2c9bcb3480a1ff7d28ab2",
        "id": 436671
    },
    {
        "content": "from typing import OrderedDict\n\n\ndef normalize_record(pairs, aligned_fields, strip=True):\n    \"\"\"\n    Return a list of pairs of (marker, value) from `pairs`, where values\n    with the same marker are recombined (i.e. unwrapped). If the marker\n    is in `aligned_fields`, spacing will also be normalized (taking the\n    length of the longest token) so that the tokens still align visually\n    in columns.\n\n    Args:\n        pairs: An iterable of (marker, value) pairs.\n        aligned_fields: A container of markers that are aligned.\n    Return:\n        The list of pairs with aligned fields normalized.\n\n    Example:\n\n    >>> data = [\n    ...     ('\\\\t', 'inu=ga   ippiki'),\n    ...     ('\\\\m', 'inu =ga  ichi -hiki'),\n    ...     ('\\\\g', 'dog =NOM one  -CLF.ANIMAL'),\n    ...     ('\\\\t', 'hoeru'),\n    ...     ('\\\\m', 'hoe  -ru'),\n    ...     ('\\\\g', 'bark -IPFV'),\n    ...     ('\\\\f', 'One dog barks.')\n    ... ]\n    >>> for (mkr, val) in normalize_record(data, set(['\\\\t', \\\\g', '\\\\m'])):\n    ...     print(mkr, val)\n    \\t inu=ga   ippiki           hoeru\n    \\m inu =ga  ichi -hiki       hoe  -ru\n    \\g dog =NOM one  -CLF.ANIMAL bark -IPFV\n    \\f One dog barks.\n    \"\"\"\n    field_data = OrderedDict()\n    # gather lines with the same marker, and keep track of the longest\n    # aligned fields at each position\n    maxlens = {}\n    for mkr, val in pairs:\n        if mkr not in field_data:\n            field_data[mkr] = []\n        if val is None:\n            continue\n        field_data[mkr].append(val)\n        i = len(field_data[mkr]) - 1\n        # this string length counts unicode combining characters, so\n        # the lengths may appear off when printed\n        if mkr in aligned_fields and len(val) > maxlens.get(i, -1):\n            maxlens[i] = len(val)\n    # join and normalize spacing (use longest length for each position)\n    mkrs = list(field_data.keys())\n    for mkr in mkrs:\n        data = field_data[mkr]\n        if data == []:\n            joined = None\n        elif mkr in aligned_fields:\n            joined = ' '.join(s.ljust(maxlens[i]) for i, s in enumerate(data))\n        else:\n            joined = ' '.join(data)\n        if strip and joined is not None:\n            joined = joined.rstrip()\n        field_data[mkr] = joined\n    return list(field_data.items())",
        "sha1": "2897762371a896ad1904ebc833f6ddcf3515cfc2",
        "id": 136654
    },
    {
        "content": "def default(value, default):\n    \"\"\"\n    Return `default` is `value` is :data:`None`, otherwise return `value`.\n    \"\"\"\n    if value is None:\n        return default\n    return value",
        "sha1": "517ffb3c6f67ad9290d8c44be5bd54a90bc4e37c",
        "id": 52995
    },
    {
        "content": "def update_rbac_role(\n    self,\n    role: str,\n    menu_items: list,\n    net_read: bool,\n) -> bool:\n    \"\"\"Create or update rbac role\n\n    .. list-table::\n        :header-rows: 1\n\n        * - Swagger Section\n          - Method\n          - Endpoint\n        * - rbacRole\n          - POST\n          - /rbac/role\n\n    .. note::\n        Swagger documentation (in 9.0.2.0) shows net_read value of\n        ``True`` to be read/write but it is flipped.\n\n    :param role: Name of the role\n    :type role: str\n    :param menu_items: List of menu items assigned for access from the\n        role\n    :type menu_items: list\n    :param net_read: Permissions for the attributes,\n        Read/Write (``False``), Read-Only (``True``)\n    :type net_read: bool\n    :return: Returns True/False based on successful call\n    :rtype: bool\n    \"\"\"\n    data = {}\n    data[role] = {\n        \"menuTypeItems\": menu_items,\n        \"net_read\": net_read,\n    }\n\n    return self._post(\n        \"/rbac/role\",\n        data=data,\n        expected_status=[204],\n        return_type=\"bool\",\n    )",
        "sha1": "7e666d1bc06ae47fd921ff8574e167875a5c2441",
        "id": 355326
    },
    {
        "content": "def topitems(iterable):\n    \"\"\" Last (top) items from a list of lists, useful to get 'top' items from a list of stacks e.g.\n        from a list of locations on a stackable game board.\n    \"\"\"\n    return [x[-1] for x in iterable]",
        "sha1": "252173360c4941cfc02622ccff51acefc67563f0",
        "id": 370797
    },
    {
        "content": "import math\n\n\ndef total_sample_splits_categorical(no_of_values):\n    \"\"\"\n    Compute total number of sample splits that can generated by categoricals.\n\n    Parameters\n    ----------\n    no_of_values : Int.\n\n    Returns\n    -------\n    no_of_splits: Int.\n\n    \"\"\"\n    no_of_splits = 0\n    for i in range(1, no_of_values):\n        no_of_splits += math.factorial(no_of_values) / (\n            math.factorial(no_of_values-i) * math.factorial(i))\n    return no_of_splits/2",
        "sha1": "705a565998c5cde4a37370e6787aa0f07d973987",
        "id": 13035
    },
    {
        "content": "def fahrenheit2kelvin(theta):\n    \"\"\"Convert temperature theta in F to K\"\"\"\n    return 5/9*(theta - 32) + 273.15",
        "sha1": "a6f1e67cf6073a1aa7b801f7aef232d1ec9b70ff",
        "id": 200931
    },
    {
        "content": "import re\n\n\ndef clean_place(value):\n    \"\"\"\n    value='Portugal, Faro, Castro Marim<BR/>Portugal'\n    returns 'Portugal', 'Faro', 'Castro Marim',\n\n    value='Portugal, Faro<BR/>Portugal'\n    returns 'Portugal', 'Faro'\n    \"\"\"\n    places = re.split('<BR/>', value)  # different locations\n\n    # split each in (country, district, council)\n    places = [re.split(', ', place) for place in places]\n    place = places[0]\n    # remove locations not determined\n    place = [location for location in place if 'n\u00e3o determinado' not in location]\n\n    # fill place with Nones if incomplete\n    while len(place) < 3:\n        place.append(None)\n    return tuple(place)",
        "sha1": "c10ee156b7e2e87f03305f69b269ac803a5dce89",
        "id": 317272
    },
    {
        "content": "def move_up(t):\n    \"\"\" A method that takes coordinates of bomb's\n    position and returns coordinates of neighbour\n    located above the bomb. It returns None if\n    there isn't such a neighbour \"\"\"\n\n    x, y = t\n    if x == 0:\n        return None\n    else:\n        return (x - 1, y)",
        "sha1": "4fcdcc31cd063e727d55116e37528498ae839337",
        "id": 613755
    },
    {
        "content": "def bin_names_to_coords_filepath(query_bin, ref_bin, results_dir):\n    \"\"\"\n    prepare a file path for results\n    :param query_bin: bin number 1 name/filepath (reference sequence)\n    :param ref_bin: bin number 2 name/filepath.  (query sequence)\n    :return:string like Acidovora-69x_Ga0081644_to_Acidovorax-79_Ga0081651\n    \"\"\"\n    return results_dir + '/' + query_bin + \"_to_\" + ref_bin",
        "sha1": "946b788acaf776322cc02aaa3b6740f198b61b4d",
        "id": 31685
    },
    {
        "content": "def _normalize_name(name):\n    \"\"\" Return normalized event/function name. \"\"\"\n    if '(' in name:\n        return name[:name.find('(')]\n\n    return name",
        "sha1": "e8799dedd18b8305a9377a3805c888502b1df2c0",
        "id": 540477
    },
    {
        "content": "def no_dilution(cf_df):\n    \"\"\"Checks if the shares of investors were NOT diluted since previous year\n    Explanation of Dilution: https://www.investopedia.com/terms/d/dilution.asp\n    cf_df = Cashflow Statement of the specified company\n    \"\"\"\n\n    try:\n        issued_stock = cf_df.iloc[cf_df.index.get_loc(\"Issuance Of Stock\"),0] # Earnings of the company through stock issuance\n    except:\n        issued_stock = 0\n    try:\n        repurchased_stock = cf_df.iloc[cf_df.index.get_loc(\"Repurchase Of Stock\"),0] # Expenditures of the company through stock repurchases\n    except:\n        repurchased_stock = 0\n    if (issued_stock + repurchased_stock <= 0):\n        return True\n    else:\n        return False",
        "sha1": "8ee3b01f5c5c61c2d47ec15ba6e34a588d4ea1ac",
        "id": 558959
    },
    {
        "content": "def calculate_version_code(values_dict, shift):\n  \"\"\"\n  Version Code is calculated based on the four version integers.\n\n  Major is for crosswalk's large update, and minor is for based chromium.\n  Major and minor will always be increasing, so use the sum of them is\n  enough.\n  For each major and minor refresh, build will reset to 0. After that,\n  the build will be increasing for 6 weeks (12 weeks if we skip one upstream\n  beta rebasing), so 100 numbers for build are enough.\n  After we branch it from trunk, the patch will be increasing for the rest of\n  this branch's life, 100 numbers are also enough since it will last for most\n  24 weeks, but the version increasing will be much less frequent after branch\n  point.\n  Shift is the last bit for different configurations we want to upload to\n  PlayStore.\n  \"\"\"\n  try:\n    major = values_dict['MAJOR']\n    minor = values_dict['MINOR']\n    build = values_dict['BUILD']\n    patch = values_dict['PATCH']\n    return (major + minor) * 100000 +\\\n           build * 1000 +\\\n           patch * 10 +\\\n           shift\n  except KeyError:\n    return 0",
        "sha1": "d5a1d24ce710372d70a2cc6d01a6a7f441dadf4d",
        "id": 373596
    },
    {
        "content": "def _node_match(node_a_attr, node_b_attr):\n    \"\"\"\n    Compares attributes of the nodes for equality.\n\n    :param node_a_attr: Attributes of first node.\n    :param node_b_attr: Attributes of second node.\n    :return: True is equal - otherwise False\n    \"\"\"\n    if node_a_attr == node_b_attr:\n        return True\n    return False",
        "sha1": "d72db14a82bf7edb28f19b321e1762d9ce027d52",
        "id": 129476
    },
    {
        "content": "def normal_intersect(p1_x, p1_y, p2_x, p2_y, px, py):\n    \"\"\"\n    Find the point at which a line through seg_p1,\n    seg_p2 intersects a normal dropped from p.\n    \"\"\"\n\n    # Special cases: slope or normal slope is undefined\n    # for vertical or horizontal lines, but the intersections\n    # are trivial for those cases\n    if p2_x == p1_x:\n        return p1_x, py\n    elif p2_y == p1_y:\n        return px, p1_y\n\n    # The slope of the segment, and of a normal ray\n    seg_slope = (p2_y - p1_y) / (p2_x - p1_x)\n    normal_slope = 0 - (1.0 / seg_slope)\n\n    # For y=mx+b form, we need to solve for b (y intercept)\n    seg_b = p1_y - seg_slope * p1_x\n    normal_b = py - normal_slope * px\n\n    # Combining and subtracting the two line equations to solve for intersect\n    x_intersect = (seg_b - normal_b) / (normal_slope - seg_slope)\n    y_intersect = seg_slope * x_intersect + seg_b\n    # Colinear points are ok!\n\n    return (x_intersect, y_intersect)",
        "sha1": "dd243dfb769cc0462c7dd29d46228fd22b880b30",
        "id": 181231
    },
    {
        "content": "def parse_x12_major_version(x12_implementation_version) -> str:\n    \"\"\"\n    Parses the x12 major version from an implementation version string.\n    If the version is invalid, an empty string is returned.\n\n    Example:\n        x = parse_x12_major_version(\"005010X279A1\")\n        print(x)\n        # prints 5010\n\n        x = parse_x12_major_version(\"00501\")\n        print(x)\n        # prints \"\"\n\n    :param x12_implementation_version: The X12 implementation version typically conveyed in ST03\n    :returns: The x12 major version or an empty string\n    \"\"\"\n    if x12_implementation_version is None or len(x12_implementation_version) < 6:\n        return \"\"\n\n    return x12_implementation_version[2:6]",
        "sha1": "ea7163825ada4d5453ca9e7dcbf915009f41f236",
        "id": 45150
    },
    {
        "content": "import re\n\n\ndef split_coord(z):\n    \"\"\"validate and split zoom coordinate.\n    coordinates must be UCSC formatted.\n    e.g. chr1:500-1000\n    chr(colon)start(hyphen)end where start <= end\n    \"\"\"\n    z=z.replace(',','')\n    zoom_coord=re.search(r'(\\S+):(\\d+)-(\\d+)',z)\n    \n    if zoom_coord==None:\n        return None\n        \n    zoom_chr,zoom_start,zoom_end=zoom_coord.groups()\n    zoom_start=int(zoom_start)\n    zoom_end=int(zoom_end)\n    \n    if(zoom_start > zoom_end):\n        return None\n        \n    return [zoom_chr,zoom_start,zoom_end]",
        "sha1": "e674d7f8fc8ea738cde123c34a7ded367061d9e1",
        "id": 503410
    },
    {
        "content": "def ricc_lrcf_solver_options(lrradi_tol=1e-10,\n                             lrradi_maxiter=500,\n                             lrradi_shifts='hamiltonian_shifts',\n                             hamiltonian_shifts_init_maxiter=20,\n                             hamiltonian_shifts_init_seed=None,\n                             hamiltonian_shifts_subspace_columns=6):\n    \"\"\"Returns available Riccati equation solvers with default solver options.\n\n    Parameters\n    ----------\n    lrradi_tol\n        See :func:`solve_ricc_lrcf`.\n    lrradi_maxiter\n        See :func:`solve_ricc_lrcf`.\n    lrradi_shifts\n        See :func:`solve_ricc_lrcf`.\n    hamiltonian_shifts_init_maxiter\n        See :func:`hamiltonian_shifts_init`.\n    hamiltonian_shifts_init_seed\n        See :func:`hamiltonian_shifts_init`.\n    hamiltonian_shifts_subspace_columns\n        See :func:`hamiltonian_shifts`.\n\n    Returns\n    -------\n    A dict of available solvers with default solver options.\n    \"\"\"\n    return {'lrradi': {'type': 'lrradi',\n                       'tol': lrradi_tol,\n                       'maxiter': lrradi_maxiter,\n                       'shifts': lrradi_shifts,\n                       'shift_options':\n                       {'hamiltonian_shifts': {'type': 'hamiltonian_shifts',\n                                               'init_maxiter': hamiltonian_shifts_init_maxiter,\n                                               'init_seed': hamiltonian_shifts_init_seed,\n                                               'subspace_columns': hamiltonian_shifts_subspace_columns}}}}",
        "sha1": "1db7c993385e2417ea3822ed7964042a5ac8c35e",
        "id": 549244
    },
    {
        "content": "def f_to_c(tempe):\n    \"\"\"Receives a temperature in Fahrenheit and returns in Celsius\"\"\"\n    return (tempe - 32) / 1.8",
        "sha1": "fa318439fac1bd0915d2dda70c3f1a5cede579ef",
        "id": 269688
    },
    {
        "content": "import re\n\n\ndef _parse_bitbucket_url(url):\n    \"\"\"\n    Handle URLs of the form:\n     - git://bitbucket.org/conducto/super.git\n     - git@bitbucket.org:conducto/super.git\n     - https://bitbucket.org/conducto/super.git\n     - git@bitbucket.org-user/conducto/super.git\n    Look for \"bitbucket.org/{owner}/{repo}.git\" or \"bitbucket.com:{owner}/{repo}.git\" or \"bitbucket.com-user:{owner}/{repo}.git\"\n    \"\"\"\n    m = re.search(r\"bitbucket\\.org(?:-.*?)?[/:]([^/]+)/(.+?)\\.git$\", url)\n    if not m:\n        raise ValueError(f\"{url} is not a valid bitbucket url.\")\n\n    owner, repo = m.group(1, 2)\n    return owner, repo",
        "sha1": "f81126c461017d97371c2e823d9fe829d6654a01",
        "id": 583534
    },
    {
        "content": "def get_line_count(file_name, chunk_size=8192*1024):\n    \"\"\"\n    Get the number of lines in a file.\n\n    Parameters:\n        file_name (str): file name\n        chunk_size (int, optional): chunk size for reading large files\n    \"\"\"\n    count = 0\n    with open(file_name, \"rb\") as fin:\n        chunk = fin.read(chunk_size)\n        while chunk:\n            count += chunk.count(b\"\\n\")\n            chunk = fin.read(chunk_size)\n    return count",
        "sha1": "ee820620c56a94cd7b432d865b49cf1aa52730d1",
        "id": 102616
    },
    {
        "content": "def make_event(data_line):\n    \"\"\"\n    Given a line of data from a tab separated data file, return a dict containing the data for the event\n    :param data_line:\n    :return: dict\n    \"\"\"\n    return dict(zip(['tag', 'label', 'url', 'start', 'end'], data_line.strip('\\n').split('\\t')))",
        "sha1": "497022600cb8fed3b23622b99356b190d88a7eaa",
        "id": 354943
    },
    {
        "content": "def lang(name, comment_symbol, multistart=None, multiend=None):\n    \"\"\"\n    Generate a language entry dictionary, given a name and comment symbol and\n    optional start/end strings for multiline comments.\n    \"\"\"\n    result = {\n        \"name\": name,\n        \"comment_symbol\": comment_symbol\n    }\n    if multistart is not None and multiend is not None:\n        result.update(multistart=multistart, multiend=multiend)\n    return result",
        "sha1": "e1e89caf4bb595e61a94df1ee70d33c39e75f486",
        "id": 688778
    },
    {
        "content": "import socket\n\n\ndef socket_is_closed(sock: socket.socket) -> bool:\n    \"\"\"\n    Checks if a socket is closed.\n\n    Parameters\n    ----------\n    sock: :class:`socket.socket`\n        The socket to check.\n    \"\"\"\n    return sock.fileno() == -1",
        "sha1": "a113f57c20cb6a8bdb4d21fb2c26fbc420b5bcd2",
        "id": 216803
    },
    {
        "content": "import torch\n\n\ndef mesh_grid(h, w, device=torch.device('cpu')):\n    \"\"\"\n    Creates a mesh grid with normalized pixel values.\n    \"\"\"\n    assert h == w, \"Only support square images for now\"\n    r = torch.arange(0.5, h, 1) / (h / 2) - 1\n    xx, yy = torch.meshgrid(r, -r)\n    return xx.to(torch.float32).to(device), yy.to(torch.float32).to(device)",
        "sha1": "0510102e16cd6c69b1aecb54c88681dfa73be81a",
        "id": 578779
    },
    {
        "content": "def manual_mean(arr):\n    \"\"\"\n    Compute mean (average) of all elements in the given 2D array.\n\n    Parameters\n    ----------\n    arr : ndarray\n\n    Returns\n    -------\n    float\n        the mean of all the elements in the array\n    \"\"\"\n    the_sum = 0\n    for i in range(0, arr.shape[0]):\n        for j in range(0, arr.shape[1]):\n            the_sum += arr[i, j]\n    return the_sum / arr.size",
        "sha1": "17cb09022a675975662b7c6c47403e0a9b2381ae",
        "id": 453264
    },
    {
        "content": "def narrow(checked, DASHRlut):\n    \"\"\"Returns dictionary mapping selected DASHRs to local drives.\n\n    :param checked: array of integer pins\n    :param DASHRlut: dictionary mapping of all locally detected DASHRs\n    :returns: dictionary mapping only user-selected DASHRs\n    \"\"\"\n    for_harvest = {}\n    for pin in checked:\n        if pin in DASHRlut:\n            for_harvest[pin] = DASHRlut[pin]\n    return for_harvest",
        "sha1": "d34d579a687c77c4c33d1b4fa064002e4e9e51f7",
        "id": 646437
    },
    {
        "content": "def create_api_host_url(host, endpoints):\n\n    \"\"\" create_api_host_url\n\n    Creates the endpoint url for the API call.\n\n    INPUTS\n\n    @host [str]: The base URL for the API call.\n\n    @endpoints [list]: The API endpoint names for the chosen method\n\n    RETURNS\n\n    @api_url [str]: The full endpoint url for the method.\n\n    \"\"\"\n\n    api_url = '/'.join((host, *endpoints))\n\n    return api_url",
        "sha1": "4f76fb5241804cb139af0607aaaf88dd7bb285b1",
        "id": 351149
    },
    {
        "content": "def collatz_len_fast(n, cache):\n    \"\"\"Slightly more clever way to find the collatz length.\n\n    A dictionary is used as a cache of previous results, and since\n    the dictionary passed in is mutable, our changes will reflect\n    in the caller.\n    \"\"\"\n    if n == 1:\n        return 1\n    if n in cache:\n        return cache[n]\n\n    if n % 2 == 0:\n        cache[n] = collatz_len_fast(n // 2, cache) + 1\n    else:\n        cache[n] = collatz_len_fast(3 * n + 1, cache) + 1\n    return cache[n]",
        "sha1": "61b82d495e76eca921280dea1b1afa460409d05a",
        "id": 550039
    },
    {
        "content": "import math\n\n\ndef circle_area(radius):\n    \"\"\"\n    Calculates the area of a circle with the given radius.\n\n    :param radius: The radius of the circle\n    :return: Area of the circle\n    \"\"\"\n    return math.pi * (radius ** 2)",
        "sha1": "82f76f997b3964a83b47a7d287f44f01f70e8d4b",
        "id": 565898
    },
    {
        "content": "import six\n\n\ndef _bytes(*args):\n    \"\"\"\n    Returns a byte array (bytes in py3, str in py2) of chr(b) for\n    each b in args\n    \"\"\"\n    if six.PY2:\n        return \"\".join(chr(c) for c in args)\n    # else: Python 3\n    return bytes(args)",
        "sha1": "59fbdc1f10ef1a5e8d3b0f43aae8f18614c5d554",
        "id": 653828
    },
    {
        "content": "import pkgutil\nimport importlib\n\n\ndef scan_for_agents(do_registration=True):\n    \"\"\"Identify and import ocs Agent plugin scripts.  This will find all\n    modules in the current module search path (sys.path) that begin\n    with the name 'ocs_plugin\\_'.\n\n    Args:\n        do_registration (bool): If True, the modules are imported,\n            which likely causes them to call register_agent_class on\n            each agent they represent.\n\n    Returns:\n        The list of discovered module names.\n\n    \"\"\"\n    items = []\n    for modinfo in pkgutil.iter_modules():\n        if modinfo.name.startswith('ocs_plugin_'):\n            items.append(modinfo.name)\n            if do_registration:\n                importlib.import_module(modinfo.name)\n    return items",
        "sha1": "0052fafa9dc3936f64b502b8e4b9384835551445",
        "id": 325548
    },
    {
        "content": "def center_of_slide_level(slide,level):\n\n    \"\"\"\n        center x,y point of input level image\n\n        slide =  input slide with Openslide type\n        level = disired level\n\n        return  center point x, y\n    \"\"\"\n\n    c_x, c_y = slide.level_dimensions[level]\n    c_x/=2\n    c_y/=2\n    return c_x, c_y",
        "sha1": "3046ce63bc824b9dd04b17e7be99d71abc980761",
        "id": 360231
    },
    {
        "content": "def validation_integer(value, obj=None):\n    \"\"\"\n   Validates that value is an integer number.\n   No change is made to the value\n    \"\"\"\n    try:\n        check = int(value)\n        return True, value\n    except:\n        return False, value",
        "sha1": "8618154468ed6e42daca97bde2cd2a749bb5b6e3",
        "id": 192673
    },
    {
        "content": "def fullscan_policy(obs):\n    \"\"\"\n    A policy function that generates only fullscan data.\n    \"\"\"\n    valid_actions = obs['valid_actions']\n    ms1_action = len(valid_actions) - 1  # last index is always the action for MS1 scan\n    return ms1_action",
        "sha1": "39172e8a421d485d790c01c14407a623026d5422",
        "id": 627971
    },
    {
        "content": "def get_high_pin_idx_pull_up(pin_input_values):\n    \"\"\"Returns the index of the first high pin value. Assumes Pull UP mode. i.e. a stronger force will pull the pin value down to 0. If no high pins were found, None is returned.\"\"\"\n    high_pin_idx = (\n        pin_input_values.index(False) if False in pin_input_values else None\n    )\n    return high_pin_idx",
        "sha1": "92cad383e5bfa7775a800629fc2650c2070177d1",
        "id": 316169
    },
    {
        "content": "def reference_value_for_covariate_mean_all_values(cov_df):\n    \"\"\"\n    Strategy for choosing reference value for country covariate.\n    This one takes the mean of all incoming covariate values.\n    \"\"\"\n    return float(cov_df[\"mean_value\"].mean())",
        "sha1": "57935c7b39e2f02f059e7f4b4a835bbe84a67081",
        "id": 39153
    },
    {
        "content": "from typing import Optional\nfrom typing import Any\n\n\ndef access_dot_path(dictionary: dict, path: str) -> Optional[Any]:\n    \"\"\"\n    Access dot-separated path in dictionary or return None\n    \"\"\"\n    dot_index = path.find('.')\n    if dot_index == -1:  # no more dots in path\n        return dictionary[path]\n    previous = path[:dot_index]  # key before first dot\n    if previous not in dictionary:\n        return None\n    element = dictionary[previous]\n    if isinstance(element, dict):\n        return access_dot_path(element, path[dot_index + 1:])",
        "sha1": "c6e4773fd486e5b51d93465d060919a2b8a9770d",
        "id": 518195
    },
    {
        "content": "import math\n\n\ndef sqroot(x):\n    \"\"\"\n    Finds the square root of the number passed in\n    \"\"\"\n    return math.sqrt(x)",
        "sha1": "868f74584e1ff28a5534331cae624f8e655d8e98",
        "id": 127203
    },
    {
        "content": "def BuildEdgeDict(surface):\n    \"\"\" Create edge dictionary from a surface.\n    The edge dictionary is indexed by a tuple \n    (vertexindex1,vertexindex2) of indices into surface.vertexes. \n    It contains a list of polygon ids that have an edge that shares\n    these two vertices. \n    \n    This function assumes that identical vertices in the surface \n    have been merged, so the vertexindex uniquely identifies the\n    vertex. \n\n    This function returns the edge dictionary\n\"\"\"\n\n    edges={}  # lists of polygon numbers, indexed by a tuple of the  vertex indices connected by the edge\n    for polynum in range(surface.vertexidx_indices.shape[0]):\n        firstidx=surface.vertexidx_indices[polynum]\n                \n        numvertices=surface.numvertices[polynum]\n        for firstvertex in range(numvertices):\n            nextvertex=(firstvertex+1) % numvertices\n            \n            firstvertexidx=surface.vertexidx[firstidx+firstvertex]\n            nextvertexidx=surface.vertexidx[firstidx+nextvertex]\n            \n            if (firstvertexidx,nextvertexidx) in edges:\n                edges[(firstvertexidx,nextvertexidx)].append((polynum,firstvertex,nextvertex))\n                pass\n            elif (nextvertexidx,firstvertexidx) in edges:\n                edges[(nextvertexidx,firstvertexidx)].append((polynum,nextvertex,firstvertex))\n                pass\n            else:\n                edges[(firstvertexidx,nextvertexidx)] = [ (polynum,firstvertex,nextvertex) ]\n                pass\n            pass\n        pass\n    return edges",
        "sha1": "f05956640a3da02b59aa3d270ec69ef955f99638",
        "id": 267335
    },
    {
        "content": "def compute_average(grades):\n    \"\"\"\n    Computes the average of all the grades in the list\n\n    Parameters: grades (list)\n    Returns: the average (float) rounded up to one decimal\n    \"\"\"\n    average = round(sum(grades)/len(grades), 1)\n    return average",
        "sha1": "3a85a59c4047ef14b64e074c00bb6e0c7d95931d",
        "id": 641106
    },
    {
        "content": "def cmp_string(val1, val2):\n    \"\"\" Compares 'val1' and 'val2' as unsigned numbers. Respects the\n    unix 'cmp' function return values (-1, 0, 1). \"\"\"\n\n    if val1 > val2:\n        return 1\n\n    if val1 < val2:\n        return -1\n\n    return 0",
        "sha1": "24fac6e184877bff1077d90d928524d70d1c0a60",
        "id": 138188
    },
    {
        "content": "def split_s3_url(s3_url):\n    \"\"\"\n    Breaks up s3 URL into\n    bucket\n    path under bucked\n    tail (file or folder)\n    :param s3URL:\n    :return: 3 tuple (bucket, folder, tail)\n    \"\"\"\n    s3_url_arr = s3_url.rsplit('/')[2:]\n    if len(s3_url_arr) > 1:\n        return s3_url_arr[0], '/'.join(s3_url_arr[1:-1]), s3_url_arr[-1]\n    elif len(s3_url_arr) == 1:\n        return s3_url_arr[0], \"\", \"\"\n    else:\n        raise ValueError(\"'{}' not a valid S3 URL\".format(s3_url))",
        "sha1": "d50a3a9fe4dd4e26784fc499d087014e0a14f57f",
        "id": 94115
    },
    {
        "content": "def skip_14(labels):\n    \"\"\"Customised label skipping for 14 quantiles.\"\"\"\n    return (\n        labels[0],\n        labels[2],\n        labels[4],\n        labels[6],\n        labels[8],\n        labels[10],\n        labels[13],\n    )",
        "sha1": "4c784d7625486d8e4a56006d22af9455f90238f9",
        "id": 606841
    },
    {
        "content": "import textwrap\n\n\ndef redent(text, indent):\n    \"\"\"Remove all common indentation from 'text' and then insert\n       'indent' at the beginning of each line. 'indent' can be either\n       a string, which is inserted as is, or a number, which is how\n       many spaces to insert.\"\"\"\n    if isinstance(indent, int):\n        indent = ' '*indent\n    return '\\n'.join(indent+line for line in textwrap.dedent(text).split('\\n'))",
        "sha1": "6aeb6c264e6e871e0aea2cb502711a4e95aa88c9",
        "id": 381643
    },
    {
        "content": "def _root_task(worker):\n    \"\"\"\n    Return the first task scheduled by the worker, corresponding to the root task\n    \"\"\"\n    return worker._add_task_history[0][0]",
        "sha1": "8a2b7e1fb6984f588611c29fd316664033d3d333",
        "id": 507460
    },
    {
        "content": "def slim_down_df(godb, df_clean, go_colname):\n    \"\"\"\n    Maps GO column to slim GO\n    :param godb: The GO database\n    :param df_clean: DataFrame with one GO term per row\n    :param go_colname: Name for the column with GO terms\n    :return: DataFrame with old GO terms replaced with slim GO terms\n    \"\"\"\n    # make a local copy, rather than modifying the original df\n    df_loc = df_clean.copy()\n    # get a panda Series object of the GO terms\n    func_series = df_loc[go_colname]\n    # convert the Series to a set, for map_set_to_slim\n    func_set = set(func_series)\n    # returns a dictionary with {<full_go_term>: <slim_go_term>, ... }\n    mapper = godb.map_set_to_slim(func_set)\n    # use map on the Series\n    df_loc.loc[:, go_colname] = func_series.map(mapper)\n    # return copy\n    return df_loc",
        "sha1": "cde763ff0495cca17d94c2f8c773f5e938e3af09",
        "id": 580516
    },
    {
        "content": "def logout_user(_):\n    \"\"\"Log user out.\"\"\"\n    return {}, 200",
        "sha1": "edb75ddc32f905c62600ef7706ccd060d02f466c",
        "id": 48310
    },
    {
        "content": "import hashlib\n\n\ndef string_to_md5(content: str) -> str:\n    \"\"\"\n    Take a string and calculate its md5 hash (as a string).\n    \"\"\"\n    encoded = content.encode(\"utf8\")\n    return hashlib.md5(encoded).hexdigest()",
        "sha1": "1949385c5f95af092147b6576647769f79318109",
        "id": 77809
    },
    {
        "content": "def document_lookup(md_files):\n    \"\"\"\n    Given a list of MarkdownDocument files, create a dictionary keyed by\n    the filename (independent of the path). It will map the filename to\n    a list of potential files.\n\n    # Parameters\n\n    md_files:list(MarkdownDocument)\n        - The list of MarkdownDocument objects to create the lookup\n        dictionary\n\n    # Return\n\n    A dictionary keyed by a filename mapped to a list of\n    MarkdownDocument objects that have that filename but are on\n    different paths.\n\n    \"\"\"\n\n    reverse = {}\n\n    for md in md_files:\n        reverse.setdefault(md.filename.name, []).append(md)\n\n    return reverse",
        "sha1": "c99121debf71aaa1e9435b664016f70929241755",
        "id": 122506
    },
    {
        "content": "def still_a_dependency(dependency):\n    \"\"\"\n    Defines the criteria for which a dependency is deemed complete (and therefore no longer a dependency).\n\n     Args:\n        dependency, Table.Row or dict. Processing row corresponding to the required input for the job in prow.\n                                     This must contain keyword accessible values for 'STATUS', and 'LATEST_QID'.\n\n    Returns:\n        bool. False if the criteria indicate that the dependency is completed and no longer a blocking factor (ie no longer\n              a genuine dependency). Returns True if the dependency is still a blocking factor such that the slurm\n              scheduler needs to be aware of the pending job.\n\n    \"\"\"\n    return dependency['LATEST_QID'] > 0 and dependency['STATUS'] != 'COMPLETED'",
        "sha1": "428eb94243e1697cdc6bb083d0f739453731505f",
        "id": 615214
    },
    {
        "content": "def make_dict(s_list):\n    \"\"\" Convert file list into a dictionary with the file path as its key, and meta data\n    as a list stored as the keys value. This format change makes searching easier. \"\"\"\n    return { l_itm['path'] : l_itm for l_itm in s_list}",
        "sha1": "5e5e2c0253444fc65d01c4558c07b6b8675bb84c",
        "id": 461860
    },
    {
        "content": "import math\n\n\ndef rollout(mdp, start_state, policy, *, max_length=math.inf):\n    \"\"\"Roll out a policy to generate (s, a) trajectories\n\n    Args:\n        mdp (gym.env): MDP instance\n        start_state (any): Starting state for the rollout\n        policy (function): Policy p(s) -> a\n\n        max_length: Maximum trajectory length\n\n    Returns:\n        (list): A single trajectory, as list of (s, a, r) pairs\n    \"\"\"\n\n    # Reset the MDP and set the initial state\n    mdp.reset()\n    mdp.state = start_state\n\n    trajectory = []\n    while True:\n\n        # Copy starting state\n        start_state = mdp.state\n\n        # Query the policy for an action\n        action = policy(mdp.state)\n\n        # Take that action\n        state, reward, done, status = mdp.step(action)\n\n        # Store the (s, a, r) tuple\n        trajectory.append((start_state, action, reward))\n\n        # Check exit conditions\n        if done or len(trajectory) > max_length:\n\n            # Append the final state\n            trajectory.append((mdp.state, None, None))\n\n            break\n\n    return trajectory",
        "sha1": "47949ba506a22d383b4e0a6b0076835a81473209",
        "id": 175449
    },
    {
        "content": "def update_detects_payload(current_payload: dict, passed_keywords: dict) -> dict:\n    \"\"\"Update the provided payload with any viable parameters provided as keywords.\"\"\"\n    if passed_keywords.get(\"assigned_to_uuid\", None):\n        current_payload[\"assigned_to_uuid\"] = passed_keywords.get(\"assigned_to_uuid\", None)\n    if passed_keywords.get(\"show_in_ui\", None):\n        current_payload[\"show_in_ui\"] = passed_keywords.get(\"show_in_ui\", None)\n    if passed_keywords.get(\"status\", None):\n        current_payload[\"status\"] = passed_keywords.get(\"status\", None)\n    if passed_keywords.get(\"comment\", None):\n        current_payload[\"comment\"] = passed_keywords.get(\"comment\", None)\n\n    return current_payload",
        "sha1": "821f50b1e034e3fef6d1980a894cc164bfdee689",
        "id": 548038
    },
    {
        "content": "import itertools\n\n\ndef take(n, seq):\n    \"\"\" The first n elements of a sequence\n\n    >>> list(take(2, [10, 20, 30, 40, 50]))\n    [10, 20]\n    \"\"\"\n    return itertools.islice(seq, n)",
        "sha1": "63eba56531a6ebef46cefb91d104ab37352452a6",
        "id": 279092
    },
    {
        "content": "import torch\n\n\ndef mse_loss(y, y_hat):\n    \"\"\"\n    Args:\n        y: the label tensor (batch_size, linear_2_out_features)\n        y_hat: the prediction tensor (batch_size, linear_2_out_features)\n\n    Return:\n        J: scalar of loss\n        dJdy_hat: The gradient tensor of shape (batch_size, linear_2_out_features)\n    \"\"\"\n    loss = torch.mean(torch.pow(y-y_hat, 2.0))\n    dJdy_hat = (2.0 * (y_hat - y))/(y.shape[0]*y.shape[1])\n\n    return loss, dJdy_hat",
        "sha1": "f182712538c36a116f80caeb05d517fad3d56de0",
        "id": 143371
    },
    {
        "content": "def reverse(input_list):\n    \"\"\"\n    Reverses a list\n\n    :param input_list: A list of strings (any list would work though)\n    :return: A list, in reverse order\n    \"\"\"\n    output_list = []\n    list_length = len(input_list)\n    for index in range(list_length-1, -1, -1):      # Pay attention to the range here; why'd I do this?\n        # print input_list[index]                   # For testing purposes only\n        output_list.append(input_list[index])\n    return output_list",
        "sha1": "b19382c034de792735da018d0a9f30af2faf589f",
        "id": 594748
    },
    {
        "content": "def file_write(file_handle, file_blocks):\n    \"\"\"A simple function to write a part of a file in chunks. It is decorated\n    with a timer to track duration.\n\n    - Args:\n        - file_handle (file): the open file to write\n        - file_blocks (file): the data to write\n\n    - Returns:\n        - [file]: returns the written blocks\n    \"\"\"\n    return file_handle.write(file_blocks)",
        "sha1": "c4787e0205888a3e9813d201a397fac660e6300a",
        "id": 254201
    },
    {
        "content": "import codecs\n\n\ndef is_known_charset(charset):\n    \"\"\"Checks if the given charset is known to Python.\"\"\"\n    try:\n        codecs.lookup(charset)\n    except LookupError:\n        return False\n    return True",
        "sha1": "af98d2a1da474e7f73fa352c75a556651ca5b5a2",
        "id": 544869
    },
    {
        "content": "def url_method_key(url: str, method: str) -> str:\n    \"\"\"\n    Generate a fake key based on url and method\n    :param url: str -> The url\n    :param method: str -> The method\n    :return: str\n    \"\"\"\n    return f\"{method.lower()}-{url}\"",
        "sha1": "5c217ecd96db3751cc21814260a023ef97feadda",
        "id": 122238
    },
    {
        "content": "def node_is_in_list(node,list_of_nodes):\n    \"\"\"\n    Check if the node is present in the list of nodes\n    \"\"\"\n    for node2 in list_of_nodes:\n        if node is node2:\n            return True\n    return False",
        "sha1": "984c6f5eb466c07fddeb4bb8fb93052b5bda62e8",
        "id": 336417
    },
    {
        "content": "def _create_get_memorized_lines(lines):\n    \"\"\"Creates a function that returns the given string.\"\"\"\n\n    def get_memorized_lines():\n        return lines\n    return get_memorized_lines",
        "sha1": "ea8fa5d82c3d875add11209f7a9237d28d76f0bc",
        "id": 423365
    },
    {
        "content": "def run_once(task, seconds=0, args=[], kwargs={}):\n    \"\"\"\n    Run a one-time task later after the specified seconds.\n\n    :param task: the task proxy\n    :param seconds: the delayed seconds before running.\n    :return: the schedule.\n    \"\"\"\n    return task.run_once(seconds, args, kwargs)",
        "sha1": "6f8b12a891c5d8d5f7710241a45766dd565010f6",
        "id": 266540
    },
    {
        "content": "def msg2dict(msg):\n  \"\"\"Convert a generic ROS message into a dictionary (un-nested)\n  :param msg: ROS message to convert\n  :rtype: dict\n  \"\"\"\n  return {k: getattr(msg, k) for k in msg.get_fields_and_field_types().keys()}",
        "sha1": "52803702efc2ea804337eb53f4c28782c3ffa424",
        "id": 639868
    },
    {
        "content": "def read_dataset(spark, file_path, file_type, file_configs={}):\n    \"\"\"\n    This function reads the input data path and return a Spark DataFrame. Under the hood, this function is based\n    on generic Load functionality of Spark SQL.\n\n    Parameters\n    ----------\n    spark\n        Spark Session\n    file_path\n        Path to input data (directory or filename).\n        Compatible with local path and s3 path (when running in AWS environment).\n    file_type\n        \"csv\", \"parquet\", \"avro\", \"json\".\n        Avro data source requires an external package to run, which can be configured with spark-submit\n        (--packages org.apache.spark:spark-avro_2.11:2.4.0).\n    file_configs\n        This optional argument is passed in a dictionary format as key/value pairs\n        e.g. {\"header\": \"True\",\"delimiter\": \"|\",\"inferSchema\": \"True\"} for csv files.\n        All the key/value pairs in this argument are passed as options to DataFrameReader,\n        which is created using SparkSession.read. (Default value = {})\n\n    Returns\n    -------\n    DataFrame\n\n    \"\"\"\n    odf = spark.read.format(file_type).options(**file_configs).load(file_path)\n    return odf",
        "sha1": "e9fb4d2195162ce880048fee262bfa489f741bbd",
        "id": 107097
    },
    {
        "content": "def count(text, character):\n    \"\"\"Return the amount of certain character in the text.\"\"\"\n    return text.count(character)",
        "sha1": "41d6b88845f472e275c946086b0891b3d5cc9f33",
        "id": 391241
    },
    {
        "content": "def CheckLongLines(input_api, output_api, maxlen=80, source_file_filter=None):\n  \"\"\"Checks that there aren't any lines longer than maxlen characters in any of\n  the text files to be submitted.\n  \"\"\"\n  bad = []\n  for f, line_num, line in input_api.RightHandSideLines(source_file_filter):\n    # Allow lines with http://, https:// and #define/#pragma/#include/#if/#endif\n    # to exceed the maxlen rule.\n    if (len(line) > maxlen and\n        not 'http://' in line and\n        not 'https://' in line and\n        not line.startswith('#define') and\n        not line.startswith('#include') and\n        not line.startswith('#pragma') and\n        not line.startswith('#if') and\n        not line.startswith('#endif')):\n      bad.append(\n          '%s, line %s, %s chars' %\n          (f.LocalPath(), line_num, len(line)))\n      if len(bad) == 5:  # Just show the first 5 errors.\n        break\n\n  if bad:\n    msg = 'Found lines longer than %s characters (first 5 shown).' % maxlen\n    return [output_api.PresubmitPromptWarning(msg, items=bad)]\n  else:\n    return []",
        "sha1": "79765e5f7d7f33a55101fc9741c59b27094d44bf",
        "id": 386486
    },
    {
        "content": "def format_date(date):\n    \"\"\"Return ISO 8601 date string for `date`\n\n    Uses format `YYYY-MM-DDThh:mm:ss.sTZD`\n\n    Parameters\n    ----------\n    date : datetime\n        Date to format\n\n    Returns\n    -------\n    str\n        ISO 8601 formatted date string\n    \"\"\"\n    return (date.strftime('%Y-%m-%dT%H:%M:%S')\n            + date.strftime('.%f')[:4] + 'Z')",
        "sha1": "a860b5872fc448f7e56e870702cefe151df1e44d",
        "id": 527971
    },
    {
        "content": "def _impaired_or_not(z_score, cutoff):\n    \"\"\" Dichotimize z-score by applying a cutoff\n\n    :param z_score: the z-score, i.e. performance relative to a reference population\n    :param cutoff: the cut-off to decide impaired (<=) or preserved (>) on the cognitive domain\n    :return: 1 if impaired, 0 if preserved\n    \"\"\"\n    if z_score <= cutoff:\n        return 1\n    else:\n        return 0",
        "sha1": "8d53497beca4f4f7962cf3a49086c366f4c64fea",
        "id": 198030
    },
    {
        "content": "import torch\n\n\ndef normalize_point_batch_to_sphere(pc: torch.Tensor, NCHW=True):\n    \"\"\"\n    normalize a batch of point clouds\n    :param\n        pc      [B, N, 3] or [B, 3, N]\n        NCHW    if True, treat the second dimension as channel dimension\n    :return\n        pc      normalized point clouds, same shape as input\n        centroid [B, 1, 3] or [B, 3, 1] center of point clouds\n        furthest_distance [B, 1, 1] scale of point clouds\n    \"\"\"\n    point_axis = 2 if NCHW else 1\n    dim_axis = 1 if NCHW else 2\n    centroid = torch.mean(pc, dim=point_axis, keepdim=True)\n    pc = pc - centroid\n    furthest_distance, _ = torch.max(\n        torch.sqrt(torch.sum(pc ** 2, dim=dim_axis, keepdim=True)), dim=point_axis, keepdim=True)\n    pc = pc / furthest_distance\n    return pc, centroid, furthest_distance",
        "sha1": "5be7aee1886fd9a66d0cc5bd6373a3baf5935690",
        "id": 88791
    },
    {
        "content": "import copy\n\n\ndef strip_revision_in_copy(doc_map):\n  \"\"\" Strip the _rev field in a deep copy of doc_map and return it.\n  \n  :param dict doc_map: A dict representation of a JSON document.\n  :return:  doc_map itself without _rev\n  \"\"\"\n  new_doc = copy.deepcopy(doc_map)\n  new_doc.pop(\"_rev\", None)\n  return new_doc",
        "sha1": "b0c5f7c2d674064effb270050aea654610eb90f4",
        "id": 635141
    },
    {
        "content": "def split_data(data, val_size=0, test_size=0):\n    \"\"\"\n    splits data to training, validation and testing parts\n    \"\"\"\n    #split based on percentage\n    ntest = int(round(len(data) * (1 - test_size)))\n    nval = int(round(len(data.iloc[:ntest]) * (1 - val_size)))\n\n    df_train, df_val, df_test = data.iloc[:nval], data.iloc[nval:ntest], data.iloc[ntest:]\n\n    return df_train, df_val, df_test",
        "sha1": "d3da49c2307fffd8a6494af76769991100a59b64",
        "id": 281670
    },
    {
        "content": "import torch\n\n\ndef logsumexp_masked(risk_scores,\n                     riskset):                     \n    \"\"\"Compute logsumexp across `axis` for entries where `mask` is true.\"\"\"\n    risk_score_masked = torch.mul(risk_scores, riskset).float()\n    # for numerical stability, substract the maximum value\n    # before taking the exponential\n    amax = torch.max(risk_score_masked, 1)[0]\n    risk_score_shift = risk_score_masked.sub(amax)\n    exp_masked = torch.mul(risk_score_shift.exp(), riskset)\n    exp_sum = torch.sum(exp_masked,1)\n    output = amax + torch.log(exp_sum)      \n    return output",
        "sha1": "76a12c93886d8900780b7f2bffd368888937624e",
        "id": 449716
    },
    {
        "content": "import time\nimport logging\n\n\ndef warn_slow(func, timelimit=60, *args, **kwargs):\n    # noinspection SpellCheckingInspection\n    \"\"\"\n    Decorator that prints a log message if the function execution\n    took longer then specified.\n\n    >>> @warn_slow  # warn if it takes more than 1 minute\n    >>> def preprocess_input_files(inputdir, tempdir):\n    ...\n    >>> @warn_slow(timelimit=600)  # warn if it takes more than 10 minutes\n    >>> def run_calculation(tempdir, outdir):\n    ...\n    >>>\n\n    :param func: the decorated function\n    :param timelimit: the time limit before warning\n    :param args: the positional args of the function\n    :param kwargs: the keyword args of the function\n    :return: the function return value\n    \"\"\"\n    start = time.perf_counter()\n    result = func(*args, **kwargs)\n    end = time.perf_counter()\n    run_time = end - start\n    if run_time > timelimit:\n        logging.warning(f\"{func.__name__} took {run_time} seconds\")\n    else:\n        logging.info(f\"{func.__name__} took {run_time} seconds\")\n    return result",
        "sha1": "c3f8db8f79f7f80b6b038190660d1e81ab8e4be0",
        "id": 146577
    },
    {
        "content": "def CI_compare(CI1, CI2):\n    \"\"\"Return +1 if CI1 > CI2, -1 if CI1 < CI2, 0 if overlapping\"\"\"\n    if CI1[1] < CI2[0]:\n        return -1\n    elif CI2[1] < CI1[0]:\n        return +1\n    else:\n        return 0",
        "sha1": "f07fe7c77a219d074d0ee46536078bbacb37b03f",
        "id": 92082
    },
    {
        "content": "import math\n\n\ndef min_l(k):\n    \"\"\"A lower bound on l for a k-path.\"\"\"\n    if k == 0:\n        return 0\n    return math.floor(k / 2) + 1",
        "sha1": "d7e0e941ce88c0f8c8d611bf5c138ade32eeb24f",
        "id": 274588
    },
    {
        "content": "import inspect\n\n\ndef _is_non_defaulted_positional_args(param: inspect.Parameter) -> bool:\n  \"\"\"Returns True if `param` is a positional argument with no default.\"\"\"\n  return ((param.kind == param.POSITIONAL_OR_KEYWORD or\n           param.kind == param.POSITIONAL_ONLY) and\n          param.default is param.empty)",
        "sha1": "a001f9130f3e5be1acad61959faf5e9ef66c19f1",
        "id": 677127
    },
    {
        "content": "import unicodedata\n\n\ndef norm_fold(astr: str) -> str:\n    \"\"\"Normalize and casefold Unicode strings for saner comparisons.\n\n    :param astr: input unicode string\n    :return: a normalized and case-folded version of the input string\n    \"\"\"\n    return unicodedata.normalize('NFC', astr).casefold()",
        "sha1": "050c3ccf2177f9f1c34e92820ce74f1dfc643a43",
        "id": 368078
    },
    {
        "content": "import logging\n\n\ndef format_mesh_terms(df):\n    \"\"\"\n    Removes unrequired columns and pivots the mesh terms data into a dictionary.\n\n    Args:\n        df (dataframe): mesh terms as returned from retrieve_mesh_terms\n\n    Returns:\n        (dict): document_id: list of mesh terms\n    \"\"\"\n    logging.info(\"Formatting mesh terms\")\n    # remove PRC rows\n    df = df.drop(df[df.term == 'PRC'].index, axis=0)\n\n    # remove invalid error rows\n    df = df.drop(df[df.doc_id.astype(str).str.contains('ERROR.*ERROR', na=False)].index, axis=0)\n\n    # pivot and remove unrequired columns\n    doc_terms = {doc_id: list(grouped.term) for doc_id, grouped in df.groupby(\"doc_id\")}\n    return doc_terms",
        "sha1": "fdc6962bbf8dc875ad00a88643ccbdf43fad16f9",
        "id": 617417
    },
    {
        "content": "def _compare_weights_of_sets(weight_new_set: int, weight_old_set: int) -> int:\n    \"\"\"\n    Method that compares the weighted sum of two sets and keeps the bigger one.\n\n    :param weight_new_set: weighted sum of the new set\n    :param weight_old_set: weighted sum of the old set\n    :return: bigger weighted sum\n    \"\"\"\n    if weight_new_set > weight_old_set:\n        return weight_new_set\n    else:\n        return weight_old_set",
        "sha1": "8f176860955a868fe1c6bb13f4f44eef56d234b7",
        "id": 333075
    },
    {
        "content": "from pathlib import Path\n\n\ndef get_filename(file_path: str) -> str:\n    \"\"\"Extract the name of the file from a file path and exclude any file extension\"\"\"\n    if not file_path:\n        raise ValueError(\"Invalid file/folder path\")\n    # Replace any backslash(es) in Windows file path with forwardslash(es)\n    file_path = file_path.replace(\"\\\\\", \"/\")\n    return Path(file_path).stem.split(\".\")[0]",
        "sha1": "facf6c28ac7d775b6d7a33d3721661580b3b5236",
        "id": 571993
    },
    {
        "content": "def find(pred, iterable):\n    \"\"\"\n    Find the first occurrence of the predicate function returning true\n    over the iterable; otherwise None.\n    >>> find(lambda e: e.startswith('g'), ['alpha', 'beta', 'gamma', 'delta'])\n    'gamma'\n    >>> find(lambda e: e.startswith('p'), ['alpha', 'beta', 'gamma', 'delta'])\n    None\n    \"\"\"\n\n    for element in iterable:\n        if pred(element):\n            return element",
        "sha1": "f0a491e4c8dfce292b604193fde25f317b0978b8",
        "id": 689455
    },
    {
        "content": "def getSQLT(timestamp):\n\t\"\"\"Make timestamp for SQLite from Python timestamp, meaning a UNIX epoch INTEGER.\n\t:param timestamp:\n\t:return: SQLite compatible timestamp in the form of a UNIX epoch INTEGER\"\"\"\n\t# I know this is a very small function, but now it's clear what SQL needs\n\treturn int(timestamp)",
        "sha1": "6fb7a1ede4b9bcbc3a92039c130a2fe557346079",
        "id": 692357
    },
    {
        "content": "import shelve\n\n\ndef load_workspace(db_path, external_namespace=None, allow_callables=False):\n    \"\"\"\n    Load a saved workspace either by returning a dict or by inserting all variables directly into the enclosing \\\n    namespace.\n\n    :param db_path: Path to the database file produced by WorkspaceSaver. Don't trust databases you didn't create!\n    :type db_path: str\n    :param external_namespace: A namespace. If globals() is used, the variables are inserted directly into the \\\n    enclosing namespace. Any other dict (including those returned by locals() and vars()) will be updated in place, \\\n    which will NOT change any namespace. If no argument is given, return a dict of the variables.\n    :type external_namespace: dict or None\n    :param allow_callables: Whether to allow callables (class and function definitions) to be loaded. Not recommended.\n    :type allow_callables: bool\n    :return: None if external_globals was specified, or a dictionary of the contents of the saved workspace.\n    :rtype: None or dict\n    \"\"\"\n\n    d = dict()\n\n    with shelve.open(db_path) as db:\n        if allow_callables:\n            d.update(db)\n        else:\n            d.update({key: value for key, value in db.items() if not hasattr(value, '__call__')})\n\n    if external_namespace:\n        external_namespace.update(d)\n    else:\n        return d",
        "sha1": "531c91c2f3d47c2fa17aaf863987ee2394ff03ab",
        "id": 524020
    },
    {
        "content": "from typing import Optional\n\n\ndef admin_obj_url(obj: Optional[object], route: str = \"\", base_url: str = \"\") -> str:\n    \"\"\"\n    Returns admin URL to object. If object is standard model with default route name, the function\n    can deduct the route name as in \"admin:<app>_<class-lowercase>_change\".\n    :param obj: Object\n    :param route: Empty for default route\n    :param base_url: Base URL if you want absolute URLs, e.g. https://example.com\n    :return: URL to admin object change view\n    \"\"\"\n    if obj is None:\n        return \"\"\n    if not route:\n        route = \"admin:{}_{}_change\".format(obj._meta.app_label, obj._meta.model_name)  # type: ignore\n    path = reverse(route, args=[obj.id])  # type: ignore\n    return base_url + path",
        "sha1": "19601794a2455cf6f76231fd3a1c932fdbe09eae",
        "id": 17283
    },
    {
        "content": "import math\n\n\ndef starting_coprime_numbers(primes: set[int]) -> list[int]:\n    \"\"\"Return the numbers up to the product of the given primes that are coprime to all of them.\n\n    Note: the length of the returned list is is exactly equal to phi(math.prod(primes)), where\n    phi is the Euler totient function.\n\n    Preconditions:\n        - primes != set()\n        - every element of primes is prime\n\n    >>> starting_coprime_numbers({2, 3})\n    [1, 5]\n    >>> starting_coprime_numbers({3, 11})\n    [1, 2, 4, 5, 7, 8, 10, 13, 14, 16, 17, 19, 20, 23, 25, 26, 28, 29, 31, 32]\n    \"\"\"\n    nums_so_far = []\n    m = math.prod(primes)\n\n    for k in range(1, m):\n        is_coprime = True\n        for p in primes:\n            if k % p == 0:\n                is_coprime = False\n        if is_coprime:\n            list.append(nums_so_far, k)\n\n    return nums_so_far",
        "sha1": "3905fdb53b01c6101d31532be22a3bacdba063fd",
        "id": 146422
    },
    {
        "content": "def _is_weblog_entry(pagename):\n    \"\"\"Return True if the page is a weblog entry.\"\"\"\n    if not pagename:\n        return\n    parts = pagename.split(\"/\")\n    if not parts[0] == \"articles\":\n        return\n    if not len(parts) == 5:\n        return\n    if parts[-1] == \"index\":\n        return\n    return True",
        "sha1": "7774239b76340c0c8b44a6097572f28b181abd60",
        "id": 160501
    },
    {
        "content": "def _fit(y, classes):\n    \"\"\"Calculate the total sum of squares for a vector y classified into\n    classes\n\n    Parameters\n    ----------\n    y : array\n        (n,1), variable to be classified\n\n    classes : array\n              (k,1), integer values denoting class membership\n\n    \"\"\"\n    tss = 0\n    for class_def in classes:\n        yc = y[class_def]\n        css = yc - yc.mean()\n        css *= css\n        tss += sum(css)\n    return tss",
        "sha1": "6b686095a392f22d7127104c5b05ac061f2231b1",
        "id": 445412
    },
    {
        "content": "def array_diff(a: list, b: list) -> list:\n\t\"\"\"\n\tDifference function, which subtracts one\n\tlist from another and returns the result.\n\n\t:param a: list a\n\t:param b: list b\n\t:return: diff between a and b\n\t\"\"\"\n\treturn [item for item in a if item not in b]",
        "sha1": "f02bf38f3ffc7b6b48767b3297ff44405e10d9c3",
        "id": 220020
    },
    {
        "content": "def getDistance(interval_a, interval_b):\n    \"\"\"Returns the distance between two intervals\"\"\"\n    return max(interval_a[0] - interval_b[1], interval_b[0] - interval_a[1])",
        "sha1": "16fc181560ec01e5bddb7da6fbb911b242126112",
        "id": 51063
    },
    {
        "content": "import json\n\n\ndef get_json(file_name):\n    \"\"\"Load JSON data from a file.\"\"\"\n    with open(file_name) as json_data:\n        return json.load(json_data)",
        "sha1": "79ecbc4859631aeed2c21eda8ae41b9eca3c6836",
        "id": 427249
    },
    {
        "content": "def split_list(input_list, num_sub_lists):\n    \"\"\"\n\n    :param input_list: List to be split\n    :param num_sub_lists: Number of sub lists to be split into\n    :return: list containing sub lists\n    \"\"\"\n    output_list = []\n    # First make empty sub lists, one for each process\n    for n in range(num_sub_lists):\n        output_list.append([])\n    # Now add file paths evenly to them\n    count = 0\n    for item in input_list:\n        output_list[count % num_sub_lists].append(item)\n        count += 1\n\n    return output_list",
        "sha1": "b44b3e8861ddbd1299da8277650a5d4ed84fa2bc",
        "id": 490679
    },
    {
        "content": "import re\n\n\ndef ipv4_address_validator(addr):\n    \"\"\"\n    Regex to validate an ipv4 address.\n    Checks if each octet is in range 0-255.\n    Returns True/False\n    \"\"\"\n\n    pattern = re.compile(\n        r\"^([1]?\\d?\\d|2[0-4]\\d|25[0-5])\\.([1]?\\d?\\d|2[0-4]\\d|25[0-5])\\.([1]?\\d?\\d|2[0-4]\\d|25[0-5])\\.([1]?\\d?\\d|2[0-4]\\d|25[0-5])$\"\n    )\n    if pattern.fullmatch(str(addr).strip().strip(\"\\n\")):\n        return True\n    else:\n        return False",
        "sha1": "6204d9d54536e510b5556d13ee259ec3d7b3fc95",
        "id": 36098
    },
    {
        "content": "def compute_struc_matching_score(matching_struc, idf_values):\n    \"\"\"\n    Computes the structural matching score.\n\n    Args:\n        matching_struc: Set of structural features that exists in both query\n        and related formula.\n        idf_values: Map of formula term and its IDF score.\n\n    Returns:\n        The structural matching score.\n    \"\"\"\n    struc_score = 0\n    for term in matching_struc:\n        if term in idf_values:\n            struc_score += idf_values.get(term)\n\n    return struc_score",
        "sha1": "63c5ee06a3fe6639eb7526c960cd4fcf82b23409",
        "id": 325727
    },
    {
        "content": "import copy\n\n\ndef fake_voc_noiser(voc):\n  \"\"\"A fake VectorOfCounts noiser that add one ID to the first bucket.\"\"\"\n  noised_voc = copy.deepcopy(voc)\n  noised_voc.stats[0] += 1\n  return noised_voc",
        "sha1": "a37f3e27482913dac82f570cf2eb7bfb22a5974f",
        "id": 400372
    },
    {
        "content": "def overlay_image(foreground_image, mask, background_image):\n  \"\"\" Overlay foreground image onto the background given a mask\n  :param foreground_image: foreground image points\n  :param mask: [0-255] values in mask\n  :param background_image: background image points\n  :returns: image with foreground where mask > 0 overlaid on background image\n  \"\"\"\n  blend_ratio = mask / 255\n  blend_ratio = blend_ratio.reshape(background_image.shape[0], background_image.shape[1], 1)\n  background_image[..., :3] = background_image[..., :3] * (1 - blend_ratio) + foreground_image[..., :3] * blend_ratio\n  return background_image",
        "sha1": "fb6b8a854e99fe984b6f57eb683a8f77a507e155",
        "id": 688365
    },
    {
        "content": "import re\n\n\ndef tokenize(text):\n    \"\"\"\n    Tokenize a text into numbers and strings.\n\n    :param text: The text to tokenize (a string).\n    :returns: A list of strings and/or numbers.\n\n    This function is used to implement robust tokenization of user input in\n    functions like :func:`.parse_size()` and :func:`.parse_timespan()`. It\n    automatically coerces integer and floating point numbers, ignores\n    whitespace and knows how to separate numbers from strings even without\n    whitespace. Some examples to make this more concrete:\n\n    >>> from humanfriendly.text import tokenize\n    >>> tokenize('42')\n    [42]\n    >>> tokenize('42MB')\n    [42, 'MB']\n    >>> tokenize('42.5MB')\n    [42.5, 'MB']\n    >>> tokenize('42.5 MB')\n    [42.5, 'MB']\n    \"\"\"\n    tokenized_input = []\n    for token in re.split(r'(\\d+(?:\\.\\d+)?)', text):\n        token = token.strip()\n        if re.match(r'\\d+\\.\\d+', token):\n            tokenized_input.append(float(token))\n        elif token.isdigit():\n            tokenized_input.append(int(token))\n        elif token:\n            tokenized_input.append(token)\n    return tokenized_input",
        "sha1": "c9ec0135d1c705ae4e02500e6c9e2359ce06b545",
        "id": 235619
    },
    {
        "content": "def chop(s, n):\n#=============\n  \"\"\"Chop characters of the front of a string.\"\"\"\n  return str(s)[n:]",
        "sha1": "5e44f764aa7724a595fb3dda7f8e1b2d4e2dc154",
        "id": 352379
    },
    {
        "content": "import hashlib\n\n\ndef check_hash(filename, hash, algo='md5'):\n    \"\"\"Check whether hash of the file content matches the expected hash.\n    Parameters\n    ----------\n    filename : str\n        Path to the file.\n    hash : str\n        Expected hash in hexadecimal digits.\n    algo: str\n        Hashing algorithm (md5, sha1, sha256, sha512)\n\n    Returns\n    -------\n    bool\n        Whether the file content matches the expected hash.\n    \"\"\"\n    algos = {\n        'md5': hashlib.md5,\n        'sha1': hashlib.sha1,\n        'sha256': hashlib.sha256,\n        'sha512': hashlib.sha512,\n    }\n    hasher = algos[algo]()\n    with open(filename, 'rb') as f:\n        while True:\n            data = f.read(1048576)\n            if not data:\n                break\n            hasher.update(data)\n\n    file_hash = hasher.hexdigest()\n    l = min(len(file_hash), len(hash))\n    return hasher.hexdigest()[0:l] == hash[0:l]",
        "sha1": "96732b43a838f2e0a7ab401b9af43be90b0b732f",
        "id": 144452
    },
    {
        "content": "def getSection(section, iraf_format=True):\n  \"\"\"Given an input string for a section in an image, it will\n     return the input as a list.  \n\n   section: An input string given a section in the image\n            Set to None to return the whole image\n\n   iraf_format: It will invert the x and y values\n\n  \"\"\"\n  #return None if section is None\n  if section is None:\n     return None\n\n  #remove the brackets\n  section=section.replace('[','')\n  section=section.replace(']','')\n\n  #loop through the axis\n  sect_list=[]\n  for s in section.split(','):\n      for t in s.split(':'):\n          sect_list.append(int(t))\n\n  #flip things around for use with python\n  if iraf_format and len(sect_list)==4:\n     return [sect_list[2]-1, sect_list[3], sect_list[0]-1, sect_list[1]]\n\n  return sect_list",
        "sha1": "527d19bda1267c7b199e4aa04205dfdea51602d8",
        "id": 546373
    },
    {
        "content": "def flatten(x):\n    \"\"\"Flatten a list of arbitrary depth. Returns a list with no sub-lists or sub-tuples.\n    If the input is not a list or a tuple, it will be returned as a one-element list.\n    \"\"\"\n    if not isinstance(x, (list, tuple)):\n        return [x]\n    else:\n        if len(x) == 0:\n            return []\n        else:\n            return flatten(x[0]) + flatten(x[1:])",
        "sha1": "461e674f5b17d2825827f550b90f0fcef90f81aa",
        "id": 213210
    },
    {
        "content": "def rename(ds, **names):\n    \"\"\"\n    Rename all variables etc that have an entry in names\n\n    Parameters\n    ----------\n    ds : xarray Dataset\n        A dataset to be renamed\n    names : dict\n        Dictionary of {old_name: new_name}\n    \"\"\"\n    for k, v in names.items():\n        if k in ds:\n            if v in ds:\n                # New name already exists\n                if all(ds[k].values == ds[v].values):\n                    ds = (\n                        ds.assign_coords({v: ds[v].rename({v: k})})\n                        .swap_dims({k: v})\n                        .drop(k)\n                    )\n            else:\n                ds = ds.rename({k: v})\n    return ds",
        "sha1": "d349850624dce8f5ddb73aa98e97875e21898a46",
        "id": 109145
    },
    {
        "content": "def _name_as_key(name):\n    \"\"\"Uppercase, stripped, no repeated interior whitespace.\"\"\"\n\n    name = name.upper().strip()\n    while '  ' in name:\n        name = name.replace('  ', ' ')\n\n    return name",
        "sha1": "58836d4f26ce83cb9eb0bc597626bc1d4fcaa7fa",
        "id": 81264
    },
    {
        "content": "def segment_by_time(sequence, segment_size, sequence_index):\n    \"\"\"Segment the sequence in segments of the indicated time length.\n\n    Segmentation will happen by time, which means that there is no guarantee\n    that the outputed segments all contain the same number of data points.\n\n    Args:\n        sequence (pandas.DataFrame):\n            Sequence to segment, passed as a multi-column ``pandas.DataFrame``.\n        segment_size (pandas.Timedelta):\n            Size of each segment, passed as a ``pandas.Timedelta`` object.\n        sequence_index (pandas.Series):\n            Data of the column that will be used as the time index for the\n            segmentation.\n\n    Returns:\n        list:\n            List of ``pandas.DataFrames`` containing each segment.\n    \"\"\"\n    sequences = []\n    start = sequence_index.iloc[0]\n    max_time = sequence_index.iloc[-1]\n    while start <= max_time:\n        end = start + segment_size\n        selected = (start <= sequence_index) & (sequence_index < end)\n        sequences.append(sequence[selected.values].reset_index(drop=True))\n        start = end\n\n    return sequences",
        "sha1": "2af547e182fae0ba103d7d9543f2bf7fd84d9be5",
        "id": 303603
    },
    {
        "content": "def show_registration_disabled_collapse(n_clicks, is_open):\n    \"\"\"\n    Toggle the registration info div.\n    \"\"\"\n    if n_clicks:\n        return not is_open\n    return is_open",
        "sha1": "b558add8fe5a0ac528d06e923a5ee8e3150d9755",
        "id": 329502
    },
    {
        "content": "import json\n\n\ndef json_points(points):\n    \"\"\"\n    Returns a list of points [(lat, lng)...] as a JSON formatted list of\n    strings.\n\n    >>> json_points([(1,2), (3,4)])\n    '[\"1,2\", \"3,4\"]'\n    \"\"\"\n    return json.dumps([\"{0},{1}\".format(point[0], point[1]) for point in points])",
        "sha1": "812de51e1e593d7f15054a389cd823fe32a96174",
        "id": 101181
    },
    {
        "content": "def floatOrNone(v, default=0.0, exctype=Exception):\n    \"\"\"Returns the float value of the given value, or default (which is normally 0.0) on error.\n    Catches exceptions of the given exctype (Exception by default)\"\"\"\n    try:\n        return float(v)\n    except exctype:\n        return default",
        "sha1": "c0b1b0152cdc39678dc437f821054e2dd9f01a61",
        "id": 684505
    },
    {
        "content": "import random\n\n\ndef generateIndices(n_blocks, N, D):\n\t\"\"\"\n\tgenerates indices for block matrix computation.\n\tChecked.\n\tInput:\n\tn_blocks: number of blocks to use.\n\tN: number of samples.\n\tD: number of genes.\n\tOutput:\n\ty_indices_to_use[i][j] is the indices of block j in sample i.\n\t\"\"\"\n\ty_indices_to_use = []\n\tidxs = list(range(D))\n\tn_in_block = int(1. * D / n_blocks)\n\n\tfor i in range(N):\n\t\tpartition = []\n\t\trandom.shuffle(idxs)\n\t\tn_added = 0\n\n\t\tfor block in range(n_blocks):\n\t\t\tstart = n_in_block * block\n\t\t\tend = start + n_in_block\n\n\t\t\tif block < n_blocks - 1:\n\t\t\t\tidxs_in_block = idxs[start:end]\n\t\t\telse:\n\t\t\t\tidxs_in_block = idxs[start:]\n\n\t\t\tpartition.append(sorted(idxs_in_block))\n\t\t\tn_added += len(idxs_in_block)\n\n\t\ty_indices_to_use.append(partition)\n\n\t\tif i == 0:\n\t\t\tprint('Block sizes', [len(a) for a in partition])\n\n\t\tassert(n_added == D)\n\n\treturn y_indices_to_use",
        "sha1": "8850db07af5811846cd80f6225b2a56b71284dc2",
        "id": 690631
    },
    {
        "content": "from typing import Iterable\n\n\ndef is_instance_of_all(obj, classes: Iterable[type]) -> bool:\n    \"\"\"\n    Returns ``True`` if the ``obj`` argument is an instance of all of the\n    classes in the ``classes`` argument.\n\n    :raises TypeError: If any element of classes is not a type.\n    \"\"\"\n    if any(not isinstance(classinfo, type) for classinfo in classes):\n        raise TypeError(\"classes must contain types\")\n    return all(isinstance(obj, classinfo) for classinfo in classes)",
        "sha1": "e2aa7fcf44dbf1c3edce8bbaef803d229403ff8c",
        "id": 311360
    },
    {
        "content": "def expand_rule_spec(rule_spec):\n    \"\"\"Creates and returns a list of ingress rule specs expanded from \n    a single rule spec (PORTS, CIDRIP) tuple whose PORTS component \n    contains port numbers and number ranges separated by commas.\"\"\"\n    port_ranges, cidrip = rule_spec\n    port_ranges = port_ranges.split(',')\n    rule_specs = []\n    for port_range in port_ranges:\n        rule_specs.append((port_range, cidrip))\n    return rule_specs",
        "sha1": "ec5bfe901f04eed8f21cab99b806f2f7df12d63a",
        "id": 342600
    },
    {
        "content": "def regex_split(original_output, regex_split_cmd):\n    \"\"\"\n    Takes in a regex string and output, returns a list of output split\n\n    :param original_output:\n    :param regex_split_cmd:\n    :return:\n    \"\"\"\n\n    def _regex_split():\n        return original_output.split(regex_split_cmd)\n\n    return _regex_split()",
        "sha1": "81042f8ea9c4f1befe7256f52cf1ec198a9a1eba",
        "id": 397137
    },
    {
        "content": "def projindex(Z,N,Index,H = 0,gamma = 0):\n    \"\"\"\n    Orthogonal projection onto the subspace of known entries\n\n    X = PROJHANKEL(Z,N,Index) determines the orthogonal projection of \n    Z onto the subspaces of matrices with entries N(Index), i.e., \n    X is the proximal mapping of the function\n\n    i_{X(Index) = N(Index)}(X),\n\n    where i_{X(Index) = N(Index)} is the convex indicator function \n    for the linear constraint X(Index) = N(Index).   \n\n    X = PROJINDEX(Z,H,gamma) allows to shift Z by -gamma*H, i.e., \n    X is projection of Z-gamma*H onto the subspaces of matrices with \n    entries N(Index) and is therefore the proximal mapping of \n    gamma*(i_{X(Index) = N(Index)}(X)+trace(X'H)).\n    \"\"\"\n    X = Z - gamma*H\n    X[Index] = N[Index]    \n    return X",
        "sha1": "6751613b05c07fcf27b383935f84b1a3bb6aa1e5",
        "id": 277566
    },
    {
        "content": "def container_ipv6(container):\n    \"\"\"\n    return the IPv6 address of a container.\n    \"\"\"\n    net_info = container.attrs[\"NetworkSettings\"][\"Networks\"]\n    if \"bridge\" in net_info:\n        return net_info[\"bridge\"][\"GlobalIPv6Address\"]\n\n    # not default bridge network, fallback on first network defined\n    network_name = list(net_info.keys())[0]\n    return net_info[network_name][\"GlobalIPv6Address\"]",
        "sha1": "c3f07123c1156b5e03fba475aa2069a49a432102",
        "id": 378725
    },
    {
        "content": "from typing import Dict\nfrom typing import Any\n\n\ndef default_style() -> Dict[str, Any]:\n    \"\"\"Define default values of the pulse stylesheet.\"\"\"\n    return {\n        'formatter.general.fig_size': [8, 6],\n        'formatter.general.dpi': 150,\n        'formatter.color.fill_waveform_d': ['#648fff', '#002999'],\n        'formatter.color.fill_waveform_u': ['#ffb000', '#994A00'],\n        'formatter.color.fill_waveform_m': ['#dc267f', '#760019'],\n        'formatter.color.fill_waveform_a': ['#dc267f', '#760019'],\n        'formatter.color.baseline': '#000000',\n        'formatter.color.barrier': '#222222',\n        'formatter.color.background': 'f2f3f4',\n        'formatter.color.annotate': '#222222',\n        'formatter.color.frame_change': '#000000',\n        'formatter.color.snapshot': '#000000',\n        'formatter.color.axis_label': '#000000',\n        'formatter.alpha.fill_waveform': 1.0,\n        'formatter.alpha.baseline': 1.0,\n        'formatter.alpha.barrier': 0.7,\n        'formatter.layer.fill_waveform': 2,\n        'formatter.layer.baseline': 1,\n        'formatter.layer.barrier': 1,\n        'formatter.layer.annotate': 4,\n        'formatter.layer.axis_label': 4,\n        'formatter.layer.frame_change': 3,\n        'formatter.layer.snapshot': 3,\n        'formatter.margin.top': 0.2,\n        'formatter.margin.bottom': 0.2,\n        'formatter.margin.left': 0.05,\n        'formatter.margin.right': 0.05,\n        'formatter.margin.between_channel': 0.1,\n        'formatter.label_offset.pulse_name': -0.1,\n        'formatter.label_offset.scale_factor': -0.1,\n        'formatter.label_offset.frame_change': 0.1,\n        'formatter.label_offset.snapshot': 0.1,\n        'formatter.text_size.axis_label': 15,\n        'formatter.text_size.annotate': 12,\n        'formatter.text_size.frame_change': 20,\n        'formatter.text_size.snapshot': 20,\n        'formatter.text_size.fig_title': 15,\n        'formatter.line_width.fill_waveform': 0,\n        'formatter.line_width.baseline': 1,\n        'formatter.line_width.barrier': 1,\n        'formatter.line_style.fill_waveform': '-',\n        'formatter.line_style.baseline': '-',\n        'formatter.line_style.barrier': ':',\n        'formatter.control.apply_phase_modulation': True,\n        'formatter.control.show_snapshot_channel': True,\n        'formatter.control.show_acquire_channel': True,\n        'formatter.control.show_empty_channel': True,\n        'formatter.unicode_symbol.frame_change': u'\\u21BA',\n        'formatter.unicode_symbol.snapshot': u'\\u21AF',\n        'formatter.latex_symbol.frame_change': r'\\circlearrowleft',\n        'formatter.latex_symbol.snapshot': '',\n        'generator.waveform': [],\n        'generator.frame': [],\n        'generator.channel': [],\n        'generator.snapshot': [],\n        'generator.barrier': []}",
        "sha1": "a516ad47fa4f90cca3533c6c7bc643ce874329b5",
        "id": 109421
    },
    {
        "content": "def check_liquidation(hh):\n    \"\"\"\n    Function to check whether there are liquidated assets to be taxed.\n\n    Parameters\n    ----------\n    hh: Hhold\n        household\n\n    Returns\n    -------\n    bool\n        True or False\n    \"\"\"\n    for p in hh.sp:\n        p.liquidation_to_tax = (p.liquidation_non_taxable > 0 or\n                                p.liquidation_cap_gains > 0 or\n                                p.liquidation_cap_losses > 0 or\n                                p.liquidation_business_exempt > 0)\n    for p in hh.sp:\n        if p.liquidation_to_tax:\n            return True\n    return False",
        "sha1": "d982a6d627282a1a9e4ad59cfe852f0a406a362f",
        "id": 517369
    },
    {
        "content": "def compose(*fs):\n    \"\"\"\n    Functional compositions of one or more functions. Eg. compose(f1, f2, f3, f4).\n\n    The innermost function:\n     - may take arbitrary (including zero) positional arguments.\n     - should produce one output value\n\n    The remaining functions (if there are more than one argument to compose):\n     - should take one input positional argument and produce one output value.\n\n    \"\"\"\n    assert len(fs) >= 1\n    *fs_outers, f_innermost = fs\n\n    if len(fs_outers) > 0:\n        return lambda *xs: compose(*fs_outers)(f_innermost(*xs))\n    else:\n        return f_innermost",
        "sha1": "5e950e920a740ef79f86f782abefa55a88a8258b",
        "id": 336829
    },
    {
        "content": "import torch\n\n\ndef get_torch_dtype(numeric_precision): \n    \"\"\"Provide torch dtype based on numeric precision string.\"\"\"\n    dtypes = {'float64': torch.float64,\n              'float32': torch.float32,\n              'float16': torch.float16,\n              'bfloat16': torch.bfloat16\n              }\n    return dtypes[numeric_precision]",
        "sha1": "2d470a27f1e6b3e0cc36a2e598a93b2a9c4d207c",
        "id": 107796
    },
    {
        "content": "def partition(seq, left, right, pivot_index):\n    \"\"\"\n    Reorders the slice with values lower than the pivot at the left side,\n    and values bigger than it at the right side.\n    Also returns the store index.\n\n    :param seq: A list of integers\n    :param left: An integer representing left index\n    :param right: An integer representing left index\n    :param pivot_index: An integer that we're pivoting off\n    :rtype: An stored_index integer\n    \"\"\"\n    pivot_value = seq[pivot_index]\n    seq[pivot_index], seq[right] = seq[right], seq[pivot_index]\n    store_index = left\n    for i in range(left, right):\n        if seq[i] < pivot_value:\n            seq[i], seq[store_index] = seq[store_index], seq[i]\n            store_index += 1\n    seq[store_index], seq[right] = seq[right], seq[store_index]\n    return store_index",
        "sha1": "5f4101a68e4543eeb9647e2b183533c57628977b",
        "id": 143154
    },
    {
        "content": "import re\n\n\ndef strip_single_paragraph(text: str):\n    \"\"\" If the provided HTML text has only a single paragraph, strip it off. \"\"\"\n\n    stripped = re.sub(r'^<p>(.*)</p>$', r'\\1', text.strip())\n    if '<p>' in stripped:\n        return text\n    return stripped",
        "sha1": "e85734789cdc319455f8cdae9b38410bbb9c089d",
        "id": 194954
    },
    {
        "content": "def dataset_is_mnist_family(dataset):\n  \"\"\"returns if dataset is of MNIST family.\"\"\"\n  return dataset.lower() == 'mnist' or dataset.lower() == 'fashion-mnist'",
        "sha1": "617cf0fb92a14d6699262d8661b280d19ac7eae3",
        "id": 281113
    },
    {
        "content": "def _includes(opt):\n    \"\"\"\n    Return the #include directives for the user-defined list of include files.\n    \"\"\"\n    includes = []\n    if opt.include_files is not None and len(opt.include_files) > 0:\n        for include_file in opt.include_files:\n            if include_file[0] == '\"' or include_file[0] == '<':\n                includes.append('#include {0}'.format(include_file))\n            else:\n                includes.append('#include \"{0}\"'.format(include_file))\n    return includes",
        "sha1": "69eb081f543f9758d25a4de9c52e9f6b91751cfe",
        "id": 620486
    },
    {
        "content": "def pix2ang(val, pixel_scale):\n    \"\"\"Transform an angle from units of pixels into angular units.\"\"\"\n    return val * pixel_scale",
        "sha1": "ae01b4ea1aa86949beabcad8cdab23fca644e1e8",
        "id": 568698
    },
    {
        "content": "import socket\n\n\ndef _reconnect(ip_port):\n    \"\"\"Return a new socket connection over TCP/IP \"\"\"\n    com = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    com.connect(ip_port)\n    return com",
        "sha1": "00d2230062705dc2b92b3646c89e62ca4e55ef2a",
        "id": 411564
    },
    {
        "content": "def getJaccard(orig, new):\n\t\"\"\"Get jaccard distance of neighbors intersecting with those in ambient space\n\torig : Original/ambient space nearest neighbor indices, from getNeighbors()\n\tnew : Latent/Comparison space nearest neighbor indices, from getNeighbors()\n\tReturns:\n\tfrac : List of Jaccard distances for each obs\n\t\"\"\"\n\tfrac = [0]*new.shape[0]\n\tfor i in range(new.shape[0]):\n\t\tinter = set(orig[i,:]).intersection(new[i,:])\n\t\tfrac[i] = 1 - len(inter)/len(set(orig[i,:]).union(new[i,:]))\n\n\treturn frac",
        "sha1": "a880bcb75a836cc1f8f7de5a69b095884a68619b",
        "id": 563670
    },
    {
        "content": "from typing import Tuple\nimport math\n\n\ndef get_xy_components(lon: float, lat: float) -> Tuple[int, int]:\n    \"\"\"For a given longitude and latitude, returns the relevant\n    components of the name of the zip file containing the SRTM data (for\n    example \"35\" and \"02\" in \"srtm_35_02.zip\").\n    \"\"\"\n    mod_lon = (int(math.floor(lon)) + 180) // 5 + 1\n    mod_lat = (64 - int(math.floor(lat))) // 5\n    return mod_lon, mod_lat",
        "sha1": "076a4339278766f59c91338fb95953455e4ae0eb",
        "id": 634337
    },
    {
        "content": "def _intTime(tStr):\n    \"\"\"\n    Converts a time given as a string containing a float into an integer representation.\n    \"\"\"\n    return int(float(tStr))",
        "sha1": "a6a79610af2f8a6e69133332e99f87acccf8e19b",
        "id": 394217
    },
    {
        "content": "import math\n\n\ndef rounding_repeats(repeats, d_multiplier):\n    \"\"\" Round number of filters based on depth multiplier. \"\"\"\n    if not d_multiplier:\n        return repeats\n    return int(math.ceil(d_multiplier * repeats))",
        "sha1": "76cd3208ca3fd86fcc87bab45c35f09a8cba6f57",
        "id": 198934
    },
    {
        "content": "def set_ignore_certificate_errors(ignore: bool) -> dict:\n    \"\"\"Enable/disable whether all certificate errors should be ignored.\n\n    Parameters\n    ----------\n    ignore: bool\n            If true, all certificate errors will be ignored.\n\n    **Experimental**\n    \"\"\"\n    return {\n        \"method\": \"Security.setIgnoreCertificateErrors\",\n        \"params\": {\"ignore\": ignore},\n    }",
        "sha1": "49f0597ee4b7bdcdd620e636704e732f52871c22",
        "id": 51906
    },
    {
        "content": "import boto\nfrom typing import Optional\n\n\ndef get_aws_zone_from_boto() -> Optional[str]:\n    \"\"\"\n    Get the AWS zone from the Boto config file, if it is configured and the\n    boto module is available.\n    \"\"\"\n    try:\n        zone = boto.config.get('Boto', 'ec2_region_name')\n        if zone is not None:\n            zone += 'a'  # derive an availability zone in the region\n        return zone\n    except ImportError:\n        pass\n    return None",
        "sha1": "eebbe2bb15f9c95def8e1e68f95c08c5a7a29137",
        "id": 645705
    },
    {
        "content": "import re\n\n\ndef no_whitespace_or_punct(line: str) -> str:\n    \"\"\"\n    Remove whitespace and punctuation from string\n\n    Args:\n        line: String to remove whitespace and punctuation from\n    Returns:\n        A string with the whitespace and punctuation removed\n    \"\"\"\n    # remove whitespace\n    newstr = line.replace(' ', '')\n\n    # remove punctuation besides \"'\" and \"+\"\n    non_phonetic_punc = r\"[.?!,*%$=\\\\/[\\]()\\\";]*\"\n    newstr = re.sub(non_phonetic_punc, '', newstr)\n\n    return newstr",
        "sha1": "64489fa702ca5a0f39dc31543f9216443129c8ff",
        "id": 212061
    },
    {
        "content": "def centerel(elsize, contsize):\n    \"\"\"Centers an element of the given size in the container of the given size.\n    Returns the coordinates of the top-left corner of the element relative to\n    the container.\"\"\"\n    w, h = elsize\n    W, H = contsize\n    x = (W-w)//2\n    y = (H-h)//2\n    return (x, y)",
        "sha1": "72aa035924ea2fe89d76073607f3156b75885410",
        "id": 83629
    },
    {
        "content": "def acc(modifier: int, number: int) -> int:\n\t\"\"\"\n\tacc adds or subtracts from accumulator\n\n\t:param modifier: Number to added/subtracted from acc\n\t:param number: current acc value\n\t:type modifier: int\n\t:type number: int\n\t:rtype: int\n\t\"\"\"\n\n\treturn number + int(modifier)",
        "sha1": "efe108a5d98c1afab620eaef2c8f2e4fe3d42623",
        "id": 592640
    },
    {
        "content": "def multi_timescale_model(request) -> str:\n    \"\"\"Fixture that provides multi-timescale models.\n\n    Returns\n    -------\n    str\n        Name of the multi-timescale model.\n    \"\"\"\n    return request.param",
        "sha1": "7c538e397bca6a18194d088614db9d9be9323eae",
        "id": 392163
    },
    {
        "content": "def quickSparqlEntry(quadro):\n    \"\"\"\n        Converts the tuple format of the data processing into a sparql query string\n        :param SpchtTriple quadro: a SpchtTriple object that contains 3 SpchtThird objects\n        :rtype: str\n        :return: a sparql query of the structure <s> <p> <o> .\n    \"\"\"\n    return f\"{str(quadro.subject)} {str(quadro.predicate)} {str(quadro.sobject)} . \\n\"",
        "sha1": "56c85c1daac108336506cc4bdb93c3f547d71bef",
        "id": 307856
    },
    {
        "content": "def format_time(d):\n    \"\"\" Format a datetime as hh:mm \"\"\"\n    return str(d.hour).zfill(2) + \":\" + str(d.minute).zfill(2)",
        "sha1": "2f3264e2169e6b3c79c884aea4ca1a82b3cc4d74",
        "id": 228748
    },
    {
        "content": "def decstate_rep(decstate):\n    \"\"\"\n    List representation of a decision state.\n    \"\"\"\n    theta = decstate[0] \n    dtheta = decstate[1]\n    return [theta, dtheta]",
        "sha1": "5d77535f4b9b32aabff0f6df32a778094c116b3f",
        "id": 171776
    },
    {
        "content": "def f(a):\n    \"\"\"Function with one parameter.\"\"\"\n    return a",
        "sha1": "ccd74187d108b3419e8b421f5f66216cef5ced39",
        "id": 124597
    },
    {
        "content": "def parse_media_file(mediafile):\n    \"\"\"\n    Get the media compounds present\n    :param mediafile: filepath to the tab-delimited media file.\n    :return: dictionary with first level of keys as compound IDs. The values are another dictionary of\n    compound name, formula, and charge.\n    \"\"\"\n\n    readmedia = {}\n    with open(mediafile, \"r\") as f:\n        for li, ls in enumerate(f):\n            # Skip header line\n            if li == 0:\n                continue\n            ls = ls.rstrip(\"\\n\")\n            cpid, name, formula, charge = ls.split(\"\\t\")\n            readmedia[cpid] = {\"name\": name, \"formula\": formula, \"charge\": charge}\n    return readmedia",
        "sha1": "62784a8738c0cd0f2c8296ba1021072ad138a591",
        "id": 446015
    },
    {
        "content": "def truncate_text(text: str, limit: int = 256) -> str:\n    \"\"\"Truncate a given `text` if the `limit` is reached\"\"\"\n    if limit <= 0:\n        raise ValueError(\"limit must be greater than 0\")\n\n    return text[:limit] + \"...\" if len(text) > limit else text",
        "sha1": "53308f70123669ae7b66f94d225830c41abc2b82",
        "id": 582058
    },
    {
        "content": "def display_iscsi_device(iqn, sess):\n    \"\"\"\n    Print the data for iscsi device identified by iqn.\n    Parameters\n    ----------\n    iqn: str\n        The iSCSI qualified name.\n    sess: OCISession\n        An oci sdk session.\n    Returns\n    -------\n        bool: True on success, False otherwise.\n    \"\"\"\n    this_compartment = sess.this_compartment()\n    this_availability_domain = sess.this_availability_domain()\n    all_volumes = this_compartment.all_volumes(this_availability_domain)\n    for vol in all_volumes:\n        try:\n            this_iqn = vol.get_iqn()\n        except Exception as e:\n            continue\n        try:\n            if this_iqn == iqn:\n                this_name = vol.get_display_name()\n                this_ocid = vol.get_ocid()\n                this_ip = vol.get_portal_ip()\n                this_port = vol.get_portal_port()\n                print('%s %s %s:%s [%s]' % (this_name, iqn, this_ip, this_port, this_ocid))\n                return True\n        except Exception as e:\n            return False",
        "sha1": "efc77ddd813fb0ece6fe5cae24b4719dcff948d2",
        "id": 628691
    },
    {
        "content": "def reverse_list (list):\n\t\"\"\"\n\t:param: list\n\t:return: list\n\tReturn a list, whose elements are in reversed order\n\te.g. reverse_list([30,40,50]) returns [50,40,30]\n\t\"\"\"\n\treversed=[]\n\t#Copy the first element of the given list into empty reversed list: reversed list is now [30]\n\treversed.append(list[0])\n\t#Insert second element 40 into reversed list at index 0, so the list now is [40,30] etc.\n\tfor i in list[1:]:\n\t\treversed.insert(0,i)\n\treturn reversed",
        "sha1": "a3370aa505e19a4e4bca76d765c8f3859ac106d2",
        "id": 696221
    },
    {
        "content": "def get_wbo(offmol, dihedrals):\n    \"\"\"\n    Returns the specific wbo of the dihedral bonds\n    Parameters\n    ----------\n    offmol: openforcefield molecule\n    dihedrals: list of atom indices in dihedral\n\n    Returns\n    -------\n    bond.fractional_bond_order: wiberg bond order calculated using openforcefield toolkit for the specific dihedral central bond\n    \"\"\"\n    offmol.assign_fractional_bond_orders(bond_order_model=\"am1-wiberg-elf10\")\n    bond = offmol.get_bond_between(dihedrals[1], dihedrals[2])\n    return bond.fractional_bond_order",
        "sha1": "82aa84036d3078826fedaec40e599ffd783a422c",
        "id": 702567
    },
    {
        "content": "def f1(precision: float, recall: float) -> float:\n    \"\"\"Calculate the F1 score.\"\"\"\n    return 2 * precision * recall / (precision + recall)",
        "sha1": "34a7674c37d91dc92974856540b8eb8c201b2697",
        "id": 149247
    },
    {
        "content": "def pca_dim_reduction(x, basis, xmean=None):\n    \"\"\"(x, basis, xmean) --> xnew\n    Dimensionality reduction with PCA.\n\n    x: N*D data matrix\n    basis: D*K basis matrix\n    xmean: 1-D vector, mean vector used in PCA, if not set, use the mean of x instead\n\n    xnew: N*K new data matrix\n    \"\"\"\n\n    if xmean is None:\n        xmean = x.mean(axis=0)\n\n    xnew = (x - xmean).dot(basis)\n    return xnew",
        "sha1": "63ce86d46c38d9d8cfda646890ae45c237e97c79",
        "id": 451865
    },
    {
        "content": "def print_array(arr):\n    \"\"\"\n    Prints the content of a list\n    :param arr: a list that needed to be printed\n    :return: the string needed to print\n    \"\"\"\n    string = ''\n    for a in arr:\n        string += str(a) + ' '\n    return string.strip(' ')",
        "sha1": "69a88e80bef8da3b452278de1b9d59493e56416f",
        "id": 153838
    },
    {
        "content": "import tempfile\nimport json\n\n\ndef _GetKeysAsJson(keys_input, session_work_dir):\n  \"\"\"Converts |keys_input| into a JSON file on disk.\n\n  Args:\n    keys_input: A dictionary or a string pointing to a JSON file. The contents\n        of either should be Skia Gold config data.\n\n  Returns:\n    A string containing a filepath to a JSON file with containing |keys_input|'s\n    data.\n  \"\"\"\n  if isinstance(keys_input, str):\n    return keys_input\n  assert isinstance(keys_input, dict)\n  keys_file = tempfile.NamedTemporaryFile(suffix='.json',\n                                          dir=session_work_dir,\n                                          delete=False).name\n  with open(keys_file, 'w') as f:\n    json.dump(keys_input, f)\n  return keys_file",
        "sha1": "1ed979fd0424ebd3205b1f3c24c234caa855062e",
        "id": 400366
    },
    {
        "content": "def is_valid_hcl(value: str) -> bool:\n    \"\"\"\n    Return if value is a valid hcl (hair color).\n\n    Parameter\n    ---------\n    value: str\n        a hcl.\n\n    Return\n    ------\n    bool\n        True if hcl is valid, False othewise.\n    \"\"\"\n    if value[0] != \"#\":\n        return False\n    return all((char.isdigit() or char in \"abcdef\" for char in value[1:]))",
        "sha1": "1b3a241acc734d9c2e51fa7a2d513668a8172201",
        "id": 172706
    },
    {
        "content": "from typing import List\n\n\ndef _bits_to_bool(bits: List[int], start: int, end: int) -> bool:\n    \"\"\"\n    interprete the given range of a bit string as a bool\n    as specified in the NMEA AIS sentence standard\n    \"\"\"\n    assert start == end\n    return bits[start] == 1",
        "sha1": "92ef1b2bb10c9fe02ef26cbca1ee39ea7b543a71",
        "id": 498753
    },
    {
        "content": "def get_users_mentions(user_mentions_array):\n    \"\"\"Get user mentions from column, put into string\"\"\"\n    output = []\n    for item in user_mentions_array:\n        output.append(item['screen_name'])\n    return \"|\".join(output)",
        "sha1": "aefd912e134303be929ead4aff6686625e72b30e",
        "id": 283850
    },
    {
        "content": "import pathlib\n\n\ndef str_to_pathlib(in_str):\n    \"\"\"\n    Handle the string to pathlib.Path casting\n    \"\"\"\n    if isinstance(in_str, pathlib.Path):\n        return in_str\n    else:\n        try:\n            return pathlib.Path(in_str)\n        except:\n            raise IOError(\"Invalid path specified.\")",
        "sha1": "20c5903f318f4c972d6d98fd9f6e91872df5cd5f",
        "id": 577445
    },
    {
        "content": "def _sorted_images(images, start_name):\n    \"\"\"Retrieve a sorted list of images with most recent first.\n    \"\"\"\n    images = [(i.name, i) for i in images if i.name.startswith(start_name)]\n    images.sort(reverse=True)\n    return [(i.id, name) for (name, i) in images]",
        "sha1": "7f878983533a31c40ced51522d5d051b75fde96d",
        "id": 654808
    },
    {
        "content": "def indent(string):\n    \"\"\"Return an indented string.\"\"\"\n    indent_char_count = 4\n    return ' '*indent_char_count + str(string)",
        "sha1": "11e6c9a2f1e80ab0dfac61b2ffbb8a05994452ca",
        "id": 501884
    },
    {
        "content": "def get_fw_setting(fw_conn, items_xpath):\n    \"\"\"Get FW Setting Info\n\n    Args:\n        fw_conn (PanDevice): A panos object for device\n        items_xpath (str): The items xpath for the setting\n\n    Returns:\n        xml_data (Element): XML data from firewall\n    \"\"\"\n    base_xpath = (\"/config/devices/entry[@name='localhost.localdomain']\"\n                  + items_xpath)\n    xml_data = fw_conn.xapi.get(xpath=base_xpath)\n    return xml_data",
        "sha1": "78b5c063c66d53436e7d20f46afa272862404026",
        "id": 88558
    },
    {
        "content": "import requests\n\n\ndef make_get_request(url, headers=None):\n    \"\"\"Wrapper for requests.get\"\"\"\n    return requests.get(url, headers=headers)",
        "sha1": "d9163c15ce08582b9951a74611dc64ddbd89b1d8",
        "id": 557113
    },
    {
        "content": "def make_flat_list(input_list):\n    \"\"\"Takes a multidimensional list and returns a one dimensional list\"\"\"\n    flat_list = []\n\n    for x in input_list:\n        if type(x) != list:\n            flat_list.append(x)\n        elif type(x) == list:\n            for y in make_flat_list(x):\n                flat_list.append(y)\n    \n    return flat_list",
        "sha1": "72a1f80f0035312a500d548a82b956327b0d2e10",
        "id": 339882
    },
    {
        "content": "from string import ascii_uppercase\n\n\ndef rhyme_designator(index):\n    \"\"\"\n    Returns a string with an uppercase letter and a modifier.\n\n    The modifier indicates how many times around the alphabet the index has\n    gone, e.g. for index = 27, the string is A'.\n    \"\"\"\n    num = len(ascii_uppercase)\n    letter = ascii_uppercase[index % num]\n    modifier = index // num\n    if modifier == 0:\n        modifier = \"\"\n    else:\n        modifier = str(modifier)\n    return letter + modifier",
        "sha1": "2e0861ed692bfb6e32b8bfa0232421cc0385c0da",
        "id": 457835
    },
    {
        "content": "def hasattrs(object, *names):\n    \"\"\"\n    Takes in an object and a variable length amount of named attributes,\n    and checks to see if the object has each property. If any of the\n    attributes are missing, this returns false.\n\n    :param object: an object that may or may not contain the listed attributes\n    :param names: a variable amount of attribute names to check for\n    :return: True if the object contains each named attribute, false otherwise\n    \"\"\"\n    for name in names:\n        if not hasattr(object, name):\n            return False\n    return True",
        "sha1": "f3a2fc308d041ed0de79e3389e30e02660a1d535",
        "id": 5997
    },
    {
        "content": "def number_of_yang_modules_that_passed_compilation(in_dict: dict, position: int, compilation_condition: str):\n    \"\"\"\n    Return the number of the modules that have compilation status equal to the 'compilation_condition'.\n\n    Arguments:\n        :param in_dict                  (dict) Dictionary of key:yang-model, value:list of compilation results\n        :param position                 (int) Position in the list where the 'compilation_condidtion' is\n        :param compilation_condition    (str) Compilation result we are looking for - PASSED, PASSED WITH WARNINGS, FAILED\n    :return: the number of YANG models which meet the 'compilation_condition'\n    \"\"\"\n    t = 0\n    for k, v in in_dict.items():\n        if in_dict[k][position - 1] == compilation_condition:\n            t += 1\n    return t",
        "sha1": "d40d10a5601589518aa179822d851628d6b24a0a",
        "id": 692689
    },
    {
        "content": "def days_between(d1, d2):\n    \"\"\"Returns the difference in days between two \n    datetime objects.\n    \"\"\"\n    return abs((d2 - d1).days)",
        "sha1": "91cd79d40b34537ab23c2579246c8ae40de11357",
        "id": 302754
    },
    {
        "content": "def _format_int(n, length=20):\n    \"\"\"\n    Return an integer with a certain length for readability\n    :param n: A integer value\n    :return: The integer value padded with spaces\n    \"\"\"\n    return f'{n: >{length}}'",
        "sha1": "04712155a00b33d17441e72ec5fcdc49cbc0c826",
        "id": 298767
    },
    {
        "content": "import re\n\n\ndef remove_non_alphabetic(text):\n    \"\"\"\n    Method used to remove all non alphabet from text\n\n    Parameters:\n    -----------------\n        text (string): Text to clean\n\n    Returns:\n    -----------------\n        text (string): Text cleaned without punctuation\n\n    \"\"\"\n\n    # removing all non alphabet chars\n    text = re.sub(\"[^a-zA-Z]+\", \" \", text)\n\n    return text",
        "sha1": "b5bcdb2bf4d73dc59b042ad91b00cc34dbc29e38",
        "id": 438904
    },
    {
        "content": "def add_mac_colon(mac_address, config):\n    \"\"\"\n    Formats a MAC address string with colons\n\n    Arguments:\n    mac_address (str) - MAC address string without colons\n    config (TcConfig) - TcConfig instance (used to determine logging level)\n\n    Returns:\n    str - Formatted MAC address\n\n    Usage:\n    >>> add_mac_colon('000000010203', config)\n    '00:00:00:01:02:03'\n    \"\"\"\n    if config.verbose > 2:\n        print(\"MAC ADDRESS IS: \\\"%s\\\"\" % mac_address)\n    return ':'.join(map(''.join, zip(*[iter(mac_address)] * 2)))",
        "sha1": "c19ccda23a82d8fbf46cefb730961172df5023ff",
        "id": 150733
    },
    {
        "content": "def hex_to_rgb_string_filter(hex):\n    \"\"\"\n    Given hex will return rgb string\n\n    E.g. #0b0c0c ==> \"11, 12, 12\"\n    \"\"\"\n    h = hex.lstrip(\"#\")\n    rgb = tuple(int(h[i : i + 2], 16) for i in (0, 2, 4))\n    return f\"{rgb[0]},{rgb[1]},{rgb[2]}\"",
        "sha1": "4a8301e6dab194c5a0ace30403951898e3c57449",
        "id": 468594
    },
    {
        "content": "def _check_callable(func, value):\n    \"\"\"Return true if func(value) returns is true or if *func* is\n    *value*.\n    \"\"\"\n    return value is func or func(value)",
        "sha1": "a24aaeb5a6c7605ae4c202ce0234da01f1eaa89f",
        "id": 95842
    },
    {
        "content": "def parse_tcp_uri(uri):\n    \"\"\"Parse tcp://<host>:<port>.\n\n    \"\"\"\n\n    try:\n        if uri[:6] != 'tcp://':\n            raise ValueError\n\n        address, port = uri[6:].split(':')\n\n        return address, int(port)\n    except (ValueError, TypeError):\n        raise ValueError(\n            f\"Expected URI on the form tcp://<host>:<port>, but got '{uri}'.\")",
        "sha1": "76120575341b5cba71304c544fdb5a759ed5fcd2",
        "id": 432712
    },
    {
        "content": "def best_ss_to_expand_greedy(new_set, supersets, ruleWeights, max_mask):\n    \"\"\" Returns index of the best superset to expand, given the rule\n        weights and the maximum allowed mask size. -1 if none possible.\n    \"\"\"\n\n    bestSuperset = None\n    bestCost = float('inf')\n\n    new_set = set(new_set)\n\n    for superset in supersets:\n        # if this merge would exceed the current mask size limit, skip it\n        if len(new_set.union(superset)) > max_mask:\n            continue\n\n        # the rule increase is the sum of all rules that involve each part added to the superset\n        cost = sum(ruleWeights[part] for part in new_set.difference(superset))\n        if cost < bestCost:\n            bestCost = cost\n            bestSuperset = superset\n\n    # if no merge is possible, return -1\n    if bestSuperset == None:\n        return -1\n\n    return supersets.index(bestSuperset)",
        "sha1": "e03a5d9d752f24b7b5188ae9babae277642bacf2",
        "id": 186572
    },
    {
        "content": "def get_solute_data_from_db(solute_spc, db):\n    \"\"\"\n    Returns solute data found from the RMG solute library and corresponding comment.\n    \"\"\"\n    if solute_spc is not None:\n        data = db.get_solute_data_from_library(solute_spc, db.libraries['solute'])\n        if data is not None:\n            solute_data = data[0]\n            solute_comment = f'From RMG-database solute library: {data[2].index}. {data[2].label}'\n        else:\n            solute_data = None\n            solute_comment = 'Not found in RMG-database'\n        return solute_data, solute_comment\n    else:\n        return None, None",
        "sha1": "4a82602f1bbcf923cb06b76c8666dc1e2898aadc",
        "id": 270434
    },
    {
        "content": "import functools\n\n\ndef wrap_thread_target(target, future):\n    \"\"\"Wrap a thread's target function with a future object.\n\n    With this, you may call ``sys.exit`` inside a thread, and re-raises\n    the ``SystemExit`` in the main thread through the future object.\n\n    Examples:\n    >>> f = Future()\n    >>> t = threading.Thread(target=wrap_thread_target(sys.exit, f))\n    >>> t.start()\n    >>> t.join()\n    >>> f.get_exception()\n    SystemExit()\n    \"\"\"\n\n    @functools.wraps(target)\n    def wrapper(*args, **kwargs):\n        # We set reraise to False because, when an exception reaches\n        # CPython runtime thread runner, CPython usually just logs its\n        # stack trace, which we prefer doing at the thread-join site.\n        with future.catching_exception(reraise=False):\n            future.set_result(target(*args, **kwargs))\n\n    return wrapper",
        "sha1": "2eb763850867ed2a74070d23aa43dee45170974f",
        "id": 530884
    },
    {
        "content": "def loadString(dset):\n    \"\"\"!\n    Load a string from an HDF5 dataset and return as a Python str object.\n\n    Since version 3.0, h5py loads UTF8 strings as `bytes` objects.\n    This function provides uniform behavior across h5py 2.0 and h5py 3.0 by\n    always returning `str` objects.\n    \"\"\"\n    s = dset[()]\n    if isinstance(s, str):\n        return s\n    return s.decode(\"utf-8\")",
        "sha1": "9997029ae7f025435f89e35da1c0e37fa211195b",
        "id": 111801
    },
    {
        "content": "def make_response(data, message, status=\"success\", error=False):\n    \"\"\"Return a standardised response object.\"\"\"\n    return {\n        \"data\": data,\n        \"message\": message,\n        \"status\": status,\n        \"error\": error\n    }",
        "sha1": "2755c9d5c3db7bed9e2e9d7f9e91aab290500a57",
        "id": 102426
    },
    {
        "content": "from functools import reduce\n\n\ndef concat(l):\n    \"\"\"\n    >>> concat([[0, 1], [2], [3, 4, 5]])\n    [0, 1, 2, 3, 4, 5]\n    \"\"\"\n    return reduce(list.__add__, l, [])",
        "sha1": "a746ad6283466bbd747db4ff2a5ce4e4bedbb0c2",
        "id": 430465
    },
    {
        "content": "def _uniquify(seq, sep='-'):\n    \"\"\"Uniquify a list of strings.\n\n    Adding unique numbers to duplicate values.\n\n    Parameters\n    ----------\n    seq : `list` or `array-like`\n        A list of values\n    sep : `str`\n        Separator\n\n    Returns\n    -------\n    seq: `list` or `array-like`\n        A list of updated values\n    \"\"\"\n\n    dups = {}\n\n    for i, val in enumerate(seq):\n        if val not in dups:\n            # Store index of first occurrence and occurrence value\n            dups[val] = [i, 1]\n        else:\n            # Increment occurrence value, index value doesn't matter anymore\n            dups[val][1] += 1\n\n            # Use stored occurrence value\n            seq[i] += (sep+str(dups[val][1]))\n\n    return(seq)",
        "sha1": "dcc264912f48df95f97649dec215d117a9f4314e",
        "id": 539039
    },
    {
        "content": "def signature(part):\n    \"\"\" return the signature of a partial object \"\"\"\n    return (part.func, part.args, part.keywords, part.__dict__)",
        "sha1": "522ae88538d6dd880492292c6f2ef169f3bbd06d",
        "id": 124057
    },
    {
        "content": "import torch\n\n\ndef fMAE(preds, targs, *kwargs):\n    \"\"\"\n    Loss function\n    Mean Absolute Error\n    \"\"\"\n    return torch.mean( torch.abs(targs - preds) )",
        "sha1": "090bbd11b672c61281edc4df61010ec7bf1fb924",
        "id": 441282
    },
    {
        "content": "import torch\n\n\ndef convert_to_tensor(array):\n    \"\"\"Converts numpy arrays and lists to Torch tensors before calculation losses\n    :param array: torch.tensor / Numpy array / List\n    \"\"\"\n    return torch.FloatTensor(array) if type(array) != torch.Tensor else array",
        "sha1": "d7f6321a62d969e3ba4546911175bb74dd3f0ffc",
        "id": 162462
    },
    {
        "content": "def find_smallest(items):\n    \"\"\"Find the smallest item's index in list.\"\"\"\n    smallest = items[0]\n    idx = 0\n    for i, v in enumerate(items):\n        if v < smallest:\n            smallest = v\n            idx = i\n    return idx",
        "sha1": "68a95439f5542fb90e6468abb6305e1c2b51c820",
        "id": 287254
    },
    {
        "content": "def rem_var(var, subs):\n    \"\"\"Deletes any substitutions of var in subs.\"\"\"\n    newsubs = subs.copy()\n    try:\n        del newsubs[var.constant()]\n    except KeyError:\n        pass\n    try:\n        del newsubs[var.variable()]\n    except KeyError:\n        pass\n    return newsubs",
        "sha1": "161c62470ad648e2d062d021b528ece4e1555509",
        "id": 65962
    },
    {
        "content": "def decode_varint_py(buffer, pos=0):\n    \"\"\" Decode an integer from a varint presentation. See\n    https://developers.google.com/protocol-buffers/docs/encoding?csw=1#varints\n    on how those can be produced.\n\n        Arguments:\n            buffer (bytearry): buffer to read from.\n            pos (int): optional position to read from\n\n        Returns:\n            (int, int): Decoded int value and next read position\n    \"\"\"\n    result = buffer[pos]\n    if not (result & 0x81):\n        return (result >> 1), pos + 1\n    if not (result & 0x80):\n        return (result >> 1) ^ (~0), pos + 1\n\n    result &= 0x7f\n    pos += 1\n    shift = 7\n    while 1:\n        b = buffer[pos]\n        result |= ((b & 0x7f) << shift)\n        pos += 1\n        if not (b & 0x80):\n            return ((result >> 1) ^ -(result & 1), pos)\n        shift += 7\n        if shift >= 64:\n            raise ValueError(\"Out of int64 range\")",
        "sha1": "e51e7f9f225ce0edd7d157239f1db9e051518bdc",
        "id": 644482
    },
    {
        "content": "def _prefixscan_combine(func, binop, pre, x, axis, dtype):\n    \"\"\"Combine results of a parallel prefix scan such as cumsum\n\n    Parameters\n    ----------\n    func : callable\n        Cumulative function (e.g. ``np.cumsum``)\n    binop : callable\n        Associative function (e.g. ``add``)\n    pre : np.array\n        The value calculated in parallel from ``preop``.\n        For example, the sum of all the previous blocks.\n    x : np.array\n        Current block\n    axis : int\n    dtype : dtype\n\n    Returns\n    -------\n    np.array\n    \"\"\"\n    # We could compute this in two tasks.\n    # This would allow us to do useful work (i.e., func), while waiting on `pre`.\n    # Using one task may guide the scheduler to do better and reduce scheduling overhead.\n    return binop(pre, func(x, axis=axis, dtype=dtype))",
        "sha1": "95eff470bc28cb55519608391686781edb2dce1c",
        "id": 479316
    },
    {
        "content": "def get_email_html(qotd_html=\"\", comic_html=\"\"):\n    \"\"\"\n    Generates beautiful HTML from the qotd & comic supplied\n    Also includes some default content like attribution, etc.\n    \"\"\"\n    return \"\"\"<html>\n    <head></head>\n    <body>\n        <p>Good morning! Here's your daily dose of inspiration & some humour :)</p>\n        {0}<br>\n        {1}<br><br>\n        <p>Cheers,<br>Team Finomena</p>\n    </body>\n    </html>\"\"\".format(qotd_html, comic_html)",
        "sha1": "ac08dcfcce73b5108308eaf3e535568ad32f180d",
        "id": 414004
    },
    {
        "content": "def prune_databag(databag):\n    \"\"\"\n    Sometimes empty projects find their way into the databag.\n    This function prunes out the empty ones.\n    \"\"\"\n    pruned = {\"projects\": []}\n\n    for project in databag[\"projects\"]:\n        if len(project[\"members\"]) > 0:\n            pruned[\"projects\"].append(project)\n\n    return pruned",
        "sha1": "0517e5ce893f080c16c0fdb5e80b985d74c9c59c",
        "id": 280013
    },
    {
        "content": "from typing import List\n\n\ndef brute_force_max_sum_subarray(array: List[int]) -> int:\n    \"\"\"\n    A brute force O(n2) algorithm to solve the maximum subarray problem.\n    :param array: the array to process\n    :return: the maximum value amongst all consecutive subarrays\n\n    >>> brute_force_max_sum_subarray([34, -50, 42, 14, -5, 86])\n    137\n    >>> brute_force_max_sum_subarray([-5, -1, -8, -9])\n    0\n    \"\"\"\n    if not array:\n        return 0\n    return max([sum(array[i1:i2]) for i1 in range(len(array)) for i2 in range(i1, len(array)+1)])",
        "sha1": "777e4edfd64479fc090e2a756c4f5733c9f477bd",
        "id": 264074
    },
    {
        "content": "def bytescl(img, bottom, top):\n    \"\"\"\n    Scale a pixel image to limits (0, 1).\n    Keyword arguments:\n    img    -- Original pixel image.\n    bottom -- Lower limit of img.\n    top    -- Upper limit of img.\n    Output(s):\n    scl_img -- Scaled image with new limits 0(min) - 1(max).\n    \"\"\"\n\n    scl_img = (((top - bottom) * (img - img.min())) / (img.max() - img.min())) + bottom\n    \n    return scl_img",
        "sha1": "e9b62e982920165be0a6897fb7f152c32737a14b",
        "id": 73514
    },
    {
        "content": "import re\n\n\ndef convert_c_source_to_bytes(input_cc_file):\n  \"\"\"Converts C++ source file to bytes (immutable).\n\n  Args:\n    input_cc_file: A .cc file to process.\n\n  Returns:\n    A bytearray corresponding to the input cc file array.\n  \"\"\"\n  pattern = re.compile(r'(((0x[0-9a-fA-F]+),?)+)')\n  model_bytearray = bytearray()\n\n  with open(input_cc_file) as file_handle:\n    for line in file_handle:\n      values_match = pattern.search(line)\n\n      if values_match is None:\n        continue\n\n      list_text = values_match.group(1)\n      values_text = filter(None, list_text.split(','))\n\n      values = [int(x, base=16) for x in values_text]\n      model_bytearray.extend(values)\n\n  return bytes(model_bytearray)",
        "sha1": "f60832e365ea33a4b1dc4c2566ac675ed56aa131",
        "id": 334794
    },
    {
        "content": "def collect_summary(cols):\n    \"\"\"Select the summary sentences that are matched with a given section into an array of sentences\"\"\"\n    section_idx, matched_summaries = cols.section_idx, cols.matched_summaries\n    collected_summary = [t for (t, s_idx) in matched_summaries if s_idx == section_idx]\n    return collected_summary",
        "sha1": "43a64b18681019a9634814d81bbb8205284b04f2",
        "id": 96981
    },
    {
        "content": "def is_js_registered(app, filename):\n    \"\"\"\n    Checks whether a given js file has been added to the Sphinx register.\n    \"\"\"\n    for js_file, _ in app.registry.js_files:\n        if filename == js_file:\n            return True\n    return False",
        "sha1": "dc80ae6852d16c8a75765fbd9bbfd8522b07ef31",
        "id": 147796
    },
    {
        "content": "import torch\n\n\ndef ones(shape):\n    \"\"\"All ones.\"\"\"\n    initial = torch.ones(shape, dtype=torch.float32)\n    return torch.nn.Parameter(initial)",
        "sha1": "19950b81d04d0625de6b7a90782903bdd43e10b2",
        "id": 88380
    },
    {
        "content": "def bin2int(data):\n    \"\"\"Convert a byte-string to an integer.\"\"\"\n\n    number = 0\n    if isinstance(data, int):  # A single byte is given\n        number += data\n    else:\n        for idx, byte in enumerate(data):\n            number += byte * (256 ** idx)\n    return number",
        "sha1": "c86ac04dfc729073b669ec5413f964e095483e11",
        "id": 311284
    },
    {
        "content": "def parse_type(_type):\n    \"\"\"Parse type str as builtin type.\"\"\"\n    if isinstance(_type, type):\n        return _type\n    elif _type == \"integer\":\n        return int\n    elif _type == \"real\":\n        return float\n    elif _type == \"string\":\n        return str\n    else:\n        raise NotImplementedError()",
        "sha1": "4e4773cf9c7255f9f054da4b5243772e84321197",
        "id": 125537
    },
    {
        "content": "def create_run_command(class_name, method_name):\n    \"\"\"\n    Creates the bash command for running a JUnit test.\n\n    Params:\n    class_name (str): The class name of the test to run.\n    method_name (str): The name of the method of the class to run.\n\n    Return:\n    str: The bash command for running.\n    \"\"\"\n    return f\"java -jar junit-platform-console-standalone-1.7.0.jar --fail-if-no-tests --disable-banner --details-theme=ascii --disable-ansi-colors -cp classes --select-method={class_name}#{method_name}\"",
        "sha1": "9b757e40e63bbdd7f3ee49d4506f319cedfa5f87",
        "id": 447321
    },
    {
        "content": "def read_POPI_points(file_name):\n    \"\"\"\n    Read the Point-validated Pixel-based Breathing Thorax Model (POPI) landmark points file.\n    The file is an ASCII file with X Y Z coordinates in each line and the first line is a header.\n\n    Args:\n       file_name: full path to the file.\n    Returns:\n       (list(tuple)): List of points as tuples.\n    \"\"\"\n    with open(file_name,'r') as fp:\n        lines = fp.readlines()\n        points = []\n        # First line in the file is #X Y Z which we ignore.\n        for line in lines[1:]:\n            coordinates = line.split()\n            if coordinates:\n                points.append((float(coordinates[0]), float(coordinates[1]), float(coordinates[2])))\n        return points",
        "sha1": "bcf5f0b447ab0fe5ab0ca7aa93cc129927425730",
        "id": 647978
    },
    {
        "content": "import uuid\n\n\ndef generate_uuid(remove_hyphen=True):\n    \"\"\"Generates a unique uuid string\n\n    Keyword arguments:\n        remove_hyphen: True if the hyphens should be removed from the uuid, False otherwise\n    \"\"\"\n    res = str(uuid.uuid4())\n    if remove_hyphen == True:\n        res = res.replace(\"-\", \"\")\n    return res",
        "sha1": "a448a7585a1ead66135b6336d28a37d416ff0762",
        "id": 443625
    },
    {
        "content": "def cog2str(cog):\n    \"\"\"\n    Get the full description for a COG category letter\n\n    Parameters\n    ----------\n    cog : str\n        COG category letter\n\n    Returns\n    -------\n    str\n        Description of COG category\n    \"\"\"\n\n    cog_dict = {\n        \"A\": \"RNA processing and modification\",\n        \"B\": \"Chromatin structure and dynamics\",\n        \"C\": \"Energy production and conversion\",\n        \"D\": \"Cell cycle control, cell division, chromosome partitioning\",\n        \"E\": \"Amino acid transport and metabolism\",\n        \"F\": \"Nucleotide transport and metabolism\",\n        \"G\": \"Carbohydrate transport and metabolism\",\n        \"H\": \"Coenzyme transport and metabolism\",\n        \"I\": \"Lipid transport and metabolism\",\n        \"J\": \"Translation, ribosomal structure and biogenesis\",\n        \"K\": \"Transcription\",\n        \"L\": \"Replication, recombination and repair\",\n        \"M\": \"Cell wall/membrane/envelope biogenesis\",\n        \"N\": \"Cell motility\",\n        \"O\": \"Post-translational modification, protein turnover, and chaperones\",\n        \"P\": \"Inorganic ion transport and metabolism\",\n        \"Q\": \"Secondary metabolites biosynthesis, transport, and catabolism\",\n        \"R\": \"General function prediction only\",\n        \"S\": \"Function unknown\",\n        \"T\": \"Signal transduction mechanisms\",\n        \"U\": \"Intracellular trafficking, secretion, and vesicular transport\",\n        \"V\": \"Defense mechanisms\",\n        \"W\": \"Extracellular structures\",\n        \"X\": \"No COG annotation\",\n        \"Y\": \"Nuclear structure\",\n        \"Z\": \"Cytoskeleton\",\n    }\n\n    return cog_dict[cog]",
        "sha1": "b796963f4d8dfd2ed70fc1f85c99ed74a1def9c5",
        "id": 415486
    },
    {
        "content": "import re\n\n\ndef clean_val(val):\n\t\"\"\"\n\tReturns cleaned value after replacing underscores (`'_'`) and asterisks (`'*'`) with\n\tan empty whitespace.\n\n\t:param val:\tRaw parsed string\n\t:type val: str\n\n\t:return:\n\t\tA :py:class:`str` object after removing underscores and asterisks\n\t\"\"\"\n\treturn re.sub(r'[_*]', ' ', val).strip()",
        "sha1": "8289cee081989cb5d6bed712c9b3d7c497298d9c",
        "id": 212655
    },
    {
        "content": "def resolve_uri_protocol(uri: str) -> str:\n    \"\"\"Require that the URI has a scheme by applying a default \"benchmark\"\n    scheme if none is set.\"\"\"\n    if \"://\" not in uri:\n        return f\"benchmark://{uri}\"\n    return uri",
        "sha1": "f505e2dc5e038aac171946d9de9534bafe578c23",
        "id": 339105
    },
    {
        "content": "import csv\n\n\ndef read_plan(name):\n    \"\"\"Reads a text file with csv format and returns a 2D list.\n\n    Args:\n        name (string): path to file to be read in\n\n    Returns:\n        2D list: parsed values\n    \n    >>> p = read_plan('drunk.plan.txt')\n    >>> len(p)\n    300\n    >>> len(p[0])\n    300\n    \"\"\"\n\n    f = open(name, newline = '')\n    reader = csv.reader(f, quoting = csv.QUOTE_NONNUMERIC)\n    plan = list(reader)\n    f.close()\n    return plan",
        "sha1": "8ba71bee37059add94e6ee865e114f0031c6d0b7",
        "id": 222756
    },
    {
        "content": "def _check_if_StrNotBlank(string):\n        \"\"\"\n        check if a sting is blank/empty\n\n        Parameters\n        ----------\n\n        Returns\n        -------\n        : boolean\n            True if string is not blank/empty\n            False if string is blank/empty\n        \"\"\"\n\n        return bool(string and string.strip())",
        "sha1": "e5de1d902f8e3931d23e04c6ba825b17d90e8d1d",
        "id": 10553
    },
    {
        "content": "from datetime import datetime\n\n\ndef days_diff(date1, date2):\n    \"\"\"\n        Find absolute diff in days between dates\n    \"\"\"\n    date1 = datetime.date(*date1)\n    date2 = datetime.date(*date2)\n    diff = date1 - date2\n\n    return abs(diff.days)",
        "sha1": "66b52430849194232dbce521abb759c968d919fc",
        "id": 253148
    },
    {
        "content": "def s_round(self, places=2):\n\t\"\"\"\n\tCorrectly round float to n decimal places.\n\t>>> s_round(4.055, 2)\n\t4.06\n\t>>> s_round(4.054, 2)\n\t4.05\n\t\"\"\"\n\treturn round(self + 10**(-2*6), places)",
        "sha1": "8af84e89f71429f012910665f605e3c9aeea0065",
        "id": 473289
    },
    {
        "content": "def square_table_while(n):\n    \"\"\"\n    Returns: list of squares less than (or equal to) N \n    \n    This function creates a list of integer squares 1*1, 2*2, ...\n    It only adds those squares that are less than or equal to N.\n    \n    Parameter n: the bound on the squares\n    Precondition: n >= 0 is a number\n    \"\"\"\n    seq = []\n    k = 0\n    while k*k < n:\n        seq.append(k*k)\n        k = k+1\n    \n    return seq",
        "sha1": "8eb94d9690f4138fb6759cf73d5a602659571b34",
        "id": 698796
    },
    {
        "content": "import json\n\n\ndef import_users_from_file(file):\n    \"\"\"Import our user data file and return the results.\"\"\"\n    with open(file, newline='', encoding=\"utf-8\") as load_file:\n        users = json.load(load_file)\n\n        return users",
        "sha1": "8357d6d284cb308be4f9645fc46f8deda817fc56",
        "id": 527669
    },
    {
        "content": "import random\n\n\ndef roll(number, sides):\n    \"\"\"Generate a random dice roll.\n    Returns the total of the roll.\n    number -- the number of dice to roll\n    sides  -- the number of side on dice to roll (4, 6, 8, 10, 12, 20)\n    \"\"\"\n    total = 0\n    for _ in range(number):\n        total += random.randint(1, sides + 1)\n    return total",
        "sha1": "2a6e8221cfe68f835022ca0e90a8af13d40f3f23",
        "id": 565899
    },
    {
        "content": "import ast\n\n\ndef get_inherits(tree):\n    \"\"\"\n    Get what superclasses this class inherits\n    This handles exact names like 'MyClass' but skips things like 'cls' and 'mod.MyClass'\n    Resolving those would be difficult\n    :param tree ast:\n    :rtype: list[str]\n    \"\"\"\n    return [base.id for base in tree.bases if type(base) == ast.Name]",
        "sha1": "ded54846d548c1a5bccd4387a958bff4d8a5fa01",
        "id": 123497
    },
    {
        "content": "def im2col_get_pixel(im, height, width, row, col, channel, pad):\n    \"\"\"\n    Args:\n        im: input image.\n        height: image height.\n        width: image width.\n        row: row index.\n        col: col index.\n        channel: channel index.\n        pad: padding length.\n    \"\"\"\n    row = row - pad\n    col = col - pad\n    if row < 0 or col < 0 or row >= height or col >= width:\n        pixel = 0\n    else:\n        pixel = im[int(col + width * (row + height * channel))]\n    return pixel",
        "sha1": "d03a0e4806e4ae102ca12525920db99fbe2bf1b1",
        "id": 123273
    },
    {
        "content": "def get_nb_articles(bib):\n    \"\"\"Description of get_nb_articles\n    Count the number of articles in the database\n    \"\"\"\n    print(\"There are\", len(bib), \"articles referenced.\")\n    return len(bib)",
        "sha1": "532cb5c6556dfd8997a31fb35ccc085ab34ce51e",
        "id": 404113
    },
    {
        "content": "from typing import Tuple\nfrom typing import Union\n\n\ndef read_next_token(line: str, pos: int) -> Tuple[Union[str, None], int]:\n  \"\"\"Read next token from line.\n\n  Args:\n    line: line.\n    pos: current position.\n\n  Returns:\n    Token (None if not found) and current position.\n  \"\"\"\n  while pos < len(line) and line[pos].isspace():\n    pos += 1\n\n  if pos >= len(line):\n    return None, pos\n\n  initial_pos = pos\n  while pos < len(line) and not line[pos].isspace():\n    pos += 1\n  return line[initial_pos:pos], pos",
        "sha1": "38c2237567f4d387b37b9a1efdfd3c434814422d",
        "id": 535629
    },
    {
        "content": "def read_messages(message_file):\n    \"\"\" (file open for reading) -> list of str\n    \n    Read and return the contents of the file as a list of messages, in the \n    order in which they appear in the file. Strip the newline from each line.\n    \n    \"\"\"\n    \n    # Store the message_file into the lst as a list of messages.\n    lst = message_file.readlines()\n    \n    # Strip the newline from each string in the list.\n    for i in range(len(lst)):\n        lst[i] = lst[i].strip()\n        \n    return lst",
        "sha1": "0e4b1a6995a6dd25ab3783b53e730d0dd446747c",
        "id": 703681
    },
    {
        "content": "def populate_nested_dictionary(dictionary, key_list, types=[]):\n    \"\"\"\n    For each key k_i in key_list, make sure that\n    dictionary[k_0][k_1][k_2]...[k_i] exists, creating objects of the\n    appropriate type in each sub-dictionary as needed.\n\n    If types is specified, then dictionary[key_list[i]] should point to an\n    object of type types[i] for all i\n\n    returns the element corresponding to dictionary[k_0][k_1]...[k_{n-1}] where\n    n is the size of key_list\n    \"\"\"\n\n    if len(key_list) > 0:\n        if key_list[0] not in dictionary:\n            if len(types) > 0:\n                dictionary[key_list[0]] = types[0]()\n            else:\n                dictionary[key_list[0]] = {}\n\n        return populate_nested_dictionary(dictionary[key_list[0]], key_list[1:],\n                                          types[1:])\n    else:\n        return dictionary",
        "sha1": "8b531db42db49809e8cb510e5a121a394dc13f17",
        "id": 100255
    },
    {
        "content": "def get_monotasks_for_stage_from_conf(conf, stage_num):\n  \"\"\"Extracts a Stage's Monotasks from a SimulatorConf.\n\n  Returns:\n    A list of Monotasks for one Macrotask from the Stage with the provided index, extracted from the\n    provided SimulatorConf.\n  \"\"\"\n  return conf.jobs[0].stages[stage_num].macrotasks[0].monotasks",
        "sha1": "2a3650851a8d293cc78e525c56dbe02e7f95b6fd",
        "id": 411883
    },
    {
        "content": "def normalize(name):\n    \"\"\" Return normalized name \"\"\"\n    return name.lower().replace('_', '-')",
        "sha1": "0b98e297d9a784c0f63bec9af8fdb5277b07f590",
        "id": 319433
    },
    {
        "content": "def wrap_quote(str):\n    \"\"\"Format quote.\"\"\"\n    return f\"> {str}\\n\"",
        "sha1": "652a44a0a9e85c795209bae69e298489f77c1333",
        "id": 113163
    },
    {
        "content": "def compute_pulse_properties(w, baseline_samples):\n    \"\"\"Compute basic pulse properties quickly\n    :param w: Raw pulse waveform in ADC counts\n    :param baseline_samples: number of samples to use for baseline computation at the start of the pulse\n    :return: (baseline, baseline_increase, noise_sigma, min, max);\n      baseline is the average of the first baseline_samples in the pulse\n      baseline_increase = baseline_after - baseline_before\n      min and max relative to baseline\n      noise_sigma is the std of samples below baseline\n    Does not modify w. Does not assume anything about inversion of w!!\n    \"\"\"\n    # Compute the baseline before and after the self-trigger\n    baseline = 0.0\n    baseline_samples = min(baseline_samples, len(w))\n    for x in w[:baseline_samples]:\n        baseline += x\n    baseline /= baseline_samples\n\n    baseline_after = 0.0\n    for x in w[-baseline_samples:]:\n        baseline_after += x\n    baseline_after /= baseline_samples\n\n    baseline_increase = baseline_after - baseline\n\n    # Now compute mean, noise, and min\n    n = 0           # Running count of samples included in noise sample\n    m2 = 0          # Running sum of squares of differences from the baseline\n    max_a = -1.0e6  # Running max amplitude\n    min_a = 1.0e6   # Running min amplitude\n\n    for x in w:\n        if x > max_a:\n            max_a = x\n        if x < min_a:\n            min_a = x\n        if x < baseline:\n            delta = x - baseline\n            n += 1\n            m2 += delta*(x-baseline)\n\n    if n == 0:\n        # Should only happen if w = baseline everywhere\n        noise = 0\n    else:\n        noise = (m2/n)**0.5\n\n    return baseline, baseline_increase, noise, min_a - baseline, max_a - baseline",
        "sha1": "f8b6d46dbed5e21dd27818c1a4845c2f61dc8f63",
        "id": 232217
    },
    {
        "content": "def get_node_information(data_node):\n    \"\"\"\n    It parses nodes in GPML xml file.\n    :param data_node: XML node of the type \"data\"\n    :return: node id and structured information of the form:\n    {\n        \"database_name\": database_name,\n        \"database_id\": database_id,\n        \"text\": text,\n        \"type\": type,\n        \"groupref\": groupref\n    }\n    \"\"\"\n    # get database and its corresponding unique identifier.\n    xref = data_node.find(\"xref\")\n    database_name = xref.attrs[\"database\"]\n    database_id = xref.attrs[\"id\"]\n\n    # get the graph id.\n    id = data_node.attrs.get(\"graphid\", None)\n    text = data_node.attrs[\"textlabel\"]\n    type = data_node.attrs.get(\"type\", \"unknown\")\n    groupref = data_node.attrs.get(\"groupref\", None)\n\n    # form a document.\n    return (id, {\n        \"database_name\": database_name,\n        \"database_id\": database_id,\n        \"text\": text,\n        \"type\": type,\n        \"groupref\": groupref\n    })",
        "sha1": "4798f9fc440082b4b9e82ffce05b4a63150dd94d",
        "id": 56118
    },
    {
        "content": "def count_all(d):\n    \"\"\"Return the total count of all structures in the structure freq data.\"\"\"\n    return sum(d.values())",
        "sha1": "b029a38f266c010a893f146bd6fb024fdef6d6ff",
        "id": 181423
    },
    {
        "content": "from functools import reduce\n\n\ndef bcc(data):\n    \"\"\"\n    Calculate BCC (Block Check Character) checksum for data\n\n    :param data:\n    :type data: string\n    :return: BCC checksum\n    :rtype: int\n    \"\"\"\n    return reduce(lambda a,b: a^b, bytearray(data))",
        "sha1": "399f4e4e8135f31029fe29addf42c4d27f5930ac",
        "id": 493529
    },
    {
        "content": "def fixture_all_countries(south_africa, nigeria, egypt, kenya):\n    \"\"\"\n    Returns a map of all country objects.\n    \"\"\"\n    return {\n        \"south_africa\": south_africa,\n        \"nigeria\": nigeria,\n        \"egypt\": egypt,\n        \"kenya\": kenya,\n    }",
        "sha1": "054cc2d98fd9d5a58a183a68c13c25208dfea4e9",
        "id": 586980
    },
    {
        "content": "def dequeueMessage(queue):\n    \"\"\"\n    Dequeue  a single  message from the queue.\n    Returns None if no message found.\n    \"\"\"\n\n    m = queue.read()\n    if m is None:\n        return None\n    else:\n        queue.delete_message(m)\n        return m.get_body()",
        "sha1": "7f8e3391d065736b49520b8d52e741629b35bfeb",
        "id": 47992
    },
    {
        "content": "import re\n\n\ndef extract_migration(script, name):\n    \"\"\"Extract given (forward, reverse) migration from script\"\"\"\n    functions_regex = (\n        r'.*(?P<forward>function forward.+{.+})'\n        r'.*(?P<reverse>function reverse.+{.+}).*'\n    )\n    matches = re.fullmatch(functions_regex, script, re.DOTALL)\n    if matches and name in matches.groupdict():\n        return matches.group(name)",
        "sha1": "5a64c229c9dc54f6055b425cb06edec21e5b7906",
        "id": 264618
    },
    {
        "content": "from typing import Optional\nfrom typing import Dict\nfrom typing import Any\nimport warnings\n\n\ndef get_settings(\n    conf_group: Optional[Dict[str, Any]], name_map: Optional[Dict[str, str]] = None\n) -> Dict[Any, Any]:\n    \"\"\"\n    Lookup configuration values config, environment or KeyVault.\n\n    Parameters\n    ----------\n    conf_group : Optional[Dict[str, Any]]\n        The configuration dictionary\n    name_map : Optional[Dict[str, str]], optional\n        Optional mapping to re-write setting names,\n        by default None\n\n    Returns\n    -------\n    Dict[Any, Any]\n        Dictionary of resolved settings\n\n    Raises\n    ------\n    NotImplementedError\n        Keyvault storage is not yet implemented\n\n    \"\"\"\n    if not conf_group:\n        return {}\n    setting_dict: Dict[str, Any] = conf_group.copy()\n\n    for arg_name, arg_value in conf_group.items():\n        target_name = arg_name\n        if name_map:\n            target_name = name_map.get(target_name, target_name)\n\n        if isinstance(arg_value, str):\n            setting_dict[target_name] = arg_value\n        elif isinstance(arg_value, dict):\n            try:\n                setting_dict[target_name] = _fetch_setting(arg_value)  # type: ignore\n            except NotImplementedError:\n                warnings.warn(\n                    f\"Setting type for setting {arg_value} not yet implemented. \"\n                )\n    return setting_dict",
        "sha1": "3c884860d077805b8c2c8b24de6932e78188613d",
        "id": 293796
    },
    {
        "content": "def are_blocking_checks(checks, ignore_warnings):\n    \"\"\"\n    Return True if checks are errors or unignored warnings.\n\n    :arg dict checks: dictionary with a list of errors/warnings per library\n    :arg bool ignore_warnings: ignores failed checks of type warning\n    \"\"\"\n    has_errors = any(p.endswith('Errors') for p in checks)\n\n    return (not ignore_warnings and checks) or has_errors",
        "sha1": "cc8d1e8ca9d0ee77b0384d3ea5863fff7d46350f",
        "id": 656987
    },
    {
        "content": "def calc_max_min_marks(marks):\n    \"\"\" Function which returns the minimum(excluding 0) and maximum score of the class in the form of a list. \"\"\"\n    result = []\n    marks.sort()\n    min_max = []\n    min_max[:] = (value for value in marks if value != 0)\n    least_score = min_max[0]\n    highest_score = min_max[-1]\n\n    result.append(least_score)\n    result.append(highest_score)\n\n    return result",
        "sha1": "92c6581ea2d967772cda043adee5c5d5ce893335",
        "id": 53528
    },
    {
        "content": "def _tag_encloses_foreign_namespace(tag: str) -> bool:\n    \"\"\"\n    Checks whether the tag encloses a foreign namespace (MathML or SVG).\n\n    https://html.spec.whatwg.org/multipage/syntax.html#foreign-elements\n    \"\"\"\n    return tag.lower() in (\"math\", \"svg\")",
        "sha1": "9f556b5f24e8081c12b84f01535a039d60df67be",
        "id": 238861
    },
    {
        "content": "from typing import Callable\nfrom typing import Tuple\nfrom typing import Mapping\nimport inspect\n\n\ndef get_function_parameters(\n    f: Callable,\n) -> Tuple[Mapping[str, inspect.Parameter], type]:\n    \"\"\"\n    Returns function's signature as parameters and the output type.\n\n    :param f: the function\n    :return: the parameters and the output type\n    :raises: ValueError: if the function is a builtin function or method\n\n    :Example:\n\n    >>> def f(a: int, b: int) -> int:\n    >>>     return a + b\n    >>> get_function_parameters(f)\n    (mappingproxy({'a': <Parameter \"a:int\">, 'b': <Parameter \"b:int\">}), int)\n    \"\"\"\n    sign = inspect.signature(f)\n    return sign.parameters, sign.return_annotation",
        "sha1": "e087c220a5b788b8081d224e1c8f2d7771a32df6",
        "id": 637188
    },
    {
        "content": "def is_overlap(object1, object2):\n    \"\"\"Detects overlap between two rectangles.\n\n    Taking two objects, detects whether they overlap.\n    Assumes the presence of the following attributes:\n    - x\n    - y\n    - width\n    - height\n    \n    Tests whether the rectangles are above/below each other, or\n    left/right of each other, and if not, they are overlapping.\n\n    This is greedy - includes collisions where the rectangles are only just touching.\n    \"\"\"\n\n    if object1.x + object1.width < object2.x:\n        return False\n    if object2.x + object2.width < object1.x:\n        return False\n    if object1.y + object1.height < object2.y:\n        return False\n    if object2.y + object2.height < object1.y:\n        return False\n    return True",
        "sha1": "648ea314a76cdd00d757239afbe1200cfad880f4",
        "id": 624678
    },
    {
        "content": "def addslashes(s, escaped_chars=None):\n    \"\"\"Add slashes for given characters. Default is for ``\\`` and ``'``.\n\n    :param s: string\n    :param escaped_chars: list of characters to prefix with a slash ``\\``\n    :return: string with slashed characters\n    :rtype: str\n\n    :Example:\n        >>> addslashes(\"'\")\n        \"\\\\'\"\n    \"\"\"\n    if escaped_chars is None:\n        escaped_chars = [\"\\\\\", \"'\", ]\n\n    # l = [\"\\\\\", '\"', \"'\", \"\\0\", ]\n    for i in escaped_chars:\n        if i in s:\n            s = s.replace(i, '\\\\' + i)\n    return s",
        "sha1": "37273fdfd4f55a9a35284a51543d1c13eb7f459b",
        "id": 329357
    },
    {
        "content": "def fib(n):\n    \"\"\"\n    Takes a term `n` and returns `fib(n)`.\n\n    :param n: the term for the sequence\n    :return: fib(n)\n    \"\"\"\n    if n == 1:\n        return 1\n    elif n == 2:\n        return 2\n    else:\n        return fib(n - 1) + fib(n - 2)",
        "sha1": "1889e7cebd33a6507d87d1221d2214346bede3ae",
        "id": 103225
    },
    {
        "content": "def _parse_see_args(topic, data):\n    \"\"\"Parse the OwnTracks location parameters, into the format see expects.\"\"\"\n    parts = topic.split('/')\n    dev_id = '{}_{}'.format(parts[1], parts[2])\n    host_name = parts[1]\n    kwargs = {\n        'dev_id': dev_id,\n        'host_name': host_name,\n        'gps': (data['lat'], data['lon'])\n    }\n    if 'acc' in data:\n        kwargs['gps_accuracy'] = data['acc']\n    if 'batt' in data:\n        kwargs['battery'] = data['batt']\n    return dev_id, kwargs",
        "sha1": "8286952baa5cec02235cc8f2bbe0f95eb5496adb",
        "id": 596379
    },
    {
        "content": "def _truncate_segments(segments, max_num_tokens, rng):\n  \"\"\"\n  Truncate sequence pair according to original BERT implementation:\n  https://github.com/google-research/bert/blob/master/create_pretraining_data.py#L391\n  \"\"\"\n  while True:\n    if sum(len(s) for s in segments)<=max_num_tokens:\n      break\n\n    segments = sorted(segments, key=lambda s:len(s), reverse=True)\n    trunc_tokens = segments[0]\n\n    assert len(trunc_tokens) >= 1\n\n    if rng.random() < 0.5:\n      trunc_tokens.pop(0)\n    else:\n      trunc_tokens.pop()\n  return segments",
        "sha1": "2ac48c1726cc27b3c6840f89d98220e944c7154c",
        "id": 217400
    },
    {
        "content": "def get_bin(x, n=0):\n    \"\"\"Binary representation of x, padded to length n\"\"\"\n    return format(x, 'b').zfill(n)",
        "sha1": "bc381e08efeff308b663589c5d661975774f892f",
        "id": 364630
    },
    {
        "content": "import logging\nimport ftplib\n\n\ndef ftp_check_directory(ftp_connection, path):\n    \"\"\"\n    Following convention with the rest of the code,\n    return 0 if it is a directory, 1 if it is not or failed to do the check\n    \"\"\"\n    response = ftp_connection.pwd()\n    if response == '':\n        return 1\n    original_directory = response\n\n    # We are NOT scp, so we won't create a file when filename is not\n    # specified (mirrors input behaviour)\n    try:\n        ftp_connection.cwd(path)\n        logging.error(\n            'Path \"%s\" at \"%s\" already exists and is a folder. \\\n            Please specify a target filename and retry',\n            path, ftp_connection.host)\n        is_directory = True\n    except ftplib.error_perm:\n        is_directory = False\n    except (ftplib.error_reply, ftplib.error_temp):\n        logging.exception('Could not check if path \"%s\" in \"%s\" is directory',\n                          path, ftp_connection.host)\n        return 1\n    try:\n        ftp_connection.cwd(original_directory)\n    except (ftplib.error_reply, ftplib.error_perm, ftplib.error_temp):\n        logging.exception(\n            'Error when checking if \"%s\" in \"%s\" was a directory',\n            path, ftp_connection.host)\n        return 1\n\n    return 0 if is_directory else 1",
        "sha1": "fba5ff58c84cf78d8c0c1fd72adb9c581d9888c2",
        "id": 217275
    },
    {
        "content": "def _valid_entries_type(_entries):\n    \"\"\"This function checks whether or not the ``archive_entries`` argument is a valid type.\n\n    .. versionadded:: 2.7.0\n\n    :param _entries: The ``archive_entries`` value from the parent function\n    :returns: Boolean value indicating whether the value is a ``dict``, ``tuple``, ``list`` or ``set``\n    \"\"\"\n    return any((isinstance(_entries, dict), isinstance(_entries, tuple),\n                isinstance(_entries, list), isinstance(_entries, set)))",
        "sha1": "af4ee79d93b4f661ad71fc87866aae50f688d1b1",
        "id": 337298
    },
    {
        "content": "def fliplr_joints(joints_3d, joints_3d_visible, img_width, flip_pairs):\n    \"\"\"Flip human joints horizontally.\n\n    Note:\n        num_keypoints: K\n\n    Args:\n        joints_3d (np.ndarray([K, 3])): Coordinates of keypoints.\n        joints_3d_visible (np.ndarray([K, 1])): Visibility of keypoints.\n        img_width (int): Image width.\n        flip_pairs (list[tuple()]): Pairs of keypoints which are mirrored\n            (for example, left ear -- right ear).\n\n    Returns:\n        tuple: Flipped human joints.\n\n        - joints_3d_flipped (np.ndarray([K, 3])): Flipped joints.\n        - joints_3d_visible_flipped (np.ndarray([K, 1])): Joint visibility.\n    \"\"\"\n\n    assert len(joints_3d) == len(joints_3d_visible)\n    assert img_width > 0\n\n    joints_3d_flipped = joints_3d.copy()\n    joints_3d_visible_flipped = joints_3d_visible.copy()\n\n    # Swap left-right parts\n    for left, right in flip_pairs:\n        joints_3d_flipped[left, :] = joints_3d[right, :]\n        joints_3d_flipped[right, :] = joints_3d[left, :]\n\n        joints_3d_visible_flipped[left, :] = joints_3d_visible[right, :]\n        joints_3d_visible_flipped[right, :] = joints_3d_visible[left, :]\n\n    # Flip horizontally\n    joints_3d_flipped[:, 0] = img_width - 1 - joints_3d_flipped[:, 0]\n    joints_3d_flipped = joints_3d_flipped * joints_3d_visible_flipped\n\n    return joints_3d_flipped, joints_3d_visible_flipped",
        "sha1": "c9c4586fd8aa26bd5cd88aaf6f052192b8ef43a9",
        "id": 196712
    },
    {
        "content": "def ComputeQ(node_weights):\n    \"\"\" Computes the value of Q (sum of node weights) given the node weights of a graph. \"\"\"\n    return node_weights.sum()",
        "sha1": "05672096e41861f2ab9f240b733bc65eb78ea2f6",
        "id": 683138
    },
    {
        "content": "def get_dummy_stratas(metadatas: dict) -> dict:\n    \"\"\"\n    Create a dummy, one-factor 'no_stratification' categorical\n    metadata variables to stratify on for each metadata table.\n\n    Parameters\n    ----------\n    metadatas : dict\n        Key     = Metadata file path.\n        Value   = Metadata table.\n\n    Returns\n    -------\n    stratas : dict\n        Key     = Metadata file path.\n        Value   = List of one dummy 'no_stratification' variable to stratify on.\n    \"\"\"\n    stratas = {}\n    for md_fp, md in metadatas.items():\n        md['no_stratification'] = 'no_stratification'\n        stratas[md_fp] = ['no_stratification']\n    return stratas",
        "sha1": "1500cc0f3078e8796debe2876872cb2e3dc3882e",
        "id": 236162
    },
    {
        "content": "import base64\n\n\ndef protect(match):\n    \"\"\"\n    Function for re.sub() that encodes the first match as base64 to prevent any further\n    minification from taking place.  Used, for example, to protect multi-line string literals.\n    \"\"\"\n    return (b'\\x01\\x02' + base64.b64encode(match.group(1).encode()) + b'\\x01\\x03').decode()",
        "sha1": "3a310067df81b06c1e62efb70d3fbbcf51f7c393",
        "id": 501469
    },
    {
        "content": "def list_to_string_with_comma(thing):\n    \"\"\"Input a list, returns every item in the list as a string with commas in between\"\"\"\n    string = \"\"\n    for item in thing:\n        string += str(item) + ','\n    return string[:-1]",
        "sha1": "a7fff928137c3f7041b030c1fe4dd099f0ff8ca2",
        "id": 14743
    },
    {
        "content": "import configparser, os\nfrom ast import literal_eval\n\n\ndef get_custom_parameters(file):\n\t\"\"\"\n\tParses a custom parameter files\n\t\"\"\"\n    \n\tconffile = configparser.ConfigParser(inline_comment_prefixes =\"#\")\n\tconffile.read(file)\n\t\n\targs = {}\n\tfor section in conffile.sections():\n\t\tfor param in conffile[section]:\n\t\t\t# literal_eval does not like strings, if crashes: set as (default) string\n\t\t\ttry: args[param] = literal_eval(conffile[section][param])\n\t\t\texcept: args[param] = conffile[section][param]\n\n\treturn args",
        "sha1": "73e958db0b0f7833cb5823027b0d95c04dcce767",
        "id": 222501
    },
    {
        "content": "import re\n\n\ndef capitalize(word):\n    \"\"\"Only capitalize the first letter of a word, even when written in\n    CamlCase.\n\n    Args:\n        word (str): Input string.\n\n    Returns:\n        str: Input string with first letter capitalized.\n    \"\"\"\n    return re.sub('([a-zA-Z])', lambda x: x.groups()[0].upper(), word, 1)",
        "sha1": "4f254696e00c24a85a20ea74fc66a32fceb541c6",
        "id": 701723
    },
    {
        "content": "def convert_tags_dicts_to_tags_list(tags_dicts):\n    \"\"\"Convert dicts of the form {key: value} to a list like [{\"Key\": key, \"Value\": value}].\"\"\"\n    tags_list = []\n    for tags_dict in tags_dicts:\n        tags_list.extend([{\"Key\": key, \"Value\": value} for key, value in tags_dict.items()])\n    return tags_list",
        "sha1": "eccce40a8b8203e36854a85fe5adfb4867a302b2",
        "id": 455120
    },
    {
        "content": "def read_all_commits(filename):\n    \"\"\"Read all commits from the given GIT log file.\"\"\"\n    commits = []\n    with open(filename) as fin:\n        for line in fin:\n            splitted = line.strip().split(\" \", 1)\n            commits.append(splitted)\n    commits.reverse()\n    return commits",
        "sha1": "fa56a84b0d5f6684fbcf362299b773d1d3939fa0",
        "id": 648392
    },
    {
        "content": "def flatten_dict(nested: dict) -> dict:\n    \"\"\"Take a nested dictionary and flatten it. For example:\n    {'a': {'b': 'c'}} will be flattened to {'a_b': c}\n    Args:\n        nested: a dictionary to be flattened\n    Returns:\n        Dict. flattened version of the original dictionary\n\n    \"\"\"\n    ans = {}\n    for key, val in nested.items():\n\n        # if val is a dict, unflatten val, recursively\n        if isinstance(val, dict):\n            flattened = flatten_dict(val)\n\n            for subkey, subval in flattened.items():\n                flattened_key = f\"{key}_{subkey}\"\n                ans[flattened_key] = subval\n        else:\n            ans[key] = val\n\n    return ans",
        "sha1": "1c3e37ed3aad9838c20a02e7757e3fe47d4e4513",
        "id": 73899
    },
    {
        "content": "import calendar\n\n\ndef to_unix(date):\n    \"\"\"Converts a datetime object to unixtime\"\"\"\n\n    return calendar.timegm(date.utctimetuple())",
        "sha1": "1809a60e23b2768e632839c8c1fd22bfdd0d2be2",
        "id": 519376
    },
    {
        "content": "def _scatter_element_add_torch(tensor, index, value):\n    \"\"\"In-place addition of a multidimensional value over various\n    indices of a tensor. Note that Torch only supports index assignments\n    on non-leaf nodes; if the node is a leaf, we must clone it first.\"\"\"\n    if tensor.is_leaf:\n        tensor = tensor.clone()\n    tensor[tuple(index)] += value\n    return tensor",
        "sha1": "f036fee603b2b268e617342e75c5649152acfab3",
        "id": 321154
    },
    {
        "content": "def left_strip(elements, element_to_strip):\n    \"\"\"Remove preceeding center color occurences.\n\n    Slice the list from the first non center color occurance to the\n    end of the list.\n\n    Args:\n        elements (List[str]): The unstriped list of elements.\n        element_to_strip (str): The element to strip from the list.\n\n    Returns:\n        List[str]: The list left striped of the element.\n\n    Example:\n        >>> left_strip(['1', '2', '2', '3'], '1')\n        ['2', '2', '3']\n        >>> left_strip(['1', '2', '2', '3'], '2')\n        ['1', 2', '2', '3']\n    \"\"\"\n    if elements and elements[0] == element_to_strip:\n        for elem in elements:\n            if elem != element_to_strip:\n                return elements[elements.index(elem):]\n\n    return elements",
        "sha1": "b455f51b376c9079cc18c9f481463acd9ca58cda",
        "id": 506349
    },
    {
        "content": "import pickle\n\n\ndef loads(value):\n    \"\"\"\n    Deserialize value using ``ujson.loads``.\n\n    :param value: str\n    :returns: output of ``json.loads``.\n    \"\"\"\n    if value is None:\n        return None\n    return pickle.loads(value)",
        "sha1": "12d9944dbadf5ab1a5f2f06e72ae59de55cf9319",
        "id": 580555
    },
    {
        "content": "import re\n\n\ndef clean_str(text: str) -> str:\n    \"\"\"\n    Args:\n        text: A unicode string\n    Returns:\n        str: lowercased, sans punctuation, non-English letters\n    \"\"\"\n    RE_PREPROCESS = r'\\W+|\\d+'\n    text = re.sub(\n        RE_PREPROCESS,\n        ' ',\n        text.lower()\n    )\n    text = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", text)\n    text = re.sub(r\"\\'s\", \" \\'s\", text)\n    text = re.sub(r\"\\'ve\", \" \\'ve\", text)\n    text = re.sub(r\"n\\'t\", \" n\\'t\", text)\n    text = re.sub(r\"\\'re\", \" \\'re\", text)\n    text = re.sub(r\"\\'d\", \" \\'d\", text)\n    text = re.sub(r\"\\'ll\", \" \\'ll\", text)\n    text = re.sub(r\"\\s{2,}\", \" \", text)\n    return text",
        "sha1": "d716c66d77ba73335f19d345879fa59f42277774",
        "id": 674769
    },
    {
        "content": "def is_sublist(smallList, bigList):\n    \"\"\"\n    Checks if smallList is a sublist of bigList\n    \"\"\"\n    def n_slices(n, list_):\n        for i in range(len(list_)+1-n):\n            yield(list_[i:i+n])\n    for slice_ in n_slices(len(smallList), bigList):\n        if slice_ == smallList:\n            return True\n    return False",
        "sha1": "3dd582acef24c604a2ff8f1da52a2877c45a5a20",
        "id": 66148
    },
    {
        "content": "def printAngle(angle, shift, molecule, alchemicalTransformation):\n    \"\"\"Generate angle line\n\n    Parameters\n    ----------\n    angle : Angle Object\n        Angle Object\n    shift : int\n        Shift produced by structural dummy atoms\n    molecule : molecule object\n        Molecule object\n    alchemicalTransformation : bool\n        True if alchemical transformation\n\n    Returns\n    -------\n    angleLine : str\n        Angle line data\n    \"\"\"\n\n    \n    k0 = angle.K0*8.3680\n    angle0 = angle.angle0\n    ftype = 1\n\n    line = ''\n\n    atomAangle = molecule.atoms[angle.atomA.serialOriginal -1].serial-shift\n    atomBangle = molecule.atoms[angle.atomB.serialOriginal -1].serial-shift\n    atomCangle = molecule.atoms[angle.atomC.serialOriginal -1].serial-shift\n\n\n    if alchemicalTransformation: \n\n        k0_B = angle.K0_B*8.3680\n        angle0_B = angle.angle0_B\n\n        line = '%5d%5d%5d%5d    %10.3f %10.3f%11.3f%11.3f\\n' % (atomAangle, atomBangle, \n            atomCangle, ftype, angle0, k0, angle0_B, k0_B)\n\n    else: \n        \n        line = '%5d%5d%5d%5d    %10.3f %10.3f\\n' % (atomAangle, atomBangle, \n            atomCangle, ftype, angle0, k0)\n\n    return line",
        "sha1": "0e4d20acce1a7f8abbd2efee07d319df2955a814",
        "id": 632252
    },
    {
        "content": "import pytz\n\n\ndef parse_timezone(tz):\n  \"\"\"\n  Parse a timezone description into a tzinfo object\n  >>> parse_timezone(\"America/Los Angeles\")\n  <DstTzInfo 'America/Los_Angeles' PST-1 day, 16:00:00 STD>\n  >>> parse_timezone(\"America/Los_Angeles\")\n  <DstTzInfo 'America/Los_Angeles' PST-1 day, 16:00:00 STD>\n  \"\"\"\n  return pytz.timezone(tz.replace(\" \",\"_\"))",
        "sha1": "7fb4f4f6506afb3f2057fae5020cb904f88e16a6",
        "id": 106920
    },
    {
        "content": "def boundary(graph, subgraph):\n    \"\"\"\n    Get the nodes in the subgraph that have neighbors in the graph but not in the subgraph.\n\n    This is useful for conditionally styling nodes at the boundary of a subgraph. For example:\n\n    ::\n\n        boundary = nxv.boundary(graph, subgraph)\n        style = nxv.Style(node=lambda u, d: {\n            'style': 'dashed' if u in boundary else 'solid',\n        })\n        nxv.render(subgraph, style)\n\n\n    :param graph: A graph.\n    :param subgraph: A subgraph of the graph.\n    :return: The nodes in the subgraph that have neighbors in the graph but not in the subgraph.\n    \"\"\"\n    if not all(graph.has_node(node) for node in subgraph.nodes()):\n        if all(subgraph.has_node(node) for node in graph.nodes()):\n            raise ValueError(\n                \"The 'graph' argument is a proper subgraph of the 'subgraph' argument. This is likely \"\n                \"because the arguments to boundary were passed in the wrong order.\"\n            )\n        else:\n            raise ValueError(\"The subgraph contains nodes not in the graph.\")\n    return {\n        node for node in subgraph.nodes() if subgraph.degree(node) < graph.degree(node)\n    }",
        "sha1": "011c30e355a208d73e0ccb5c3e1ccd91b41b4e74",
        "id": 612126
    },
    {
        "content": "def to_bits(k):\n    \"\"\"Return a generator that returns the bits of k, starting from the\n    least significant bit, using True for 1s, and False for 0s.\n    \"\"\"\n    k_binary = bin(k)[2:]\n    return (bit == '1' for bit in k_binary[::-1])",
        "sha1": "d4c739b5945be986c11b3a52a86a5e1dbaa85423",
        "id": 481461
    },
    {
        "content": "def move_note(inst, rule):\n    \"\"\"\n    Moves individual notes from a src to a dest.\n    :param inst:\n    :param rule:\n    :return:\n    \"\"\"\n    for note_rule in rule['note_rules']:\n        src_note = note_rule['old']\n        dest_note = note_rule['new']\n\n        for note in inst.notes:\n            if note.pitch == src_note:\n                note.pitch = dest_note\n\n    return inst",
        "sha1": "fcaa31cb77e7e9884a06ebfdcb5cda0a503291ff",
        "id": 138135
    },
    {
        "content": "import mimetypes\n\n\ndef _is_good_file_for_multiqc(fpath):\n    \"\"\"Returns False if the file is binary or image.\"\"\"\n    # Use mimetypes to exclude binary files where possible\n    (ftype, encoding) = mimetypes.guess_type(fpath)\n    if encoding is not None:\n        return False\n    if ftype is not None and ftype.startswith('image'):\n        return False\n    return True",
        "sha1": "08e925e49c3c5d3f3c23e25ac3760b79e22fb415",
        "id": 301431
    },
    {
        "content": "import itertools\n\n\ndef to_combinations(list_of_objs: list):\n    \"\"\"\n    Create array of all combinations of a list of length n of the shape (1, 2, ..., n)\n    :param list_of_objs:\n    :return: Combinations\n    \"\"\"\n    combinations = []\n\n    for i in range(2, len(list_of_objs)):\n        combinations.extend(itertools.combinations(list_of_objs, i))\n\n    return combinations",
        "sha1": "066369efbe6543da4f553ff937d4451a5480a8ee",
        "id": 687321
    },
    {
        "content": "def prepare_data_for_maintability_index(maintainability_index):\n    \"\"\"Prepare data (values, labels, colors) for the maintainability index chart.\"\"\"\n    filtered_index = {k: v for k, v in maintainability_index.items()\n                      if v > 0 and k != \"status\"}\n\n    labels = sorted(filtered_index.keys())\n\n    fractions = [filtered_index[key] for key in labels]\n\n    colors = ('#60a060', 'yellow', 'red')\n\n    return labels, fractions, colors",
        "sha1": "329a5264dda9dbe7d342ae5fd5205b30833c0eb4",
        "id": 318839
    },
    {
        "content": "import torch\n\n\ndef conv(input, weight):\n    \"\"\"\n    Returns the convolution of input and weight tensors,\n          where input contains sequential data.\n          The convolution is along the sequence axis.\n          input is of size [batchSize, inputDim, seqLength]\n    \"\"\"\n    output = torch.nn.functional.conv1d(input=input, weight=weight)\n    return output",
        "sha1": "e213be11c423ff63a1ebffda55331298fcf53443",
        "id": 709203
    },
    {
        "content": "import torch\n\n\ndef gaussian2D(radius, sigma=1, dtype=torch.float32, device='cpu'):\n    \"\"\"Generate 2D gaussian kernel.\n\n    Args:\n        radius (int): Radius of gaussian kernel.\n        sigma (int): Sigma of gaussian function. Default: 1.\n        dtype (torch.dtype): Dtype of gaussian tensor. Default: torch.float32.\n        device (str): Device of gaussian tensor. Default: 'cpu'.\n\n    Returns:\n        h (Tensor): Gaussian kernel with a\n            ``(2 * radius + 1) * (2 * radius + 1)`` shape.\n    \"\"\"\n    x = torch.arange(\n        -radius, radius + 1, dtype=dtype, device=device).view(1, -1)\n    y = torch.arange(\n        -radius, radius + 1, dtype=dtype, device=device).view(-1, 1)\n\n    h = (-(x * x + y * y) / (2 * sigma * sigma)).exp()\n\n    h[h < torch.finfo(h.dtype).eps * h.max()] = 0\n    return h",
        "sha1": "a419844b34a6dcad9bca34de4b6b4e49c6805633",
        "id": 332068
    },
    {
        "content": "from typing import OrderedDict\n\n\ndef _groups_fail_table(groups):\n    \"\"\"Returns a dict associating each element to the groups it can fail.\n    This function is used both in ``plot_elements_in_group`` and\n    ``find_logical_saboteurs``.\n\n    Parameters\n    ----------\n    groups\n      Ordered dict of the form {group_name: [elements in groups]}.\n\n    Returns\n    -------\n    fail_table\n      A dict {element_name: list[True False False ...]} where list[i] is True\n      if the element is part of the i-th group in the provided ordered\n      dictionary ``groups``.\n    \"\"\"\n    groups = list(groups.values())\n    all_elements = []\n    for group in groups:\n        for element in group:\n            if element not in all_elements:\n                all_elements.append(element)\n    return OrderedDict(\n        [\n            (element, [(element in group) for group in groups])\n            for element in all_elements\n        ]\n    )",
        "sha1": "2fda833cc84198186442208c1deb366b0fe7cb80",
        "id": 259397
    },
    {
        "content": "import time\n\n\ndef calculate_dt(start_time: float) -> float:\n    \"\"\"Calculate the delta time\n\n    Args:\n        start_time (float): the time the game started\n\n    Returns:\n        float: the delta time\n    \"\"\"\n    now = time.time()\n    return now - start_time",
        "sha1": "46225fb5f1a1a08f2f453d5b5a4e4195528feb7e",
        "id": 456353
    },
    {
        "content": "def process_test_result(passed, info, is_verbose, exit):\n    \"\"\"\n    Process and print test results to the console.\n    \"\"\"\n    # if the environment does not contain necessary programs, exit early.\n    if passed is False and \"spellbook: command not found\" in info[\"stderr\"]:\n        print(f\"\\nMissing from environment:\\n\\t{info['stderr']}\")\n        return None\n    elif passed is False:\n        print(\"FAIL\")\n        if exit is True:\n            return None\n    else:\n        print(\"pass\")\n\n    if is_verbose is True:\n        print(f\"\\tcommand: {info['command']}\")\n        print(f\"\\telapsed time: {round(info['total_time'], 2)} s\")\n        if info[\"return_code\"] != 0:\n            print(f\"\\treturn code: {info['return_code']}\")\n        if info[\"stderr\"] != \"\":\n            print(f\"\\tstderr:\\n{info['stderr']}\")\n\n    return passed",
        "sha1": "9f059e6c4bbcc3a49babab07a9cdd5d17b6e975f",
        "id": 621331
    },
    {
        "content": "import hashlib\n\n\ndef get_md5_hash(filepath, blocksize=1024 * 1024):\n    \"\"\"Get MD5 hash of input file.\"\"\"\n    m5 = hashlib.md5()\n    with open(filepath, 'rb') as f:\n        chunk = f.read(blocksize)\n        while chunk:\n            m5.update(chunk)\n            chunk = f.read(blocksize)\n    return m5.hexdigest()",
        "sha1": "c68bfb6ccfa4d3b69b4d14220e197c46f07f9314",
        "id": 301639
    },
    {
        "content": "def update_c_tools_version(main, file):\n    \"\"\" Update Version import\n    \"\"\"\n    if main.replace_in_file(file, \"from conans.model.version import Version\", \"from conans.tools import Version\"):\n        main.output_result_update(title=\"Update 'Version' import\")\n        return True\n    return False",
        "sha1": "12592d0496273ac42c0e26a7c4481255013cb4e0",
        "id": 597536
    },
    {
        "content": "def run_inference_for_epoch(data_loaders, losses, periodic_interval_batches):\n    \"\"\"\n    runs the inference algorithm for an epoch\n    returns the values of all losses separately on supervised and unsupervised parts\n    \"\"\"\n    num_losses = len(losses)\n\n    # compute number of batches for an epoch\n    sup_batches = len(data_loaders[\"sup\"])\n    unsup_batches = len(data_loaders[\"unsup\"])\n    batches_per_epoch = sup_batches + unsup_batches\n\n    # initialize variables to store loss values\n    epoch_losses_sup = [0.] * num_losses\n    epoch_losses_unsup = [0.] * num_losses\n\n    # setup the iterators for training data loaders\n    sup_iter = iter(data_loaders[\"sup\"])\n    unsup_iter = iter(data_loaders[\"unsup\"])\n\n    # count the number of supervised batches seen in this epoch\n    ctr_sup = 0\n    for i in range(batches_per_epoch):\n\n        # whether this batch is supervised or not\n        is_supervised = (i % periodic_interval_batches == 1) and ctr_sup < sup_batches\n\n        # extract the corresponding batch\n        if is_supervised:\n            (xs, ys) = next(sup_iter)\n            ctr_sup += 1\n        else:\n            (xs, ys) = next(unsup_iter)\n\n        # run the inference for each loss with supervised or un-supervised\n        # data as arguments\n        for loss_id in range(num_losses):\n            if is_supervised:\n                new_loss = losses[loss_id].step(xs, ys)\n                epoch_losses_sup[loss_id] += new_loss\n            else:\n                new_loss = losses[loss_id].step(xs)\n                epoch_losses_unsup[loss_id] += new_loss\n\n    # return the values of all losses\n    return epoch_losses_sup, epoch_losses_unsup",
        "sha1": "0434a749b3e3cebacc771e6e929464cdc3267ae1",
        "id": 476529
    },
    {
        "content": "def count_occurances(comment, word):\n    \"\"\"\n    A helper function to get the number of words in a comment.\n    \"\"\"\n    comment = comment.replace('?', ' ')\n    comment = comment.replace('.', ' ')\n    comment = comment.replace('-', ' ')\n    comment = comment.replace('/', ' ')\n    a = comment.split(\" \")\n    count = 0\n    for i in range(len(a)):\n        if (word == a[i]):\n            count = count + 1\n    return count",
        "sha1": "b4c5c5e0a7792e29811937a1e77161196bd9856c",
        "id": 631990
    },
    {
        "content": "from typing import Counter\n\n\ndef recount_answers(group):\n    \"\"\"Count the number of positive answers of a group.\n\n    Only the questions to which everyone in a group answered\n    yes to count.\n    \"\"\"\n    people = len(group)\n    line = ''.join([n.strip('\\n') for n in group])\n    count = Counter(line)\n    number = 0\n    for n, item in count.items():\n        if item == people:\n            number += 1\n    return number",
        "sha1": "f5695311d36ae71d4dca2dc57a720f216e725701",
        "id": 222737
    },
    {
        "content": "import requests\n\n\ndef get_uniprot(accession, \n                uniprot_url='https://www.uniprot.org/uniprot/{0}.txt'):\n    \"\"\"\n    Retrieve Uniprot Annotation file from Uniprot ID e.g. Q15858\n    \"\"\"\n    try:\n        results = requests.get(\n            uniprot_url.format(accession.strip()), allow_redirects=True\n        )\n    except ValueError:\n        raise ValueError('no Uniprot results retrieved for {0}'.format(accession))\n    if results:\n        return results.content.decode(\"utf-8\")\n    else:\n        raise ValueError('no Uniprot results retrieved for {0}'.format(accession))",
        "sha1": "852d801c7110e14d0e33d188828bd4c839194589",
        "id": 25835
    },
    {
        "content": "def _line_exists(wavelengths, y, z, x0, xRes,fluxMin):\n    \"\"\"For a group of lines finds if the there is some change in flux greater\n    than some minimum at the same redshift with different initial wavelengths\n\n    **Parameters**\n\n    :wavelengths: (N) ndarray\n\n        array of initial wavelengths to check\n\n    :y: (N) ndarray\n\n        flux array to check\n\n    :x0: float\n\n        wavelength of the first value in y\n\n    :xRes: float\n\n        difference in wavelength between consecutive cells in flux array\n\n    :fluxMin: float\n\n        maximum flux to count as a line existing.\n\n    **Returns**\n\n    :flag: boolean\n\n        value indicating whether all lines exist. True if all lines exist\n    \"\"\"\n\n    #Iterate through initial wavelengths\n    for wl in wavelengths:\n        #Redshifted wavelength\n        redWl = (z+1)*wl\n\n        #Index of the redshifted wavelength\n        indexRedWl = (redWl-x0)/xRes\n\n        #Check to see if even in flux range\n        if indexRedWl > len(y):\n            return False\n\n        #Check if surpasses minimum absorption bound\n        if y[int(indexRedWl)]>fluxMin:\n            return False\n\n    return True",
        "sha1": "ce8d9ae6bdcd5d7863aab5d29653dbb968852ebf",
        "id": 357262
    },
    {
        "content": "def requirements_file_to_list(fn=\"requirements.txt\"):\n    \"\"\"read a requirements file and create a list that can be used in setup.\n\n    \"\"\"\n    with open(fn, 'r') as f:\n        return [x.rstrip() for x in list(f) if x and not x.startswith('#')]",
        "sha1": "7bf8f1d183685945e08d64bf2430279ad9effb52",
        "id": 648969
    },
    {
        "content": "def get_pieces(struct, splits):\n    \"\"\"Breaks up the structure at fixed positions, returns the pieces.\n    \n    struct: structure string in sprinzl format\n    splits: list or tuple of positions to split on\n\n    This is a helper function for the sprinzl_to_vienna function.\n    \n    struct = '...===...===.'\n    splits = [0,3,7,-1,13]\n    pieces -> ['...','===.','..===','.']\n    \"\"\"\n    pieces = []\n    for x in range(len(splits)-1):\n        pieces.append(struct[splits[x]:splits[x+1]])\n    return pieces",
        "sha1": "19533958c8c7c4901db95613c936fd6b934db871",
        "id": 388264
    },
    {
        "content": "def Beta(x, norm=1., beta=1., r=1.):\n    \"\"\"\n    Beta profile.\n\n    Parameters\n    ----------\n    x : number\n        The input number for calculation.\n    norm : number\n        The normalization at the center of the cluster.\n    beta : number\n        The beta parameter.\n    r : number\n        The core radius.\n\n    References\n    ----------\n    Cavaliere, A. & Fusco-Femiano, R. 1976, A&A, 500, 95\n    \"\"\"\n    result = norm * (1 + (x / r) ** 2) ** (0.5 - 3 * beta)\n    return result",
        "sha1": "a8b1af35f7d49889df227155e7e63a20a920e372",
        "id": 480165
    },
    {
        "content": "def extract_words(crossword, separator = \"#\"):\n    \"\"\"List words needed to colve the crossword\n    :param crossword: puzzle to extract words from\n    :param separator: character used as BLANK\n    :raise ValueError: if arguments have incorrect value\n    :raise TypeError: if arguments have incorrect type\n    :returns: list of words serted from shortest to longest alphabetically\n    \"\"\"\n    if (type(crossword) is not list) or (False in [type(row) is list for row in crossword]):\n        raise TypeError(\"Incorrect type of crossword\")\n    height = len(crossword)\n    width = len(crossword[0])\n    if (False in [len(row) == width for row in crossword]):\n        raise ValueError(\"Crossword is not matrix\")\n    if not (separator.isascii() and len(separator)==1):\n        raise TypeError(\"Separator has to be single character\")\n    \n    words = []\n    for i in range(height):\n        for j in range(width):\n            #vertical\n            if (0==i or separator==crossword[i-1][j]) and separator!=crossword[i][j]:\n                length = 1\n                while i + length < height and separator!=crossword[i+length][j]:\n                    length+=1\n                if 1 < length:\n                    words.append(\"\".join([crossword[i+x][j] for x in range(length)]))\n            #horizontal\n            if (0==j or separator==crossword[i][j-1]) and separator!=crossword[i][j]:\n                length = 1\n                while j + length < width and separator!=crossword[i][j+length]:\n                    length+=1\n                if 1 < length:\n                    words.append(\"\".join(crossword[i][j:(j+length)]))\n                    \n    words.sort(key = lambda word:(len(word),word))\n    \n    return words",
        "sha1": "c35f13548043286835c7af520d72233ffa94eb23",
        "id": 332595
    },
    {
        "content": "def create_action(type_: str, value: str):\n    \"\"\"Create an action for clients to use.\"\"\"\n    return {\"type\": type_, \"value\": value}",
        "sha1": "08c922d054dad8594ad4df77c432a29305ce60d1",
        "id": 577933
    },
    {
        "content": "def table_name_suffix(load_table_s3_prefix: str) -> str:\n    \"\"\"\n    Table name suffix should be '__ct' if the S3 prefix is from change tracking.\n\n    Parameters\n    ----------\n    load_table_s3_prefix : str\n        The load's prefix as determined by its S3 key.\n\n    Returns\n    -------\n    str\n        \"__ct\" or \"\" depending on the load's prefix\n    \"\"\"\n\n    return \"__ct\" if load_table_s3_prefix.endswith(\"__ct/\") else \"\"",
        "sha1": "16df14ef505ea2bdc42d396510bd49ffa2fe33cf",
        "id": 623151
    },
    {
        "content": "def pathToString(filepath):\n    \"\"\"\n    Coerces pathlib Path objects to a string (only python version 3.6+)\n    any other objects passed to this function will be returned as is.\n    This WILL NOT work with on Python 3.4, 3.5 since the __fspath__ dunder\n    method did not exist in those verisions, however psychopy does not support\n    these versions of python anyways.\n\n    :Parameters:\n\n    filepath : str or pathlib.Path\n        file system path that needs to be coerced into a string to\n        use by Psychopy's internals\n\n    :Returns:\n\n    filepath : str or same as input object\n        file system path coerced into a string type\n    \"\"\"\n    if hasattr(filepath, \"__fspath__\"):\n        return filepath.__fspath__()\n    return filepath",
        "sha1": "4cd41579cfa4369d5fad06521723bf970ff6d502",
        "id": 660829
    },
    {
        "content": "def get_score_list(lines, start_index=0):\n    \"\"\"\n    :param start_index: Optional index where to start the score checking.\n    :param lines: List of lines containing binary numbers.\n    :return: List of scores for each line index, score is +1 for '1' and -1 for '0'\n             e.g. line '101' gets score [1, -1, 1] and then line '001' changes it to [0, -2, 2].\n    \"\"\"\n    score_list = [0 for _ in range(len(lines[0]))]\n\n    for line in lines:\n        for i in range(start_index, len(line)):\n            if line[i] == '0':\n                score_list[i] -= 1\n            else:\n                score_list[i] += 1\n\n    return score_list",
        "sha1": "fc648f26a78e3614f024b257e54a6025f4341c76",
        "id": 656869
    },
    {
        "content": "import requests\nimport json\n\n\ndef post_request(url: str, headers: dict, path: str, data: dict)->dict:\n    \"\"\"\n    Perform an HTTP POST request\n    :param url: the url to POST to\n    :param headers:a dict containing the ZT auth header\n    :param path: the path to POST to\n    :param data: a dict containing data to post\n    :return: a dict converted from the returned JSON object\n\n    raises a requests.exceptions.HTTPError when the response code is not 200\n\n    \"\"\"\n    _url = url + path\n    r = requests.post(_url, data=json.dumps(data), headers=headers)\n    r.raise_for_status()\n    return r.json()",
        "sha1": "77a1d8293a951a9aa3e622594dedcb564049c08c",
        "id": 96604
    },
    {
        "content": "import hashlib\n\n\ndef sha256(path):\n    \"\"\"Return the sha256 digest of a file.\"\"\"\n    h = hashlib.sha256()\n    with open(path, 'rb') as f:\n        for chunk in iter(lambda: f.read(8192), b''):\n            if not chunk: break\n            h.update(chunk)\n    return h.hexdigest()",
        "sha1": "97ec85f000ff9db7dbdbffab78ede1dd3b12fc65",
        "id": 652648
    },
    {
        "content": "def read_file(file_path):\n    \"\"\"Reads in a Python requirements file.\n    Ignore empty lines, comments and editable requirements\n\n    :param str file_path: path to requirements file\n\n    :returns: mapping from a project to its pinned version\n    :rtype: dict\n\n    \"\"\"\n    data = {}\n    with open(file_path) as file_h:\n        for line in file_h:\n            line = line.strip()\n            if line and not line.startswith('#') and not line.startswith('-e'):\n                project, version = line.split('==')\n                if not version:\n                    raise ValueError(\"Unexpected syntax '{0}'\".format(line))\n                data[project] = version\n    return data",
        "sha1": "4a3cc35ad1a59f6fb569ffb01a30b729a6963164",
        "id": 55031
    },
    {
        "content": "def findExpression(lines, expr, startPos=0):\n    \"\"\" Searches an array of strings for an expression. returning the line idx that contains it\"\"\"\n    idx = -1\n    if expr == \"\":\n        return idx\n\n    for i in range(startPos, len(lines)):\n        if expr in lines[i]:\n            idx = i\n            break  # Line is found, bail from the loop.\n    return idx",
        "sha1": "175c7626b6ccb5c794fad5cfa83d856d0dc1099f",
        "id": 552614
    },
    {
        "content": "def extract_chembl(line) -> str:\n    \"\"\"\n    Extract smiles from chembl tsv\n\n    Returns:\n        SMILES string\n    \"\"\"\n    return line.split('\\t')[1]",
        "sha1": "51c68a350a5c10b723633ba1322d393f4b8eb74d",
        "id": 519085
    },
    {
        "content": "def locus_tag(ncrna):\n    \"\"\"\n    Get the locus tag of the gene, if present.\n    \"\"\"\n    return ncrna.get(\"gene\", {}).get(\"locusTag\", None)",
        "sha1": "76e39c3904e3d2621d253f8f59cd0e8640f18ef7",
        "id": 649553
    },
    {
        "content": "def get_subset(options, dataset, class_a, class_b):\n    \"\"\"\n    Returns a subset of the dataset that only contains the\n    classes (class_a) and (class_b)\n    \"\"\"\n    x_train, y_train = dataset\n    train_idx = (y_train == class_a) ^ (y_train == class_b)\n\n    subset_train_x = x_train[train_idx]\n    subset_train_y = y_train[train_idx]\n\n    # relabel to +/-1\n    subset_train_y[subset_train_y == class_a] = -1\n    subset_train_y[subset_train_y == class_b] = 1\n\n    return subset_train_x, subset_train_y",
        "sha1": "0081af880de54aded079edb46207ba46f94e0ea4",
        "id": 45955
    },
    {
        "content": "from pathlib import Path\n\n\ndef _get_config_path() -> Path:\n    \"\"\"\n    Get '.fetchmerc' configuration file path\n    \"\"\"\n    home = Path.home()\n\n    return home / '.fetchmerc'",
        "sha1": "e02a59f78b2e8a27b4e8635aa1faaf8534f9e1bd",
        "id": 597305
    },
    {
        "content": "def exists(var):\n    \"\"\"\n    Return a boolean value that indicate if a variable exists or not.\n\n    Parameters\n    ----------\n    var\n        Variable to test\n\n    Returns\n    -------\n    bool\n        Boolean that indicate the existance or not of the variable\n    \"\"\"\n    try:\n        var\n    except NameError:\n        return False\n    else:\n        return True",
        "sha1": "8d54378096746e808380a9e2d39d9133242e36c1",
        "id": 376408
    },
    {
        "content": "def head_of_list(x):\n    \"\"\"Takes a list, returns the first item in that list.\n    If x is empty, return None\n\n    >>> head_of_list([1, 2, 3, 4])\n    1\n    >>> head_of_list([]) is None\n    True\n    \"\"\" \n\n    return x[0] if x else None",
        "sha1": "63939c21cac53eb756565716518b13d1c6224d9b",
        "id": 111215
    },
    {
        "content": "import unicodedata\n\n\ndef string_width(string):\n    \"\"\"Measure rendering width of string.\n       Count ZENKAKU-character as 2-point and non ZENKAKU-character as 1-point\n    \"\"\"\n    widthmap = {'Na': 1, 'N': 1, 'H': 1, 'W': 2, 'F': 2, 'A': 2}\n    return sum(widthmap[unicodedata.east_asian_width(c)] for c in string)",
        "sha1": "89e15d853f3e5caf5961f1635104f7647203bfb2",
        "id": 648766
    },
    {
        "content": "def parse_value(s):\n  \"\"\"Parse a given string to a boolean, integer or float.\"\"\"\n  if s.lower() == 'true':\n    return True\n  elif s.lower() == 'false':\n    return False\n  elif s.lower() == 'none':\n    return None\n\n  try:\n    return int(s)\n  except:  # pylint: disable=bare-except\n    pass\n\n  try:\n    return float(s)\n  except:  # pylint: disable=bare-except\n    return s",
        "sha1": "08c1132169bb1dd52ce5421b938c108c71c0449e",
        "id": 505003
    },
    {
        "content": "from typing import Dict\nfrom typing import List\n\n\ndef get_in(d: Dict, keys: List, default=None):\n    \"\"\"Returns the value in a nested dict, where keys is a list of keys.\n\n    >>> get_in({\"a\": {\"b\": 1}}, [\"a\", \"b\"])\n    1\n    >>> get_in({\"a\": [0, 1, 2]}, [\"a\", 1])\n    1\n    >>> get_in({\"a\": {\"b\": 1}}, [\"a\", \"x\"], \"not found\")\n    'not found'\n\n    \"\"\"\n    if not keys:\n        return d\n    try:\n        return get_in(d[keys[0]], keys[1:], default)\n    except (KeyError, IndexError):\n        return default",
        "sha1": "bd60966ee65c196c5554bd915b6c274a40d266ae",
        "id": 434338
    },
    {
        "content": "def get_simulator_name(csv_file):\n    \"\"\"Convert log file name to the simulator name\n    :param csv_file: Transaction log file name\n    :return: Simulator name\n    \"\"\"\n    elements = csv_file.split(\"_\")\n    return \"_\".join(elements[:4])",
        "sha1": "643dca323bc4773bb208b770ee9575c51eba31e0",
        "id": 257218
    },
    {
        "content": "def _dataset_fields(geno):\n    \"\"\"\n    return the dataset metadata fields created for dataset definition geno\n    \"\"\"\n    return {'title': geno['title'], 'notes': geno.get('notes', '')}",
        "sha1": "1a0bc5b1be143628ab58984fa84c014ab2a91b33",
        "id": 305002
    },
    {
        "content": "def _permute(vector, permutation):\n    \"\"\"Permute vector according to given ordering of indices.\n\n    Args:\n        vector: 1-D array-like object\n        permutation: any permutation of indices 0...len(vector)\n    Returns:\n        permuted vector\n    Examples:\n        _permute([4, 5, 6], [0, 2, 1]) == [4, 6, 5]\n    \"\"\"\n    return [vector[i] for i in permutation]",
        "sha1": "f4eac9f530e2dfcaaac09b50d9294efc19501dfd",
        "id": 239336
    },
    {
        "content": "def getRelativePositionTupleToAncestor(win, ancestor):\n    \"\"\"\n    Calculates relative pixel position of win to ancestor where ancestor\n    is either parent, grandparent, grandgrandparent ... or None for\n    absolute position.\n    Returns a tuple with the position.\n    \"\"\"\n    resultx = 0\n    resulty = 0\n    while win is not None and win is not ancestor:\n        x, y = win.GetPositionTuple()\n        resultx += x\n        resulty += y\n        \n        win = win.GetParent()\n\n    return (resultx, resulty)",
        "sha1": "bac9802515095f6def8f2d86c7151f7bf62cfdc3",
        "id": 511852
    },
    {
        "content": "import torch\n\n\ndef compute_pairwise_distances(x, y):\n    \"\"\" Computes the squared pairwise Euclidean distances between x and y.\n    Args:\n      x: a tensor of shape [num_x_samples, num_features]\n      y: a tensor of shape [num_y_samples, num_features]\n    Returns:\n      a distance matrix of dimensions [num_x_samples, num_y_samples]\n    Raise:\n      ValueError: if the inputs do no matched the specified dimensions.\n    \"\"\"\n\n    if not len(x.size()) == len(y.size()) == 2:\n        raise ValueError('Both inputs should be matrices.')\n    if x.size()[1] != y.size()[1]:\n        raise ValueError('The number of features should be the same.')\n\n    # By making the `inner' dimensions of the two matrices equal to 1 using\n    # broadcasting then we are essentially substracting every pair of rows\n    # of x and y.\n    norm = lambda x: torch.sum(x * x, 1)\n    return norm(x.unsqueeze(2) - y.t())",
        "sha1": "ed1716f19e713959a96468acf8eac7bae89df50a",
        "id": 247912
    },
    {
        "content": "def anagrams(passphrase: str) -> bool:\n    \"\"\"Check if there are anagrams in passphrase.\"\"\"\n    words = passphrase.split()\n    return len(words) == len({\"\".join(sorted(word)) for word in words})",
        "sha1": "47d7b57f19a5348e22154079f4688b58b7cce093",
        "id": 345156
    },
    {
        "content": "def issequence(obj):\n    \"\"\" True if given object is a list or a tuple. \"\"\"\n    return isinstance(obj, (list, tuple))",
        "sha1": "bb7b2f62d4648f979c0497d47fe8cc41595675a2",
        "id": 320033
    },
    {
        "content": "import re\n\n\ndef is_image_file(file_name):\n    \"\"\"\n    Checks for ``.png``, ``.jpg``, ``.jgpeg``, ``.gif`` file extensions.\n\n    Arguments:\n        file_name (str): Image file name.\n\n    Returns:\n        bool: Returns ``True`` if the file name has image extension. ``False`` otherwise.\n    \"\"\"\n    regex = re.compile(r\"(.+)\\.(png|jpg|jpeg|gif)$\", re.IGNORECASE)\n    result = regex.match(file_name)\n    return True if result else False",
        "sha1": "82f62598eae1e5c5e3910bad9968f6d7ad839545",
        "id": 281825
    },
    {
        "content": "import pathlib\nfrom typing import Optional\nfrom typing import List\n\n\ndef files_to_list(file_path: pathlib.Path, separator: Optional[str] = '|') -> List[pathlib.Path]:\n    \"\"\"Returns list of file path, which are listed in input file (one file path at a line).\n\n    Args:\n        file_path: Path to the input file.\n        separator: Each line from the input file will be split at this separator. The first (0 index) item will\n            be treated as an audio file path. If None, lines will note be split (the whole line is an audio file path)\n\n    Returns: list of file paths\n\n    \"\"\"\n    from_dir = file_path.parent\n    with file_path.open() as f:\n        if separator is not None:\n            file_lines = [line.split('|')[0].strip() for line in f.readlines()]\n        else:\n            file_lines = [line.strip() for line in f.readlines()]\n\n        file_paths = [from_dir / file_path for file_path in file_lines]\n\n    return file_paths",
        "sha1": "cc8ede8117c66d40ced0efe6aed42ea0df2a4575",
        "id": 113497
    },
    {
        "content": "def format_for_type(format, type):\n  \"\"\"\n  Returns the format appropriate to the given type\n\n  Parameters\n  ----------\n  format: str or dict\n    If it is str, just return it.\n    Dict should has the form { type : format_for_the_type } + { None : default_format }\n  \"\"\"\n  if isinstance(format, dict):\n     if type in format:\n        return format[type]\n     return format[None]\n  return format",
        "sha1": "b24eeaa01ef9bb38a5781822a2c6d124fed5a16e",
        "id": 532146
    },
    {
        "content": "from typing import Tuple\nfrom typing import Optional\nfrom typing import Set\nimport re\n\n\ndef _parse_requirement_for_extra(\n    requirement: str,\n) -> Tuple[Optional[str], Optional[Set[str]]]:\n    \"\"\"Given a requirement string, returns the requirement name and set of extras, if extras specified.\n    Else, returns (None, None)\n    \"\"\"\n\n    # https://www.python.org/dev/peps/pep-0508/#grammar\n    extras_pattern = re.compile(\n        r\"^\\s*([0-9A-Za-z][0-9A-Za-z_.\\-]*)\\s*\\[\\s*([0-9A-Za-z][0-9A-Za-z_.\\-]*(?:\\s*,\\s*[0-9A-Za-z][0-9A-Za-z_.\\-]*)*)\\s*\\]\"\n    )\n\n    matches = extras_pattern.match(requirement)\n    if matches:\n        return (\n            matches.group(1),\n            {extra.strip() for extra in matches.group(2).split(\",\")},\n        )\n\n    return None, None",
        "sha1": "7445ae28b695501ac0e8a637e59e4fca065d1f05",
        "id": 374542
    },
    {
        "content": "def cipher(text, shift, encrypt=True):\n    \"\"\"\n    Encrypt and decrypt letter text by using the Caesar cipher.\n\n    Parameters\n    ----------\n    text : string\n      A string only contains letters from alphabet\n    shift : int\n      A number that you wish to replace each original letter to the letter with that number of positions down the alphabet.\n    encrypt: boolean\n      A boolean value with true indicates encrypt the text and false indicates decrypt the text\n    \n    Returns\n    -------\n    string\n      The encrypt or decrypt letter text.\n\n    Examples\n    --------\n    >>> from cipher_yh3395 import cipher\n    >>> cipher('abc', 1)\n    'bcd'\n    >>> cipher('bcd', 1, False)\n    'abc\n    \"\"\"\n        \n    alphabet = 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ'\n    new_text = ''\n    for c in text:\n        index = alphabet.find(c)\n        if index == -1:\n            new_text += c\n        else:\n            new_index = index + shift if encrypt == True else index - shift\n            new_index %= len(alphabet)\n            new_text += alphabet[new_index:new_index+1]\n    return new_text",
        "sha1": "60cadd09a063af63cdd360ad62535161eab7d1bb",
        "id": 664270
    },
    {
        "content": "def date2str(dt, format_str=None):\n    \"\"\"\n    Convert list of datetime objects to legible strings\n\n    Parameters\n    ----------\n    dt : datetime.datetime\n        Single or list of datetime object(s)\n    format_str : string\n        Timestamp string formatting, default: '%Y-%m-%d %H:%M:%S.%f'. \n        See datetime.strftime documentation for timestamp string formatting\n\n    Returns\n    -------\n    time : string\n        Converted timestamps\n\n    \"\"\"\n    if format_str is None:\n        format_str = '%Y-%m-%d %H:%M:%S.%f'\n\n    if not isinstance(dt, list):\n        dt = [dt]\n\n    return [t.strftime(format_str) for t in dt]",
        "sha1": "aee6ed28d3a23c02ae05885543170de0854a7e4a",
        "id": 64572
    },
    {
        "content": "import pickle\n\n\ndef load_object(filename):\n    \"\"\"Load an object, e.g. a list of returns to be plotted.\"\"\"\n    with open(filename, 'rb') as fp:\n        return pickle.load(fp)",
        "sha1": "56e9875aed48ec121e9be2a3047c2c185839b30b",
        "id": 542923
    },
    {
        "content": "import hashlib\n\n\ndef sha1_hash(value):\n    \"\"\"Calculates the SHA1 has of a string\n\n            :arg:\n                value (str): String to be hashed\n\n            :return\n                (str): SHA1 hash\n        \"\"\"\n\n    # add salt to the value\n    salt = \"!@3`tHy:'hj6^&7m4qG9[6\"\n    salted_value = value + salt\n\n    # convert string to bytes\n    value = str.encode(salted_value)\n\n    # calculate a SHA1 hash\n    hash_object = hashlib.sha1(value)\n    hashed_value = hash_object.hexdigest()\n    return hashed_value",
        "sha1": "2c4226f0475eeec1baca84b3d844635e721e0741",
        "id": 594877
    },
    {
        "content": "def is_random(name):\n    \"\"\"\n    Determine if a user has a randomly generated display name.\n    \"\"\"\n    if len(name.split('_')) and name.startswith('user'):\n        return True\n    else:\n        return False",
        "sha1": "9cd2d35eabfcbb1c21ffb269f1976a2c17d8905c",
        "id": 570484
    },
    {
        "content": "import importlib\n\n\ndef _RelativeImport(module_relpath):\n  \"\"\"Returns a module imported by its path relative to this package.\"\"\"\n  return importlib.import_module('.' + module_relpath, package=__package__)",
        "sha1": "08f7ee554122f244bbb52363e1166c6c7336df2a",
        "id": 514178
    },
    {
        "content": "import re\n\n\ndef Extract_Numbers(string):\n    \"\"\"Function to extract the individual numbers form within a string \"\"\"\n    \n    List = [int(s) for s in re.findall(r'\\d+', string)]\n    # Pythonic language for obtaining the integers that may appear in a string\n    \n    return List\n    # Return the list of integers. Remianing local variables are wiped",
        "sha1": "c15b7374bc6f0655df503f461bc8af2477949bfc",
        "id": 646688
    },
    {
        "content": "def find_argument(inp):\n    \"\"\"\n    Find the start of the Arguments list.\n    \"\"\"\n\n    line = next(inp)\n    count = 1\n    while 'Arguments' not in line:\n        line = next(inp)\n\n        # We assume we're going to find this within two lines of the original\n        # location of the input\n        # - stop looking if we don't.\n        count += 1\n        if count > 2:\n            return False\n\n    return True",
        "sha1": "d789843de75db8b449437d9579b8a21f72427326",
        "id": 612006
    },
    {
        "content": "def _gen_perm_Numpy(order, mode):\n    \"\"\"\n    Generate the specified permutation by the given mode.\n\n    Parameters\n    ----------\n    order : int\n        the length of permutation\n    mode : int\n        the mode of specific permutation\n\n    Returns\n    -------\n    list\n        the axis order, according to Kolda's unfold\n\n    \"\"\"\n    tmp = list(range(order - 1, -1, -1))\n    tmp.remove(mode)\n    perm = [mode] + tmp\n    return perm",
        "sha1": "df2511a8b9278c3018808a259f202e42c8a5462f",
        "id": 690996
    },
    {
        "content": "def check_Nbyzan(opts, P):\n    \"\"\" \n        Check and get the number of Byzantine machines that\n        we are going to simulate \n\n        Parameters :\n        -----------\n        opts : str\n            Options passed by the command prompt\n\n        P : int\n            Total number of machines (nodes or workers).\n            1 coodinator ans the ramaining are workers\n\n        Return :\n        -------\n        n_byzantines : int (entire natural)\n            Number of byzantine machines that we\n            are going to simulate\n    \"\"\"\n\n    if len(opts) == 0:\n        n_byzantines = 0;\n        \n    n_byzantines = int(opts[0][1]);\n    \n    if n_byzantines < 0 or n_byzantines > P - 1:\n        raise ValueError(\"Number of byzantine must be an integer \"\n                         \"< number of workers or >= 0\");\n           \n    return n_byzantines;",
        "sha1": "aecbcaa8bd7febb59971d27fa81e7d1b678d0ae9",
        "id": 96477
    },
    {
        "content": "def join_base_url_and_query_string(base_url: str, query_string: str) -> str:\n    \"\"\"Joins a query string to a base URL.\n\n    Parameters\n    ----------\n    base_url: str\n        The URL to which the query string is to be attached to.\n    query_string: str\n        A valid query string.\n\n    Returns\n    -------\n    str\n        The base url with attached the query string.\n    \"\"\"\n    if base_url.endswith(\"/\"):\n        base_url = base_url[:-1]\n    return f\"{base_url}?{query_string}\"",
        "sha1": "c1c0dcd67fa32bc71134807ad66874aacd51c00d",
        "id": 658793
    },
    {
        "content": "def get_dense_memory_usage(matrix):\n    \"\"\"\n    Return the number of bytes need to store the equivalent dense\n    representation of a sparse matrix\n    \"\"\"\n    return matrix.dtype.itemsize * matrix.shape[0] * matrix.shape[1]",
        "sha1": "156ffee98de175ec64756f98dded7ec631dfbbea",
        "id": 329407
    },
    {
        "content": "import pickle\n\n\ndef decoder(code_4digit):\n    \"\"\"\n    Transform a 4-digit string code or a number into a filename (document_global)\n    Use dict from pickle file\n    Errors from dict search are controlled and displayed\n\n    :param code_4digit: string (4-letter) or number\n    :return: old filename string (None if any error occurs)\n    \"\"\"\n\n    with open('../data/recoder_dict_tradefocus.bak', 'rb') as file_open:\n        deco_dict = pickle.load(file_open)\n\n    old_filename = 'None'\n\n    try:\n\n        if type(code_4digit) == type('string'):\n            old_filename = deco_dict[code_4digit]\n            print(old_filename)\n        elif type(code_4digit) == type(4):\n            old_filename = deco_dict['{:04d}'.format(code_4digit)]\n            print(old_filename)\n        else:\n            print('Input type not recognized, please enter a 4-digit number or a 4-word string')\n\n    except Exception as e:\n        print('Error decoding!!! \\n\\t{}:{}'.format(type(e).__name__, str(e)))\n\n    return old_filename",
        "sha1": "0e336d9faf6bbadab1978ea27d5f25f7bea556a0",
        "id": 252804
    },
    {
        "content": "from typing import Dict\nfrom typing import List\nfrom typing import Set\nfrom typing import Tuple\n\n\ndef _validate_reply(\n    reply: Dict[str, List[str]], performatives_set: Set[str]\n) -> Tuple[bool, str]:\n    \"\"\"\n    Evaluate whether the reply structure in a protocol specification is valid.\n\n    :param reply: Reply structure of a dialogue.\n    :param performatives_set: set of all performatives in the dialogue.\n\n    :return: Boolean result, and associated message.\n    \"\"\"\n    performatives_set_2 = performatives_set.copy()\n\n    for performative in reply.keys():\n        if performative not in performatives_set_2:\n            return (\n                False,\n                \"Performative '{}' specified in \\\"reply\\\" is not defined in the protocol's speech-acts.\".format(\n                    performative,\n                ),\n            )\n        performatives_set_2.remove(performative)\n\n    if len(performatives_set_2) != 0:\n        return (\n            False,\n            \"No reply is provided for the following performatives: {}\".format(\n                performatives_set_2,\n            ),\n        )\n\n    return True, \"Reply structure is valid.\"",
        "sha1": "2243e60edd6497a7f699676be5fe765711db4134",
        "id": 34563
    },
    {
        "content": "def HtmlColorCode_to_strings(value):\n    \"\"\"\n    Convert a HTML color code\n    like '#FFCC00' or 'FFCC00'\n    to a list of strings ['FF', 'CC', '00']\n    \"\"\"\n    x = 1 * value.startswith('#')\n    return [value[x:x+2], value[x+2:x+4], value[x+4:x+6]]",
        "sha1": "2752720c8ef64b7ceb18cbcd3da97a80e2cd1e8c",
        "id": 464417
    },
    {
        "content": "def redirect_github_issue(match):\n    \"\"\" Custom routing rule to redirect /issue/NNN to the corresponding\n    issue on GitHub \"\"\"\n    return 'https://github.com/PlaidWeb/Publ/issues/' + match.group(1), True",
        "sha1": "8def5537bc013d3962d2a67fe10e228e6f3643b7",
        "id": 473916
    },
    {
        "content": "def get_ext(type):\n    \"\"\"Returns file extension\n\n    Args:\n        type (str): FILE_EXT from API response\n\n    Returns:\n        [str]: file extension string\n    \"\"\"\n    if type == \"MP3\":\n        return \"mp3\"\n    else:\n        return \"flac\"",
        "sha1": "a3fee4c6833cb80e08840cb71ea57e7c74f6badb",
        "id": 569427
    },
    {
        "content": "def replace_last(s, old, new, maxtimes=1):\n    \"\"\"Replace the last (n) occurence(s) of an expression in a string\"\"\"\n    tokens = s.rsplit(old, maxtimes)\n    return new.join(tokens)",
        "sha1": "dd5d006dec8f123898711d91b5ae704f76a75961",
        "id": 499109
    },
    {
        "content": "def dictfetchone(cursor):\n    \"\"\"convert SQL result to dict instead of a list.\"\"\"\n    columns = [col[0] for col in cursor.description]\n    result = [dict(zip(columns, row)) for row in cursor.fetchall()]\n    return result[0]",
        "sha1": "dce59d133dcaa913b63a9c537112be30109d1b26",
        "id": 443695
    },
    {
        "content": "from pathlib import Path\n\n\ndef ensure_extension(filename, extension):\n    \"\"\"\n        Ensures of the extension of a file\n\n        >>> ensure_extension('foo.pdf', 'pdf')\n        'foo.pdf'\n        >>> ensure_extension('bar.csv', 'pdf')\n        'bar.pdf'\n    \"\"\"\n    return str(Path(filename).with_suffix(f\".{extension}\"))",
        "sha1": "d76e896d320e1bcf6750b758379887cc70757c34",
        "id": 421960
    },
    {
        "content": "def floor_division(x, y):\n    \"\"\"\n    Compatible division which act the same behaviour in Python3 and Python2,\n    whose result will be a int value of floor(x / y) in Python3 and value of\n    (x / y) in Python2.\n\n    Args:\n        x(int|float) : The number to divide.\n        y(int|float) : The number to be divided\n\n    Returns:\n        division result of x // y\n    \"\"\"\n    return x // y",
        "sha1": "152376d1e2092186e505e0c42d7df0023eed42f7",
        "id": 492768
    },
    {
        "content": "import hashlib\n\n\ndef _generate_verifier(muuid, address):\n    \"\"\"Internal function for generating hash value from Maya UUID and address\n\n    Arguments:\n        muuid (str): Maya UUID string\n        address (str): Previous generated address id from node\n\n    Note:\n        Faster then uuid5.\n\n    \"\"\"\n    hasher = hashlib.sha1()\n    hasher.update(muuid + \":\" + address)\n    return hasher.hexdigest()",
        "sha1": "05f8d956badd318d9a362624855f8dd1c1ba04af",
        "id": 419550
    },
    {
        "content": "def validate_tags(data: list) -> bool:\n    \"\"\"\n    Validate input for the ``tags`` field.\n\n    * It must be a list\n    * Must be at least 3 characters\n    * May not end nor start with whitespace\n\n    Args:\n        data (list): The data to be validated.\n\n    Returns:\n        bool: Validation passed.\n\n    Raises:\n        ValueError: Validation failed.\n    \"\"\"\n    if not isinstance(data, list):\n        raise ValueError(f\"Not a list ({data})\")\n    for value in data:\n        if not isinstance(value, str):\n            raise ValueError(f\"All list entries must be str ({value})\")\n        if len(value) < 3:\n            raise ValueError(\"Must be at least three characters\")\n        if len(value) != len(value.strip()):\n            raise ValueError(\"May not start nor end with whitespace\")\n    return True",
        "sha1": "2788e8c2730efd190f0b9c644afd6d70eeda8105",
        "id": 278089
    },
    {
        "content": "from datetime import datetime\n\n\ndef get_datetime_now_formatted() -> str:\n    \"\"\" Gets a string of the current date and time\n    in format: Jul 11 2019: 1:28 PM\"\"\"\n    return datetime.now().strftime(\"%b %d %Y, %-I:%M %p\")",
        "sha1": "363e406805e2ed098aad7518cce3daf72fefff09",
        "id": 121455
    },
    {
        "content": "import math\n\n\ndef bearing(lat1: float, lon1: float, lat2: float, lon2: float) -> float:\n    \"\"\"Calculate bearing from lat1/lon2 to lat2/lon2\n\n    Arguments:\n        lat1 {float} -- Start latitude\n        lon1 {float} -- Start longitude\n        lat2 {float} -- End latitude\n        lon2 {float} -- End longitude\n\n    Returns:\n        float -- bearing in degrees\n    \"\"\"\n    rlat1 = math.radians(lat1)\n    rlat2 = math.radians(lat2)\n    rlon1 = math.radians(lon1)\n    rlon2 = math.radians(lon2)\n    dlon = math.radians(lon2-lon1)\n\n    b = math.atan2(math.sin(dlon)*math.cos(rlat2),math.cos(rlat1)*math.sin(rlat2)-math.sin(rlat1)*math.cos(rlat2)*math.cos(dlon)) # bearing calc\n    bd = math.degrees(b)\n    br,bn = divmod(bd+360,360) # the bearing remainder and final bearing\n\n    return bn",
        "sha1": "9148bc2726247cedf8a7de7279dac47316f2864f",
        "id": 703536
    },
    {
        "content": "def getIndex(header, item):\n\t\"\"\"Get index of variable\n\t\"\"\"\n\tfor i, elem in enumerate(header):\n\t\tif elem.lower() == item.lower():\n\t\t\treturn i\n\treturn None",
        "sha1": "c7ca274add641cb6efea4d507448eb78e1eec12b",
        "id": 58003
    },
    {
        "content": "def CSourceForArrayData(values, formatter, margin=4, width=80):\n  \"\"\"Turn an array of values into a C source array data definition.\n\n  Args:\n    values: Array of input values.\n    formatter: Formatting function, applied to each input value to get a\n      C-source description of the value.\n    margin: Left-side margin / indentation level.\n    width: Maximum line width.\n  Returns:\n    A string containing the data definition as a C source fragment.\n  \"\"\"\n  read_pos = 0\n  read_len = len(values)\n  write_pos = margin\n  line_start = ' ' * margin\n  # Account for the margin + one final comma.\n  max_width = width - margin - 1\n  out = ''\n  while read_pos < read_len:\n    out += line_start\n    write_pos = 0\n    comma = ''\n    while read_pos < read_len:\n      item = comma + formatter(values[read_pos])\n      if write_pos + len(item) > max_width:\n        break  # Too long, break line before this item.\n      out += item\n      read_pos += 1\n      write_pos += len(item)\n      comma = ','\n    if read_pos == read_len:\n      break\n    out += ',\\n'\n  return out",
        "sha1": "602c0cc6fab36647db2b6cddd5e51af920672f2f",
        "id": 641992
    },
    {
        "content": "import grp\n\n\ndef gid_to_name(gid):\n    \"\"\"\n    Find the group name associated with a group ID.\n\n    :param gid: The group ID (an integer).\n    :returns: The group name (a string) or :data:`None` if :func:`grp.getgrgid()`\n              fails to locate a group for the given ID.\n    \"\"\"\n    try:\n        return grp.getgrgid(gid).gr_name\n    except Exception:\n        return None",
        "sha1": "abde4a17fccd88add0392a4185f2db40753ccae0",
        "id": 36522
    },
    {
        "content": "def gene_sample_count(insertions, name='n_samples'):\n    \"\"\"Calculates the number of samples with insertions for each gene.\n\n    Parameters\n    ----------\n    insertions : pd.DataFrame\n        Insertion dataframe.\n    name : str\n        Name to use for the returned series.\n\n    Returns\n    -------\n    pd.Series\n        Gene sample counts for the given insertion set.\n    \"\"\"\n\n    count = insertions.groupby('gene_name')['sample'].nunique()\n    count.name = name\n    return count",
        "sha1": "44987a561394d2d454906fed07b6cb6471761d7e",
        "id": 374522
    },
    {
        "content": "import torch\n\n\ndef convert_2Djoints_to_gaussian_heatmaps_torch(joints2D,\n                                                img_wh,\n                                                std=4):\n    \"\"\"\n    :param joints2D: (B, N, 2) tensor - batch of 2D joints.\n    :param img_wh: int, dimensions of square heatmaps\n    :param std: standard deviation of gaussian blobs\n    :return heatmaps: (B, N, img_wh, img_wh) - batch of 2D joint heatmaps (channels first).\n    \"\"\"\n    device = joints2D.device\n\n    xx, yy = torch.meshgrid(torch.arange(img_wh, device=device),\n                            torch.arange(img_wh, device=device))\n    xx = xx[None, None, :, :].float()\n    yy = yy[None, None, :, :].float()\n\n    j2d_u = joints2D[:, :, 0, None, None]  # Horizontal coord (columns)\n    j2d_v = joints2D[:, :, 1, None, None]  # Vertical coord (rows)\n    heatmap = torch.exp(-(((xx - j2d_v) / std) ** 2) / 2 - (((yy - j2d_u) / std) ** 2) / 2)\n    return heatmap",
        "sha1": "26607acb1f756b840a8b970ade0ccdf01bf286e9",
        "id": 30587
    },
    {
        "content": "def application_name(config):\n    \"\"\"Returns the application_name from the configuration.\n\n    :param config: Configuration to extract the application_name from.\n    :type config: dict\n\n    :returns: The application_name from the configuration.\n    :rtype: str\n\n    \"\"\"\n\n    return config['application_name']",
        "sha1": "7897023f50e5438812f258696404db65b97fdd37",
        "id": 547049
    },
    {
        "content": "import base64\n\n\ndef _b64_encode(text):\n    \"\"\"Encode a string to base64. Unlike base64.b64encode,\n    input and output are utf-8 strings. \"\"\"\n    return base64.b64encode(text.encode('utf-8')).decode('utf-8')",
        "sha1": "2a7275da4812ec3c690a41bec89adb5cd0ed1291",
        "id": 184859
    },
    {
        "content": "def bottom_up_fibonacci(n: int, return_ith: bool = False) -> object:\n    \"\"\"Returns the nth fibonacci number if return_ith=False, else it returns a\n    list containing all the ith fibonacci numbers, for i=0, ... , n.\n\n    For example, suppose return_ith == True and n == 5, then this function\n    returns [0, 1, 1, 2, 3, 5]. If return_ith == False, it returns simply 5.\n\n    Note: indices start from 0 (not from 1).\n\n    This function uses a dynamic programing \"bottom up\" approach: we start by\n    finding the optimal solution to smaller sub-problems, and from there, we\n    build the optimal solution to the initial problem.\n\n    Time complexity: O(n).\"\"\"\n    if n == 0:\n        return n if not return_ith else [n]\n    if n == 1:\n        return n if not return_ith else [0, n]\n\n    fib = [0] * (n + 1)\n    fib[0] = 0\n    fib[1] = 1\n\n    for i in range(2, n + 1):\n        fib[i] = fib[i - 1] + fib[i - 2]\n\n    return fib[-1] if not return_ith else fib",
        "sha1": "d4f1a1c3a0c60e577d059acb8a8ac15e7fc9a6a5",
        "id": 247284
    },
    {
        "content": "def calculate_transaction_size(n_of_txin, n_of_txout):\n    \"\"\"Transaction size in bytes based on inputs and outputs for non segwit addresses\n    \"\"\"\n\n    return (n_of_txin * 180) + (n_of_txout * 34) + 10 + n_of_txin",
        "sha1": "a19205a5c9296dde0d762c894f337ce6f164d243",
        "id": 514800
    },
    {
        "content": "import time\nimport json\n\n\nasync def tts(ws, text, voice):\n    \"\"\"Send a text-to-speech request on an established websocket connection.\n\n    :param ws: an established websocket connection\n    :param text: the text to be converted to an WAVE file\n    :param voice: voice used to generate the WAVE file\n    :return: the TTS response.\n       >>> if resp['payload']['success'] is True then resp['payload']['audio_url']  # contains the converted audio URL.\n       >>> if resp['payload']['success'] is False then resp['payload']['reason']  # contains the failure reason.\n    \"\"\"\n    tts_request = {\n        'emit':    \"tts\",\n        'payload': {\n            'text':      text,\n            'voice':     voice,\n            'timestamp': int(time.time())\n        }\n    }\n    await ws.send(json.dumps(tts_request))\n    return json.loads(await ws.recv())",
        "sha1": "8b318f2524de729bc1038d7fb5385b0762e82bf2",
        "id": 525587
    },
    {
        "content": "def complex_impedance(z, XR):\n    \"\"\"\n    Returns the complex impedance from z (in %) and the X/R ratio.\n    \"\"\"\n    z = float(abs(z))\n    XR = float(abs(XR))\n    real = (z**2/(1+XR**2))**0.5\n    try:\n        imag = (z**2/(1+1/XR**2))**0.5\n    except ZeroDivisionError:\n        imag = 0.0\n    return complex(real, imag)",
        "sha1": "9647c0dc769612173d83d97b2e50704b3f46170c",
        "id": 102620
    },
    {
        "content": "def printable_cmd(c):\n  \"\"\"Converts a `list` of `str`s representing a shell command to a printable \n  `str`.\"\"\"\n  return \" \".join(map(lambda e: '\"' + str(e) + '\"', c))",
        "sha1": "b5e8a68fc535c186fdbadc8a669ed3dec0da3aee",
        "id": 2198
    },
    {
        "content": "def count(grid, c):\n    \"\"\"\n    Count the occurrences\n    of an object \"c\" in\n    the 2D list \"grid\".\n    \"\"\"\n    acc = 0\n    for row in grid:\n        for elem in row:\n            acc += c == elem\n    return acc",
        "sha1": "6a497b5d052ce8e1d2619f2278010ecd41126a42",
        "id": 9725
    },
    {
        "content": "from typing import Dict\n\n\ndef get_config_input(user_cfg: Dict[str, dict]) -> Dict[str, dict]:\n    \"\"\"\n    Get the input configuration\n\n    :param user_cfg: user configuration\n    :type user_cfg: dict\n    :return cfg: partial configuration\n    :rtype cfg: dict\n    \"\"\"\n\n    cfg = {}\n\n    if \"input\" in user_cfg:\n        cfg[\"input\"] = user_cfg[\"input\"]\n\n    return cfg",
        "sha1": "bbcaf21c3c337b14e33ac9ce6469368987eaac4b",
        "id": 695275
    },
    {
        "content": "def check_textbound_overlap(anns):\n    \"\"\"\n    Checks for overlap between the given TextBoundAnnotations.\n    Returns a list of pairs of overlapping annotations.\n    \"\"\"\n    overlapping = []\n\n    for a1 in anns:\n        for a2 in anns:\n            if a1 is a2:\n                continue\n            if a2.start < a1.end and a2.end > a1.start:\n                overlapping.append((a1,a2))\n\n    return overlapping",
        "sha1": "e10618fc2cc1642ffd295ed59f939a120094254a",
        "id": 687735
    },
    {
        "content": "def perm(lst):\n    \"\"\" Returns a list of all permutations of lst.\"\"\"\n    if len(lst) == 1:\n        return [lst]\n    result = []\n    for e in lst:\n        without_e = [x for x in lst if x != e]\n        sub_answer = perm(without_e)\n        result.extend([[e] + sub for sub in sub_answer])\n    return result",
        "sha1": "0f58dd885e023376fdd13374f73d95acd76077af",
        "id": 597439
    },
    {
        "content": "def build_deed_url(license_code, version, jurisdiction_code, language_code):\n    \"\"\"\n    Return a URL to view the deed specified by the inputs. Jurisdiction\n    and language are optional.\n    \"\"\"\n    # UGH. Is there any way we could do this with a simple url 'reverse'? The URL regex would\n    # be complicated, but we have unit tests to determine if we've got it right.\n    # See test_templatetags.py.\n\n    # https://creativecommons.org/licenses/by-sa/4.0/\n    # https://creativecommons.org/licenses/by-sa/4.0/deed.es\n    # https://creativecommons.org/licenses/by/3.0/es/\n    # https://creativecommons.org/licenses/by/3.0/es/deed.fr\n\n    if jurisdiction_code:\n        if language_code == \"en\" or not language_code:\n            return f\"/licenses/{license_code}/{version}/{jurisdiction_code}/\"\n        else:\n            return f\"/licenses/{license_code}/{version}/{jurisdiction_code}/deed.{language_code}\"\n    else:\n        if language_code == \"en\" or not language_code:\n            return f\"/licenses/{license_code}/{version}/\"\n        else:\n            return f\"/licenses/{license_code}/{version}/deed.{language_code}\"",
        "sha1": "d72f806144c4000fa0758482dbda1b05fa208dc9",
        "id": 509491
    },
    {
        "content": "def insert_ones(y, segment_end_ms, Ty=1375):\n    \"\"\"\n    Update the label vector y. The labels of the 50 output steps strictly after the end of the segment\n    should be set to 1. By strictly we mean that the label of segment_end_y should be 0 while, the\n    50 followinf labels should be ones.\n\n\n    Arguments:\n    y -- numpy array of shape (1, Ty), the labels of the training example\n    segment_end_ms -- the end time of the segment in ms\n\n    Returns:\n    y -- updated labels\n    \"\"\"\n\n    # duration of the background (in terms of spectrogram time-steps)\n    segment_end_y = int(segment_end_ms * Ty / 10000.0)\n    # Add 1 to the correct index in the background label (y)\n    y[0, segment_end_y+1:segment_end_y+51] = 1\n\n    return y",
        "sha1": "12a7fd034ede8ccc5245aed2983f4ec87b2768e7",
        "id": 427035
    },
    {
        "content": "def impedance(vp, rho):\n    \"\"\"\n    Compute acoustic impedance.\n    \"\"\"\n    return vp * rho",
        "sha1": "6be115c3d92b394a5b98cbf502d2bc753522a8a6",
        "id": 67309
    },
    {
        "content": "def _merge_dict_except_none(dict_a, dict_b):\n    \"\"\"Returns dict_a updated with any key/value pairs from dict_b where the\n    value is not None.\n\n    Does not mutate arguments. Also, please don't drink and drive.\n    \"\"\"\n    dict_b_filtered = {k: v for k, v in dict_b.iteritems() if v is not None}\n    return dict(dict_a, **dict_b_filtered)",
        "sha1": "0eca52afabcbe04688bef5970bb2409ab3ca9cd9",
        "id": 383681
    },
    {
        "content": "from typing import Sized\nfrom typing import Iterable\nfrom typing import Mapping\n\n\ndef is_listy(x):\n    \"\"\"\n    returns a boolean indicating whether the passed object is \"listy\",\n    which we define as a sized iterable which is not a map or string\n    \"\"\"\n    return isinstance(x, Sized) and isinstance(x, Iterable) and not isinstance(x, (Mapping, type(b''), type('')))",
        "sha1": "9261569c6a68695f1c67da6b5fe100f8bf0c1f47",
        "id": 103704
    },
    {
        "content": "import re\n\n\ndef preprocess_text(text):\n    \"\"\" changes text to lowercase and strips special characters/punctuation \"\"\"\n    text = re.sub(r'[^A-Za-z0-9\\.]+', ' ', text).strip()\n    text = re.sub(r'\\.\\s', ' ', text)\n    return text.lower()",
        "sha1": "171eadaf9f134940f68604f836200839413ffa08",
        "id": 168503
    },
    {
        "content": "def classify_design_space(action: str) -> int:\n    \"\"\"\n    The returning index corresponds to the list stored in \"count\":\n    [sketching, 3D features, mating, visualizing, browsing, other organizing]\n\n    Formulas for each design space action:\n        sketching = \"Add or modify a sketch\" + \"Copy paste sketch\"\n        3D features = \"Commit add or edit of part studio feature\" + \"Delete part studio feature\"\n                    - \"Add or modify a sketch\"\n        mating = \"Add assembly feature\" + \"Delete assembly feature\" + \"Add assembly instance\"\n                    + \"Delete assembly instance\"\n        visualizing = \"Start assembly drag\" + \"Animate action called\"\n        browsing = Opening a tab + Creating a tab + Deleting a tab + Renaming a tab\n        other organizing = \"Create version\" + \"Cancel Operation\" + \"Undo Redo Operation\"\n                    + \"Merge branch\" + \"Branch workspace\" + \"Update version\"\n\n    :param action: the action to be classified\n    :return: the index of the action type that this action is accounted for; if the action does not\n            belong to any category, return -1\n\n            Note:   \"Add or modify a sketch\" is special (+1 for sketching and -1 for 3D features),\n                    return -10\n    \"\"\"\n    # Creating a sketch is special as it affects both the sketching and the 3D features counts\n    if action == \"Add or modify a sketch\":\n        return -10\n    # Sketching\n    elif action == \"Copy paste sketch\":\n        return 0\n    # 3D features\n    elif action in [\"Commit add or edit of part studio feature\",\n                    \"Delete part studio feature\"]:\n        return 1\n    # Mating\n    elif action in [\"Add assembly feature\", \"Delete assembly feature\", \"Add assembly instance\"\n                    \"Delete assembly instance\"]:\n        return 2\n    # Visualizing\n    elif action in [\"Start assembly drag\", \"Animate action called\"]:\n        return 3\n    # Browsing\n    elif \"Tab\" in action and (\"opened\" in action or \"created\" in action or \"deleted\" in action or\n                              \"renamed\" in action):\n        return 4\n    # Other organizing\n    elif action in [\"Create version\", \"Cancel Operation\", \"Undo Redo Operation\", \"Merge branch\",\n                    \"Branch workspace\", \"Update version\"]:\n        return 5\n    # Not classified (Optional: print out the unclassified actions)\n    else:\n        return -1",
        "sha1": "22dc68aa23258691b0d4b9f1b27a9e8451b275d9",
        "id": 706870
    },
    {
        "content": "def quit(msg=''):\n    \"\"\"Return a valid QUIT message with an optional reason.\"\"\"\n    return 'QUIT :%s' % msg",
        "sha1": "6d9e3ef4881595a604eb300577d5ee55f76837fe",
        "id": 325048
    },
    {
        "content": "from datetime import date\n\n\ndef make_dir(parent_path, *dir_name, add_date=True):\n    \"\"\"\n    Make a new directory\n    Parameters\n    ----------\n    parent_path : str\n    dir_name : str\n         (optional), if not exists, files will be saved in parent_dir\n    add_date : bool\n        make a sub-dir with a date\n\n    Returns\n    -------\n    save_path : path\n    \"\"\"\n\n    global save_path\n    if dir_name:\n        if add_date:\n            today = date.today()\n            save_path = parent_path / dir_name[0] / today.strftime(\"%Y-%m-%d\")  # 2020-07-04\n        else:\n            save_path = parent_path / dir_name[0]\n    else:\n        if add_date:\n            today = date.today()\n            save_path = parent_path / today.strftime(\"%Y-%m-%d\")  # 2020-07-04\n        else:\n            save_path = parent_path\n\n    # print(save_path)\n    if not save_path.exists():\n        save_path.mkdir(parents=True)\n    return save_path",
        "sha1": "d739d06c465b599b23ac0e3eeee277a8cd166a3d",
        "id": 401195
    },
    {
        "content": "import json\n\n\ndef fetch_metadata(fname_json, field):\n    \"\"\"\n    Return specific field value from json sidecar.\n    :param fname_json: str: Json file\n    :param field: str: Field to retrieve\n    :return: value of the field.\n    \"\"\"\n    with open(fname_json) as f:\n        metadata = json.load(f)\n    if field not in metadata:\n        KeyError(\"Json file {} does not contain the field: {}\".format(fname_json, field))\n    else:\n        return metadata[field]",
        "sha1": "0898d76f11f435c967591fbe803c6ebaff20ea7e",
        "id": 94102
    },
    {
        "content": "import json\n\n\ndef load_contract(file_abi, file_bin, file_address=None):\n    \"\"\"\n    Load contract from disk. Returns: address, ABI and Bin from the contract\n    \"\"\"\n    if file_address is not None:\n        try:\n            contract_address = json.load(open(file_address, 'r'))[\"address\"]\n        except FileNotFoundError:\n            contract_address = None\n    else:\n        contract_address = None\n    abi = json.load(open(file_abi, 'r'))\n    contract_bin = json.load(open(file_bin, 'r'))[\"bin\"]\n\n    return contract_address, abi, contract_bin",
        "sha1": "893a60318ee10acd5ad67147e6f33e233340ef6c",
        "id": 523584
    },
    {
        "content": "def isalnum(text):\n    \"\"\"\n    Checks if all characters in ``text`` are alphanumeric and there is at least one character\n    \n    A character c is alphanumeric if one of the following returns True: :func:`isalpha`, \n    :func:`isdecimal`,:func:`isdigit`, or, :func:`isnumeric`.\n    \n    :param text: The string to check\n    :type text:  ``str``\n    \n    :return: True if all characters in ``text`` are alphanumeric and there is at least one character, False otherwise.\n    :rtype:  ``bool``\n    \"\"\"\n    assert isinstance(text,str), '%s is not a string' % text\n    return text.isalnum()",
        "sha1": "bf63dc89522398e8c8a4d91b39dbdb37d61edc28",
        "id": 692831
    },
    {
        "content": "def fmt_time(ts, msp=False):\n    \"\"\"Converts nanosecond timestamps to timecodes.\n    \n    msp = Set timecodes for millisecond precision if True\n    \n    \"\"\"\n    s = ts / 10 ** 9\n    m = s // 60\n    s = s % 60\n    h = m // 60\n    m = m % 60\n    if msp:\n        return '{:02.0f}:{:02.0f}:{:06.3f}'.format(h, m, s)\n    else:\n        return '{:02.0f}:{:02.0f}:{:012.9f}'.format(h, m, s)",
        "sha1": "782162a9bb9b3d62956686563361154adb618776",
        "id": 558302
    },
    {
        "content": "import re\nimport inspect\n\n\ndef _get_task_path(wrapped, instance) -> str:\n    \"\"\"Get the synthetic URL path for a task, based on the `wrapt` parameters.\"\"\"\n    funcname = wrapped.__name__\n    if funcname.startswith(\"_\") and not funcname.endswith(\"_\"):\n        funcname = re.sub(r\"^_+\", repl=\"\", string=funcname, count=1)\n\n    if instance is None:\n        return funcname\n    else:\n        if inspect.isclass(instance):\n            return \"/\".join([instance.__name__, funcname])\n        else:\n            return \"/\".join([instance.__class__.__name__, funcname])",
        "sha1": "16ca96d29abddfa104afc5a0ec466e0bd1d202dc",
        "id": 8936
    },
    {
        "content": "def onepositive(dist):\n    \"\"\"\n    Is there only one positive count in `dist`?\n    \"\"\"\n    npos = 0\n    for n in dist:\n        if n > 0:\n            npos += 1\n            if npos > 1:\n                return False\n    return True",
        "sha1": "0979fbab0bd583a771df0618b8c30d752ba0d89b",
        "id": 555786
    },
    {
        "content": "def find_element_in_list(array, element):\n    \"\"\"\n    Finds an element in an array. Does not crash if not found\n\n    Args:\n        array (list): list to be searched\n        element (any): element to be found\n\n    Returns:\n        int: Index of the element or None if the element is not found.\n    \"\"\"\n    try:\n        index = array.index(element)\n        return index\n    except ValueError:\n        return None",
        "sha1": "066be4f7b284ce6199dcbde9fff542061c72855c",
        "id": 355298
    },
    {
        "content": "import socket\n\n\ndef is_ipv6(ip: str) -> bool:\n    \"\"\"\n    \u5224\u65adip\u662f\u5426\u4e3a\u53ef\u8fde\u63a5\u7684ipv6\n\n    Determine whether the IP is a connectable IPv6\n    :param ip: like fe80::1862:5a79:a8a0:aae5 etc.\n    :return: bool\n    \"\"\"\n    try:\n        socket.inet_pton(socket.AF_INET6, ip)\n    except socket.error:  # not a valid ip\n        return False\n    return True",
        "sha1": "dbaa2c9894c48664d6e0c473ef0d76d3de01cb51",
        "id": 455535
    },
    {
        "content": "def convert_phrase_to_url(phrase):\n    \"\"\" Converts a phrase such as word1 word2 to a wikipedia URL of the form \n    http://en.wikipedia.org/wiki/word1_word2 and returns it\"\"\"\n    prefix = 'http://en.wikipedia.org/wiki/'\n    url = prefix + '_'.join(phrase.split())\n    return url",
        "sha1": "57633b17c06a4aad6e8ff3911a3e520390ad4952",
        "id": 692617
    },
    {
        "content": "import copy\n\n\ndef zero_pad(img, pwx, pwy):\n    \"\"\"Pads a given image with zero at the border.\"\"\"\n    \n    padded_img = copy.deepcopy(img) \n    \n    \"\"\"\n    Let's assume, padded_img = [ [1,1,1],\n                                 [1,1,1],\n                                 [1,1,1]\n                               ]\n    \"\"\"\n    \n    for i in range(pwx):\n        padded_img.insert(0, [0 for value in enumerate(padded_img[i])])                 # add a row of 0 in the top\n        padded_img.insert(len(padded_img), [0 for value in enumerate(padded_img[-1])])  # add a row of 0 in the bottom\n    \n    \"\"\"\n    Now, padded_img = [ [0,0,0]\n                        [1,1,1],\n                        [1,1,1],\n                        [1,1,1],\n                        [0,0,0]\n                      ]\n    \"\"\"\n     \n    \n    for i, row in enumerate(padded_img):\n        for j in range(pwy):\n            row.insert(0, 0)             # add a 0 in the beginning of each row\n            row.insert(len(row), 0)      # add a 0 in the ending of each row\n    \n    \"\"\"\n    Now, padded_img = [ [0, 0,0,0, 0]\n                        [0, 1,1,1, 0],\n                        [0, 1,1,1, 0],\n                        [0, 1,1,1, 0],\n                        [0, 0,0,0, 0]\n                      ]\n    \"\"\"\n    return padded_img",
        "sha1": "b9bdfca7fdeb9c0a83cb57c42516b57a05fb0d14",
        "id": 589337
    },
    {
        "content": "def get_training_or_validation_split(samples, labels, validation_split, subset):\n  \"\"\"Potentially restict samples & labels to a training or validation split.\n\n  Args:\n    samples: List of elements.\n    labels: List of corresponding labels.\n    validation_split: Float, fraction of data to reserve for validation.\n    subset: Subset of the data to return.\n      Either \"training\", \"validation\", or None. If None, we return all of the\n      data.\n\n  Returns:\n    tuple (samples, labels), potentially restricted to the specified subset.\n  \"\"\"\n  if not validation_split:\n    return samples, labels\n\n  num_val_samples = int(validation_split * len(samples))\n  if subset == 'training':\n    print('Using %d files for training.' % (len(samples) - num_val_samples,))\n    samples = samples[:-num_val_samples]\n    labels = labels[:-num_val_samples]\n  elif subset == 'validation':\n    print('Using %d files for validation.' % (num_val_samples,))\n    samples = samples[-num_val_samples:]\n    labels = labels[-num_val_samples:]\n  else:\n    raise ValueError('`subset` must be either \"training\" '\n                     'or \"validation\", received: %s' % (subset,))\n  return samples, labels",
        "sha1": "54e636991d1048cd30177cf470c42387a8e364fb",
        "id": 416935
    },
    {
        "content": "def compute_iou(bboxA, bboxB):\n    \"\"\"compute iou of two bounding boxes\n\n    Args:\n      bboxA(list): coordinates of box A (i,j,w,h)\n      bboxB(list): coordinates of box B (i,j,w,h)\n\n    Return:\n      float: iou score\n    \"\"\"\n\n    ix = max(bboxA[0], bboxB[0])\n    iy = max(bboxA[1], bboxB[1])\n    mx = min(bboxA[0] + bboxA[2], bboxB[0] + bboxB[2])\n    my = min(bboxA[1] + bboxA[3], bboxB[1] + bboxB[3])\n    area_inter = max(mx - ix, 0) * max(my - iy, 0)\n    area_A = bboxA[2] * bboxA[3]\n    area_B = bboxB[2] * bboxB[3]\n\n    iou = float(area_inter) / float(area_A + area_B - area_inter)\n    return iou",
        "sha1": "e32aa77208f7b4cd31c7a0eb2a6adca25c332917",
        "id": 110203
    },
    {
        "content": "import collections\n\n\ndef _name_regions(regions, vertex_slice):\n    \"\"\"Convert a list of regions into the correct form for a method expecting\n    two dictionaries describing the regions.\n    \"\"\"\n    regions = collections.OrderedDict({\n        i: r for i, r in enumerate(regions, start=1) if r is not None\n    })\n    region_args = collections.defaultdict(\n        lambda: ((vertex_slice, ), {})\n    )\n\n    return regions, region_args",
        "sha1": "188d8aa071d6921642f785aade485d54bcda39b4",
        "id": 617822
    },
    {
        "content": "def add_cluster_root_dir_as_positional_argument(arg_parser):\n    \"\"\"Add root_dir as root output directory for `cluster`.\"\"\"\n    helpstr = \"An directory to store temporary and output cluster files\" + \\\n              \" (e.g., in SA3.2, tasks/pbtranscript.tasks.separate_flnc-0/0to1kb_part0/cluster_out; in SA2.3, data/clusterOutDir/)\"\n    arg_parser.add_argument(\"root_dir\", help=helpstr, type=str)\n    return arg_parser",
        "sha1": "9caeff57d8175c0fb91127d5c911eabd8a0666e0",
        "id": 508506
    },
    {
        "content": "def make_dispatcher(prefix, logger=None):\n    \"\"\" Create a function which will dispatch arguments to specially\n    named handler methods on an object.\n\n    Parameters\n    ----------\n    prefix : str\n        The string to prefix to all dispatch names to construct the\n        name of the handler method.\n\n    logger : logging.Logger, optional\n        A logger to use for logging handler lookup misses.\n\n    Returns\n    -------\n    result : types.FunctionType\n        A function with the signature func(obj, name, *args). Calling\n        it is equivalent to `getattr(obj, prefix + name)(*args)`\n\n    \"\"\"\n    def dispatcher(obj, name, *args):\n        handler = getattr(obj, prefix + name, None)\n        if handler is not None:\n            handler(*args)\n        elif logger is not None:\n            msg = \"no dispatch handler found for '%s' on `%s` object\"\n            logger.warn(msg % (name, obj))\n    dispatcher.__name__ = prefix + '_dispatcher'\n    return dispatcher",
        "sha1": "c1523b48f40701bf47b5d9fe532c5c2b1ff815a3",
        "id": 367771
    },
    {
        "content": "def unique_list(seq):\n    \"\"\"\n    Find the unique elements in a list or other sequence\n    while maintaining the order. (i.e., remove any duplicated\n    elements but otherwise leave it the same)\n\n    Method from:\n    https://stackoverflow.com/questions/480214/how-do-you-remove-duplicates-from-a-list-whilst-preserving-order\n\n    Parameters\n    ----------\n    seq: list or sequence\n        Any input object that can be iterated\n\n    Returns\n    -------\n    L: list\n        a new list of the unique objects\n    \"\"\"\n    seen = set()\n    seen_add = seen.add\n    return [x for x in seq if not (x in seen or seen_add(x))]",
        "sha1": "e031c262fd51aea577394bbaaeb97225835fd448",
        "id": 246046
    },
    {
        "content": "def compute_files_to_download(client_hashes, server_hashes):\n    \"\"\"\n    Given a dictionary of file hashes from the client and the\n    server, specify which files should be downloaded from the server\n\n    :param client_hashes: a dictionary where the filenames are keys and the\n                          values are md5 hashes as strings\n    :param server_hashes: a dictionary where the filenames are keys and the\n                          values are md5 hashes as strings\n    :return: a list of 2 lists -> [to_dload, to_delete]\n             to_dload- a list of filenames to get from the server\n             to_delete- a list of filenames to delete from the folder\n\n    Note: we will get a file from the server if a) it is not on the\n    client or b) the md5 differs between the client and server\n\n    Note: we will mark a file for deletion if it is not available on\n    the server\n\n    \"\"\"\n    to_dload, to_delete = [], []\n    for filename in server_hashes:\n        if filename not in client_hashes:\n            to_dload.append(filename)\n            continue\n        if client_hashes[filename] != server_hashes[filename]:\n            to_dload.append(filename)\n\n    for filename in client_hashes:\n        if filename not in server_hashes:\n            to_delete.append(filename)\n\n    return [to_dload, to_delete]",
        "sha1": "f67220d82852edbba751ca794457adfa347483be",
        "id": 328443
    },
    {
        "content": "def create_header_str(*args):\n    \"\"\"\n    create header str with delimiters\n    :param args: values which should be in header\n    :return: values concatenated as str with '|' as delimiter\n    \"\"\"\n    header_str = '|'\n    for arg in args:\n        header_str = header_str + str(arg) + '|'\n    return header_str",
        "sha1": "1480125a68dce371444758e179237ee4ee4dd94a",
        "id": 315793
    },
    {
        "content": "def rotate_table(df):\n    \"\"\"\n    Given a pandas dataframe, rotates values 180 degrees (keeping indices).\n    \n    Keyword arguments:\n    :param df: pandas dataframe (no default)\n    :return: rotated pandas dataframe\n    \"\"\"\n\n    # Flip rows\n    flipped_df = df.copy()\n    row_count = len(df.index)\n    loop_i = 1\n    for row in df.iterrows():\n        index = row_count - loop_i\n        flipped_df.iloc[index, :] = row[1].values\n        loop_i += 1\n    \n    # Flip columns\n    rotated_df = flipped_df.copy()\n    column_count = len(flipped_df.columns)\n    loop_i = 0\n    for column in flipped_df:\n        index = column_count - loop_i\n        rotated_df[index] = flipped_df[column]\n        loop_i += 1\n    \n    print('Rotated table 180 degrees.')\n    return rotated_df",
        "sha1": "72fb4203e8f4dd58b030f84c35b9689f00926f97",
        "id": 277463
    },
    {
        "content": "import torch\n\n\ndef simclr_collator(batch):\n    \"\"\"\n    This collator is used in SimCLR approach.\n\n    The collators collates the batch for the following input (each image has k-copies):\n        input: [[img1_0, ..., img1_k], [img2_0, ..., img2_k], ..., [imgN_0, ..., imgN_k]]\n        output: [img1_0, img2_0, ....., img1_1, img2_1,...]\n\n\n    Input:\n        batch: Example\n                batch = [\n                    {\"data\": [img1_0, ..., img1_k], \"label\": [lbl1, ]},        #img1\n                    {\"data\": [img2_0, ..., img2_k], \"label\": [lbl2, ]},        #img2\n                    .\n                    .\n                    {\"data\": [imgN_0, ..., imgN_k], \"label\": [lblN, ]},        #imgN\n                ]\n\n                where:\n                    img{x} is a tensor of size: C x H x W\n                    lbl{x} is an integer\n\n    Returns: Example output:\n                output = [\n                    {\n                        \"data\": torch.tensor([img1_0, img2_0, ....., img1_1, img2_1,...]) ..\n                    },\n                ]\n    \"\"\"\n    assert \"data\" in batch[0], \"data not found in sample\"\n    assert \"label\" in batch[0], \"label not found in sample\"\n\n    data = [x[\"data\"] for x in batch]\n    labels = [torch.tensor(x[\"label\"]) for x in batch]\n    data_valid = [torch.tensor(x[\"data_valid\"]) for x in batch]\n    data_idx = [torch.tensor(x[\"data_idx\"]) for x in batch]\n    num_positives, num_images = len(data[0]), len(data)\n\n    output_data, output_label, output_data_valid, output_data_idx = [], [], [], []\n    for pos in range(num_positives):\n        for idx in range(num_images):\n            output_data.append(data[idx][pos])\n            output_label.append(labels[idx][pos])\n            output_data_valid.append(data_valid[idx][pos])\n            output_data_idx.append(data_idx[idx][pos])\n\n    output_batch = {\n        \"data\": [torch.stack(output_data)],\n        \"label\": [torch.stack(output_label)],\n        \"data_valid\": [torch.stack(output_data_valid)],\n        \"data_idx\": [torch.stack(output_data_idx)],\n    }\n    return output_batch",
        "sha1": "c7cc46b2b4fa25377465d2c93d97f0c05c12511e",
        "id": 402886
    },
    {
        "content": "def rivers_with_station(stations):\n    \"\"\"\n    Function that, given a list of station objects,\n    returns a container with the names of the rivers with a monitoring station.\n    \n    Args:\n        stations (list): List of stations (MonitoringStation).\n    \n    Returns:\n        set: Set of names of rivers with a monitoring station.\n    \"\"\"\n\n    return {station.river for station in stations}",
        "sha1": "a764cd72e1d5c1a4b62a1c20d3bd7d1b3e2678a3",
        "id": 559644
    },
    {
        "content": "def is_unlucky_ticket(ticket: str) -> bool:\n    \"\"\"\n    \u041f\u0440\u043e\u0432\u0435\u0440\u044f\u0435\u0442, \u044f\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u043b\u0438 \u0448\u0435\u0441\u0442\u0438\u0437\u043d\u0430\u0447\u043d\u044b\u0439 \u043d\u043e\u043c\u0435\u0440 \u0431\u0438\u043b\u0435\u0442\u0430 \u043d\u0435\u0441\u0447\u0430\u0441\u0442\u043b\u0438\u0432\u044b\u043c.\n    \u041d\u043e\u043c\u0435\u0440 \u0431\u0438\u043b\u0435\u0442\u0430 \u044f\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u043d\u0435\u0441\u0447\u0430\u0441\u0442\u043b\u0438\u0432\u044b\u043c, \u0435\u0441\u043b\u0438 \u0441\u0443\u043c\u043c\u0430 \u0435\u0433\u043e \u0446\u0438\u0444\u0440 \u0434\u0435\u043b\u0438\u0442\u0441\u044f \u043d\u0430 13 \u0431\u0435\u0437 \u043e\u0441\u0442\u0430\u0442\u043a\u0430.\n\n    :param ticket: \u0428\u0435\u0441\u0442\u0438\u0437\u043d\u0430\u0447\u043d\u044b\u0439 \u043d\u043e\u043c\u0435\u0440 \u0431\u0438\u043b\u0435\u0442\u0430.\n    :return: \u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442 \u043f\u0440\u043e\u0432\u0435\u0440\u043a\u0438.\n    \"\"\"\n\n    if len(ticket) != 6:\n        raise ValueError(\"\u041d\u043e\u043c\u0435\u0440 \u0431\u0438\u043b\u0435\u0442\u0430 \u0434\u043e\u043b\u0436\u0435\u043d \u0431\u044b\u0442\u044c \u0448\u0435\u0441\u0442\u0438\u0437\u043d\u0430\u0447\u043d\u044b\u043c\")\n\n    return sum([int(number) for number in ticket]) % 13 == 0",
        "sha1": "57d994bb8d1f88271f29abf61c0b221f737adaf6",
        "id": 583704
    },
    {
        "content": "import base64\n\n\ndef number_to_base64(data: int) -> bytes:\n    \"\"\"Convert an integer to base64-encoded bytes in big endian order.\n\n    The base64 encoding used here is the Base64urlUInt encoding defined in RFC\n    7515 and 7518, which uses the URL-safe encoding characters and omits all\n    padding.\n\n    Parameters\n    ----------\n    data : `int`\n        Arbitrarily large number\n\n    Returns\n    -------\n    result : `bytes`\n        The equivalent URL-safe base64-encoded string corresponding to the\n        number in big endian order.\n    \"\"\"\n    bit_length = data.bit_length()\n    byte_length = bit_length // 8 + 1\n    data_as_bytes = data.to_bytes(byte_length, byteorder=\"big\", signed=False)\n    return base64.urlsafe_b64encode(data_as_bytes).rstrip(b\"=\")",
        "sha1": "dae8ce434e30eeb95e43ba2297a4ee2621e6e7bf",
        "id": 339014
    },
    {
        "content": "def convert_indices_to_symbols(symbol_table, indices):\n    \"\"\"Converts indices to symbols by looking them up in the symbol table.\n\n    Args:\n        symbol_table (SymbolTable): The symbol table.\n        indices (List[int]): The list of indices.\n\n    Returns:\n        List[str]: The list of symbols corresponding to the given indices.\n\n    Raises:\n        KeyError: If an index is not found in the symbol table.\n    \"\"\"\n    syms = []\n    for idx in indices:\n        sym = symbol_table.find_symbol(idx)\n        if sym == \"\":\n            raise KeyError(\"Index {} is not found in the symbol table.\"\n                           .format(idx))\n        syms.append(sym)\n    return syms",
        "sha1": "285e7eb6aca695dc2b2a7d58222945c7e52257d2",
        "id": 359970
    },
    {
        "content": "def currency_clean_helper(currency, value):\n\t\"\"\"Used to validate that a currency value works for a \n\tgive currency.  Should be called from a forms clean() method.\n\n\tReturns (value, errors)\n\t\"\"\"\n\twhole = value[0]\n\tfrac = str(value[1]) if len(value) == 2 else None\n\tif frac and len(frac) > currency.decimal_places:\n\t\treturn None, \"Too many decimal places (%s) for currency %s\" % (\n\t\t\t\tlen(frac), currency)\n\n\tif not frac:\n\t\tfrac = '0' * currency.decimal_places\n\telif len(frac) < currency.decimal_places:\n\t\tfrac += '0' * (currency.decimal_places - len(frac))\n\n\treturn int(str(whole) + frac), None",
        "sha1": "db51c6969316264065bb973a1871021775f40f6c",
        "id": 693005
    },
    {
        "content": "def tx_lines_by_period_cycle_init(mod, period, cycle):\n    \"\"\"\n    Re-arrange the 3-dimensional PRDS_CYCLES_TX_DCOPF set into a 1-dimensional\n    set of TX_DCOPF, indexed by PRD_CYCLES.\n    \"\"\"\n    txs = list(tx for (p, c, tx) in mod.PRDS_CYCLES_TX_DCOPF\n               if c == cycle and p == period)\n    return txs",
        "sha1": "25e9ec11bcbcebe21ac290b0fc8e32b73b28945f",
        "id": 424978
    },
    {
        "content": "def replace_null(x, replace = None):\n    \"\"\"\n    Replace null values\n\n    Parameters\n    ----------\n    x : Expr, Series\n        Column to operate on\n\n    Examples\n    --------\n    >>> df = tp.Tibble(x = [0, None], y = [None, None])\n    >>> df.mutate(x = tp.replace_null(col('x'), 1))\n    \"\"\"\n    if replace == None: return x\n    return x.fill_null(replace)",
        "sha1": "6aa7906524b40633bf05ca0178891da249310fff",
        "id": 508326
    },
    {
        "content": "def pipeline_none(model):\n    \"\"\"No transforms pipeline\"\"\"\n    return model",
        "sha1": "acadfcaa775ecd161911a6fab07d245bd28cf581",
        "id": 53486
    },
    {
        "content": "import requests\n\n\ndef offline_token_using_password(token_endpoint, client_id, username,\n                                 password):\n    \"\"\"Get offine token using password.\"\"\"\n    response = requests.post(\n        token_endpoint,\n        data={\n            'grant_type': 'password',\n            'scope': ['offline_access', 'openid'],\n            'client_id': client_id,\n            'username': username,\n            'password': password,\n        })\n    return response.json()",
        "sha1": "db802596940a9d020bb13e93b0546ce9a48aea40",
        "id": 605290
    },
    {
        "content": "def duration360days(Ndays):\n    \"\"\"\n    Convers a number o days into duration tuple (years, months, days)\n    in a 360/30 day counting convention.\n\n    :param Ndays:   Number of days\n    :return:        Tuple (years, monhts, days)\n    :type Ndays:    int\n    :rtype:         tuple(int, int, int)\n\n    Example:\n    >>> from m2py.finance import dtime as dt\n    >>> dt.duration360days(1321)\n    >>> (3, 8, 1)\n\n    1321 days = 3 years, 8 months and 1 day\n    \"\"\"\n\n    years = int(Ndays / 360)\n    rem = Ndays % 360\n    months = int(rem / 30)\n    days = rem % 30\n\n    return (years, months, days)",
        "sha1": "88f9b0a49aa84f262dfdf0ac5ce2540f7b6fd2b5",
        "id": 512725
    },
    {
        "content": "def maze_nghbrs(vert, maze_path):\n    \"\"\" for vert find all neighbouring verts that are connected by\n    an edge in maze_path\"\"\"\n\n    nghbrs = [link_edge.other_vert(vert)\n              for link_edge in vert.link_edges\n              if link_edge in maze_path]\n    return nghbrs",
        "sha1": "26c077058b031bf6cc63f3bc9d012da934f045f0",
        "id": 225426
    },
    {
        "content": "def calc_posterior(likelihood, prior, norm_list):\n\n\n    \"\"\"\n    Calculate the posterior probability given likelihood,\n    prior, and normalization\n    \n    Positional Arguments:\n        likelihood -- float, between 0 and 1\n        prior -- float, between 0 and 1\n        norm_list -- list of tuples, each tuple has two values\n            the first value corresponding to the probability of a value of \"b\"\n            the second value corresponding to the probability of \n                a value of \"a\" given that value of \"b\"\n    Example:\n        likelihood = .8\n        prior = .3\n        norm_list = [(.25 , .9), (.5, .5), (.25,.2)]\n        print(calc_posterior(likelihood, prior, norm_list))\n        # --> 0.45714285714285713\n    \"\"\"\n    Pa = 0\n    \n    for t in norm_list:\n        x = t[0] * t[1]\n        Pa+=x\n\n    return (likelihood*prior)/Pa",
        "sha1": "cd39b242971ec7dc74fce61effe9199b2e312461",
        "id": 290583
    },
    {
        "content": "import torch\n\n\ndef compute_intersection(a, b):\n    \"\"\"Compute intersection between boxes.\n\n    Args:\n        a: tensor (m, 4)\n        b: tensor (n, 4)\n\n    Returns:\n        tensor (m, n)\n    \"\"\"\n    # (m, 1, 2)\n    a_min = a[:, 0:2].unsqueeze(1)\n    # (1, n, 2)\n    b_min = b[:, 0:2].unsqueeze(0)\n    # (m, n, 2)\n    lb = torch.max(a_min, b_min)\n\n    # (m, 1, 2)\n    a_max = a[:, 2:4].unsqueeze(1)\n    # (1, n, 2)\n    b_max = b[:, 2:4].unsqueeze(0)\n    # (m, n, 2)\n    ub = torch.min(a_max, b_max)\n\n    # (m, n)\n    inter = (ub - lb).clamp(min=0).prod(2)\n    return inter",
        "sha1": "f008173dbe8f5bf13dc2f65ee0f4ebcb9d2828be",
        "id": 382133
    },
    {
        "content": "import math\n\n\ndef xy_to_CCT_hernandez1999(xy):\n    \"\"\"\n    Returns the correlated colour temperature :math:`T_{cp}` from given\n    *CIE XYZ* colourspace *xy* chromaticity coordinates using\n    *Javier Hernandez-Andres, Raymond L. Lee, Jr., and Javier Romero (1999)*\n    method.\n\n    Parameters\n    ----------\n    xy : array_like\n        *xy* chromaticity coordinates.\n\n    Returns\n    -------\n    numeric\n        Correlated colour temperature :math:`T_{cp}`.\n\n    References\n    ----------\n    .. [10]  `Calculating correlated color temperatures across the entire gamut\n            of daylight and skylight chromaticities\n            <http://www.ugr.es/~colorimg/pdfs/ao_1999_5703.pdf>`_,\n            DOI: http://dx.doi.org/10.1364/AO.38.005703\n\n    Examples\n    --------\n    >>> xy_to_CCT_hernandez1999((0.31271, 0.32902))  # doctest: +ELLIPSIS\n    6500.0421533...\n    \"\"\"\n\n    x, y = xy\n\n    n = (x - 0.3366) / (y - 0.1735)\n    CCT = (-949.86315 +\n           6253.80338 * math.exp(-n / 0.92159) +\n           28.70599 * math.exp(-n / 0.20039) +\n           0.00004 * math.exp(-n / 0.07125))\n\n    if CCT > 50000:\n        n = (x - 0.3356) / (y - 0.1691)\n        CCT = (36284.48953 +\n               0.00228 * math.exp(-n / 0.07861) +\n               5.4535e-36 * math.exp(-n / 0.01543))\n\n    return CCT",
        "sha1": "a1e0b08af5ae19d73b9a66dbea0aaa8886135e53",
        "id": 382112
    },
    {
        "content": "def render_tags(project):\n    \"\"\"\n    Render project's tags.\n    \"\"\"\n    data = {'tags': []}\n    if not project.tags:\n        return data\n    data['tags'] = project.tags.split(',')\n    return data",
        "sha1": "7096e1545a7ac2d60fa7e031764a8bb0d31da17f",
        "id": 605077
    },
    {
        "content": "def _ParseBodyAsDict(body):\n  \"\"\" Parse the specified body as a dictionary with each element in a line, and\n  key value pairs separated by '='.\n\n  Args:\n    body: The string with the HTTP body to parse.\n\n  Returns:\n    A dictionary with the body contents.\n  \"\"\"\n  return dict(line.split('=') for line in body.split('\\n') if line)",
        "sha1": "c8d3d8fee55d873d5d6676f8ab0a9c448ef6469c",
        "id": 161398
    },
    {
        "content": "def validate_single_input(df, idx_col=\"project\", msg=\"\"):\n    \"\"\"\n    Check whether there is only 1 input per index.\n\n    Example: check that there is only 1 load point per project in the heat\n    rate inputs DataFrame.\n\n    :param df: DataFrame to check. Must have column idx_col.\n    :param idx_col: str or list of str, the index column, defaults to \"project\"\n    :param msg: str, optional error message clarification.\n    :return: List of error messages for each index with invalid inputs.\n    \"\"\"\n\n    results = []\n\n    n_inputs = df.groupby(idx_col).size()\n    invalids = (n_inputs > 1)\n    if invalids.any():\n        bad_idxs = invalids.index[invalids]\n        print_bad_idxs = \", \".join(bad_idxs)\n        results.append(\n            \"{}(s) '{}': Too many inputs! Maximum 1 input per {}. {}\"\n            .format(idx_col, print_bad_idxs, idx_col, msg)\n        )\n\n    return results",
        "sha1": "624be8792c3ecfeb0ef0568c98349ff1855ad0e2",
        "id": 186219
    },
    {
        "content": "from typing import Counter\n\n\ndef a_scramble(str_1,str_2):\n    \"\"\"Test if all the letters of word a are contained in word b\"\"\"\n    str_1 = str_1.upper()\n    str_2 = str_2.upper()\n    letters = Counter(str_1)\n    letters.subtract(Counter(str_2))\n    return all(v >= 0 for v in letters.values())",
        "sha1": "e9e088b4c2e18f7efb6ba5749c05eeede4531bef",
        "id": 402081
    },
    {
        "content": "def dtIsArticle(dt):\n    \"\"\"\n    Checks if determiner is indefinite or definite English article\n    Argument:\n        dt: the determiner name\n    Return:\n        True if determiner is indefinite or definite English article\n    \"\"\"\n    return dt.lower() in ['a', 'an', 'the']",
        "sha1": "ab9553aa275eeb5d838d1ffe0b5332051f64e59f",
        "id": 88483
    },
    {
        "content": "import requests\n\n\ndef intentar_descarga(link):\n    \"\"\"Intenta descargar un archivo desde el link proporcionado.\"\"\"\n\n    r = requests.get(link)\n\n    if r.ok:\n        return r\n    else:\n        raise Exception(\"Error en la descarga de \" + link)",
        "sha1": "f547ad6e31afd6aaa072cb86598701fcdb3d2bd8",
        "id": 287826
    },
    {
        "content": "from typing import Callable\n\n\ndef fixture_mean(request) -> Callable:\n    \"\"\"Mean function of a random process.\"\"\"\n    return request.param[1]",
        "sha1": "c217a803ce237799889f28d69adeca2a6b29fdb2",
        "id": 383670
    },
    {
        "content": "import math\n\n\ndef date_label(i):\n    \"\"\"\n    Helper funtion, label 67 quarters, from 2000q1 to 2016q3.\n    Takes current iteration and returns quarter label.\n    \"\"\"\n    dlab = '{0}q{1}'\n    # count years\n    yr = 2000 + math.floor(i/4)\n    # count quarters\n    qt = i % 4+1\n\n    return(dlab.format(yr, qt))",
        "sha1": "79206c11628ad22adca0fee85cda4885711901cb",
        "id": 621425
    },
    {
        "content": "def _should_update_date(verified_mode):\n    \"\"\" Returns whether or not the verified mode should be updated. \"\"\"\n    return not(verified_mode is None or verified_mode.expiration_datetime_is_explicit)",
        "sha1": "c2ad192fb5bb8da27fd754466660cb819ac141a6",
        "id": 92876
    },
    {
        "content": "def is_no_overlap(*set_list):\n    \"\"\"\n    Test if there's no common item in several set.\n    \"\"\"\n    return sum([len(s) for s in set_list]) == len(set.union(*[set(s) for s in set_list]))",
        "sha1": "300253d499c22bb398837c9766a9f5d8f7b5c720",
        "id": 20790
    },
    {
        "content": "import csv\n\n\ndef file_to_dicts_lists(file_path):\n    \"\"\"Function make list of dictionaries with quotes from given file\n\n    Args:\n        file_path (str): Path to file with qoutes\n\n    Returns:\n        list: List with dictionaries with qoutes\n    \"\"\"\n    with open(file_path, \"r\") as f:\n        dicts_list = []\n        reader = csv.DictReader(f)\n\n        for line in reader:\n            dicts_list.append(line)\n    \n    return dicts_list",
        "sha1": "a74622ec09b2cdf1a1c463c77f540507df25fb84",
        "id": 232557
    },
    {
        "content": "def keras_fit(model, X_train, Y_train, X_test, Y_test, batch_size=128,\n              nb_classes=None, epochs=12, fLOG=None):\n    \"\"\"\n    Fits a :epkg:`keras` model.\n\n    @param      model       :epkg:`keras` model\n    @param      X_train     training features\n    @param      Y_train     training target\n    @param      X_test      test features\n    @param      Y_test      test target\n    @param      batch_size  batch size\n    @param      nb_classes  nb_classes\n    @param      epochs      number of iterations\n    @param      fLOG        logging function\n    @return                 model\n    \"\"\"\n    # numpy.random.seed(1337)  # for reproducibility\n\n    if nb_classes is None:\n        nb_classes = Y_train.shape[1]\n        if fLOG:\n            fLOG(\"[keras_fit] nb_classes=%d\" % nb_classes)\n    try:\n        model.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs,\n                  verbose=1, validation_data=(X_test, Y_test))\n    except Exception:  # pylint: disable=W0703\n        model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=epochs,\n                  verbose=1, validation_data=(X_test, Y_test))\n    return model",
        "sha1": "2a61fdeb227932d8864c0ec5fb45bb70411dd802",
        "id": 114754
    },
    {
        "content": "def get_dec_inp_targ_seqs(sequence, max_len, start_id, stop_id):\n    \"\"\"\n    Given the reference summary as a sequence of tokens, return the input sequence for the decoder, and the target sequence which we will use to calculate loss. The sequence will be truncated if it is longer than max_len. The input sequence must start with the start_id and the target sequence must end with the stop_id (but not if it's been truncated).\n    Args:\n      sequence: List of ids (integers)\n      max_len: integer\n      start_id: integer\n      stop_id: integer\n    Returns:\n      inp: sequence length <=max_len starting with start_id\n      target: sequence same length as input, ending with stop_id only if there was no truncation\n    \"\"\"\n    inp = [start_id] + sequence[:]\n    target = sequence[:]\n    if len(inp) > max_len:  # truncate\n        inp = inp[:max_len]\n        target = target[:max_len]  # no end_token\n    elif len(inp) == max_len:\n        target.append(stop_id)\n    else:\n        target.append(stop_id)  # end token\n        inp.append(stop_id)  # end token\n    # assert len(inp) == len(target)\n    return inp, target",
        "sha1": "2cb851596a7674f4001d00c89da6419b1093534a",
        "id": 545527
    },
    {
        "content": "from pathlib import Path\nfrom typing import Union\nimport mimetypes\n\n\ndef list_mimetypes(path: Path,\n                   mime_types: tuple[str, ...] = (\"image/jpeg\",),\n                   only_print: bool = True) -> Union[filter, None]:\n    \"\"\"\\\n    Listar los ficheros de \"path\" cuyo mime type est\u00e9 en \"mime_types\".\n\n    Parameters\n    ----------\n    path : Path\n        El directorio que se quiere listar.\n    mime_types : tuple[str, ...], optional\n        Los mimetypes que se listar\u00e1n. Por defecto es (\"image/jpeg\",).\n    only_print : bool, optional\n        Si es True la salida se env\u00eda a la salida est\u00e1ndar, en caso\n        contrario retorna un objeto filter. The default is True.\n\n    Returns\n    -------\n    None o filter\n\n    \"\"\"\n    if only_print:\n        print(f\"Directorio {path}\\n\")\n        for file in Path(path).iterdir():\n            if mimetypes.guess_type(file)[0] in mime_types:\n                print(file.name)\n    else:\n        # Una \"inner function\"\n        def mimefilter(f):\n            return mimetypes.guess_type(f)[0] in mime_types\n        return filter(mimefilter, Path(path).iterdir())",
        "sha1": "eb7867325c5f88d01f10a63d086496b98285f74e",
        "id": 524924
    },
    {
        "content": "def nb_samples_per_dit_decim(Fs=8000, code_speed=13, decim=7.69):\n    \"\"\" One dit of time at w wpm is 1.2/w.\n        Returns a tuple (raw samples per dit, expected decimation factor)\n        Overlap is nfft - decimation factor\n    \"\"\"\n    t_dit = 1.2 / code_speed\n    return int(t_dit * Fs), int(t_dit * Fs) / decim",
        "sha1": "16464396e57a07c1d849d95d88aff9bd57e12aa5",
        "id": 563006
    },
    {
        "content": "def isDescendant(node1, node2):\n    \"\"\"\n    Checks if node1 is descendant of node2\n\n    :param node1:\n    :param node2:\n    :return: True if node1 is descendant of node2, False otherwise\n    \"\"\"\n    if node2 is None:\n        return False\n    if node1 is node2:\n        return True\n    return isDescendant(node1, node2.left) if node1.value < node2.value else isDescendant(node1, node2.right)",
        "sha1": "83e90e33b763a0f7706a41abaa71a01d30b036ac",
        "id": 62672
    },
    {
        "content": "def ListServerCas(sql_client, sql_messages, instance_ref):\n  \"\"\"Calls the list server CAs endpoint and returns the response.\"\"\"\n  return sql_client.instances.ListServerCas(\n      sql_messages.SqlInstancesListServerCasRequest(\n          project=instance_ref.project, instance=instance_ref.instance))",
        "sha1": "d6f386213ce1ce1f5a32919db5f438845eb0374b",
        "id": 580814
    },
    {
        "content": "import random\n\n\ndef generate_random_even_num(min_num, max_num):\n    \"\"\"Generates a random even integer between min_num and max_num\n\n    Args:\n        min_num (int): The minimum the number can be.\n        max_num (int): The maximum the number can be.\n\n    Returns:\n        (int): an even number.\n    \"\"\"\n\n    num = random.randint(min_num, max_num)\n    if num % 2 == 0:\n        return num\n    return num + 1",
        "sha1": "57fc2285d9d9f1315738c6ae923c9ce20b16f13c",
        "id": 311771
    },
    {
        "content": "def ice_cream_parlor(m, arr):\n    \"\"\"Hackerrank Problem: https://www.hackerrank.com/challenges/icecream-parlor/problem\n\n    Sunny and Johnny like to pool their money and go to the ice cream parlor. Johnny never buys the same flavor that\n    Sunny does. The only other rule they have is that they spend all of their money.\n\n    Given a list of prices for the flavors of ice cream, select the two that will cost all of the money they have.\n\n    Args:\n        m (int): Total amount of pooled money\n        arr (list): List of the costs of the different flavors of ice cream\n\n    Returns:\n        list: List containing the indices of the two flavors of ice cream that add up to the total pooled money\n    \"\"\"\n    for i in range(len(arr)):\n        for j in range(i+1, len(arr)):\n            if arr[i] + arr[j] == m:\n                return [i+1, j+1]",
        "sha1": "218c120136ee3bf92e35b93a7a10a61191efe848",
        "id": 424887
    },
    {
        "content": "import math\n\n\ndef intersection_angle(m1, m2):\n    \"\"\"\n    Computes intersection angle between two slopes.\n    \"\"\"\n    return math.degrees(math.atan((m2-m1) / (1+m1*m2)))",
        "sha1": "244192d3d1fe74130d64350606e765d8f2d4831b",
        "id": 545
    },
    {
        "content": "def agg_loss(method):\n    \"\"\"\n    Extract aggregate loss from full name of method.\n\n    Args:\n        method (str): full name of method\n\n    Returns:\n        str: aggregate loss\n    \"\"\"\n    if method[:6] == \"hinge_\":\n        return method[6:]\n    if method[:9] == \"logistic_\":\n        return method[9:]\n    if method[:7] == \"sklearn\":\n        return method[7:]",
        "sha1": "a6d8c0c9162788db02cee60aaaee8361595aeaeb",
        "id": 647279
    },
    {
        "content": "def read_file(fname):\n    \"\"\"\n    Get a file into a list of strings (one entry=one line)\n    \"\"\"\n    with open(fname) as f:\n        lines = f.readlines()\n    return lines",
        "sha1": "d0322839e510c46209b6716739a6f371292e17a5",
        "id": 657160
    },
    {
        "content": "def extract_value(value):\n    \"\"\"\n    Given UAVCAN Value object, returns the value and its type\n    \"\"\"\n    if   hasattr(value, 'boolean_value'): return (bool(value.boolean_value), bool)\n    elif hasattr(value, 'integer_value'): return (int(value.integer_value), int)\n    elif hasattr(value, 'real_value'):    return (float(value.real_value), float)\n    elif hasattr(value, 'string_value'):  return (str(value.string_value), str)\n    else:                                 return (None, None)",
        "sha1": "2741c37de06e8eb3071149f070e11e55211b721d",
        "id": 172090
    },
    {
        "content": "def rgba_from_argb_int(color):\n    \"\"\"\n    Converts ARGB int into RGBA tuple.\n    \n    Returns:\n        (int, int, int, int)\n            Red, green, blue and alpha channels.\n    \"\"\"\n    \n    a = ((color >> 24) & 0xFF) / 255.\n    r = (color >> 16) & 0xFF\n    g = (color >> 8) & 0xFF\n    b = color & 0xFF\n    \n    return r, g, b, a",
        "sha1": "a92d676f89678556a3382af4642408c5cb72f06c",
        "id": 258171
    },
    {
        "content": "import torch\n\n\ndef train_loop(dataloader, model, loss_fn, optimizer, device):\n    \"\"\"\n    training loop\n    Arguments:\n        dataloader: pytorch dataloader for training\n        model: pytorch model\n        loss_fn: the loss function to train the model\n        optimizer: the optimizer to train the model\n        device: pytorch device (cuda or cpu)\n    Returns:\n        mean_loss(float): the mean loss value\n        correct(float): the mean accuracy\n    \"\"\"\n    model.train()\n    size = len(dataloader.dataset)\n    total_loss = 0\n    correct = 0\n    n_iter = len(dataloader)\n    is_cuda = device.type.find('cuda')!=-1\n    for batch, (X, y, _) in enumerate(dataloader):\n        if is_cuda:\n            X = X.to(device)\n            y = y.to(device)\n        # Compute prediction and loss\n        pred = model(X)\n        loss = loss_fn(pred, y)\n\n        # get the accuracy\n        with torch.no_grad():\n            correct += (pred.softmax(1).argmax(1) == y).type(torch.float).sum().item()\n            total_loss += loss.item()\n\n        # Backpropagation\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if batch % 2 == 0:\n            loss, current = loss.item(), batch * dataloader.batch_size\n            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n    mean_loss = total_loss/n_iter\n    correct /= size\n    print(f\"Train Stats: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {mean_loss:>8f}\")\n    return mean_loss,correct",
        "sha1": "fcc8915209db69b45899ace83dad61a5f027a4a2",
        "id": 73208
    },
    {
        "content": "def _segmentrepr(obj):\n    \"\"\"\n    >>> _segmentrepr([1, [2, 3], [], [[2, [3, 4], [0.1, 2.2]]]])\n    '(1, (2, 3), (), ((2, (3, 4), (0.1, 2.2))))'\n    \"\"\"\n    try:\n        it = iter(obj)\n    except TypeError:\n        return \"%g\" % obj\n    else:\n        return \"(%s)\" % \", \".join(_segmentrepr(x) for x in it)",
        "sha1": "003970affda8acf1e8db149f8f6b40f32006ed8d",
        "id": 274816
    },
    {
        "content": "from datetime import datetime\n\n\ndef exceeds_threshold(last_run_created, threshold):\n    \"\"\"\n    Check whether a Mill run has exceeded the run threshold.\n    This explicitly ignores timezones since the datetime should be UTC.\n    Granularity smaller than a second is ignored.\n\n    :param last_run_created:\n        A datetime object corresponding to the last run.\n    :param threshold:\n        A threshold to check against in hours.\n    :return:\n        True if last_run_created is older than the threshold, and False otherwise.\n        Returns False if the offset is exactly equal to the threshold.\n    \"\"\"\n    last_run_age = datetime.now() - last_run_created\n    last_run_hours = (last_run_age.days * 24) + (last_run_age.seconds / 3600)\n    return last_run_hours > int(threshold)",
        "sha1": "a9fee3e59a35d43c7f0a1e133cf22fea9cdf74ca",
        "id": 73107
    },
    {
        "content": "from typing import List\n\n\ndef parse_coverage(lines: List[str]) -> List[str]:\n    \"\"\"\n    Return a list of source files impacted by a test run.\n\n    The method reads the output of a py.test --cov run and looks for\n    all source files that have > 0% coverage.\n\n    :param lines: Output lines from py.test\n    :return: List of test files.\n    \"\"\"\n    sources = []\n    for line in lines:\n        cols = line.split()\n        if (len(cols) is 4) and (cols[0].endswith('.py')) and ('%' in cols[3]):\n            if int(cols[3].strip('%')) > 0:\n                sources.append(cols[0])\n    return sources",
        "sha1": "6b479a00920e9c95711da541bdeba0ce91c35373",
        "id": 232561
    },
    {
        "content": "def format_fold_run(rep=None, fold=None, run=None, mode=\"concise\"):\n    \"\"\"Construct a string to display the repetition, fold, and run currently being executed\n\n    Parameters\n    ----------\n    rep: Int, or None, default=None\n        The repetition number currently being executed\n    fold: Int, or None, default=None\n        The fold number currently being executed\n    run: Int, or None, default=None\n        The run number currently being executed\n    mode: {\"concise\", \"verbose\"}, default=\"concise\"\n        If \"concise\", the result will contain abbreviations for rep/fold/run\n\n    Returns\n    -------\n    content: Str\n        A clean display of the current repetition/fold/run\n\n    Examples\n    --------\n    >>> format_fold_run(rep=0, fold=3, run=2, mode=\"concise\")\n    'R0-f3-r2'\n    >>> format_fold_run(rep=0, fold=3, run=2, mode=\"verbose\")\n    'Rep-Fold-Run: 0-3-2'\n    >>> format_fold_run(rep=0, fold=3, run=\"*\", mode=\"concise\")\n    'R0-f3-r*'\n    >>> format_fold_run(rep=0, fold=3, run=2, mode=\"foo\")\n    Traceback (most recent call last):\n        File \"reporting.py\", line ?, in format_fold_run\n    ValueError: Received invalid mode value: 'foo'\"\"\"\n    content = \"\"\n\n    if mode == \"verbose\":\n        content += format(\"Rep\" if rep is not None else \"\")\n        content += format(\"-\" if rep is not None and fold is not None else \"\")\n        content += format(\"Fold\" if fold is not None else \"\")\n        content += format(\"-\" if fold is not None and run is not None else \"\")\n        content += format(\"Run\" if run is not None else \"\")\n        content += format(\": \" if any(_ is not None for _ in [rep, fold, run]) else \"\")\n        content += format(rep if rep is not None else \"\")\n        content += format(\"-\" if rep is not None and fold is not None else \"\")\n        content += format(fold if fold is not None else \"\")\n        content += format(\"-\" if fold is not None and run is not None else \"\")\n        content += format(run if run is not None else \"\")\n    elif mode == \"concise\":\n        content += format(\"R\" if rep is not None else \"\")\n        content += format(rep if rep is not None else \"\")\n        content += format(\"-\" if rep is not None and fold is not None else \"\")\n        content += format(\"f\" if fold is not None else \"\")\n        content += format(fold if fold is not None else \"\")\n        content += format(\"-\" if fold is not None and run is not None else \"\")\n        content += format(\"r\" if run is not None else \"\")\n        content += format(run if run is not None else \"\")\n    else:\n        raise ValueError(\"Received invalid mode value: '{}'\".format(mode))\n\n    return content",
        "sha1": "ce14f51a97552d1292cf4fd1e3431ba36929d445",
        "id": 641554
    },
    {
        "content": "def newPage(doc, pno=-1, width=595, height=842):\n    \"\"\"Create and return a new page object.\n    \"\"\"\n    doc._newPage(pno, width=width, height=height)\n    return doc[pno]",
        "sha1": "f908615439b911805d98d45e164ebe41565b5a1a",
        "id": 695217
    },
    {
        "content": "def create_document(bookmark):\n    \"\"\"Creates a Document (a dict) for the search engine\"\"\"\n\n    return {\n        \"id\": str(bookmark.id),\n        \"title\": bookmark.title or \"\",\n        \"notes\": bookmark.notes or \"\",\n        \"tags\": \", \".join([tag.name for tag in bookmark.tags]),\n    }",
        "sha1": "a3af46f6827b725f949056de0372e2d55121874f",
        "id": 683938
    },
    {
        "content": "def string_intersection_v2(s1, s2):\n    \"\"\"\n    The strings are converted into sets and then return the intersection.\n    :param s1:\n    :param s2:\n    :return: sorted set\n    \"\"\"\n    return set(s1).intersection(s2)",
        "sha1": "6eb63fecff73a6a543a202b5b5f29f488307d5d5",
        "id": 461684
    },
    {
        "content": "def filter_outliers(df, columns, percentile=0.95):\n    \"\"\"\n    Remove top 5% of values from each of the selected columns\n\n    Parameters\n    ----------\n    df : dataframe\n    columns : list\n        Column names to perform the filter operation on\n    percentile : float\n        Value from 0 to 1\n\n    Returns\n    -------\n    DataFrame\n        Filtered dataframe\n    \"\"\"\n    filtered_df = df.copy()\n    for col in columns:\n        filter_value = df[col].quantile(percentile)\n        filtered_df = filtered_df.loc[filtered_df[col] < filter_value]\n\n    return filtered_df",
        "sha1": "83c2923e03ee1b241f41b186c53d258f3c16818f",
        "id": 503308
    },
    {
        "content": "from typing import Iterable\nfrom typing import Mapping\n\n\ndef listlike(x):\n    \"\"\"Checks if an object is a list/tuple/set/array etc. Strings and\n    mappings (e.g. dicts) are not considered list-like.\n    \"\"\"\n    return isinstance(x, Iterable) and not isinstance(x, (str, Mapping))",
        "sha1": "67f5eb8411b774d1836549f5c92b3ab8f68a4952",
        "id": 545530
    },
    {
        "content": "def frame2ms(f, frame_rate=30):\n    \"\"\"Convert a frame number to a time in ms\"\"\"\n    return f * 1000 / frame_rate",
        "sha1": "11515b68fe61a63a7a2e72bcdc280c52217d237d",
        "id": 248127
    },
    {
        "content": "def clean_ensembl_id(identifier):\n    \"\"\"\n    Formats an ensembl gene identifier to drop the version number.\n\n    E.g., ENSG00000002822.15 -> ENSG00000002822\n\n    Args:\n        identifier (str)\n\n    Returns:\n        identifier (str)\n\n    \"\"\"\n    return identifier.split('.')[0].upper()",
        "sha1": "abaef14f4bedd33bd9714c9e7df3504652a9899c",
        "id": 57387
    },
    {
        "content": "import collections\n\n\ndef _find_candidate_matches(unmatched_issues, results_list):\n    \"\"\"Returns a dictionary with issue candidates\n\n    For example, let's say we find a new command injection issue in a file\n    which used to have two.  Bandit can't tell which of the command injection\n    issues in the file are new, so it will show all three.  The user should\n    be able to pick out the new one.\n\n    :param unmatched_issues: List of issues that weren't present before\n    :param results_list: main list of current Bandit findings\n    :return: A dictionary with a list of candidates for each issue\n    \"\"\"\n\n    issue_candidates = collections.OrderedDict()\n\n    for unmatched in unmatched_issues:\n        issue_candidates[unmatched] = [\n            i for i in results_list if unmatched == i\n        ]\n\n    return issue_candidates",
        "sha1": "07d371f7d98930fbf4dd39de8082e40629d407ba",
        "id": 257549
    },
    {
        "content": "def get_type(element):\n    \"\"\"\n    Get the type of object as a string\n    :return: String\n    \"\"\"\n    return str(element.__class__.__name__).lower()",
        "sha1": "cc89e69474b2cf81448cdf8ab59c2d9b5649b417",
        "id": 339312
    },
    {
        "content": "def _translate_tags_grobid_to_IOB(tag):\n    \"\"\"\n    Convert labels as used by GROBID to the more standard IOB2 \n    \"\"\"\n    if tag.endswith('other>'):\n        # outside\n        return 'O'\n    elif tag.startswith('I-'):\n        # begin\n        return 'B-'+tag[2:]\n    elif tag.startswith('<'):\n        # inside\n        return 'I-'+tag\n    else:\n        return tag",
        "sha1": "020fe2e8d6cd5bc928fa9edd98d100939a94d533",
        "id": 348015
    },
    {
        "content": "def crc_xmodem_update(crc, data):\n    \"\"\" Calculate CRC for 1 byte.\n\n    Polynomial: x^16 + x^12 + x^5 + 1 (0x1021)\n\n    Initial value: 0x0\n\n    Example:: python\n\n        crc_xmodem_update( crc, 0x34 )\n\n    :param crc: 16 bit CRC value\n    :param data: 8 bit data value\n    \"\"\"\n    # crc must not be higher than 16 bits\n    crc = 0xFFFF & crc\n\n    # data must not be higher than 8 bits\n    data = 0xFF & data\n\n    crc = crc ^ (data << 8)\n    for i in range(8):\n        if crc & 0x8000:\n            crc = (crc << 1) ^ 0x1021\n        else:\n            crc = crc << 1\n\n    # Return only 16 bits\n    return 0xFFFF & crc",
        "sha1": "089de6fe142b3a68389b4ee52976869c01759e8f",
        "id": 343091
    },
    {
        "content": "import mimetypes\n\n\ndef guess_mime_type(filepath):\n    \"\"\"Guess the MIME type for the given file path. If no reasonable guess can\n    be determined, `application/octet-stream` is returned.\n\n    Args:\n        filepath: path to the file\n\n    Returns:\n        the MIME type string\n    \"\"\"\n    return mimetypes.guess_type(filepath)[0] or \"application/octet-stream\"",
        "sha1": "e797da8eb55e2a0843b4d2e58812c3c0481708ed",
        "id": 657540
    },
    {
        "content": "def combo_to_int(value, replace_string='None', replace_with=0):\n    \"\"\"\n    Used for the values of combo quants with numerical values and one string value.\n    The default use case is to replace the 'None' option with a 0.\n    \"\"\"\n    if value == replace_string:\n        return int(replace_with)\n    else:\n        return int(value)",
        "sha1": "7a8b49062a015689c67e83bc00ea9da6da0eeffb",
        "id": 602625
    },
    {
        "content": "def get_github_scores(github):\n    \"\"\"\n    Calculates GitHub Contribution, Activity and Reputation Scores.\n    :param github: GitHub Instance.\n    :return: Tuple of individual scores.\n    \"\"\"\n    github_contribution_score = 0\n    github_activity_score = 0\n    github_reputation_score = 0\n    if github.exists():\n        github = github[0]\n        github_contribution_score = github.contribution_score\n        github_activity_score = github.activity_score\n        github_reputation_score = github.reputation_score\n    return github_contribution_score, github_activity_score, github_reputation_score",
        "sha1": "57ef3fdec58240fc03f6d04f9042e25d845d5717",
        "id": 350036
    },
    {
        "content": "from datetime import datetime\n\n\ndef get_minutes(time_end: datetime, time_start: datetime) -> int:\n    \"\"\"\n    Converts 2 dates to minutes\n    Thank you kite\n    https://www.kite.com/python/answers/how-to-calculate-a-time-difference-in-minutes-in-python#:~:text=Subtract%20one%20datetime%20object%20from,the%20time%20difference%20in%20minutes.\n\n    :param time_end: End timestamp\n    :param time_start: Start timestamp\n    :return: Minutes between them\n    \"\"\"\n    time_delta = (time_end - time_start)\n    total_seconds = time_delta.total_seconds()\n    return int(total_seconds / 60)",
        "sha1": "58f65031897f962798fae78db92c6b11aeeac536",
        "id": 401545
    },
    {
        "content": "import time\n\n\ndef _wait_and_get_responses(job_ids, batch_client):\n    \"\"\"Waits on all given jobs to terminate; returns the job descriptions.\"\"\"\n    job_responses = {}\n    for job in job_ids:\n        terminated = False\n        while not terminated:\n            response = batch_client.describe_jobs(jobs=[job])\n            job_response = response[\"jobs\"][0]\n            status = job_response[\"status\"]\n            if status in {\"FAILED\", \"SUCCEEDED\"}:\n                job_responses[job] = job_response\n                terminated = True\n            else:\n                time.sleep(1)\n    return job_responses",
        "sha1": "4cf9a23dd9b03652b4336d9f4e7d49bd7b212a58",
        "id": 359318
    },
    {
        "content": "def get_source_data_path(config, subdir_num):\n    \"\"\"\n    Return the source data path for the given subdir number\n\n    :param config: Config dictionary\n    :param subdir_num: Number of subdir\n    :return: Full path to given source data\n    \"\"\"\n    # assume data is a list corresponding with subdirs\n    data_paths = config[\"source_data_paths\"]\n    if not isinstance(data_paths, list):\n        raise RuntimeError(\n            \"Unexpected, source_data_paths entry in config should be a list corresponding with subdirs\"\n        )\n    if subdir_num >= len(data_paths):\n        raise RuntimeError(\n            \"Unexpected, could not get source_data_paths entry %d, found %d in list\"\n            % (subdir_num, len(data_paths))\n        )\n\n    return data_paths[subdir_num]",
        "sha1": "ec568f5656a4b216152fe3de6f3ee557cd7defaf",
        "id": 57777
    },
    {
        "content": "def merge_dicts(a, b):\n    \"\"\"Recursively merges two dictionaries, allowing overlapping keys.\"\"\"\n    merged = dict(a)\n    for key, value in b.items():\n        if key in merged:\n            merged[key] = merge_dicts(merged[key], value)\n        else:\n            merged[key] = value\n    return merged",
        "sha1": "ccb2bdb82a749fdfc7bd720b82e9f6d4179c34d5",
        "id": 262381
    },
    {
        "content": "def convert_by_vocab_dict(vocab_dict, items):\n    \"\"\"\n    Converts a sequence of [tokens|ids] according to the vocab dict.\n    \"\"\"\n    output = []\n    for item in items:\n        if item in vocab_dict:\n            output.append(vocab_dict[item])\n        else:\n            output.append(vocab_dict[\"<unk>\"])\n    return output",
        "sha1": "38001a3be3c4abeceb0fba5cb9488ab2f7e93ef3",
        "id": 621701
    },
    {
        "content": "def calc_nhd(tdiff):\n    \"\"\"Return the number of positive elements of an xarray along the time dim.\n\n    \"\"\"\n    nhd_yr = tdiff.where(tdiff > 0).count(dim='time')\n    return nhd_yr",
        "sha1": "0237f5686f6b49a465cd4af536c81cdab53f8674",
        "id": 64478
    },
    {
        "content": "def sort_module_names(modlst):\n    \"\"\"\n    Sort a list of module names in order of subpackage depth.\n\n    Parameters\n    ----------\n    modlst : list\n      List of module names\n\n    Returns\n    -------\n    srtlst : list\n      Sorted list of module names\n    \"\"\"\n\n    return sorted(modlst, key=lambda x: x.count('.'))",
        "sha1": "f8c5f1d9485e84b3dca9607bddcbea174d990432",
        "id": 532273
    },
    {
        "content": "def _check_shape_matmul(mat, order):\n    \"\"\"\n        Checks size of 'mat' and reshapes if necessary.\n    \n        Args:\n            mat - A list/list of lists representing a vector/matrix (see 'matmul')\n            order - Indicates whether mat is a left/right matrix/vector such that\n                    we can broadcast appropriately.\n    \n        Returns:\n            (m,n) - Tuple giving the shape of mat\n            broadcast - Boolean indicating whether we should broadcast list\n    \"\"\"\n    \n    broadcast_list = False\n    if isinstance(mat[0],list):\n        m = len(mat)\n        n = len(mat[0])\n    elif order == 'left':\n        m = 1\n        n = len(mat)\n        broadcast_list = True\n    else:  # order == 'right'\n        m = len(mat)\n        n = 1\n        broadcast_list = True\n    \n    return m, n, broadcast_list",
        "sha1": "c50a2e231b1256a8d831f7100c6c528dcf3b08e6",
        "id": 163726
    },
    {
        "content": "def get_children(tok):\n    \"\"\"\n    Get tok's children as list.\n    \"\"\"\n    return list(tok.children)",
        "sha1": "4b451363d1b520e6b62b0be72afc2ea1ca08621d",
        "id": 494689
    },
    {
        "content": "def is_valid_name(name, parameters, kw):\n    \"\"\"Checks if name is a valid script variable name. Returns the error.\"\"\"\n    if not name.isidentifier():\n        return \"Variable name '{}' is not a valid identifier.\".format(name)\n    if name in kw.KEYWORDS:\n        return \"Variable name '{}' is a keyword.\".format(name)\n    behaviors = parameters.get(kw.BEHAVIORS)\n    if behaviors is not None and name in behaviors:\n        return \"Variable name '{}' equals a behavior name.\".format(name)\n    stimulus_elements = parameters.get(kw.STIMULUS_ELEMENTS)\n    if stimulus_elements is not None and name in stimulus_elements:\n        return \"Variable name '{}' equals a stimulus element name.\".format(name)\n    return None",
        "sha1": "9bfe5b75f2068757c0b434664bbfe65f0d16e8ae",
        "id": 676559
    },
    {
        "content": "def xStr(value, default=''):\n    \"\"\"\n    Extended str() adding a default result, if the input is None\n    \"\"\"\n    return default if (value is None) else str(value)",
        "sha1": "3ba950b312051abf94be259b9b4c1274a0fc47d8",
        "id": 122846
    },
    {
        "content": "def calculate_precision(tp, fp):\n    \"\"\"\n    :param tp: int\n        Number of True Positives\n    :param fp: int\n        Number of False Positives\n    :return: float\n        Precision\n    \"\"\"\n    if tp + fp == 0:\n        return 0\n    return tp / (tp + fp)",
        "sha1": "9d8f49c4b1027cfd18fdbcc88fc14b6ef59bee49",
        "id": 215630
    },
    {
        "content": "def format_stats(stats: dict) -> str:\n    \"\"\"Prepares stats for logging.\"\"\"\n    return \", \".join([f\"{k}: {v} commits\" for k, v in stats.items()])",
        "sha1": "8c554012e7ff2db2ba6394dcb87541ab668ed31c",
        "id": 691468
    },
    {
        "content": "def normalize(probs):\n    \"\"\"Normalizes a list of probabilities, so that they sum up to 1\"\"\"\n    prob_factor = 1 / sum(probs)\n    return [prob_factor * p for p in probs]",
        "sha1": "52ff964125858f40512d1cddccf7311b138d961d",
        "id": 82415
    },
    {
        "content": "def are_present(tags, line):\n    \"\"\"bool: Returns True if all tags are present in line.\"\"\"\n    return all(tag in line for tag in tags)",
        "sha1": "42c96dabfa78ee5fb13aafe68977813889bfee16",
        "id": 315801
    },
    {
        "content": "def pull_rows(client, dref, table_name, start_index=0, count=40000):\n    \"\"\"\n    Query count rows starting at index from table_name in dataset of \n    established bigquery client\n\n    client: bigquery client connection\n    dref: bigquery dataset reference\n    table_name: bigquery dataset table name\n    start_index: starting index of query\n    count: how many rows to query\n\n    Returns a list of bigquery row instances\n    \"\"\"\n    table_ref = dref.table(table_name)\n    table = client.get_table(table_ref)\n    results = [x for x in client.list_rows(table, start_index=start_index, max_results=count)]\n\n    return results",
        "sha1": "21ff6903a216e7ba56ec1221bb4eed6645214016",
        "id": 128538
    },
    {
        "content": "def role_ids_for_account(dynamo_table, account_number):\n    \"\"\"\n    Get a list of all role IDs in a given account by querying the Dynamo secondary index 'account'\n\n    Args:\n        account_number (string)\n\n    Returns:\n        list: role ids in given account\n    \"\"\"\n    role_ids = set()\n\n    results = dynamo_table.query(IndexName='Account',\n                                 KeyConditionExpression='Account = :act',\n                                 ExpressionAttributeValues={':act': account_number})\n    role_ids.update([return_dict['RoleId'] for return_dict in results.get('Items')])\n\n    while 'LastEvaluatedKey' in results:\n        results = dynamo_table.query(IndexName='Account',\n                                     KeyConditionExpression='Account = :act',\n                                     ExpressionAttributeValues={':act': account_number},\n                                     ExclusiveStartKey=results.get('LastEvaluatedKey'))\n        role_ids.update([return_dict['RoleId'] for return_dict in results.get('Items')])\n    return role_ids",
        "sha1": "73ee88813d7ae6ee5a48f89c7e37fe2cdb8e1093",
        "id": 653497
    },
    {
        "content": "import re\n\n\ndef convert_excp_to_err_code(excp_name):\n    \"\"\"Convert Exception class name (CamelCase) to error-code (Snake-case)\"\"\"\n    words = re.findall(r'[A-Z]?[a-z]+|[A-Z]{2,}(?=[A-Z][a-z]|\\d|\\W|$)|\\d+',\n                       excp_name)\n    return '-'.join([str.lower(word) for word in words])",
        "sha1": "0eef10b2c2004510574f28d9177eef6f5c6046e9",
        "id": 189534
    },
    {
        "content": "def csrmvExIsAligned(a, x, y=None):\n    \"\"\"Check if the pointers of arguments for csrmvEx are aligned or not\n\n    Args:\n        a (cupy.cusparse.csr_matrix): Matrix A.\n        x (cupy.ndarray): Vector x.\n        y (cupy.ndarray or None): Vector y.\n\n        Check if a, x, y pointers are aligned by 128 bytes as\n        required by csrmvEx.\n\n    Returns:\n        bool: ``True`` if all pointers are aligned.\n              ``False`` if otherwise.\n\n    \"\"\"\n\n    if a.data.data.ptr % 128 != 0:\n        return False\n    if a.indptr.data.ptr % 128 != 0:\n        return False\n    if a.indices.data.ptr % 128 != 0:\n        return False\n    if x.data.ptr % 128 != 0:\n        return False\n    if y is not None and y.data.ptr % 128 != 0:\n        return False\n    return True",
        "sha1": "5d4423b4134c645eda6d31014c9cf230ce9e4cee",
        "id": 255755
    },
    {
        "content": "import re\n\n\ndef is_contain_zh(val):\n    \"\"\" \u5224\u65ad\u53c2\u6570\u662f\u5426\u5305\u542b\u4e2d\u6587 \"\"\"\n    zh_pattern = re.compile(u'[\\u4e00-\\u9fa5]+')\n    match = zh_pattern.search(str(val))\n    return True if match else False",
        "sha1": "e6a99244f54a8f61529341ce5869ff73ac804fb3",
        "id": 256645
    },
    {
        "content": "def get_output_at(output, at):\n\t\"\"\"\n\tget one output\n\n\t\"\"\"\n\treturn output[at]",
        "sha1": "e5c138a23836d0a98ca54f3363ad455bc3753455",
        "id": 572998
    },
    {
        "content": "import json\n\n\ndef get_blocks_from_s3_ref(blocks_s3_ref: str, s3_client) -> list:\n    \"\"\"Return a list of blocks from an S3 reference file.\"\"\"\n    blocks = json.loads(s3_client.get_object_from_s3(blocks_s3_ref))\n    for b in blocks:\n        if 'parentBlockIndex' in b:\n            del b['parentBlockIndex']\n        if 'blockIndex' in b:\n            del b['blockIndex']\n    return blocks",
        "sha1": "1b685ba120bbd10a4e1c7c58ad9bbc61ea39e63d",
        "id": 56574
    },
    {
        "content": "def marginal_topic_distrib(doc_topic_distrib, doc_lengths):\n    \"\"\"\n    Return marginal topic distribution ``p(T)`` (topic proportions) given the document-topic distribution (theta)\n    `doc_topic_distrib` and the document lengths `doc_lengths`. The latter can be calculated with\n    :func:`~tmtoolkit.bow.bow_stats.doc_lengths`.\n\n    :param doc_topic_distrib: document-topic distribution; shape NxK, where N is the number of documents, K is the\n                              number of topics\n    :param doc_lengths: array of size N (number of docs) with integers indicating the number of terms per document\n    :return: array of size K (number of topics) with marginal topic distribution\n    \"\"\"\n    unnorm = (doc_topic_distrib.T * doc_lengths).sum(axis=1)\n    return unnorm / unnorm.sum()",
        "sha1": "b9593782a176d4f98eeaf606dee8f0ab1a82f22b",
        "id": 366050
    },
    {
        "content": "def nested_loops(a):\n  \"\"\"A test function illustrating nested loops.\"\"\"\n  for i in range(a):\n    while True:\n      break\n      unreachable = 10\n    for j in range(i):\n      for k in range(j):\n        if j * k > 10:\n          continue\n          unreachable = 5\n      if i + j == 10:\n        return True\n  return False",
        "sha1": "716b55e1900410f435b5c2fe3ad0925130f17921",
        "id": 391818
    },
    {
        "content": "def _is_items(lst):\n    \"\"\"Is ``lst`` an items list?\n    \"\"\"\n    try:\n        return [(a, b) for a, b in lst]\n    except ValueError:\n        return False",
        "sha1": "1a8362cd5c91dd799469682d729d2f9c735dd795",
        "id": 672738
    },
    {
        "content": "def rotations(S):\n    \"\"\"\n    Returns all the rotations of a string\n    \"\"\"\n    L=list(S)\n    L2=list()\n    for i in range(0, len(L)):\n        L2.append(''.join(L[i:] + L[:i]))\n    return L2",
        "sha1": "a59c1d1daa9df37581ef670c2744351c61bd83c9",
        "id": 62629
    },
    {
        "content": "def qualified_field(alias, field):\n    \"\"\"\n    Qualifies the SQL `field` with `alias`. If `alias` is empty,\n    then no qualification is used. (Just `field` is returned.)\n    \"\"\"\n    if not alias:\n        return field\n    else:\n        return '%s.%s' % (alias, field)",
        "sha1": "8061057296a40b4e7fb361cc00c0aa1f4bb75abd",
        "id": 140331
    },
    {
        "content": "import json\n\n\ndef _parse_graph(graph_json):\n    \"\"\"Parse and extract the NNVM graph.\n\n    Parameters\n    ----------\n    graph_json : str or graph class\n        The graph to be deployed in json format output by nnvm graph.\n\n    Returns\n    -------\n    nodes_list : list\n        List of all the nodes presented in the graph\n\n    heads_list : list\n        List of all output nodes presented in the graph\n\n    shapes_list: list\n        List of shape of each nodes presented in the graph\n\n    dltype_list: list\n        List of data type of each nodes presented in the graph\n    \"\"\"\n    json_obj = json.loads(graph_json)\n    nodes_list = json_obj['nodes']\n    dltype_list = json_obj['attrs']['dltype']\n    shapes_list = json_obj['attrs']['shape']\n    heads_list = json_obj['heads']\n    return nodes_list, dltype_list, shapes_list, heads_list",
        "sha1": "a0a434fe12c2e31be6fbff6e1b4a407a89ff5089",
        "id": 308404
    },
    {
        "content": "def get_assignment_id(data_store, assignment_resource_id):\n  \"\"\"Returns an assignment ID string from an assignment resource ID.\n\n  Args:\n    data_store: data_store.DataStore to read from if the assignment ID is not\n      cached.\n    assignment_resource_id: resource_id.FalkenResourceId of the assignment.\n\n  Returns:\n    assignment_id string.\n  \"\"\"\n  assignment = data_store.read(assignment_resource_id)\n  return assignment.assignment_id",
        "sha1": "e6983eb1a9d132b5975c769738a3169979fccc88",
        "id": 195345
    },
    {
        "content": "def percent_pos_to_step_num(percent_pos, max_steps):\n    \"\"\" Calculate step number from percentage position.   Note that 100% means fully up.\"\"\"\n    return round((1 - percent_pos) * max_steps)",
        "sha1": "32bc641a55c0d07970e33b7e604852c24db4f4d0",
        "id": 396258
    },
    {
        "content": "def is_1D_graph(g):\n    \"\"\"\n    Check a graph to see if it is 1D, or\n    a chain of nodes with one or zero parents\n    and children.\n\n    Parameters\n    ------------\n    g : networkx Graph\n\n    Returns\n    ------------\n    is_1D : bool\n        Is graph 1D or not\n    \"\"\"\n    # check degree of sucessors\n    for v in g.succ.values():\n        if len(v) not in [0, 1]:\n            return False\n\n    # check degree of predecessors\n    for v in g.pred.values():\n        if len(v) not in [0, 1]:\n            return False\n\n    # made it through all checks\n    return True",
        "sha1": "6967ca4e7bceb68841b8a2275f16bc0911d675d4",
        "id": 86015
    },
    {
        "content": "def luminance_to_contrast_ratio(luminance1, luminance2):\n    \"\"\"Calculate contrast ratio from a pair of relative luminance.\n\n    :param luminance1: Relative luminance\n    :type luminance1: float\n    :param luminance2: Relative luminance\n    :type luminance2: float\n    :return: Contrast ratio\n    :rtype: float\n    \"\"\"\n    (l1, l2) = sorted((luminance1, luminance2), reverse=True)\n    return (l1 + 0.05) / (l2 + 0.05)",
        "sha1": "bdd0adb7a01dc822a517f7472aa678749d994030",
        "id": 222856
    },
    {
        "content": "def isnum(x):\n    \"\"\"Test whether an object is an instance of a built-in numeric type.\"\"\"\n    for T in int, float, complex:\n        if isinstance(x, T):\n            return 1\n    return 0",
        "sha1": "1c38df6baa8f863465ecc0ae3cc327fbada19f2c",
        "id": 518672
    },
    {
        "content": "import inspect\n\n\ndef signatures_union(\n    init_sig: inspect.Signature, method_sig: inspect.Signature\n) -> inspect.Signature:\n    \"\"\"Returns a Union of the constructor and method signature.\n\n    Args:\n        init_sig (inspect.Signature): Constructor signature\n        method_sig (inspect.Signature): Method signature\n\n    Returns:\n        A Union of the the two Signatures as a single Signature\n    \"\"\"\n\n    def key(param):\n        # all params are keyword or positional\n        # move the params without defaults to the front\n        if param.default is inspect._empty:\n            return -1\n        return 1\n\n    params = list(init_sig.parameters.values()\n                 ) + list(method_sig.parameters.values())\n    params.sort(key=key)\n    return inspect.Signature(\n        parameters=params, return_annotation=method_sig.return_annotation\n    )",
        "sha1": "dd2bce01909005cbbeff6e9e49487f994c1b7b14",
        "id": 417288
    },
    {
        "content": "from typing import Union\nimport requests\nimport random\n\n\ndef new_passphrase(\n    words: int = 4,\n    num: int = 1,\n    seperator: str = \"-\",\n    dice_rolls: int = 4,\n    dice_sides: int = 6,\n    list_url: str = \"https://www.eff.org/files/2016/09/08/eff_short_wordlist_1.txt\",\n) -> Union[str, list]:\n    \"\"\"Generates one or more secure passphrases.\n\n    Args:\n        words (int, optional): The number of words to comprise the resultant passphrase. Defaults to 4.\n        num (int, optional): The number of passphrases to generate. Defaults to 1.\n        seperator (str, optional): The string separator between words. Defaults to '-'.\n        dice_rolls (int, optional): Per the word list, how many dice rolls for each word lookup. Defaults to 4.\n        dice_sides (int, optional): Per the word list, how many sides does the dice have. Defaults to 6.\n        list_url (str, optional): The word list. Tab delimited, two column. Defaults to \"https://www.eff.org/files/2016/09/08/eff_short_wordlist_1.txt\".\n\n    Returns:\n        str: One Passphrase\n        list: Multiple Passphrases\n    \"\"\"\n    data = (requests.get(list_url)).text.splitlines()\n    eff_list = dict([l.split(\"\\t\") for l in data])\n    str_out = \"\"\n    list_out = []\n    for _ in range(num):\n        for w in range(words):\n            result = \"\"\n            for _ in range(dice_rolls):\n                result += str(random.randint(1, dice_sides))\n            if w == 0:\n                str_out += f\"{eff_list[result]}\"\n            else:\n                str_out += f\"{seperator}{eff_list[result]}\"\n        if num == 1:\n            return str_out\n        else:\n            list_out.append(str_out)\n            str_out = \"\"\n    return list_out",
        "sha1": "de9f72c0d889c612a8e8e0c061b5c604c53213e8",
        "id": 597110
    },
    {
        "content": "def and_(a, b):\n    \"\"\"\n    Combines two :any:`Filters<Filter>` with an \"and\" operation, matching\n    any results that match both of the given filters.\n\n    :param a: The first filter to consider.\n    :type a: Filter\n    :param b: The second filter to consider.\n    :type b: Filter\n\n    :returns: A filter that matches both a and b\n    :rtype: Filter\n    \"\"\"\n    return a.__and__(b)",
        "sha1": "50c29034f2a48fc12ed20a088b452a83b22e942f",
        "id": 228846
    },
    {
        "content": "def replace_corrupted_instruction(script, instruction_index):\n\n    \"\"\"\n    Return the copy of a script with the command (JMP or NOP) of a given instruction replaced.\n\n    \"jmp\" -> \"nop\"\n    \"nop\" -> \"jmp\"\n    \"\"\"\n\n    # Copy the script and get the instruction by index.\n    new_script = script.copy()\n    instruction = script[instruction_index]\n\n    # Replace the instruction.\n    old, new = (\"jmp\", \"nop\") if \"jmp\" in instruction else (\"nop\", \"jmp\")\n    new_script[instruction_index] = instruction.replace(old, new)\n\n    # Return the new script.\n    return new_script",
        "sha1": "583f3cdd3dc7a05d232e2c87825c3e166b253535",
        "id": 638611
    },
    {
        "content": "from pathlib import Path\nimport yaml\n\n\ndef load_plot_these_methods_config(path):\n    \"\"\"\n    Loads PLOT_THESE_METHODS.yaml present at `path`.\n\n    Args:\n        path (str): path where config files can be found.\n\n    Returns:\n        (set): a set of method that needs to be plotted. an empty set if the file is not found. \n    \"\"\"\n    include_methods = Path(path).resolve() / \"PLOT_THESE_METHODS.yaml\"\n    if include_methods.exists():\n        with open(str(include_methods), \"rb\") as f:\n            plot_these_methods = yaml.safe_load(f)\n        return set([x for x, plot in plot_these_methods.items() if plot])\n\n    return {}",
        "sha1": "17f5951d82a3ad24747cbe868743d436275b6082",
        "id": 160088
    },
    {
        "content": "def mutate(df, **kwargs):\n    \"\"\"\n    Creates new variables (columns) in the DataFrame specified by keyword\n    argument pairs, where the key is the column name and the value is the\n    new column value(s).\n\n    Args:\n        df (pandas.DataFrame): data passed in through the pipe.\n\n    Kwargs:\n        **kwargs: keys are the names of the new columns, values indicate\n            what the new column values will be.\n\n    Example:\n        diamonds >> mutate(x_plus_y=X.x + X.y) >> select_from('x') >> head(3)\n\n              x     y     z  x_plus_y\n        0  3.95  3.98  2.43      7.93\n        1  3.89  3.84  2.31      7.73\n        2  4.05  4.07  2.31      8.12\n    \"\"\"\n\n    return df.assign(**kwargs)",
        "sha1": "874fec9cc6d756c57b9002fcb7c5c3131a29a9a8",
        "id": 67588
    },
    {
        "content": "def bids_for_doc(doc_top_list, td, reviewers):\n    \"\"\"\n    Generate bids for the document for all provided reviewers\n    \"\"\"\n    bids = []\n    for rev in reviewers:\n        rev_top_dict = dict(td.rev_top[rev.name()])\n        score = 0\n        for t_id, t_prob in doc_top_list:\n            score += rev_top_dict.get(t_id, 0) * t_prob\n        bids.append(score)\n    return bids",
        "sha1": "5dbcc3246734c444147c75b0eaaf4db2c30b42d3",
        "id": 643637
    },
    {
        "content": "def tone_filter_color_assigner(color_list: list) -> list:\n    \"\"\"\n    Author: Jeremy Trendoff\n\n    Returns the RGB values of the colors selected in the \n    two, three tone filters.\n\n    Accepts a list of colors inputed by the user and assigns the related RGB \n    value to the coorisponding position in a list.\n\n    >>> tone_filter_color_assigner(['red', 'blue'])\n    [(255,0,0), (0,255,0)]\n    \"\"\"\n\n    # Variables\n    colors = ['black', 'white', 'red', 'lime', 'blue', 'yellow', 'cyan',\n              'magenta', 'gray']\n    rgbs = [(0, 0, 0), (255, 255, 255), (255, 0, 0), (0, 255, 0), (0, 0, 255),\n            (255, 255, 0), (0, 255, 255), (255, 0, 255), (128, 128, 128)]\n\n    return_list = []\n\n    # Color Processing\n    for color in color_list:\n        for i in range(len(colors)):\n            if (color == colors[i]):\n                return_list.append(rgbs[i])\n\n    return return_list",
        "sha1": "ec1791d72a8acba94077d954d4078cb4c4982ced",
        "id": 538973
    },
    {
        "content": "from pathlib import Path\nimport yaml\nimport json\n\n\ndef print_results_from_evaluation_dirs(work_dir_path: Path, run_numbers: list,\n                                       print_results_only: bool = False) -> None:\n    \"\"\"Print the aggregated results from multiple evaluation runs.\"\"\"\n\n    def float_representer(dumper, value):\n        text = '{0:.4f}'.format(value)\n        return dumper.represent_scalar(u'tag:yaml.org,2002:float', text)\n\n    yaml.add_representer(float, float_representer)\n\n    for run_number in run_numbers:\n        eval_dir_path = work_dir_path / f'evaluation_{run_number}'\n        eval_file_name = f'evaluation_results_{run_number}.json'\n        print(f'--- Evaluation summary run {run_number} ---')\n        with open(eval_dir_path / eval_file_name, 'r') as infile:\n            results = json.load(infile)\n            test_set_name = results['test_set_name']\n            if print_results_only:\n                results = {key: val for key, val in results.items() if 'result' in key}\n                results['test_set_name'] = test_set_name\n            print(yaml.dump(results))",
        "sha1": "4be2d893da5f321390c4b49cd4283c0b6f98b4d5",
        "id": 7463
    },
    {
        "content": "def USA_map(go,tls,df):\n    \"\"\"\n    Fucntion to create an interactive USA map visualization that break down\n    the number of jobs for each state\n    \"\"\"\n\n    fig = go.Figure(data=go.Choropleth(locations=df['CODE'],\n                                       z = df['counts'].astype(float),\n                                       locationmode = 'USA-states',\n                                       colorscale = 'Reds',\n                                       colorbar_title = \"No of jobs\"))\n\n    fig.update_layout(title_text = 'Jobs Distribution around the US',title_x=0.5,geo_scope='usa')\n    return fig",
        "sha1": "0af857f49d6214882f17123bb688cfa7ccf7e286",
        "id": 365917
    },
    {
        "content": "def extract_masked_part_from_spectrogram(mask, spectrogram):\n    \"\"\" Extract the masked part of the spectrogram\n    \"\"\"\n    return spectrogram[:,mask]",
        "sha1": "76be4d93aca3684edfe374bcce0ce09b833ab687",
        "id": 697624
    },
    {
        "content": "def get_instr_pos(qobj, exp_index, name):\n    \"\"\"Return all locations of QasmQobjInstruction in a Qobj experiment.\n\n    The return list is sorted in reverse order so iterating over it\n    to insert new items will work as expected.\n\n    Args:\n        qobj (Qobj): a Qobj object\n        exp_index (int): The index of the experiment in the qobj\n        name (str): QasmQobjInstruction name to find\n\n    Returns:\n        list[int]: A list of positions where the QasmQobjInstruction is located.\n    \"\"\"\n    # Check only the name string of the item\n    positions = [i for i, val in enumerate(qobj.experiments[exp_index].instructions)\n                 if val.name == name]\n    return positions",
        "sha1": "8abf69db8db879aaca5133a01d3e68ba7a19296c",
        "id": 449160
    },
    {
        "content": "def hello(friend_name):\n    \"\"\"\n    Return hello message for a friend\n    :param: friend_name: the person's name\n    :return: String containing the message\n    \"\"\"\n    return f'Hello, {friend_name}!'",
        "sha1": "fe9d859644ee3505087aab060cdb5261a1c0afe4",
        "id": 107807
    },
    {
        "content": "def _doublet(plist, J):\n    \"\"\"\n    Applies a *J* coupling to each signal in a list of (frequency, intensity)\n    signals, creating two half-intensity signals at +/- *J*/2.\n\n    Parameters\n    ---------\n    plist : [(float, float)...]\n        a list of (frequency{Hz}, intensity) tuples.\n    J : float\n        The coupling constant in Hz.\n\n    Returns\n    -------\n    [(float, float)...]\n        a list of (frequency, intensity) tuples.\n    \"\"\"\n    res = []\n    for v, i in plist:\n        res.append((v - J / 2, i / 2))\n        res.append((v + J / 2, i / 2))\n    return res",
        "sha1": "5b88ec2889bcbfecf6001e1c84fabe835ed4f678",
        "id": 693569
    },
    {
        "content": "def convective_mt_coeff(Dm, dp, epsilon, Re, Sc):\n    \"\"\"\n    Estimate the convective mass transfer coefficient using Wilson and\n    Geankoplis correlation.\n\n    .. math::  k_f = \\\\frac{Sh D_m}{d_p}\n\n    where\n\n    .. math::  Sh = 1.09 \\\\varepsilon^{-1} Re^{0.33} Sc^{0.33}, \\\\quad 0.0015 < Re < 55\n    .. math::  Sh = 0.25 \\\\varepsilon^{-1} Re^{0.69} Sc^{0.33}, \\\\quad 55 < Re < 1050\n\n    Parameters\n    ----------\n    Dm : float\n        Molecular diffusion coefficient [m^2/s]\n    dp : float\n        Particle diameter [m]\n    epsilon : float\n        Bed porosity [-]\n    Re : float\n        Reynolds number [-]\n    Sc : float\n        Schmidt number [-]\n\n    Returns\n    -------\n    kf : float\n        Convective mass transfer coefficient [m/s]\n\n    Raises\n    ------\n    ValueError\n        If Re is not in range 0.0015 - 1050.\n\n    Example\n    -------\n    >>> convective_mt_coeff(4.79e-10, 5e-6, 0.335, 7.21e-3, 1452.0)\n    6.77e-4\n\n    References\n    ----------\n    E.J. Wilson, and C.J. Geankoplis. Liquid mass transfer at very low\n    Reynolds numbers in packed beds. Ind. Eng. Chem. Fund., 1966, 5 (1), 9.\n\n    D.M Ruthven. Principles of Adsorption and Adsorption Processes.\n    John Wiley & Sons, Inc., New York, 1984.\n    \"\"\"\n    if 0.0015 < Re < 55:\n        Sh = 1.09 * epsilon**-1 * Re**0.33 * Sc**0.33\n    elif 55 <= Re < 1050:\n        Sh = 0.25 * epsilon**-1 * Re**0.69 * Sc**0.33\n    else:\n        raise ValueError(\n            f'Correlation not applicable in the given conditions. \\n'\n            f'Re must be in rage 0.0015 < Re < 1050. It is {Re}. \\n'\n        )\n    kf = Sh * Dm / dp\n    return kf",
        "sha1": "5949c3c70b53e159e6cb39ceee8a86f4f9b6f490",
        "id": 466965
    },
    {
        "content": "def getChild(elem, index):\n    \"\"\"\n    Get a child of the DOM element by specifying an index.\n    \"\"\"\n    count = 0\n    child = elem.firstChild\n    while child:\n        next = child.nextSibling\n        if child.nodeType == 1:\n            if index == count:\n                return child\n            count += 1\n        child = next\n    return None",
        "sha1": "e593fbb91507fde374e3fd235d2e1490c0ba566e",
        "id": 384949
    },
    {
        "content": "import logging\n\n\ndef docker_obj_status(name, obj_type, docker_helper, logger=None):\n    \"\"\"\n    Determine if a specific docker object like an image, container, or network\n    exists, and if it is \"owned\" by the current project\n\n    Args:\n        name: str or list, of the name(s) of the object\n        obj_type: str, one of 'network', 'container', 'image', or 'image_bare'\n            NOTE:\n                'image' includes the tag in the image name. Like: 'stuff:1.0'\n                'image_bare' is the image name without the tag.\n        docker_helper: DockerHelper, to use for querying docker objects\n        logger: Logger, for to use for logging\n    Return:\n        list of dicts.\n        dict structure:\n            {\n                'name': str, of the object name\n                'exists': bool, whether and object with 'name' exists.\n                'owned': bool, whether the object is owned by the specific project\n            }\n    \"\"\"\n    if logger:\n        log = logger\n    else:\n        log = logging.getLogger('docker_obj_status')\n    result = list()\n    res_tmpl = {\n        'name': None,\n        'exists': False,\n        'owned': False\n    }\n    owned_obj = list()\n    all_obj = list()\n    if obj_type == 'network':\n        owned_networks = docker_helper.get_networks()[1]\n        owned_obj = [net['name'] for net in owned_networks]\n        all_networks = docker_helper.get_networks(return_all=True)[1]\n        all_obj = [net['name'] for net in all_networks]\n    elif obj_type == 'container':\n        owned_containers = docker_helper.get_containers()[1]\n        owned_obj = [cont['name'] for cont in owned_containers]\n        all_containers = docker_helper.get_containers(return_all=True)[1]\n        all_obj = [cont['name'] for cont in all_containers]\n    elif obj_type == 'image':\n        owned_obj = docker_helper.get_images()[1]\n        all_obj = docker_helper.get_images(return_all=True)[1]\n    elif obj_type == 'image_bare':\n        owned_images = docker_helper.get_images()[1]\n        owned_obj = [image.split(':')[0] for image in owned_images]\n        all_images = docker_helper.get_images(return_all=True)[1]\n        all_obj = [image.split(':')[0] for image in all_images]\n    else:\n        log.warning(\"Unknown docker object type: '%s'\", obj_type)\n    if isinstance(name, list):\n        for nme in name:\n            res = dict(res_tmpl)\n            res['name'] = nme\n            if nme in owned_obj:\n                res['exists'] = True\n                res['owned'] = True\n            elif nme in all_obj:\n                res['exists'] = True\n            result.append(res)\n    else:\n        res = dict(res_tmpl)\n        res['name'] = name\n        if name in owned_obj:\n            res['exists'] = True\n            res['owned'] = True\n        elif name in all_obj:\n            res['exists'] = True\n        result.append(res)\n    return result",
        "sha1": "d481ec289dd0255edbef3aae2eb99e68b4758ea6",
        "id": 321919
    },
    {
        "content": "def create_hello_world_cpp() -> str:\n    \"\"\"\n    Fixture which returns a simple cpp hello world code\n    \"\"\"\n\n    return '#include<iostream>\\n using namespace std; int main() {cout << \"Hello World\"; return 0;}'",
        "sha1": "c86c0643dddefc9d653e73ab8e24dfbfe80af172",
        "id": 88500
    },
    {
        "content": "def __parse_gfc_entry(line):\n    \"\"\"Return the values for both coefficients in a GFC file line.\"\"\"\n    sline = line.split()\n    n = int(sline[1])\n    m = int(sline[2])\n\n    return n, m, float(sline[3]), float(sline[4])",
        "sha1": "036da71761cd5403adf744f68ac9af1ff276ceb6",
        "id": 85776
    },
    {
        "content": "def getProperty( data, name, default ):\n    \"\"\"\n    Read a property from an LDAP data structure.\n    Return a default value if name-value pair does not exist.\n    \"\"\"\n    try:\n        return data[name]\n    except (IndexError, KeyError):\n        return default",
        "sha1": "0486dc6f8c56650976231f9aa40c8d25af5fb2e3",
        "id": 489132
    },
    {
        "content": "from datetime import datetime\n\n\ndef dt_to_stamp(dt):\n\t\"\"\" Return UTC timestamp for datetime dt. \"\"\"\n\treturn (dt - datetime(1970, 1, 1)).total_seconds()",
        "sha1": "c0760a9b9dc7e0805eb87edb5ddc3865c0e13646",
        "id": 323337
    },
    {
        "content": "def getPagesLinkingToPage(page, corpus):\n    \"\"\"\n    Returns a list of all the pages that link to a given page\n    \"\"\"\n    linkingPages = []\n\n    for memberPage in corpus:\n        if(page in corpus[memberPage]):\n            linkingPages.append(memberPage)\n\n    return linkingPages",
        "sha1": "1d479ee18aba86c6112edf37b1860b6d37af35b6",
        "id": 422231
    },
    {
        "content": "def find_nested_path(field, es_mapping):\n    \"\"\"\n    Returns path to 'highest' level nested field, in other words the first field mapped with type=nested\n    found by traversing the given field from the *top* level.\n\n    This function relies on information about the structure of the es_mapping to extract\n    the *path to the object who's mapping is nested*. The comments within this function try to explain\n    that structure. This information is needed to construct nested queries (it is the path).\n    It returns None if the given (sub)field is not a member of a type=nested mapped field.\n\n    :param field: the *full path* to the field we are filtering/aggregating on.\n                        For example: \"experiments_in_set.biosample.biosource.individual.organism.name\"\n    :param es_mapping: dictionary representation of the es_mapping of the type we are searching on\n    :return: path for nested query or None\n    \"\"\"\n    location = es_mapping\n    possible_nested_paths = []\n    path = []\n    for cursor in field.split('.'):\n        if cursor == 'raw':  # if we get to this point we're definitely at a leaf and should stop\n            break\n        if cursor not in location:  # its possible we are at a sub-embedded object boundary. Check if it has properties.\n            if 'properties' not in location:  # if it doesn't have properties, there's nowhere to go, so return None.\n                return None\n            location = location['properties']  # else move location forward, but do not add it to the PATH\n        if cursor not in location:  # if we still don't see our 'level', we are not a nested field\n            break   # accumulated path will be discarded (not added to possible_nested_paths)\n        location = location[cursor]\n        path.append(cursor)\n        if location.get('type', None) == 'nested':  # this could be a path\n            possible_nested_paths.append('.'.join(path))\n    # the last path added is the closest in proximity to the field and thus is correct\n    return possible_nested_paths[-1] if possible_nested_paths else None",
        "sha1": "501cf9771d023e74c32502df4b1fc8f18169a25c",
        "id": 396714
    },
    {
        "content": "def replicaset_config(client):\n    \"\"\"\n    Return the replicaset config document\n    https://docs.mongodb.com/manual/reference/command/replSetGetConfig/\n    \"\"\"\n    rs = client.admin.command('replSetGetConfig')\n    return rs",
        "sha1": "5d2b3e2801fa96b6186a24edb00d849285dd9045",
        "id": 166900
    },
    {
        "content": "def read_file(file_path):\n    \"\"\"\n    Reads input file.\n\n    Args:\n        file_path (str): path of input file.\n\n    Returns:\n        list: content of the file.\n    \"\"\"\n    with open(file_path, 'r') as file:\n        return file.read().strip().split('\\n')",
        "sha1": "27008dcbd9bd9336240e68c9514ae6170c18df78",
        "id": 28795
    },
    {
        "content": "def selection_sort(items):\n    \"\"\"Sort given items by finding minimum item, swapping it with first\n    unsorted item, and repeating until all items are in sorted order.\n    Running time: O(n**2) As it loops through the whole array for each element\n    Memory usage: O(1) Sorting is done in place on the array\n    \"\"\"\n    # Loop through each index\n    for i in range(len(items)):\n        # Find the smallest unsorted value\n        ind = items.index(min(items[i:]), i)\n        # Switch it with the current index\n        items[ind], items[i] = items[i], items[ind]\n    return items",
        "sha1": "d7396df561adc9805d5e91dcbfa3ddd3e67018a8",
        "id": 146046
    },
    {
        "content": "def get_metadata_filename(preprocess_id, version=None):\n    \"\"\"Return a filename used for a preprocess download\"\"\"\n    assert preprocess_id, \"preprocess_id cannot be None\"\n    assert str(preprocess_id).isdigit(), \"preprocess_id must be numeric\"\n\n    if not version:\n        version = '1.0'\n\n    version = str(version).replace('.', '-')\n\n    fname = 'metadata_%s_v%s.json' % (preprocess_id, version)\n\n    return fname",
        "sha1": "27c4727db2fe59a1793d2693d8a52ada357c264d",
        "id": 396453
    },
    {
        "content": "def format_result(result, info):\n    \"\"\"\n    Formats the given resultset.\n\n    @param result: the result to format.\n    @param info:   return only the header information.\n    \"\"\"\n\n    if info:\n\n        return '\\n'.join([\n            'Domain: ' + str(result.domain),\n            'Request ID: ' + str(result.request_id),\n            'State: ' + str(result.state),\n            'Comment: ' + str(result.comment),\n            'Created: ' + str(result.created)])\n\n    else:\n        return str(result)",
        "sha1": "f95c7ef1ba2634721e681e1b6ef4949b67c4e825",
        "id": 108914
    },
    {
        "content": "def deduplicate(sequence):\n    \"\"\"\n    Return a list with all items of the input sequence but duplicates removed.\n\n    Leaves the input sequence unaltered.\n    \"\"\"\n    return [element\n            for index, element in enumerate(sequence)\n            if element not in sequence[:index]]",
        "sha1": "53f8c1933ef5afe0c2b7571bd379214437d17339",
        "id": 454772
    },
    {
        "content": "from typing import OrderedDict\n\n\ndef sort_dict_by_mrv(d):\n    \"\"\"Sort dictionary according to minimun remaining values heuristic.\"\"\"\n    return OrderedDict(sorted(d.items(), key=lambda x: len(x[1]) if isinstance(x[1], set) else 0))",
        "sha1": "0158fad1f33f2845977ef78406be4c03d83b3f53",
        "id": 390409
    },
    {
        "content": "def usage(err=''):\n    \"\"\"  Prints the Usage() statement for the program    \"\"\"\n    m = '%s\\n' %err\n    m += '  Default usage is to seach the Scan library tree for your branch.\\n'\n    m += ' '\n    m += '    walklib  \\n'\n    m += '      or\\n'\n    m += '    walklib -w tech_rules \\n'\n    m += '      or\\n'\n    return m",
        "sha1": "63468270bc890eb9715b6f4bccbad4babf404cb5",
        "id": 474757
    },
    {
        "content": "def reduce_by_multiple(trajectory, integer):\n    \"\"\"\n    keep only control points in given trajectory which are multiple of @integer\n    :param trajectory: array that describes the trajectory\n    :param integer: the multiple used for reducing\n    :return: the reduced trajectory\n    \"\"\"\n    reduced = trajectory[0:len(trajectory) - 1:integer]\n    if len(reduced) == 0:\n        return trajectory\n    if reduced[-1] != trajectory[len(trajectory) - 1]:\n        reduced.append(trajectory[len(trajectory) - 1])\n    return reduced",
        "sha1": "7447b3f3d96fe5d65e28d7630fed5ca88a365109",
        "id": 492546
    },
    {
        "content": "def get_story_memcache_key(story_id, version=None):\n    \"\"\"Returns a memcache key for the story.\n\n    Args:\n        story_id: str. ID of the story.\n        version: str. Schema version of the story.\n\n    Returns:\n        str. The memcache key of the story.\n    \"\"\"\n    if version:\n        return 'story-version:%s:%s' % (story_id, version)\n    else:\n        return 'story:%s' % story_id",
        "sha1": "adba3b1b04e95847d94b593a0d49eaa6867eefbf",
        "id": 114814
    },
    {
        "content": "import torch\n\n\ndef smooth_l1_loss(bbox_pred, bbox_targets, bbox_inside_weights, bbox_outside_weights, beta=1.0):\n    \"\"\"\n    SmoothL1(x) = 0.5 * x^2 / beta      if |x| < beta\n                  |x| - 0.5 * beta      otherwise.\n    1 / N * sum_i alpha_out[i] * SmoothL1(alpha_in[i] * (y_hat[i] - y[i])).\n    N is the number of batch elements in the input predictions\n    \"\"\"\n    box_diff = bbox_pred - bbox_targets\n    in_box_diff = bbox_inside_weights * box_diff\n    abs_in_box_diff = torch.abs(in_box_diff)\n    smoothL1_sign = (abs_in_box_diff < beta).detach().float()\n    in_loss_box = smoothL1_sign * 0.5 * torch.pow(in_box_diff, 2) / beta + \\\n                  (1 - smoothL1_sign) * (abs_in_box_diff - (0.5 * beta))\n    out_loss_box = bbox_outside_weights * in_loss_box\n    loss_box = out_loss_box\n    N = loss_box.size(0)  # batch size\n    loss_box = loss_box.view(-1).sum(0) / N\n    return loss_box",
        "sha1": "04b809b51b1cd5fe3aab4a3c8b5288752932cd06",
        "id": 576479
    },
    {
        "content": "def create_line_item_config(name, order_id, placement_ids, ad_unit_ids, cpm_micro_amount, sizes, hb_bidder_key_id,\n                            hb_pb_key_id, hb_bidder_value_id, hb_pb_value_id, currency_code='USD'):\n  \"\"\"\n  Creates a line item config object.\n\n  Args:\n    name (str): the name of the line item\n    order_id (int): the ID of the order in DFP\n    placement_ids (arr): an array of DFP placement IDs to target\n    ad_unit_ids (arr): an array of DFP ad unit IDs to target\n    cpm_micro_amount (int): the currency value (in micro amounts) of the\n      line item\n    sizes (arr): an array of objects, each containing 'width' and 'height'\n      keys, to set the creative sizes this line item will serve\n    hb_bidder_key_id (int): the DFP ID of the `hb_bidder` targeting key\n    hb_pb_key_id (int): the DFP ID of the `hb_pb` targeting key\n    currency_code (str): the currency code (e.g. 'USD' or 'EUR')\n  Returns:\n    an object: the line item config\n  \"\"\"\n\n  # Set up sizes.\n  creative_placeholders = []\n  \n  for size in sizes:\n    creative_placeholders.append({\n      'size': size\n    })\n\n  # Create key/value targeting for Prebid.\n  # https://github.com/googleads/googleads-python-lib/blob/master/examples/dfp/v201802/line_item_service/target_custom_criteria.py\n  # create custom criterias\n\n  hb_bidder_criteria = {\n    'xsi_type': 'CustomCriteria',\n    'keyId': hb_bidder_key_id,\n    'valueIds': [hb_bidder_value_id],\n    'operator': 'IS'\n  }\n\n  hb_pb_criteria = {\n    'xsi_type': 'CustomCriteria',\n    'keyId': hb_pb_key_id,\n    'valueIds': [hb_pb_value_id],\n    'operator': 'IS'\n  }\n\n  # The custom criteria will resemble:\n  # (hb_bidder_criteria.key == hb_bidder_criteria.value AND\n  #    hb_pb_criteria.key == hb_pb_criteria.value)\n  top_set = {\n    'xsi_type': 'CustomCriteriaSet',\n    'logicalOperator': 'AND',\n    'children': [hb_bidder_criteria, hb_pb_criteria]\n  }\n\n  # https://developers.google.com/doubleclick-publishers/docs/reference/v201802/LineItemService.LineItem\n  line_item_config = {\n    'name': name,\n    'orderId': order_id,\n    # https://developers.google.com/doubleclick-publishers/docs/reference/v201802/LineItemService.Targeting\n    'targeting': {\n      'inventoryTargeting': {},\n      'customTargeting': top_set,\n    },\n    'startDateTimeType': 'IMMEDIATELY',\n    'unlimitedEndDateTime': True,\n    'lineItemType': 'PRICE_PRIORITY',\n    'costType': 'CPM',\n    'costPerUnit': {\n      'currencyCode': currency_code,\n      'microAmount': cpm_micro_amount\n    },\n    'creativeRotationType': 'EVEN',\n    'primaryGoal': {\n      'goalType': 'NONE'\n    },\n    'creativePlaceholders': creative_placeholders,\n  }\n  if placement_ids is not None:\n    line_item_config['targeting']['inventoryTargeting']['targetedPlacementIds'] = placement_ids\n\n  if ad_unit_ids is not None:\n    line_item_config['targeting']['inventoryTargeting']['targetedAdUnits'] = [{'adUnitId': id} for id in ad_unit_ids]\n\n  return line_item_config",
        "sha1": "ab26c245fa513bbb7d3db18b3042fbfd07682e76",
        "id": 108345
    },
    {
        "content": "def convert_acsii(text):\n    \"\"\"\n    convert input text or number into a list of acsii code\n    :param text: text or number\n    :return: list of acsii code for each digit / character in the input\n    \"\"\"\n    return [ord(i) for i in str(text)]",
        "sha1": "9b7f6ae1b3ca0f4dedb3f19536bdf7f46c39fe25",
        "id": 296484
    },
    {
        "content": "def _sabr_implied_vol_hagan_A4(\n        underlying, strike, maturity, alpha, beta, rho, nu):\n    \"\"\"_sabr_implied_vol_hagan_A4\n    One of factor in hagan formua.\n    See :py:func:`sabr_implied_vol_hagan`.\n\n    A_{4}(K, S; T)\n        & := &\n            1\n            +\n            \\left(\n                \\\\frac{(1 - \\\\beta)^{2}}{24}\n                    \\\\frac{\\alpha^{2}}{(SK)^{1-\\\\beta}}\n                + \\\\frac{1}{4}\n                    \\\\frac{\\\\rho\\\\beta\\nu\\alpha}{(SK)^{(1-\\\\beta)/2}}\n                + \\\\frac{2 - 3\\\\rho^{2}}{24}\\nu^{2}\n            \\\\right) T\n\n    :param float underlying:\n    :param float strike:\n    :param float maturity:\n    :param float alpha:\n    :param float beta:\n    :param float rho:\n    :param float nu:\n\n    :return: value of factor.\n    :rtype: float.\n    \"\"\"\n    one_minus_beta = 1.0 - beta\n    one_minus_beta_half = one_minus_beta / 2.0\n    one_minus_beta2 = one_minus_beta ** 2\n\n    numerator1 = one_minus_beta2 * alpha * alpha\n    denominator1 = 24.0 * ((underlying * strike) ** one_minus_beta)\n    term1 = numerator1 / denominator1\n\n    numerator2 = rho * beta * nu * alpha\n    denominator2 = 4.0 * ((underlying * strike) ** one_minus_beta_half)\n    term2 = numerator2 / denominator2\n\n    term3 = (2.0 - 3.0 * rho * rho) * nu * nu / 24.0\n\n    return 1.0 + (term1 + term2 + term3) * maturity",
        "sha1": "704f4db60570cc18ad16b17d9b6ba93747d373d4",
        "id": 29466
    },
    {
        "content": "import math\n\n\ndef _rescale_path(path, depth):\n    \"\"\"Rescales the input path by depth! ** (1 / depth), so that the last\n    signature term should be roughly O(1).\n\n    Parameters\n    ----------\n    path : np.ndarray\n        Input path of shape [N, L, C].\n    depth : int\n        Depth the signature will be computed to.\n\n    Returns\n    -------\n    np.ndarray:\n        Tensor of the same shape as path, corresponding to the scaled path.\n    \"\"\"\n    coeff = math.factorial(depth) ** (1 / depth)\n    return coeff * path",
        "sha1": "0d57eb62609f66b10f3eb1053d48061d15779df5",
        "id": 68870
    },
    {
        "content": "from typing import Tuple\n\n\ndef optional_type(s: str) -> Tuple[str, bool]:\n    \"\"\"Determine if a type is optional and parse the actual type name.\"\"\"\n    if s.endswith(\"(optional)\"):\n        return s[: s.find(\"(optional)\")].strip(), True\n    return s, False",
        "sha1": "a9b143010bb9a22989a3997813c9a135b0d2144e",
        "id": 180043
    },
    {
        "content": "def get_role( trans, id ):\n    \"\"\"Get a Role from the database by id.\"\"\"\n    # Load user from database\n    id = trans.security.decode_id( id )\n    role = trans.sa_session.query( trans.model.Role ).get( id )\n    if not role:\n        return trans.show_error_message( \"Role not found for id (%s)\" % str( id ) )\n    return role",
        "sha1": "b58ef0e6f82c22062f775e7368bf9511f342df66",
        "id": 598538
    },
    {
        "content": "def crop(img, boundaries):\n    \"\"\"Crop the image to the given boundaries.\"\"\"\n    minx, miny, maxx, maxy = boundaries\n    return img[miny:maxy, minx:maxx]",
        "sha1": "bbf8316f898dc72cac85acedde0faef7657afb47",
        "id": 408098
    },
    {
        "content": "def metadata_entry_to_tuple(metadata_entry):\n    \"\"\"Converts a lxml.objectify.ObjectifiedElement metadata entry to a tuple.\n\n    The metadata entry object is converted into a simple (key, value) tuple.\n\n    :param lxml.objectify.ObjectifiedElement metadata_entry: an object\n        containing MetadataEntry XML data. This tag is a child tag of\n        EntityType.Metadata XML.\n\n    :return: a key-value tuple respresnting the metadata entry. Both key and\n        value will be strings.\n\n    :rtype: tuple\n    \"\"\"\n    key = metadata_entry.Key.text\n    value = metadata_entry.TypedValue.Value.text\n    return (key, value)",
        "sha1": "3f2f40e0eaeeec57ee2817b4d63c3bfbdcc00e22",
        "id": 199311
    },
    {
        "content": "def validate_float_or_None(s):\n    \"\"\"convert s to float, None or raise\"\"\"\n    # values directly from the rc file can only be strings,\n    # so we need to recognize the string \"None\" and convert\n    # it into the object. We will be case-sensitive here to\n    # avoid confusion between string values of 'none', which\n    # can be a valid string value for some other parameters.\n    if s is None or s == 'None':\n        return None\n    try:\n        return float(s)\n    except ValueError:\n        raise ValueError('Could not convert \"%s\" to float or None' % s)",
        "sha1": "3eb634e98ac8244fb527ab109836df10eb31a969",
        "id": 530198
    },
    {
        "content": "def go_match_fuzzy(name, string):\n    \"\"\"Check if string matches name using approximation.\"\"\"\n    if not string:\n        return False\n\n    name_len = len(name)\n    string_len = len(string)\n\n    if string_len > name_len:\n        return False\n    if name_len == string_len:\n        return name == string\n\n    # Attempt to match all chars somewhere in name\n    prev_index = -1\n    for i, char in enumerate(string):\n        index = name.find(char, prev_index+1)\n        if index == -1:\n            return False\n        prev_index = index\n    return True",
        "sha1": "1d70bbd692a4d2caabbbd983fc28b505d6e7ebc1",
        "id": 189466
    },
    {
        "content": "def reduce_score(score):\n    \"\"\"Convert tuple into 1-dimension score.\"\"\"\n    moves, pushes, steps = score\n    return moves + pushes + steps",
        "sha1": "8b8c7ac382b69e6336aebabe981e7b736ddd6830",
        "id": 310818
    },
    {
        "content": "def norm_array(Array, Minval=0.001):\n    \"\"\"Normalise array by largest abs value if >Minval (avoids trying to renormalise zero array)\"\"\"\n    greatest = 0.0\n    flatarray = Array.flat\n    for i in range(len(flatarray)):\n        if abs(flatarray[i]) > abs(greatest):\n            greatest = flatarray[i]\n    if abs(greatest) > Minval:\n        newarray = Array / greatest\n    else:\n        newarray = Array\n    return newarray",
        "sha1": "ba2a6076146bb782364469b7f4557d23a03120ce",
        "id": 116787
    },
    {
        "content": "from math import sqrt\nfrom math import pow\n\n\ndef standard_deviation(value_list):\n    \"\"\"Return standard deviation.\"\"\"\n    n = len(value_list)\n    average_value = sum(value_list) * 1.0 / n\n    return sqrt(sum([pow(e - average_value, 2) for e in value_list]) * 1.0 / (n - 1))",
        "sha1": "a9080971a9fac3b497c9ee8647f85b24b8edf3b1",
        "id": 174573
    },
    {
        "content": "def lowest_index(arr, target):\n    \"\"\" Finds the lowest index of target in arr. If target in arr, returns lowest\n    index i such that arr[i] == target, else returns index i where it should be\n    inserted while keeping arr sorted.\n\n    Args:\n        arr array to search target in\n        target value\n    Returns:\n        index, where 0 <= index <= len(arr)\n    Preconditions:\n        arr == sorted(arr)\n        < is supported between target and elements of arr\n    \"\"\"\n    # initialize search range\n    start, end = 0, len(arr)\n\n    # maintain solution in range [start, end]\n    while (start < end):\n        mid = (start + end) // 2\n        if arr[mid] < target:\n            start = mid + 1\n        else:\n            end = mid\n\n    return end",
        "sha1": "f7e7e394d5e17b7638152f354e4d2166b3e1bad3",
        "id": 240589
    },
    {
        "content": "from sympy.printing.lambdarepr import lambdarepr\n\n\ndef lambdastr(args, expr):\n    \"\"\"\n    Returns a string that can be evaluated to a lambda function.\n\n    >>> from sympy.abc import x, y, z\n    >>> from sympy.utilities.lambdify import lambdastr\n    >>> lambdastr(x, x**2)\n    'lambda x: (x**2)'\n    >>> lambdastr((x,y,z), [z,y,x])\n    'lambda x,y,z: ([z, y, x])'\n\n    \"\"\"\n\n    #XXX: This has to be done here because of circular imports\n\n    # Transform everything to strings.\n    expr = lambdarepr(expr)\n    if isinstance(args, str):\n        pass\n    elif hasattr(args, \"__iter__\"):\n        args = \",\".join(str(a) for a in args)\n    else:\n        args = str(args)\n\n    return \"lambda %s: (%s)\" % (args, expr)",
        "sha1": "6263f09ace7a6e44813985f8cf2162d61e0830cf",
        "id": 153993
    },
    {
        "content": "def stations_by_river(stations, river):\n    \"\"\"Takes a list of stations and returns a list of all the station names\n    on a specific river in alphabetic order\"\"\"\n    station_names = []\n    for station in stations:\n        if station.river == river:\n            station_names.append(station.name)\n    station_names = sorted(station_names)\n    return station_names",
        "sha1": "078e83affc54b90f2a58ad46cefdd895d9f8c1e6",
        "id": 700740
    },
    {
        "content": "def validate_elasticinferenceaccelerator_type(elasticinferenceaccelerator_type):\n    \"\"\"\n    Validate ElasticInferenceAccelerator for Instance\n    Property: ElasticInferenceAccelerator.Type\n    Property: LaunchTemplateElasticInferenceAccelerator.Type\n    \"\"\"\n\n    VALID_ELASTICINFERENCEACCELERATOR_TYPES = (\n        \"eia1.medium\",\n        \"eia1.large\",\n        \"eia1.xlarge\",\n    )\n    if elasticinferenceaccelerator_type not in VALID_ELASTICINFERENCEACCELERATOR_TYPES:\n        raise ValueError(\n            \"Elastic Inference Accelerator Type must be one of: %s\"\n            % \", \".join(VALID_ELASTICINFERENCEACCELERATOR_TYPES)\n        )\n    return elasticinferenceaccelerator_type",
        "sha1": "93adc962f3c8b50e509187258c843f269e17de60",
        "id": 213657
    },
    {
        "content": "def run_command(command, instance, key, client):\n    \"\"\"Executes the given command on the given EC2 instance\n\n    Parameters\n    ----------\n    command : str\n        The command to run (e.g. ``python run_myscript.py``)\n    instance : obj\n        A ``boto3`` AWS EC2 instance object.\n    key : obj\n        A ``paramiko.rsakey.RSAKey`` object.\n    client : obj\n        A ``paramiko.client.SSHClient`` object.\n\n    Returns\n    -------\n    output : str\n        The standard output from running the command\n    errors : str\n        The standard error output from running the command\n    \"\"\"\n\n    client.connect(hostname=instance.public_dns_name, username='ec2-user', pkey=key)\n    stdin, stdout, stderr = client.exec_command(command)\n    output = stdout.read()\n    errors = stderr.read()\n\n    # Make output a more readable\n    output = output.decode(\"utf-8\")\n    output = output.replace('\\t', '  ').replace('\\r', '').replace(\"\\'\", \"\").split('\\n')\n    errors = errors.decode(\"utf-8\")\n    errors = errors.replace('\\t', '  ').replace('\\r', '').replace(\"\\'\", \"\").split('\\n')\n\n    return output, errors",
        "sha1": "803ffba5f2bb4ed9c418f4c8a704d30a3c22a8d1",
        "id": 557416
    },
    {
        "content": "def strip_fname(name):\n    \"\"\"\n    strip wrapper (e.g. F${}, V${}, P${}) from name to get ID.\n    :param name: string to strip\n    :return: element name without wrapper text\n    \"\"\"\n    return name[3:-1]",
        "sha1": "24d5e2b1a45110598ca6cfde1bcfc7ee3c2d9d36",
        "id": 117937
    },
    {
        "content": "def create_condition_resource(condition_id: str,\n                              patient_reference: dict,\n                              onset_datetime: str,\n                              condition_code: dict,\n                              severity = None) -> dict:\n    \"\"\"\n    Create condition resource following the FHIR format\n    (http://www.hl7.org/implement/standards/fhir/condition.html)\n    \"\"\"\n    condition_resource = {\n        \"resourceType\": \"Condition\",\n        \"id\": condition_id,\n        \"subject\": patient_reference,\n        \"code\": condition_code,\n    }\n\n    if severity:\n        condition_resource[\"severity\"] = severity\n\n    if onset_datetime:\n        condition_resource[\"onsetDateTime\"] = onset_datetime\n\n    return condition_resource",
        "sha1": "609615a1dfd58a1156bddf5de9be6d32dc5b9c21",
        "id": 398014
    },
    {
        "content": "def _run_command(cmd,sched=None):\n    \"\"\"\n    Run the command, either locally or via a scheduler\n\n    Arguments:\n      cmd (Command): command to run\n      sched (SimpleScheduler): optional, a scheduler\n        to use to run the command\n    \"\"\"\n    print(\"Running %s\" % cmd)\n    if sched is None:\n        retcode,output = cmd.subprocess_check_output()\n    else:\n        job = sched.submit(cmd)\n        job.wait()\n        retcode = job.exit_code\n    return retcode",
        "sha1": "71017f8e97f0477c556dcd08f9b706227b730885",
        "id": 274338
    },
    {
        "content": "def format_code(code):\n    \"\"\"\n    Generates the string for a code block of the example page, formatted for a\n    'sphinx-gallery' example script.\n    Parameters\n    ----------\n    code : string\n        The python source code to be formatted.\n    Returns\n    -------\n    string : The formatted string for the example script.\n    \"\"\"\n    block_str = ''\n    for line in code:\n        block_str += line + '\\n'\n    return block_str + '\\n'",
        "sha1": "668c77a6923236f53c6be5465a22c77a2b1cd5dc",
        "id": 169674
    },
    {
        "content": "import click\n\n\ndef end_option(func):\n    \"\"\"Option for setting an end date in the search.\"\"\"\n    return click.option(\n        '--end',\n        'end_date',\n        type=click.DateTime(formats=['%Y-%m-%d']),\n        help=\"Show sites before this date (YYYY-MM-DD).\"\n    )(func)",
        "sha1": "4daf7347b4faf387016bc524943db50b0279404e",
        "id": 369204
    },
    {
        "content": "def container_for_key(key):\n    \"\"\" Determines what type of container is needed for `key` \"\"\"\n    try:\n        int(key)\n        return []\n    except ValueError:\n        return {}",
        "sha1": "3d2b6b2f0787596e71810714f152797f2903d094",
        "id": 227138
    },
    {
        "content": "import math\n\n\ndef std(lst):\n    \"\"\"\n    Helper function that calculates the standard deviation of a numeric sequence\n    \"\"\"\n    average = sum(lst) / float(len(lst))\n    variance = sum(list(map(lambda x: (x - average) ** 2, lst))) / float(len(lst))\n    stdev = math.sqrt(variance)\n\n    return stdev",
        "sha1": "060be5305eda97f2054862f2ca07c688498245f5",
        "id": 248977
    },
    {
        "content": "def create_intern(conn, intern):\n    \"\"\"\n    Create a new intern\n    :param conn:\n    :param intern:\n    :return: intern id\n    \"\"\"\n\n    sql = ''' INSERT INTO interns(last_name,first_name,position_applied,school,program,date_of_entry)\n              VALUES(?,?,?,?,?,?) '''\n    cur = conn.cursor()\n    cur.execute(sql, intern)\n    conn.commit()\n    return cur.lastrowid",
        "sha1": "0f87ab2e7c0719f7411aff1ee2f3e88363ea61be",
        "id": 560049
    },
    {
        "content": "def tpu_ordinal_fn(shard_index_in_host, replicas_per_worker):\n  \"\"\"Return the TPU ordinal associated with a shard.\"\"\"\n  return shard_index_in_host % replicas_per_worker",
        "sha1": "773313750ce78cf5d32776752cb75201450416ba",
        "id": 6065
    },
    {
        "content": "def prop_val_bounds(printer, ast):\n    \"\"\"Prints an infimum / supremum property \"(inf|sup): prop\".\"\"\"\n    type_str = ast[\"type\"]\n    pred_str = f'{{{printer.ast_to_string(ast[\"predicate\"])}}}' if ast.get(\"predicate\") else \"\"\n    exprs_str = \", \".join(map(lambda expr: printer.uppaal_c_printer.ast_to_string(expr), ast[\"exprs\"]))\n    return f'{type_str}{pred_str}: {exprs_str}'",
        "sha1": "0c4f7bedde7a5ea7f5bb701428935450089eb994",
        "id": 461506
    },
    {
        "content": "def convert_case(list_of_cards):\n    \"\"\"\n    Converts to upper case rank ,lower case suit\n    :param list_of_cards:\n    :return:\n    \"\"\"\n\n    for idx, card in enumerate(list_of_cards):\n        list_of_cards[idx] = card[0].upper() + card[1].lower()\n\n    return list_of_cards",
        "sha1": "558b5c45ee26b250fa7a8459caf68f43f4cb471b",
        "id": 493969
    },
    {
        "content": "import unicodedata\n\n\ndef normalize(wikipedia_url):\n  \"\"\"Unicode normalize the wikipedia title.\n\n  Args:\n    wikipedia_url: The original title\n\n  Returns:\n    The unicode normalized title\n  \"\"\"\n  return unicodedata.normalize('NFC', wikipedia_url)",
        "sha1": "65edd1d1fa2643765e634474e68ef3607c37d024",
        "id": 450296
    },
    {
        "content": "def _GetChannelSet(five_g, width):\n  \"\"\"Returns a tuple of target channels based on spectrum and width given.\n\n  Args:\n    five_g: (bool) Whether target is 5GHz radio.\n    width: (int) Channel width, eg. 20, 40, 80.\n  \"\"\"\n  if five_g:\n    if width == 20:\n      return (36, 40, 44, 48, 52, 56, 60, 64, 100, 104, 108, 112, 116, 120, 124,\n              128, 132, 136, 140, 144, 149, 153, 157, 161, 165)\n    elif width == 40:\n      return (36, 44, 52, 60, 100, 108, 116, 124, 132, 140, 149, 157)\n    elif width == 80:\n      return (36, 52, 100, 116, 132, 149)\n\n  # we return 2.4G channels otherwise.\n  return (1, 6, 11)",
        "sha1": "9b5436266a47d52eced41ac8e90471c3cf0dc9f9",
        "id": 63057
    },
    {
        "content": "from typing import List\nfrom typing import Any\n\n\ndef getListStr(data: List[Any], separator: str = ', ') -> str:\n    \"\"\" Return a human readable list string (for printing purposes).\n\n    Args:\n        data: list to transform to string.\n        separator: separator to join list items with.\n    \"\"\"\n    dataStr: List[str] = []\n    for item in data:\n        dataStr.append(str(item))\n    readableStr = separator.join(dataStr)\n\n    return readableStr",
        "sha1": "0f1986630dfc4e6d46ee953b0031a9ec35228c52",
        "id": 198736
    },
    {
        "content": "def get_x2(oi, ei):\n    \"\"\"\n    Function to calculate the statistical test for the categorical data analysis,\n    it is a chi squared value.\n    \n    Parameters:\n    --------------------------\n    oi : list\n        Frequency from the observed events.\n    ei : list\n        Frequency from the expected events.\n    \n    Returns:\n    --------------------------\n    x2 : double\n        Chi squared value which represents the calculated statistical test.\n    \"\"\"\n    if (any(isinstance(el, list) for el in oi)): # Validate if the list is a list of lists\n        x2 = 0\n    \n        for i in range(len(oi)):\n            for j in range(len(oi[i])):\n                x2 += (oi[i][j] - ei[i][j])**2 / ei[i][j]\n            \n        return x2\n    else:\n        k = len(oi)\n        x2 = sum([((oi[x] - ei[x])**2 / ei[x]) for x in range(k)])\n        return x2",
        "sha1": "e97eead0e631db246f7c6cc2112e26510e7ab572",
        "id": 331992
    },
    {
        "content": "def count_by(x, n):\n    \"\"\" Return a sequence of numbers counting by `x` `n` times. \"\"\"\n    return range(x, x * n + 1, x)",
        "sha1": "6bca9902f78f454da6a33cb40054a87f9832830a",
        "id": 139415
    },
    {
        "content": "def map_rcs_to_snake(nh, nv, row, col, spin):\n    \"\"\"Mapping (row, column, spin-type) to snake encoding.\n\n    Args:\n        nhoriz -- number of horizontal sites\n        nvert -- number of vertical sites\n        row -- row location of the qubit in the lattice\n        col -- column location of the qubit in the lattice\n        spin -- spin-type: up or down\n\n    Returns:\n        Number of snake encoded qubit\n    \"\"\"\n    return 2 * nh * row + col + nh * spin",
        "sha1": "c33da0d0f12049ec79a26bd54cdd413133990025",
        "id": 272851
    },
    {
        "content": "def max_value(tab): \n    \"\"\"\n    Brief: computes the max with positiv value\n    Args: \n        tab: a liste of numeric value exepcts at least one positive valus, raise expection\n    Return: the max value\n    Raises :\n        ValueError if no positive value is found\n    \"\"\"\n    if not(isinstance(tab, list)):\n        raise ValueError('Expected a list as input')\n    \n    numberMaxi=0\n        \n    for val in tab:\n        if val > numberMaxi:\n            numberMaxi=val\n    return numberMaxi",
        "sha1": "82a6e83df070a8e32ecce6f12a7e0a03307b81ce",
        "id": 646588
    },
    {
        "content": "def get_var_val(key, ii, varDict):\n    \"\"\"Gets an input in the likes of ${var} and returns the corresponding var value from the dict\n\n    Parameters\n    ----------\n    key: string\n        unparsed key of var\n    ii: int\n        current iteration idx\n    varDict: dict\n        variable dictionary\n\n    Returns\n    -------\n    string\n        variable value as string\n\n    \"\"\"\n    res = varDict.get(key[2:-1], '0')[ii]\n    return str(res)",
        "sha1": "81df687051e4fa3bcf1d1e6f08bdfa430138ee29",
        "id": 619399
    },
    {
        "content": "def format_obj(obj, layout='{}', sep=None):\n    \"\"\"Given an obj or dict[obj] and a format, returns formatted strings.\n\n    :param obj: iterable[obj] or obj, containing info to print.\n    :param layout: str, the format one object should be printed with, defaults to '{}'\n    :param sep: str, the delimeter for joining, defaults to None.\n    :raise ValueError: Raises ValueError if layout passed is invalid.\n    :return: list[str] or str, containing necessary information. Returns the default layout if no object is passed.\n    \"\"\"    \n    # If layout is None, default to '{}'\n    if layout is None or not isinstance(layout, str):\n        raise ValueError(\"Cannot print object without formatted string.\")\n        \n    # If object is None, return the layout without any preprocessing.\n    if obj is None:\n        return layout\n\n    def format_item(item):        \n        return layout.format(item)\n    \n    def format_iterable(list_obj):        \n        return [format_item(item) for item in list_obj]    \n    \n    try:\n        results = format_iterable(obj)\n        if len(results) == 0:\n            return layout\n        \n        if sep is None:\n            return results\n        else:\n            return sep.join(results)\n    except TypeError as e:\n        print(f'`{obj}` is not an iterable. Formatting single item.')\n        pass\n\n    return format_item(obj)",
        "sha1": "5718038917ec1769d57bf0e0b10c58eb927e2abc",
        "id": 564547
    },
    {
        "content": "def chunks(l, n):\n    \"\"\"\n    returns list of list of length n.\n    E.g. chunks([1, 2, 3, 4, 5], 2) returns [[1, 2],  [3, 4], [5]]\n    \"\"\"\n    return [l[i:i+n] for i in range(0, len(l), n)]",
        "sha1": "24e71aca504e1fe5c25c95026e4ac8ca8cb047df",
        "id": 150871
    },
    {
        "content": "def get_all_sectors_of_narratives(narratives):\n    \"\"\"Get all defined sectors of all narratives\n\n    ARguments\n    --------\n    narratives : list\n        All defined narratives\n\n    Returns\n    -------\n    all_sectors : list\n        All sectors\n    \"\"\"\n    all_sectors = set()\n    for narrative in narratives:\n        all_sectors.add(narrative['sector'])\n    all_sectors = list(all_sectors)\n\n    return all_sectors",
        "sha1": "5893ae8c510ddb2e4efe4d40c1f2b5c8e7d232f3",
        "id": 231921
    },
    {
        "content": "from datetime import datetime\n\n\ndef get_current_year() -> int:\n    \"\"\"Get's the current year basing on the system time\"\"\"\n    return datetime.now().year",
        "sha1": "595bab7f6bbbc3b3d084c9577dab5b16c0d05feb",
        "id": 218969
    },
    {
        "content": "def perimeter_square(length):\n    \"\"\"\n    .. math::\n\n        perimeter = 4 * length\n\n    Parameters\n    ----------\n    length: float\n        length of one side of a square\n\n    Returns\n    -------\n    perimeter: float\n        perimeter of the square\n    \"\"\"\n    return length * 4",
        "sha1": "07b8eefed384636a254c1ca84d861bebf80782ec",
        "id": 651751
    },
    {
        "content": "def getReportPath(job, reportType):\n    \"\"\"\n    Return the filename for the error report.\n    Does not include the folder to avoid conflicting with the S3 getSignedUrl method.\n    \"\"\"\n    path = 'submission_{}_{}_{}_report.csv'.format(job.submission_id, job.file_type.name, reportType)\n    return path",
        "sha1": "8fb2b34be8ce615120387bdee48b6855d97d0d60",
        "id": 640014
    },
    {
        "content": "def get_digit_c(number, digit):\n    \"\"\" Given a number, returns the digit in the specified position, does not accept out of range digits \"\"\"\n    return int(str(number)[-digit])",
        "sha1": "6e38e80fa6f2714cd6fd83ae01b208848dd55e2b",
        "id": 670739
    },
    {
        "content": "def uniquify(lst):\n    \"\"\"\n    Make the elements of a list unique by inserting them into a dictionary.\n    This must not change the order of the input lst.\n    \"\"\"\n    dct = {}\n    result = []\n    for k in lst:\n        if k not in dct:\n            result.append(k)\n        dct[k] = 1\n    return result",
        "sha1": "022635ac8da8351089e55c1df9411451d442b425",
        "id": 544724
    },
    {
        "content": "def dict_subtract(d1, d2):\n    \"\"\"Subtract one dictionary from another.\n\n    Args:\n        d1 (dict): First dictionary.\n        d2 (dict): Second dictionary.\n\n    Returns:\n        dict: `d1 - d2`.\n    \"\"\"\n    if set(d1.keys()) != set(d2.keys()):\n        raise ValueError(\"Dictionaries have different keys.\")\n    return {k: d1[k] - d2[k] for k in d1.keys()}",
        "sha1": "a8098f66ce1ca85803c90d9cfc058ae96b3f8123",
        "id": 28369
    },
    {
        "content": "def split_every_n_characters(n: int, s: str) -> list:\n    \"\"\"Split a string every `n` characters.\n    :param n: the number that determines the length of output strings\n    :param s: any string\n    :return: a list of strings where all of them are `n` characters long (except the last one)\n    :link: https://stackoverflow.com/a/9475354\n    \"\"\"\n    return [s[i:i+n] for i in range(0, len(s), n)]",
        "sha1": "b3d2e477244b92da53444024c7154cb64a196dce",
        "id": 432113
    },
    {
        "content": "def quad_BACT_to_gradient(bact, l):\n    \"\"\"Convert a SLAC quad BACT (kG) into BMAD b1_gradient T/m units\"\"\"\n    return -bact/(10.0*l)",
        "sha1": "2d0946f338ee32bea1eaa259f61eaceba268042b",
        "id": 260974
    },
    {
        "content": "import re\n\n\ndef find_keywords(df, reg_dict):\n    \"\"\"\n    Per domain in `reg_dict`, find all matches for the \"joined domain regex\" in the `all_text` column of the dataframe. Store the matches in a column with the domain name.\n\n    Parameters\n    ----------\n    df: DataFrame\n        dataframe with the column `all_text`\n    reg_dict: dict\n        dictionary with domains as keys and a \"joined domain regex\" as values\n\n    Returns\n    -------\n    DataFrame\n        dataframe with the new columns containing matches\n    \"\"\"\n    for k, v in reg_dict.items():\n        df[k] = df.all_text.str.findall(v, flags=re.IGNORECASE)\n    return df",
        "sha1": "87aca5d3e52e294ef689978143ad528619d65809",
        "id": 599347
    },
    {
        "content": "def hack_ncbi_fasta_name(pipe_name):\n    \"\"\"Turn 'gi|445210138|gb|CP003959.1|' into 'CP003959.1' etc.\n\n    For use with NCBI provided FASTA and GenBank files to ensure\n    contig names match up.\n\n    Or Prokka's *.fna and *.gbk files, turning 'gnl|Prokka|contig000001'\n    into 'contig000001'\n    \"\"\"\n    if pipe_name.startswith(\"gi|\") and pipe_name.endswith(\"|\"):\n        return pipe_name.split(\"|\")[3]\n    elif pipe_name.startswith(\"gnl|\") and pipe_name.count(\"|\")==2:\n        return pipe_name.split(\"|\")[2]\n    else:\n        return pipe_name",
        "sha1": "3b384b101a63fc2babd2b7b26b603565d71a8496",
        "id": 49283
    },
    {
        "content": "def get_dict(d, key, default=None):\n    \"\"\"\n    Getter for dictionaries. Does a get on dictionary items\n    instead of attributes.\n    \"\"\"\n    return d.get(key, default)",
        "sha1": "731d180aa973470e47b2e18101746ac81cafeeb9",
        "id": 327401
    },
    {
        "content": "def _beautify_message(msg):\n    \"\"\"\n    Function for cleaning the received message. Removes everything not allowed by the specification, adds some required\n    fields and renames some to follow the specification.\n\n    :param msg: Received message as dictionary\n\n    :return: Cleaned message.\n\n    \"\"\"\n    msg[\"type\"] = \"Message\"\n    msg.pop(\"_id\")\n    msg[\"id\"] = msg.pop(\"mid\")\n    if \"attachments\" in msg:\n        if len(msg[\"attachments\"]) > 0:\n            for item in msg[\"attachments\"]:\n                item[\"id\"] = item.pop(\"aid\")\n        elif len(msg[\"attachments\"]) == 0:\n            del msg[\"attachments\"]\n    return msg",
        "sha1": "0dfeb2d80308d04e0a4e951db113412298624207",
        "id": 610073
    },
    {
        "content": "def calculate_max_diff(col_1, col_2):\n    \"\"\"Get a maximum difference between two columns\n\n    Parameters\n    ----------\n    col_1 : Pandas.Series\n        The first column\n    col_2 : Pandas.Series\n        The second column\n\n    Returns\n    -------\n    Numeric\n        Numeric field, or zero.\n    \"\"\"\n    try:\n        return (col_1.astype(float) - col_2.astype(float)).abs().max()\n    except:\n        return 0",
        "sha1": "8c81adaf2e3680cc192a15a47746f654533b43c3",
        "id": 537461
    },
    {
        "content": "def _parse_fasta_input(fasta: str) -> dict:\n    \"\"\"\n        The expected input of the protein sequences are in FASTA format.\n        http://genetics.bwh.harvard.edu/pph/FASTA.html\n        Parse the input to retrieve the entries and sequences.\n\n    :param fasta: the FASTA formatted string.\n    :return: Return a dictionary of each entry and its corresponding sequences.\n    \"\"\"\n\n    entry_dict = {}\n    entries = fasta.split(\">\")\n\n    for entry in entries:\n        if entry == \"\":\n            continue\n        entry = entry.replace(\"\\r\", \"\").replace(\" \", \"\")\n\n        entry_split_by_newline = entry.split(\"\\n\")\n\n        if len(entry_split_by_newline) < 1:\n            continue\n\n        entry_dict[entry_split_by_newline[0]] = \"\".join(entry_split_by_newline[1:])\n\n    return entry_dict",
        "sha1": "9da935f8bd22afecbbbcc890417e9338b729aeb5",
        "id": 402955
    },
    {
        "content": "def parse_key_value_list(kv_string_list, error_fmt, error_func):\n    \"\"\"Parse a list of strings like ``KEY=VALUE`` into a dictionary.\n\n    :param kv_string_list: Parse a list of strings like ``KEY=VALUE`` into a\n                           dictionary.\n    :type kv_string_list: [str]\n    :param error_fmt: Format string accepting one ``%s`` argument which is the\n                      malformed (i.e. not ``KEY=VALUE``) string\n    :type error_fmt: str\n    :param error_func: Function to call when a malformed string is encountered.\n    :type error_func: function(str)\n    \"\"\"\n    ret = {}\n    for value in kv_string_list:\n        try:\n            k, v = value.split('=', 1)\n            ret[k] = v\n        except ValueError:\n            error_func(error_fmt % (value,))\n    return ret",
        "sha1": "0aa2df08d75efbc5981ffa3f92d815f6cdd84816",
        "id": 60388
    },
    {
        "content": "def collect_vertices(geometry):\n    \"\"\"Returns all vertices within the given geometry as a list.\"\"\"\n    vertices = []\n    vertices_iter = geometry.vertices()\n    while vertices_iter.hasNext():\n        vertices.append(vertices_iter.next())\n    return vertices",
        "sha1": "19381cb28a475e62f47240a1cadef481a5f34a44",
        "id": 645154
    },
    {
        "content": "import platform\n\n\ndef get_platform_specific_library_name(universal_library_name, static = False):\n    \"\"\"Forms the platform-specific library name from a universal library name\n\n    @param  universal_library_name  Universal name of the library that will be converted\n    @param  static                  Whether the name is for a static library\n    @returns The platform-specific library name\n    @remarks\n      A universal library name is just the name of the library without extension,\n      using dots to separate words - for example My.Awesome.Stuff. Depending on the platform,\n      this might get turned into My.Awesome.Stuff.dll or libMyAwesomeStuff.so\"\"\"\n\n    if platform.system() == 'Windows':\n\n        if static:\n            return universal_library_name + \".lib\"\n        else:\n            return universal_library_name + \".dll\"\n\n    else:\n\n        # Because Linux tools automatically add 'lib' and '.a'/'.so'\n        return universal_library_name.replace('.', '')\n\n        #if static:\n        #    return 'lib' + universal_library_name.replace('.', '') + '.a'\n        #else:\n        #    return 'lib' + universal_library_name.replace('.', '') + '.so'",
        "sha1": "99a320e36c2fe5035dc8d75761180343f0cf92d3",
        "id": 142582
    },
    {
        "content": "import torch\n\n\ndef _reduce_list(val_list, red_func=torch.cat):\n    \"\"\"\n    Applies reduction function to given list. If each element in the list is\n    a Tensor, applies reduction function to all elements of the list, and returns\n    the output Tensor / value. If each element is a tuple, applies reduction\n    function to corresponding elements of each tuple in the list, and returns\n    tuple of reduction function outputs with length matching the length of tuple\n    val_list[0].\n    \"\"\"\n    if isinstance(val_list[0], torch.Tensor):\n        return red_func(val_list)\n    assert isinstance(val_list[0], tuple), \"Elements to be reduced can only be\"\n    \"either Tensors or tuples containing Tensors.\"\n    final_out = []\n    for i in range(len(val_list[0])):\n        final_out.append(_reduce_list([val_elem[i] for val_elem in val_list], red_func))\n    return tuple(final_out)",
        "sha1": "5ebdace78bf4dbd7b89559529db1ac7aba7ca7b6",
        "id": 264361
    },
    {
        "content": "import csv\n\n\ndef read_csv(path):\n    \"\"\" Return list of strings per comma.\n\n    :param str path: csv located path\n    :rtype list[str]\n    \"\"\"\n    args = []\n    with open(path, \"r\") as f:\n        reader = csv.reader(f)\n        for row in reader:\n            for r in row:\n                args.append(r)\n        return args",
        "sha1": "ee2ec97a87a30af2ac5c2d6c051929aaa2cfe4b1",
        "id": 374694
    },
    {
        "content": "def is_hovering(rect, mouse_pos):\n    \"\"\"Checks if a mouse is hovering over a rect\"\"\"\n    if (\n        rect.left <= mouse_pos[0] <= rect.right\n        and rect.top <= mouse_pos[1] <= rect.bottom\n    ):\n        return True\n    return False",
        "sha1": "e17f4b8ee6e0b5f174473aa88388ff81ab67ea66",
        "id": 697824
    },
    {
        "content": "def invert_ax(ax):\n    \"\"\" Inverts x and y data on a plot axis, and flips limits.  Most useful for\n    having an oriented axes in correlation side plots.\n    \"\"\"\n    for line in ax.lines:\n        xd = line.get_xdata()\n        yd = line.get_ydata()\n        line.set_xdata(yd)\n        line.set_ydata(xd)\n\n    xlim = ax.get_xlim()\n    ylim = ax.get_ylim()\n    ylim = (ylim[-1], ylim[0])\n    ax.set_xlim(ylim)\n    ax.set_ylim(xlim)\n    return ax",
        "sha1": "badc8cbe4e0602f8a70af7cb82b23273b4186b16",
        "id": 423211
    },
    {
        "content": "def p_fromlist(python_list: list) -> str:\n    \"\"\"Formatting list into paragraph in HTML\n\n    Args:\n        python_list (list): python list to format\n\n    Returns:\n        str: HMTL code for the list\n    \"\"\"\n    return '<p>' + ', '.join(python_list) + '</p>'",
        "sha1": "f77db13828b4bc1f11ecc629f220ccce19f2e496",
        "id": 289164
    },
    {
        "content": "import re\n\n\ndef _htmlPingbackURI(fileObj):\n    \"\"\"Given an interable object returning text, search it for a pingback URI\n    based upon the search parameters given by the pingback specification.\n\n    Namely, it should match the regex:\n      <link rel=\"pingback\" href=\"([^\"]+)\" ?/?>\n    (source: http://www.hixie.ch/specs/pingback/pingback)\n\n    We could search the text using an actual HTML parser easily enough, or\n    expand the regex to be a little more forgiving, but for the moment we'll\n    follow the spec.\"\"\"\n    regex = re.compile('<link rel=\"pingback\" href=\"([^\"]+)\" ?/?>', re.I) # might as well be case-insensitive\n    for line in fileObj:\n        m = regex.search(line)\n        if m != None:\n            uri = m.group(1)\n            # The pingback spec demands we expand the four allowed entities,\n            # but no more.\n            uri = uri.replace(\"&lt;\", \"<\")\n            uri = uri.replace(\"&gt;\", \">\")\n            uri = uri.replace(\"&quot;\", '\"')\n            uri = uri.replace(\"&amp;\", \"&\")\n            return uri\n    return None",
        "sha1": "342f401cb955af511040be30cb56c80d05dfeeda",
        "id": 682113
    },
    {
        "content": "def max_factor(num):\n    \"\"\"Find the maximum prime factor.\"\"\"\n    factor = 2\n    while factor * factor <= num:\n        while num % factor == 0:\n            num /= factor\n        factor += 1\n    if (num > 1):\n        return num\n    return factor",
        "sha1": "3b29d27d77454c1a52f483936b6a6bd94b98e306",
        "id": 386640
    },
    {
        "content": "import re\n\n\ndef form_date_parameter(date: str, name: str) -> str:\n    \"\"\"Form the date parameter for the API query after checking whether given date is valid.\n\n    Args:\n        date (str): date in format YYYY-MM-DD\n        name (str): either 'startDate' or 'endDate', used to differentiate API query parameters\n\n    Raises:\n        ValueError: raised if the given date is not valid\n\n    Returns:\n        str: the name and and date formatted as a query parameter\n    \"\"\"\n    date_regexp = (\n        r\"^(\\d{4}-((0[13578]|1[02])-(0[1-9]|[12][0-9]|3[01])|(0[469]|11)-\"\n        r\"(0[1-9]|[12][0-9]|30)|02-(0[1-9]|1\\d|2[0-8]))|(\\d{2}(0[48]|[2468][048]|[13579][26])|\"\n        r\"([02468][048]|[1359][26])00)-02-29)\"\n    )\n    match = re.match(date_regexp, date)\n    if match is None:\n        raise ValueError(\"{} {} is an invalid date.\".format(name, date))\n    return f\"{name}={date}&\"",
        "sha1": "1a229254731ca43748e1f6939a54cd2d029fee3a",
        "id": 672876
    },
    {
        "content": "def count_words(s, n):\n    \"\"\"Return the n most frequently occuring words in s.\"\"\"\n\n    # Count the number of occurences of each word in s\n    words = s.split() # split words into list on spaces\n    uniquewords = []\n\n    # for every word in the list\n    for word in words:\n        iswordunique = True\n\n        # check to see if that word has been seen before\n        for ii, pair in enumerate(uniquewords):\n\n            # if that word has been seen before\n            if pair[0] == word:\n\n                # update the count of that word\n                uniquewords[ii] = (pair[0], pair[1] + 1)\n                iswordunique = False\n                break\n\n        # if word wasn't seen, add it to the list\n        if iswordunique:\n            uniquewords.append((word,1)) \n\n    # Sort the occurences in descending order (alphabetically in case of ties)\n    uniquewordsalph = sorted(uniquewords, key=lambda word: word[0])\n    uniquewordsdesc = sorted(uniquewordsalph, key=lambda word: word[1], reverse=True)\n\n    # Return the top n words as a list of tuples (<word>, <count>)\n    return uniquewordsdesc[0:n]",
        "sha1": "7ebf352867ab1ee2508332adfb86c3104653955b",
        "id": 156778
    },
    {
        "content": "from pathlib import Path\n\n\ndef all_files(dir, pattern):\n    \"\"\"Recursively finds every file in 'dir' whose name matches 'pattern'.\"\"\"\n    return [f.as_posix() for f in [x for x in Path(dir).rglob(pattern)]]",
        "sha1": "45f12cda2e16cb745d99d2c8dfb454b32130e1c8",
        "id": 709929
    },
    {
        "content": "import re\n\n\ndef parseVersion(versionString):\n  \"\"\"\n    Parses a version string into its components\n    @ In, versionStirng, str, the version string to parse\n    @ Out, version, list, list of components as integers and strings\n  \"\"\"\n  version = []\n  for part in re.split(r'([0-9]+|[a-z]+|\\.)',versionString):\n    if len(part) == 0 or part == \".\":\n      continue\n    try:\n      version.append(int(part))\n    except ValueError:\n      version.append(part)\n  return version",
        "sha1": "5d95ac05f7e222522964617edc75adfd335138d3",
        "id": 147971
    },
    {
        "content": "def split_words(text : str) -> list:\n    \"\"\" Breaks up a command input such as 'hello foo bar' into individual words\"\"\"\n    command_text = text.strip()\n    commands = command_text.split(' ')\n    return commands",
        "sha1": "1bc27d5ec5f4c805eb22b84d9b916d1dca0f9312",
        "id": 35385
    },
    {
        "content": "def get_geo(twitter_msg):\n    \"\"\"\n    Generates GoogleMap link if msg has location data.\n    :param twitter_msg: Twitter Status Object\n    :return: string with gm link if possible or empty string otherwise\n    \"\"\"\n    try:\n        x, y = twitter_msg.place[\"bounding_box\"][\"coordinates\"][0][0]\n        return \"https://www.google.com/maps/place/{},{}\".format(y, x)\n    except Exception as e:\n        return \"\"",
        "sha1": "037262c8d8bfae2f280b66f1fa13467d4cda1e9d",
        "id": 685606
    },
    {
        "content": "def overflow(keyword):\n    \"\"\"Validation for the ``overflow`` property.\"\"\"\n    return keyword in ('auto', 'visible', 'hidden', 'scroll')",
        "sha1": "e9825260cee76e44bb629efafb18c4f956e944ff",
        "id": 606639
    },
    {
        "content": "def _check_minsize(fa, minsize):\n    \"\"\"\n    Raise ValueError if there is any sequence that is shorter than minsize.\n    If minsize is None the size will not be checked.\n    \"\"\"\n    if minsize is None:\n        return fa\n\n    for name, seq in fa.items():\n        if len(seq) < minsize:\n            raise ValueError(f\"sequence {name} is shorter than {minsize}\")\n\n    return fa",
        "sha1": "b6f5171fa8b547e1006ad45c6e7bd9c4af397747",
        "id": 467740
    },
    {
        "content": "def match_str(text, matches):\n    \"\"\" Return first matching element of list 'matches' in 'text', otherwise None \"\"\"\n    for match in matches:\n        if match in text:\n            return match\n    return None",
        "sha1": "0a4b23fa36088cfac9f596ea47534f046c5ddc62",
        "id": 376784
    },
    {
        "content": "def get_dates_range_str(df, col_name='date'):\n    \"\"\"Format a range of dates to a string yyyymmdd --> yyyymmdd\"\"\"\n    return df[col_name].min().strftime('%Y-%m-%d -> ') + df[col_name].max().strftime('%Y-%m-%d')",
        "sha1": "2c8c5a43fa017448aa889a8ee5ca0c397347718e",
        "id": 558765
    },
    {
        "content": "def get_plot_ground_truth_coverages(ground_truths, plot_id):\n    \"\"\"Extract ground truths coverages for specified plot.\"\"\"\n    coverages = (\n        ground_truths[ground_truths[\"Name\"] == plot_id][\n            [\"COUV_BASSE\", \"COUV_SOL\", \"COUV_INTER\", \"COUV_HAUTE\"]\n        ].values\n        / 100\n    )\n    return coverages.astype(float).squeeze()",
        "sha1": "926f3bde34aa74a5c745f3c33cc6183a0d66d21a",
        "id": 460827
    },
    {
        "content": "def getCorrespondingWindow(index, windows):\n    \"\"\"\n    Finds the corresponding window to the each predicted anomaly\n    :param index (type:int): index of predicted anomaly point\n    :param windows (type list of tuples (start, end)): list of true anomaly windows\n    :return (typ: tuple): anomaly window if point lies inside it else the preceding window\n    \"\"\"\n    \n    for i, window in enumerate(windows):\n        if window[0] <= index and (i == len(windows)-1 or index < windows[i+1][0]):\n            return window",
        "sha1": "ab40559e6ed71e55a7ccd6a4cd82f66063dd383b",
        "id": 104972
    },
    {
        "content": "def safe_readable(handle):\n    \"\"\"Attempts to find if the handle is readable without throwing an error.\"\"\"\n    try:\n        status = handle.readable()\n    except (OSError, ValueError):\n        status = False\n    return status",
        "sha1": "ac0c5918d2c323df741b771c5cb31203eb018f10",
        "id": 542668
    },
    {
        "content": "def read_metadata(metadata_df, index):\n    \"\"\"Returns a dictionary of useful metadata.\"\"\"\n    file_path = metadata_df['file_path'].values[index]\n    base_name_pkl = file_path.split('/')[-1]\n\n    metadata_dict = {\n        'base_name_pkl': base_name_pkl,\n        'classifier': metadata_df['full_classifier'].values[index],\n        'obj_type': metadata_df['object_type'].values[index],\n        'obj_name': metadata_df['object_name'].values[index]\n    }\n\n    return metadata_dict",
        "sha1": "1cb9586adec9e39936d4018a09334db6791d0d6e",
        "id": 557991
    },
    {
        "content": "import logging\n\n\ndef _get_task_counts(rows):\n    \"\"\"Calculcate number of true/false tasks and maximum achievable score.\"\"\"\n    count_true = count_false = 0\n    max_score = None\n    for row in rows:\n        if not row.id.property:\n            logging.info(\"Missing property for task %s.\", row.id)\n            continue\n        expected_result = row.id.expected_result\n        if not expected_result:\n            continue\n        if expected_result.result is True:\n            count_true += 1\n        elif expected_result.result is False:\n            count_false += 1\n        row_max_score = row.id.property.max_score(expected_result)\n        if row_max_score is not None:\n            max_score = row_max_score + (max_score or 0)\n\n    return max_score, count_true, count_false",
        "sha1": "b9b6dd4f8abcbe2eee9275ae465047448e1a5ac7",
        "id": 52043
    },
    {
        "content": "def _remove_prefix(node):\n    \"\"\"replaces e.g. x101 or u101 with 101\"\"\"\n    assert node[0] == \"x\" or node[0] == \"u\"\n    return node[1:]",
        "sha1": "8a4c39619b0d9010a4ad98e3f6f28b92af9bd85c",
        "id": 244760
    },
    {
        "content": "from typing import List\n\n\ndef create_cave(depth: int, tx: int, ty: int) -> List[List[int]]:\n    \"\"\"\n    Creates the cave according to the cave generation rules.\n\n    Since the cave is essentially infinite a constant size padding is applied\n    around the target coordinates to make the pathfinding feasible. Note that\n    there needs to be a padding because the optimal path can overshoot the\n    target. The padding size for this input was found simply by starting with\n    a very large value and progressively decreasing it until a value small\n    enough was found which produces the correct pathfinding result but is\n    still relatively quick to compute.\n    \"\"\"\n    PADDING = 50\n    cave = [[0] * (tx + PADDING) for _ in range(ty + PADDING)]\n\n    for y in range(ty + PADDING):\n        for x in range(tx + PADDING):\n            index = None\n            if y == 0 and x == 0:\n                index = 0\n            elif y == 0:\n                index = x * 16807\n            elif x == 0:\n                index = y * 48271\n            elif y == ty and x == tx:\n                index = 0\n\n            if index is None:\n                cave[y][x] = (cave[y-1][x] * cave[y][x-1] + depth) % 20183\n            else:\n                cave[y][x] = (index + depth) % 20183\n\n    return cave",
        "sha1": "20537fab61614aece67b20f9d33bd8ade3259637",
        "id": 21151
    },
    {
        "content": "def whitelist(squat_candidates, whitelist_filename=\"whitelist.txt\"):\n    \"\"\"Remove whitelisted packages from typosquat candidate list.\n\n    Args:\n        squat_candidates (dict): dict of packages and potential typosquatters\n        whitelist_filename (str): file location for whitelist\n\n    Returns:\n        dict: packages and post-whitelist potential typosquatters\n    \"\"\"\n    # Create whitelist\n    whitelist = []\n    with open(whitelist_filename, \"r\") as file:\n        for line in file:\n            # Strip out end of line character\n            whitelist.append(line.strip(\"\\n\"))\n\n    # Remove packages contained in whitelist\n    whitelist_set = set(whitelist)\n    for pkg in squat_candidates:\n        new_squat_candidates_set = set(squat_candidates[pkg]) - whitelist_set\n        new_squat_candidates_list = list(new_squat_candidates_set)\n        # Update typosquat candidate list\n        squat_candidates[pkg] = new_squat_candidates_list\n\n    return squat_candidates",
        "sha1": "5d8305d7ee721988420a6035d2df9e0b52404b7b",
        "id": 17804
    },
    {
        "content": "import math\nimport torch\n\n\ndef encode_position(input, levels, inc_input=True):\n    \"\"\"\n    For each scalar, we encode it using a series of sin() and cos() functions with different frequency.\n        - With L pairs of sin/cos function, each scalar is encoded to a vector that has 2L elements. Concatenating with\n          itself results in 2L+1 elements.\n        - With C channels, we get C(2L+1) channels output.\n    :param input:   (..., C)            torch.float32\n    :param levels:  scalar L            int\n    :return:        (..., C*(2L+1))     torch.float32\n    \"\"\"\n\n    # this is already doing 'log_sampling' in the official code.\n    result_list = [input] if inc_input else []\n    for i in range(levels):\n        temp = 2.0**i * input * math.pi # (..., C)\n        result_list.append(torch.sin(temp))  # (..., C)\n        result_list.append(torch.cos(temp))  # (..., C)\n\n    result_list = torch.cat(result_list, dim=-1)  # (..., C*(2L+1)) The list has (2L+1) elements, with (..., C) shape each.\n    return result_list",
        "sha1": "8e2f9e21dd83a0df1679f49495c3de92641bc3dd",
        "id": 577449
    },
    {
        "content": "import struct\n\n\ndef _pack_asf_image(mime, data, type=3, description=\"\"):\n    \"\"\"Pack image data for a WM/Picture tag.\n    \"\"\"\n    tag_data = struct.pack('<bi', type, len(data))\n    tag_data += mime.encode(\"utf-16-le\") + b'\\x00\\x00'\n    tag_data += description.encode(\"utf-16-le\") + b'\\x00\\x00'\n    tag_data += data\n    return tag_data",
        "sha1": "085f5461f5e39388247e151428a35b9107a4f0e5",
        "id": 445414
    },
    {
        "content": "import uuid\nimport json\nimport requests\n\n\ndef set_azure_cloudcheckr_application_service_assignment(AzureApiBearerToken, AzureReaderRoleId, AzureCloudCheckrApplicationServicePrincipalId, AzureSubscriptionId):\n    \"\"\"\n    Sets the previously created CloudCheckr application to have a reader role assignment.\n\n    https://docs.microsoft.com/en-us/azure/role-based-access-control/role-assignments-rest\n    \"\"\"\n\n    RoleAssignmentId = str(uuid.uuid1())\n\n    api_url = \"https://management.azure.com/subscriptions/\" +  AzureSubscriptionId + \"/providers/Microsoft.Authorization/roleAssignments/\" + RoleAssignmentId + \"?api-version=2015-07-01\"\n    authorization_value = \"Bearer \" + AzureApiBearerToken\n    role_assignment_data = json.dumps({\"properties\": {\"principalId\": AzureCloudCheckrApplicationServicePrincipalId, \"roleDefinitionId\": AzureReaderRoleId}})\n\n    response = requests.put(api_url, headers={\"Authorization\": authorization_value, \"Content-Type\": \"application/json\"}, data=role_assignment_data)\n    print(response.json())\n\n    if \"properties\" in response.json():\n        properties = response.json()[\"properties\"]\n        if \"roleDefinitionId\" in properties:\n            return properties[\"roleDefinitionId\"]\n    print(\"Failed to set role assignment for the CloudCheckr Application to the specified subscription\")\n    return None",
        "sha1": "1d0462f885810977f9ec40e950d8d7df3e0471a0",
        "id": 84102
    },
    {
        "content": "def splinter_window_size(splinter_webdriver, splinter_window_size):\n    \"\"\"\n    Prevent pytest-splinter from crashing with Chrome.\n\n    \"\"\"\n    if splinter_webdriver == 'chrome':\n        return None\n\n    return splinter_window_size",
        "sha1": "a3991608e985fc1f94173b92960f1e2cdc047130",
        "id": 266000
    },
    {
        "content": "def returnVarLevels(df, var):\n\n    \"\"\"\n    Returns the unique values/levels of the given variable.\n    \"\"\"\n    \n    return df[var].unique().tolist()",
        "sha1": "02792cf80d363312b00305f6bddc61ddffdd4a67",
        "id": 310492
    },
    {
        "content": "def gamma_from_alpha(alpha):\n    \"\"\"\n    Adiabatic index from spectral index, Sedov-Taylor expansion.\n\n    Parameters\n    ----------\n    alpha : spectral index\n    \"\"\"\n    gamma = (4./5)*(2.*alpha + 1.)\n    return gamma",
        "sha1": "ac1e264c452d446d98a80b7fa5c99d65e6c744a2",
        "id": 354984
    },
    {
        "content": "def median(lst, presorted=False):\n    \"\"\"\n    Returns the median of a list of float/int numbers. A lot of our median\n    calculations are done on presorted data so the presorted flag can be\n    set to skip unnecessary sorting.\n    :param lst: list\n    :param presorted: Boolean\n    :return: float\n    \"\"\"\n    if not presorted:\n        sorted_lst = sorted(lst)\n    else:\n        sorted_lst = lst\n    n = len(lst)\n    if n < 1:\n        return None\n    if n % 2 == 1:\n        return sorted_lst[n//2]\n    else:\n        return sum(sorted_lst[n//2-1:n//2+1])/2.0",
        "sha1": "320ac6df1bf912f7ac723e7037b9fcfa9d9c8ee7",
        "id": 610653
    },
    {
        "content": "def get_plot_dict_p_s(ch_var):\n    \"\"\"Returns a dictionary of key parameters for plotting based on varied component.\"\"\"\n    d = {}\n    # polyelectrolyte density varied\n    if ch_var == 'p':\n        d = {'ch_var':'p', 'ch_fix':'s', 'order':[0,1], 'name_var':'Polymer'}\n    # salt density varied\n    elif ch_var == 's':\n        d = {'ch_var':'s', 'ch_fix':'p', 'order':[1,0], 'name_var':'Salt'}\n    else:\n        print('invalid ch_var character: choose s or p.')\n\n    return d",
        "sha1": "4a986336e17569261c14883a7afe44280e3729d0",
        "id": 334085
    },
    {
        "content": "def Not(thing):\n    \"\"\"Not(thing) is a nicer way of saying thing.Not()\"\"\"\n    return thing.Not()",
        "sha1": "f67ad1ce2206637598132ffe2d9a86a666d625e2",
        "id": 277386
    },
    {
        "content": "def _filter_kwargs(prefix, kwargs):\n  \"\"\"Return new kwargs matching a prefix, or empty dict if no matches.\"\"\"\n  tmp_kwargs = {}\n  for kwarg, value in kwargs.items():\n    if prefix in kwarg:\n      kwarg = kwarg.lstrip(prefix)\n      kwarg = kwarg.lstrip('_')\n      tmp_kwargs[kwarg] = value\n  return tmp_kwargs",
        "sha1": "5ff205dd7f8e0d7eed69cb244090fcf6d391dae6",
        "id": 173807
    },
    {
        "content": "import secrets\n\n\ndef generate_password(character_set, length):\n    \"\"\"\n    Uses the secretes module to pick a given number of characters\n    from the available character set. https://docs.python.org/3/library/secrets.html\n    \"\"\"\n    # \"complexity is the worst enemy of security\" - Bruce Schneier.\n    return ''.join(secrets.choice(character_set) for _character in range(length))",
        "sha1": "35056b5ffb44577abe077e1d5c0a9ec0b010c1b2",
        "id": 326409
    },
    {
        "content": "def myhasattr(obj, name, _marker=object()):\n    \"\"\"Make sure we don't mask exceptions like hasattr().\n\n    We don't want exceptions other than AttributeError to be masked,\n    since that too often masks other programming errors.\n    Three-argument getattr() doesn't mask those, so we use that to\n    implement our own hasattr() replacement.\n    \"\"\"\n    return getattr(obj, name, _marker) is not _marker",
        "sha1": "e5a92ae96147d50914fe4f0c677e1e56e6ba26d9",
        "id": 433710
    },
    {
        "content": "def _is_int_tuple(value):\n  \"\"\"Return whether the value is a tuple of integers.\"\"\"\n  return isinstance(value, tuple) and all(isinstance(v, int) for v in value)",
        "sha1": "0a9b1908676a3154acd07790ec1559e26255a206",
        "id": 251007
    },
    {
        "content": "def stringify_parsed_email(parsed):\n    \"\"\"\n    Convert a parsed email tuple into a single email string\n    \"\"\"\n    if len(parsed) == 2:\n        return f\"{parsed[0]} <{parsed[1]}>\"\n    return parsed[0]",
        "sha1": "6552987fe6a06fdbb6bd49e5d17d5aadaae3c832",
        "id": 4267
    },
    {
        "content": "def str_to_int_list(string):\n    \"\"\"\n    :param str string:  A string read by the config file.\n\n    :return:  A list of integers.\n    :rtype:   list\n\n    Utility function used to convert a string to a list of integers\n    \"\"\"\n    list = []\n    parts = string.split(',')\n\n    for part in parts:\n        part = part.replace('[', '')\n        part = part.replace(']', '')\n        part = part.strip()\n\n        number = int(part)\n        list.append(number)\n\n    return list",
        "sha1": "019e25bbe1bc6b6640cfc823bccdadd2ffbf2d2b",
        "id": 598182
    },
    {
        "content": "def flatten(L):\n    \"\"\"\n    Flattens a list.\n\n    Example:\n\n    >>> flatten([a,b,[c,d,[e,f]]])\n    [a,b,c,d,e,f]\n    \"\"\"\n    if not isinstance(L, list):\n        return [L]\n\n    if L == []:\n        return L\n\n    return flatten(L[0]) + flatten(L[1:])",
        "sha1": "39292b7f686158fc9163a11c4b981e2f438a0fea",
        "id": 187055
    },
    {
        "content": "def number(number: str, fsep: str, tsep: str) -> str:\n    \"\"\"\n    Format a number using the provided float and thousands separators.\n\n        >>> number(\"1 000 000 000 000\", \",\", \" \")\n        '1 000 000 000 000'\n        >>> number(\"1000000\", \",\", \" \")\n        '1 000 000'\n        >>> number(\"1000000\", \".\", \"\")\n        '1000000'\n        >>> number(\"1000000\", \".\", \",\")\n        '1,000,000'\n        >>> number(\"-1000000\", \",\", \" \")\n        '\u22121 000 000'\n        >>> number(\"-1000000\", \"\", \"\")\n        '\u22121000000'\n        >>> number(\"\u22121000000\", \".\", \",\")\n        '\u22121,000,000'\n        >>> number(\"4.54609\", \",\" , \" \")\n        '4,54609'\n        >>> number(\"4.54609\", \".\" , \",\")\n        '4.54609'\n        >>> number(\"22905\", \",\" , \".\")\n        '22.905'\n    \"\"\"\n    # Remove superfluous spaces\n    number = number.replace(\" \", \"\")\n\n    # Handle unicode minus U+2212 character before doing the conversion\n    number = number.replace(\"\u2212\", \"-\")\n\n    # Convert\n    try:\n        # Integer\n        res = f\"{int(number):,}\"\n    except ValueError:\n        # Float\n        res = f\"{float(number):,}\"\n\n    # Replace the current thousands separator with \"|\";\n    # then replace the dot with the float separator;\n    # and lastly replace the \"|\" with the deisred thousands separator.\n    # This 3-steps-replacement is needed for when separators are replacing each other.\n    res = res.replace(\",\", \"|\").replace(\".\", fsep).replace(\"|\", tsep)\n\n    # Always return unicode minus U+2212 character for negative numbers\n    return res.replace(\"-\", \"\u2212\")",
        "sha1": "eb68d3e7fc344ed3eab94ccb0ccaf9a493c89d27",
        "id": 99921
    },
    {
        "content": "def XmlAttr(xnNode,sAttrName,bRequired):\n\t\"\"\"Read XML attribute value.\n\t[xnNode]: XML node, containing attribute value.\n\t[sAttrName]: string, name of attribute to read.\n\t[bRequired]: bool, attribute is required; Exception if attribute missing. \n\t<retval>: string, attribute value.\n\t<except>: Exception, if attribute required and missing.\n\t\"\"\"\n\tsAttrVal=xnNode.get(sAttrName);\n\tif(sAttrVal==None):\n\t\tif(bRequired): raise Exception(\"Required attribute '\"+sAttrName+\"' missing\");\n\telse: sAttrVal=sAttrVal.strip()\n\treturn sAttrVal",
        "sha1": "f2b4181be9815ba533f1497cfbccf10ddf56e2ad",
        "id": 298226
    },
    {
        "content": "def format_name(f_name, l_name):\n    \"\"\"Take a first and last name and format it\n    to return the title case version of the name.\n\n    Args:\n        f_name ([string])\n        l_name ([string])\n\n    Returns:\n        f_name + l_name in title\n    \"\"\"\n    if f_name == \"\" or l_name == \"\":\n        return \"You not enter a valid name!\"\n    full_name = f\"{f_name} {l_name}\"\n    return full_name.title()",
        "sha1": "59c7a9b135d0001c7bc1a378bbcbfb68a4dfa36a",
        "id": 36431
    },
    {
        "content": "import re\n\n\ndef replaceall(replace_dict, string):\n    \"\"\"\n    replaceall will take the keys in replace_dict and replace string with their corresponding values. Keys can be regular expressions.\n    \"\"\"\n    replace_dict = dict((re.escape(k), v) for k, v in list(replace_dict.items()))\n    pattern = re.compile(\"|\".join(list(replace_dict.keys())))\n    return pattern.sub(lambda m: replace_dict[re.escape(m.group(0))], string)",
        "sha1": "66c6ec299476986011de21a5f28a613c44435d33",
        "id": 41894
    },
    {
        "content": "import click\n\n\ndef _click_exception_with_exit_code(name: str, message: str, exitcode: int):\n    \"\"\"\n    create an exception\n    :param name: exception name\n    :param message: exception message\n    :param exitcode: exitcode\n    :return: new exception object\n    \"\"\"\n\n    class _ClickException(click.ClickException):\n        exit_code = exitcode\n\n    return type(name, (_ClickException,), {})(message)",
        "sha1": "39ef0f538bea27b8ebcbde1dbc53f64ccc89cc03",
        "id": 206966
    },
    {
        "content": "def binary(x: int, pre: str='0b', length: int=8):\n    \"\"\"\n    Return the binary representation of integer x\n    Input:\n        x: an integer of any size\n        pre: the prefix for the output string, default 0b\n        length: length of the output in binary if its representation has smaller length\n                default is 8 i,e, 2**8=256 int, a byte\n    Return:\n        The binary representation of integer x with a minimum lenght of \"length\"\n        padded with trailing 0s\n    \"\"\"\n    return '{0}{{:{1}>{2}}}'.format(pre, 0, length).format(bin(x)[2:])",
        "sha1": "287e5bb87f31b71ad7ccd1cf65fab729794eeef4",
        "id": 25262
    },
    {
        "content": "def characterLevelIndex(X, digits_word):\n    \"\"\"\n        Map each character present in the dataset into an unique integer. All digits are mapped into a single array.\n\n        :param X: Data to retrieve characters from\n        :param digits_word: Words regrouping all digits\n        :return: A dictionary where each character is maped into a unique integer, the maximum number of words in the data, the maximum of characters in a word\n    \"\"\"\n\n    # Create a DETERMINISTIC set of all characters\n    used = set()\n    all_chars = [c for s in X for w in s for c in w if c not in used and (used.add(c) or True)]\n\n    # Create an index for each character\n    # The index 1 is reserved for the digits, regrouped under the word param `digits_word`\n    char2ind = {char: index for index, char in enumerate(all_chars, 2)}\n    ind2char = {index: char for index, char in enumerate(all_chars, 2)}\n\n    # To deal with out-of-vocabulary words\n    char2ind.update({digits_word:1})\n    ind2char.update({1:digits_word})\n\n    # For padding\n    maxWords = max([len(s) for s in X])\n    maxChar  = max([len(w) for s in X for w in s])\n    print(\"Maximum number of words in a sequence  :\", maxWords)\n    print(\"Maximum number of characters in a word :\", maxChar)\n\n    return char2ind, maxWords, maxChar",
        "sha1": "535491f7835ff4d108f3373e17c7eb5865fd21e2",
        "id": 580617
    },
    {
        "content": "def MockIterIfs(arg):\n  \"\"\"Mock out pynetlinux.Interface.Iterifs, return no interfaces.\"\"\"\n  return []",
        "sha1": "6d8baafe6bf571b72f72f665eee4c12defa939da",
        "id": 81537
    },
    {
        "content": "def sort_font(fontlist):\n\t\"\"\"\n\tReturns a new list that is first sorted by the font's compatibleFamilyName3\n\t(Windows compatible name), and secondly by the font's macStyle (style name).\n\t\"\"\"\n\treturn sorted(fontlist, key=lambda font: (font.compatibleFamilyName3, font.macStyle))",
        "sha1": "57f1e6ab15a8a338ecb5f623c24b6ddd7d11ce4b",
        "id": 391321
    },
    {
        "content": "def enforce_use_of_all_cpus(model):\n    \"\"\"For sklearn models which have an `n_jobs` attribute,\n    set to -1. This will force all cores on the machine to be\n    used.\n\n    Args:\n        model : sklearn model\n            A trainable sklearn model\n\n    Returns:\n        model : sklearn model\n            Model 'as is' with `n_jobs` set to one if it\n            exists\n\n    \"\"\"\n    setattr(model, 'n_jobs', -1)\n    return model",
        "sha1": "6fb7878700ffc2fea960432ed76f6d1d90638a32",
        "id": 28203
    },
    {
        "content": "import six\n\n\ndef get_probs_for_labels(labels, prediction_results):\n  \"\"\" Given ML Workbench prediction results, get probs of each label for each instance.\n\n  The prediction results are like:\n  [\n    {'predicted': 'daisy', 'probability': 0.8, 'predicted_2': 'rose', 'probability_2': 0.1},\n    {'predicted': 'sunflower', 'probability': 0.9, 'predicted_2': 'daisy', 'probability_2': 0.01},\n    ...\n  ]\n\n  Each instance is ordered by prob. But in some cases probs are needed for fixed\n  order of labels. For example, given labels = ['daisy', 'rose', 'sunflower'], the\n  results of above is expected to be:\n  [\n    [0.8, 0.1, 0.0],\n    [0.01, 0.0, 0.9],\n    ...\n  ]\n  Note that the sum of each instance may not be always 1. If model's top_n is set to\n  none-zero, and is less than number of labels, then prediction results may not contain\n  probs for all labels.\n\n  Args:\n    labels: a list of labels specifying the order of the labels.\n    prediction_results: a pandas DataFrame containing prediction results, usually returned\n        by get_prediction_results() call.\n\n  Returns:\n    A list of list of probs for each class.\n  \"\"\"\n\n  probs = []\n  if 'probability' in prediction_results:\n    # 'probability' exists so top-n is set to none zero, and results are like\n    # \"predicted, predicted_2,...,probability,probability_2,...\n    for i, r in prediction_results.iterrows():\n      probs_one = [0.0] * len(labels)\n      for k, v in six.iteritems(r):\n        if v in labels and k.startswith('predicted'):\n          if k == 'predict':\n            prob_name = 'probability'\n          else:\n            prob_name = 'probability' + k[9:]\n          probs_one[labels.index(v)] = r[prob_name]\n      probs.append(probs_one)\n    return probs\n  else:\n    # 'probability' does not exist, so top-n is set to zero. Results are like\n    # \"predicted, class_name1, class_name2,...\n    for i, r in prediction_results.iterrows():\n      probs_one = [0.0] * len(labels)\n      for k, v in six.iteritems(r):\n        if k in labels:\n          probs_one[labels.index(k)] = v\n      probs.append(probs_one)\n    return probs",
        "sha1": "f18580a5aba09df56fbb9b45b8ca5753eeba0d62",
        "id": 10956
    },
    {
        "content": "def format_decomposition_string(decomposition_string, delimiter=\";\"):\n    \"\"\"Returns a formatted text string representation of a query decomposition\n        for manually reviewing HIT results.\n    \n    Parameters\n    ----------\n    decomposition_string : str\n        decompositiong string with delimited steps \n    delimiter : str\n        delimiter at the end of decomposition steps\n    \n    Returns\n    -------\n    str\n        Returns formatted text string, numbered steps, each step in a new line\n    \"\"\"\n    text_decomposition = \"\"\n    # remove last delimiter\n    if decomposition_string[-1] == delimiter:\n        decomposition_string = decomposition_string[:-1]\n    # break into steps\n    steps = decomposition_string.split(delimiter)\n    for i in range(len(steps)):\n        text_decomposition += '\\n\\t' + str(i+1) + '. ' + steps[i]\n    return text_decomposition",
        "sha1": "0218b7b40c4a52e3a32020d05f2c8f8c2e6ae5e6",
        "id": 526428
    },
    {
        "content": "import math\n\n\ndef phaseNearTargetPhase(phase,phase_trgt):\n\t\"\"\"\n\tAdds or subtracts 2*math.pi to get the phase near the target phase.\n\t\"\"\"\n\tpi2 = 2*math.pi\n\tdelta = pi2*int((phase_trgt - phase)/(pi2))\n\tphase += delta \n\tif(phase_trgt - phase > math.pi): \n\t\tphase += pi2\n\t\treturn phase\n\tif(phase_trgt - phase < -math.pi):\n\t\tphase -= pi2\n\treturn phase",
        "sha1": "dbd1a7c291b47703fa64756c986815d2cf706677",
        "id": 691132
    },
    {
        "content": "def dict_path_get(dicttree, path, default=None):\n    \"\"\"Use a /-separated path to query a structure of nested dicts.\n\n    Note: If the path does not exist, this will return None instead of\n    raising an exception.\n\n    >>> nested = {\n    ...     'a': {\n    ...         'b': {\n    ...             'c': 'foo'\n    ...         }\n    ...     }\n    ... }\n    ...\n    >>> dict_path_get(nested, 'a/b/c')\n    'foo'\n\n    \"\"\"\n    d = dicttree\n    try:\n        for k in path.split('/'):\n            d = d[k]\n    except KeyError:\n        d = default\n    return d",
        "sha1": "f8dce1f2838f687a72c7431a48ed3764adb91cb2",
        "id": 332870
    },
    {
        "content": "def get_montmort_numbers(max, mod):\n    \"\"\"\n    0 ... max \u307e\u3067\u306e\u30e2\u30f3\u30e2\u30fc\u30eb\u6570\n    \u6574\u6570 1, 2, 3, \u2026, n \u3092\u8981\u7d20\u3068\u3059\u308b\u9806\u5217\u306b\u304a\u3044\u3066\u3001i \u756a\u76ee (i \u2264 n) \u304c i \u3067\u306a\u3044\u9806\u5217 (\u5b8c\u5168\u9806\u5217, \u652a\u4e71\u9806\u5217) \u306e\u500b\u6570\u3002\n    1, 0, 1, 2, 9, 44, 265, 1854, 14833, 133496, ...\n    https://ja.wikipedia.org/wiki/\u5b8c\u5168\u9806\u5217\n    https://oeis.org/A000166\n    :param int max:\n    :param int mod:\n    \"\"\"\n    dp = [0] * (max + 1)\n    dp[0] = 1\n    dp[1] = 0\n    for i in range(2, len(dp)):\n        dp[i] = (dp[i - 1] + dp[i - 2]) * (i - 1) % mod\n    return dp[:max + 1]",
        "sha1": "abf34166813782200d41126c8bfdb645a8a66da2",
        "id": 416754
    },
    {
        "content": "def handle_aligns(aligns):\n    \"\"\"Converts comma delimited string of alignments to tuple\n\n    Arguments:\n        aligns: comma delimited string of 'l', 'r', 'c' or empty string\n    Returns:\n        aligns_tuple: tuple of alignments or None\n    \"\"\"\n    if not aligns:\n        aligns_tuple = None\n    else:\n        aligns_tuple = tuple(aligns.lower().replace(\" \", \"\").split(\",\"))\n    return aligns_tuple",
        "sha1": "892adca706e338e4cad39c570c73612f9e775640",
        "id": 585612
    },
    {
        "content": "def get_csv_row(table_row, file_link):\n    \"\"\"\n    Get a file's name, download link, and size\n    :param table_row: A 'tr' tag object\n    :param file_link: A file's download link\n    :return: A dictionary whose pairs are 'filename':<filename>, 'download_link':<download link, and 'filesize':<size>\n    \"\"\"\n    keys = ['filename', 'download_link', 'filesize']\n    elements = table_row.find_all('td')\n    csv_row = elements[1].a['href'], file_link, elements[3].text\n    return dict(zip(keys, csv_row))",
        "sha1": "c87280071df319c0fc5234f0f969b3beb3d3f272",
        "id": 91020
    },
    {
        "content": "import re\n\n\ndef get_next_page(current_page):\n    \"\"\"Return next page based on current page url.\n\n    Parameters\n    ----------\n    current_page : str\n        Current page url.\n\n    Returns\n    -------\n    str\n    \"\"\"\n    regex = r\"(?<=page=)\\d+\"\n    next_page_number = str(int(*re.findall(regex, current_page)) + 1)\n    return re.sub(regex, next_page_number, current_page)",
        "sha1": "d395904c95d70de8b069fc16e2204e97fa9eeaa7",
        "id": 132527
    },
    {
        "content": "import yaml\n\n\ndef get_creation_options(config: str, driver: str):\n    \"\"\"\n    Gets a list of options for a specific format or returns None.\n    :param config: The configuration for a datasource.\n    :param driver: The file format to look for specific creation options.\n    :return: A tuple of None or the first value is list of warp creation options, and\n     the second value is a list of translate create options.\"\"\"\n    if config:\n        conf = yaml.safe_load(config) or dict()\n        params = conf.get(\"formats\", {}).get(driver, {})\n        return params.get(\"warp_params\"), params.get(\"translate_params\")\n    return None, None",
        "sha1": "9b4c31f13850ce22b91e3d0a8634eea97c505f9d",
        "id": 126394
    },
    {
        "content": "import string\n\n\ndef is_valid_file_name(str_input):\n    \"\"\"Returns if str is valid file name.\n    May only contain: ascii_lowercase, ascii_uppercase, digits, dot, dash, underscore\n    \"\"\"\n    allowed = set(string.ascii_lowercase + string.ascii_uppercase + string.digits + '.-_')\n    if str_input is not '':\n        return set(str_input) <= allowed\n    return False",
        "sha1": "1f0499245417b7b39283e1be11512c10ac1db367",
        "id": 647925
    },
    {
        "content": "def flux_error(g, num_flux, true_flux):\n    \"\"\"Computes discrete L2-error norm for the face-centered fluxes\"\"\"\n\n    A = g.face_areas\n    error = (A * (true_flux - num_flux) ** 2).sum() ** 0.5 / (\n        A * true_flux ** 2\n    ).sum() ** 0.5\n\n    return error",
        "sha1": "4c9e635badae1d5f1d16999ecbf3f0e610c552a6",
        "id": 421191
    },
    {
        "content": "def has_bash_info(filename):\n    \"\"\"Check if a file can be executed with bash.\n    \n    This basically checks if the first line contains #!/bin/bash or #!/bin/sh.\n    \"\"\"\n    with open(filename, 'r') as infile:\n        first_line = infile.readline()\n\n    return first_line.strip() in [\"#!/bin/bash\", \"#!/bin/sh\"]",
        "sha1": "3a1caa499cbe66cbeb52711aa01ee308c783712f",
        "id": 525304
    },
    {
        "content": "def bytechr(i):\n    \"\"\"Return bytestring of one character with ordinal i; 0 <= i < 256.\"\"\"\n    if not 0 <= i < 256:\n        if not isinstance(i, int):\n            raise TypeError('an integer is required')\n        else:\n            raise ValueError('bytechr() arg not in range(256)')\n    return chr(i).encode('latin1')",
        "sha1": "0949f417ec521edb4e3edef103e099ef43057869",
        "id": 26313
    },
    {
        "content": "def crop(img, box):\n    \"\"\"\n    Crop regoin of box in image.\n    Input:\n        img: np.array\n        box: (left, top, width, height), int\n    Return:\n        croped_img: np.array    \n    \"\"\"\n    croped_img = img[box[1]:(box[1]+box[3]), box[0]:(box[0]+box[2])]\n    return croped_img",
        "sha1": "56e1497245c265d2bdca76ed683414996cbf41eb",
        "id": 53860
    },
    {
        "content": "def minutes_readable(minutes):\n    \"\"\"\n    convert the duration in minutes to a more readable form\n\n    Args:\n        minutes (float | int): duration in minutes\n\n    Returns:\n        str: duration as a string\n    \"\"\"\n    if minutes <= 60:\n        return '{:0.0f} min'.format(minutes)\n    elif 60 < minutes < 60 * 24:\n        minutes /= 60\n        if minutes % 1:\n            fmt = '{:0.1f} h'\n        else:\n            fmt = '{:0.0f} h'\n        return fmt.format(minutes)\n    elif 60 * 24 <= minutes:\n        minutes /= 60 * 24\n        if minutes % 1:\n            fmt = '{:0.1f} d'\n        else:\n            fmt = '{:0.0f} d'\n        return fmt.format(minutes)\n    else:\n        return str(minutes)",
        "sha1": "9115070cb17786cbb866d7849ad2b261ad6652e6",
        "id": 571031
    },
    {
        "content": "import mimetypes\n\n\ndef get_mimetype(filename):\n    \"\"\"\n    helper function to return MIME type of a given file\n\n    :param filename: filename (with extension)\n\n    :returns: MIME type of given filename\n    \"\"\"\n\n    return mimetypes.guess_type(filename)[0]",
        "sha1": "d40bef48f3745909c14dfae106ce40ba41f7a80d",
        "id": 140310
    },
    {
        "content": "from typing import List\nfrom typing import Any\n\n\ndef __get_command(name: str, commands: List[Any], strip: bool) -> Any:\n    \"\"\"Pick a command from the list either by short name or name.\"\"\"\n    selector = 'short_name' if strip else 'name'\n    for command in commands:\n        if command[selector] == name:\n            return command",
        "sha1": "a7c436c954c2f54840d8d0547ee6fcd80ff615d3",
        "id": 427477
    },
    {
        "content": "def is_palindrome(number):\n    \"\"\"\n    Check if a number is a palindrome.\n\n    :param number: The int to check.\n\n    :returns: True if the number is a palindrome, else False.\n    \"\"\"\n    number_string = str(number)\n    reversed_number = ''.join(reversed(number_string))\n    return number_string == reversed_number",
        "sha1": "a1842ec14f095adc6d5c3ea6c1e7e137e457adc7",
        "id": 102100
    },
    {
        "content": "def _get_tree_expression_rec(chunks) -> str:\n    \"\"\"\n    Build a 'Binary Expression Tree' for the ffmpeg pts selection\n\n    :param chunks: List of chunks that have the format [oldStart, oldEnd, newStart, newEnd]\n    :return: Binary tree expression to calculate the speedup for the given chunks\n    \"\"\"\n    if len(chunks) > 1:\n        split_index = int(len(chunks) / 2)\n        center = chunks[split_index]\n        return 'if(lt(N,{}),{},{})'.format(center[0],\n                                           _get_tree_expression_rec(chunks[:split_index]),\n                                           _get_tree_expression_rec(chunks[split_index:]))\n    else:\n        chunk = chunks[0]\n        local_speedup = (chunk[3] - chunk[2]) / (chunk[1] - chunk[0])\n        offset = - chunk[0] * local_speedup + chunk[2]\n        return 'N*{}{:+}'.format(local_speedup, offset)",
        "sha1": "b46374b41d661460743b08e83a8a2db791e11253",
        "id": 560327
    },
    {
        "content": "def get_GiB(x: int):\n    \"\"\"return x GiB.\"\"\"\n    return x * (1 << 30)",
        "sha1": "1405e34ac74a5f7156fdebc90e2b1dabd1ff5025",
        "id": 562295
    },
    {
        "content": "def dsu_sort(idx, seq):\n    \"\"\"Sorts a list of tuples according to the idx column using a Decorate-Sort-Undecorate method\"\"\"\n\n    for i, e in enumerate(seq):\n        seq[i] = (e[idx], e)\n    seq.sort()\n    seq.reverse()\n    for i, e in enumerate(seq):\n        seq[i] = e[1]\n    return seq",
        "sha1": "a20f28aa10522d9653a85421104755f31861218c",
        "id": 15741
    },
    {
        "content": "from pathlib import Path\n\n\ndef data_dir(test_dir: Path) -> Path:\n    \"\"\"\n    Create a directory for storing the mock data set.\n    \"\"\"\n\n    _data_dir = test_dir / 'data'\n    _data_dir.mkdir(exist_ok=True)\n    return _data_dir",
        "sha1": "3b204816252a2c87698197a416a4e2de218f639d",
        "id": 709136
    },
    {
        "content": "def get_next(data):\n    \"\"\"\n    Get next element from facebook JSON response,\n    or return None if no next present.\n    \"\"\"\n    try:\n        return data['paging']['next']\n    except KeyError:\n        return None",
        "sha1": "e4041383b524eba8a8e210cf2061d1e339980831",
        "id": 564636
    },
    {
        "content": "def get_attribute_gender_ethnicity(data, path_col, col_suffix=\"\"):\n    \"\"\"\n    Given a panda.DataFrame that has one column which contains path to each\n    gender-ethnicity image, extract four information about the image and append\n    to different columns. These columns are:\n\n    attribute column (att + col_suffix) - contains ethnicity_gender information\n    ethnicity column (e + col_suffix) - contains ethnicity information\n    gender column (g + col_suffix) - contains gender information\n    label column (a + col_suffix) - ethnicity + gender abbreviated information\n\n    Parameters\n    ----------\n    data        pandas.DataFrame has column 'path_col' containing file paths\n    path_col    column name of the column that contains path to each image file\n    col_suffix  suffix to add to attribute, ethnicity, gender, and label columns\n\n    Returns\n    -------\n    data:       pandas.DataFrame w/ attribute, ethnicity, gender, label columns\n    \"\"\"\n    attribute_col = f\"att{col_suffix}\"\n    ethnicity_col = f\"e{col_suffix}\"\n    gender_col = f\"g{col_suffix}\"\n    label_col = f\"a{col_suffix}\"\n\n    data[attribute_col] = data[path_col].apply(lambda x: x.split(\"/\")[0])\n    data[ethnicity_col] = data[attribute_col].apply(\n        lambda x: x.split(\"_\")[0][0].upper()\n    )\n    data[gender_col] = data[attribute_col].apply(lambda x: x.split(\"_\")[1][0].upper())\n    data[label_col] = data[ethnicity_col] + data[gender_col]\n\n    for col in [attribute_col, ethnicity_col, gender_col, label_col]:\n        data[col] = data[col].astype(\"category\")\n\n    return data",
        "sha1": "ce97533a69672422d21a10038e9d401d44ca7454",
        "id": 466098
    },
    {
        "content": "def mark_backend_unsupported(func):\n    \"\"\"Mark a method as being not supported by a backend.\"\"\"\n    func._oculy_backend_unsupported = True\n    return func",
        "sha1": "5d1053dfcfeaf7a078a0b8cb87321152d9ee1111",
        "id": 667009
    },
    {
        "content": "from typing import Tuple\n\n\ndef extract_package_name_and_version(line: str) -> Tuple[str, str]:\n    \"\"\"Returns the name and the version of the package. If the version is not\n    provided, it is set to Not provided.\n    \n    Parameters\n    ----------\n\n    package_line: str\n        Line extracted from the conda environment file of the form package=version or package==version\n            \n    Returns\n    -------\n\n    Tuple[str, str] \n        Name and version of the package\"\"\"\n\n    split_line = line.split(\"=\")\n    name = split_line[0]\n    # It is mandatory to use the last element of the list\n    # for the version as pip packages are written name==version\n    version = split_line[-1] if len(split_line) > 1 else \"Not provided\"\n\n    return (name, version)",
        "sha1": "933afc797672725e2ab779e2fcff5f89087fe9d7",
        "id": 154269
    },
    {
        "content": "import re\n\n\ndef parseCookieFile(cookiefile):\n    \"\"\"Parse a cookies.txt file and return a dictionary of key value pairs\n    compatible with requests.\"\"\"\n\n    cookies = {}\n    with open (cookiefile, 'r') as fp:\n        for line in fp:\n            if not re.match(r'^\\#', line):\n                lineFields = line.strip().split('\\t')\n                cookies[lineFields[5]] = lineFields[6]\n    return cookies",
        "sha1": "6345f7edc1504f9def38c2b6cc881512327da56d",
        "id": 222951
    },
    {
        "content": "def chirp_mass(mass1, mass2):\n    \"\"\"\n    Takes two masses and calculates the corresponding chirpmass.\n    Args:\n        mass1: Mass 1\n        mass2: Mass 2\n\n    Returns:\n        chirpmass: The chirpmass that corresponds to mass1, mass2\n    \"\"\"\n\n    return (mass1 * mass2) ** (3 / 5) / (mass1 + mass2) ** (1 / 5)",
        "sha1": "41ccb3e573f361538c7591942b445e629b4c6c84",
        "id": 184679
    },
    {
        "content": "def peal_speed_to_blow_interval(peal_minutes: float, num_bells: int) -> float:\n    \"\"\" Calculate the blow interval from the peal speed, assuming a peal of 5040 changes \"\"\"\n    peal_speed_seconds = peal_minutes * 60\n    seconds_per_whole_pull = peal_speed_seconds / 2520  # 2520 whole pulls = 5040 rows\n    return seconds_per_whole_pull / (num_bells * 2 + 1)",
        "sha1": "4127b57089f8f348cfeea46c763a2b7bbb089684",
        "id": 58988
    },
    {
        "content": "from typing import List\n\n\ndef convert_to_frequencies(bins: List[int], sample_rate: float, fft_size: int) -> List[float]:\n    \"\"\"\n    Convert the sparse list of bins provided to a list of actual frequencies\n\n    :param bins: A sparse list of bins from an fft, fftshift() already applied\n    :param sample_rate: The sample rate in sps\n    :param fft_size: The number of bins in the full fft\n    :return: A list of frequencies\n    \"\"\"\n\n    # map bins to frequencies, remember our bins are now +- values around zero\n    bin_hz = sample_rate / fft_size\n    freqs = [(bin_value - fft_size // 2) * bin_hz for bin_value in bins]\n    return freqs",
        "sha1": "29db1d63d4f97ec20d363df0300b4d906521381c",
        "id": 531448
    },
    {
        "content": "from typing import Any\n\n\ndef keep_each_tree_predicate(\n    y: Any, raw_predictions: Any, previous_loss: float, current_loss: float\n) -> bool:\n    \"\"\"This predicate will always return True.\n\n    This implies that every new tree will be used in the ensemble.\"\"\"\n    return True",
        "sha1": "78b6c3c3867c36361eab4c2d5a136ef5aaf62b68",
        "id": 121467
    },
    {
        "content": "def convert_subscripts(old_sub, symbol_map):\n    \"\"\"Convert user custom subscripts list to subscript string according to `symbol_map`.\n\n    Examples\n    --------\n    >>>  oe.parser.convert_subscripts(['abc', 'def'], {'abc':'a', 'def':'b'})\n    'ab'\n    >>> oe.parser.convert_subscripts([Ellipsis, object], {object:'a'})\n    '...a'\n    \"\"\"\n    new_sub = \"\"\n    for s in old_sub:\n        if s is Ellipsis:\n            new_sub += \"...\"\n        else:\n            # no need to try/except here because symbol_map has already been checked\n            new_sub += symbol_map[s]\n    return new_sub",
        "sha1": "4a97647cb14726ba3b0bc109c626b8c8cc1b5815",
        "id": 668294
    },
    {
        "content": "def limit_rgb(in_v, limiter):\n    \"\"\"\n    Takes a value and it's maximum potential and maps it to 0-255\n    :param in_v: input value\n    :param limiter: value's maximum potential\n    :return: int\n    \"\"\"\n    out_v = int((in_v/limiter) * 255)\n    return out_v",
        "sha1": "59269f079d4b6849fd3f1a893d7826b3d8e301bb",
        "id": 200682
    },
    {
        "content": "def extract_weights(network):\n    \"\"\"\n    Will return the current parameters of the network along with their names.\n    Inputs:     - A neural network object (i.e. from torch.nn.Module)\n    \n    Outputs:    - A tuple containing the tensor in each parameter of the network at the moment the function was called.\n                - A tuple containing the names of each parameter.\n    \"\"\"\n    params = []\n    names = []\n    for name, param in network.named_parameters():\n        params.append(param.detach().clone())\n        names.append(name)\n    params = tuple(params)\n    names = tuple(names)\n    return params, names",
        "sha1": "8a2c8135bcdf656a7cf3165fab73cf15cfbefefd",
        "id": 214564
    },
    {
        "content": "def convert_prob_labels(list_of_probabilities, list_of_labels):\n    \"\"\"Converts lists of probabilities and labels so that they are tied together.\n\n    Args:\n        list_of_probabilities(list): a list of probabilities sent in from the predictor.\n        list_of_labels(list): a list of labels sent in from the predictor.\n\n    Returns:\n        probabilities_list(list): A list of the top 5 probabilities of the predictor.\n        class_list(list): A list of the art styles corresponding to with the probabilities.\n    \"\"\"\n    probabilities_dict = dict(zip(list_of_labels, list_of_probabilities))\n    sorted_probabilities_dict = dict(sorted(probabilities_dict.items(), key=lambda item: item[1], reverse=True))\n    class_list = list(sorted_probabilities_dict.keys())[:5]\n    probabilities_list = list(sorted_probabilities_dict.values())[:5]\n    return probabilities_list, class_list",
        "sha1": "6bb6a87143ca59235a0fa246bdf4132e831647c8",
        "id": 471278
    },
    {
        "content": "import random\n\n\ndef random_string(length, letters=\"abcdefghijklmnopqrstuvwxyz\"):\n    \"\"\"\n    Returns a random string  of length `length` from a set of letters.\n    \"\"\"\n    return \"\".join([random.choice(letters) for _ in range(length)])",
        "sha1": "05791734997e44159fda61fd62fb83ec2c9cc138",
        "id": 220555
    },
    {
        "content": "def remaining_G7_8_cap(k12):\n    \"\"\"\n    Remaining enrollment capacity available for grades 7-8.\n\n    \"\"\"\n    return (k12['G7_8_cap'] - k12['G7_8']).clip(0)",
        "sha1": "901faee930f7f7b5b885f2852f0fe4c13dc289a8",
        "id": 111206
    },
    {
        "content": "def parse_genome_size(gs_file: str) -> int:\n    \"\"\"\n    Parse genome size from input file\n\n    Args:\n        gs_file (str): File containing the genome size of the sample\n\n    Returns:\n        int: genome size\n    \"\"\"\n    with open(gs_file, 'rt') as gs_fh:\n        return int(gs_fh.readline().rstrip())",
        "sha1": "565783c8581ef70355f77903fb3668ff82041135",
        "id": 325840
    },
    {
        "content": "def create_buffer(node):\n    \"\"\"Create circular buffer from node.\"\"\"\n    node['next'] = node\n    node['prev'] = node\n    return node",
        "sha1": "39e7a6acab17c27d89bf62464b8c5ff56d35ba8d",
        "id": 278765
    },
    {
        "content": "def is_core_type(type_):\n    \"\"\"Returns \"true\" if the type is considered a core type\"\"\"\n    return type_.lower() in {\n        'int', 'long', 'int128', 'int256', 'double',\n        'vector', 'string', 'bool', 'true', 'bytes', 'date'\n    }",
        "sha1": "3ef8c43869c0eb717cca04defd69a569a38f4afa",
        "id": 217102
    },
    {
        "content": "def get_parents(cur, term_id):\n    \"\"\"Return a set of parents for a given term ID.\"\"\"\n    cur.execute(\n        f\"\"\"SELECT DISTINCT object FROM statements\n            WHERE stanza = '{term_id}' AND predicate IN ('rdfs:subClassOf', 'rdfs:subPropertyOf')\n            AND object NOT LIKE '_:%'\"\"\"\n    )\n    return set([x[0] for x in cur.fetchall()])",
        "sha1": "31992acc6dfa72a60e4c9b522c665956e7c2c0c1",
        "id": 398565
    },
    {
        "content": "def match_server_args_factory(tick_rate: int, realtime: bool, observations_only: bool, env_config_string: str):\n    \"\"\" Helper factory to make a argument dictionary for servers with varying ports \"\"\"\n\n    def match_server_args(port):\n        arg_dict = {\n            \"tick_rate\": tick_rate,\n            \"port\": port,\n            \"realtime\": realtime,\n            \"observations_only\": observations_only,\n            \"config\": env_config_string\n        }\n        return arg_dict\n\n    return match_server_args",
        "sha1": "5059d0a4224067f485455a54f4e6d1f83ba68531",
        "id": 11633
    },
    {
        "content": "def str_null_empty(value: str) -> str:\n    \"\"\"\n    Test if a string is null or empty\n\n    Args:\n        value (str): string to test\n\n    Raises:\n        TypeError: if ``value`` is not a string.\n        ValueError: if ``value`` is null or empty.\n\n    Returns:\n        str: ``value``\n    \"\"\"\n    if not isinstance(value, str):\n        raise TypeError(\"value is required to be a string instance.\")\n    s = value.strip()\n    if len(s) > 0:\n        return value\n    raise ValueError(\"must not be null or empty string\")",
        "sha1": "2263c95b8c032bd1e5466b9350bb4cfa502b170c",
        "id": 506831
    },
    {
        "content": "def f_to_c(f):\n    \"\"\"\n    Convert fahrenheit to celcius\n    \"\"\"\n    return (f - 32.0) / 1.8",
        "sha1": "12df6accdfd901e35c3ddd4086b98f6d0cce79ef",
        "id": 666836
    },
    {
        "content": "import torch\n\n\ndef correlate_local(sparse_descriptors: torch.Tensor, dense_descriptors: torch.Tensor):\n    \"\"\"Compute local correlation maps.\n    Args:\n        * sparse_features: Sparse descriptors of size [N x C]\n        * dense_features: Batch of local descriptor maps of size [B x C x H x W]\n    Returns:\n        * local_cmaps: The concatenated local correspondence maps, of size [B x H x W]\n    \"\"\"\n    batch, channels, height, width = dense_descriptors.shape\n    local_cmaps = torch.bmm(\n        sparse_descriptors.unsqueeze(1), dense_descriptors.view(batch, channels, -1)\n    ).view(batch, height, width)\n    return local_cmaps",
        "sha1": "4cdea3781476d5d693deed49bda1c35ea4c85de6",
        "id": 338809
    },
    {
        "content": "def normalize(df):\n    \"\"\" Normalizes data to [0,1].\n    \"\"\"\n    # copy the dataframe\n    df_norm = df.copy()\n    # apply min-max scaling\n    for column in df_norm.columns:\n        df_norm[column] = (df_norm[column] - df_norm[column].min()) / (df_norm[column].max() - df_norm[column].min())\n\n    return df_norm",
        "sha1": "67ed8ac72df8750b34ff87c132d22b04df9be918",
        "id": 38576
    },
    {
        "content": "import types\nimport asyncio\n\n\ndef loop_apply_coroutine(loop, func: types.FunctionType, *args, **kwargs) -> object:\n    \"\"\"\n    Call a function with the supplied arguments.\n    If the result is a coroutine, use the supplied loop to run it.\n    \"\"\"\n    if asyncio.iscoroutinefunction(func):\n        future = asyncio.ensure_future(\n            func(*args, **kwargs), loop=loop)\n\n        loop.run_until_complete(future)\n        return future.result()\n    else:\n        return func(*args, **kwargs)",
        "sha1": "d77a70540237f690e712e30b93b53b363907b678",
        "id": 704049
    },
    {
        "content": "def bytes_to_text(s, encoding):\n    \"\"\"\n    Convert bytes objects to strings, using the given encoding. Illegally\n    encoded input characters are replaced with Unicode \"unknown\" codepoint\n    (\\ufffd).\n\n    Return any non-bytes objects without change.\n    \"\"\"\n    if isinstance(s, bytes):\n        return str(s, encoding, 'replace')\n    else:\n        return s",
        "sha1": "2b5b45be32725070d14c180b6dde6039c292b598",
        "id": 96853
    },
    {
        "content": "def check_num(num):\n    \"\"\"Checks if num is smaller than 20\"\"\"\n    return num < 20",
        "sha1": "08bff87189cd9b4e51ff8af03668ae08364982eb",
        "id": 406859
    },
    {
        "content": "import torch\n\n\ndef get_all_pairs_indices(labels):\n    \"\"\"\n    Given a tensor of labels, this will return 4 tensors.\n    The first 2 tensors are the indices which form all positive pairs\n    The second 2 tensors are the indices which form all negative pairs\n    \"\"\"\n    labels1 = labels.unsqueeze(1)\n    labels2 = labels.unsqueeze(0)\n    matches = (labels1 == labels2).byte()\n    diffs = matches ^ 1\n    matches -= torch.eye(matches.size(0)).byte().to(labels.device)\n    a1_idx = matches.nonzero()[:, 0].flatten()\n    p_idx = matches.nonzero()[:, 1].flatten()\n    a2_idx = diffs.nonzero()[:, 0].flatten()\n    n_idx = diffs.nonzero()[:, 1].flatten()\n    return a1_idx, p_idx, a2_idx, n_idx",
        "sha1": "b388c79990267bf52519fd583b822ade988abe97",
        "id": 255544
    },
    {
        "content": "import unicodedata\nimport re\n\n\ndef slugify(string):\n    \"\"\"Slugifies a string.\n    Based on public domain code from https://github.com/zacharyvoase/slugify\n    \"\"\"\n    normalized_string = str(unicodedata.normalize(\"NFKD\", string))\n    no_punctuation = re.sub(r\"[^\\w\\s-]\", \"\", normalized_string).strip().lower()\n    slug = re.sub(r\"[-\\s]+\", \"-\", no_punctuation)\n    return slug",
        "sha1": "27ef8a800f89dc69b3cec2a653cdfdb248a6232b",
        "id": 121749
    },
    {
        "content": "def ReadIgnoreFile(infile):\n    \"\"\"Parses a file indicating which communities should be skipped by\n    RemoveLowComplexityRepeats.\n\n    The numbers of communities to skip should be separated by commas\n    Lines beginning with # will be treated as comments and skipped\n    (eg providing justification for skipping these communities).\n    \"\"\"\n    ignore_list=[]\n    inhandle=open(infile, 'r')\n    for line in inhandle:\n        #Skip comments\n        if line[0]=='#': continue\n        ignore_list+=line.split(',')\n\n    inhandle.close()\n    return ignore_list",
        "sha1": "98b25c8b26c0d4a8c320e66a55628c8e62c996c0",
        "id": 533690
    },
    {
        "content": "def _execute(presto_cur, presto_stmt):\n    \"\"\"\n    Wrapper around the prestodb.dbapi.Cursor.execute() method\n    Params:\n        presto_cur (prestodb.dbapi.Cursor) : presto connection cursor\n        presto_stmt (str) : presto SQL statement\n    Returns:\n        prestodb.dbapi.Cursor : Cursor after execute method called\n    \"\"\"\n    presto_cur.execute(presto_stmt)\n    return presto_cur",
        "sha1": "fc7cebdbf1acfd2ae24a2b0b72dba88c66b81259",
        "id": 403615
    },
    {
        "content": "def author_clean(author):\n    \"\"\" Clean an author and return the formatted string \"\"\"\n    replace = [\".\", \";\", \" \", \",\", \"_\", \"-\"]\n    author_split = author.strip().split(\",\")\n    clean_author = \"\"\n    if len(author_split) >= 2:\n        last_name = author_split[0]\n        first_name = author_split[1]\n        for rep in replace:\n            first_name = first_name.replace(rep, \"\")\n        clean_author = last_name + \" \" + first_name\n    else:\n        for rep in replace:\n            clean_author = author.replace(rep, \"\")\n\n    return clean_author",
        "sha1": "85f43ab0d8d2c6f40508a3770aad0d24c75daaa1",
        "id": 633893
    },
    {
        "content": "def tmp(tmpdir_factory):\n    \"\"\"Create a common temp directory.\"\"\"\n    return str(tmpdir_factory.mktemp('tmp_test'))",
        "sha1": "3901ede63f669752622f7273f876347218b0011c",
        "id": 19022
    },
    {
        "content": "def get_task_runs(task_id, task_runs_json):\n\n    \"\"\"\n    Search through all task runs to get the set matching the input task id\n\n    :param task_id: task.json['id'] for the task we want the associated task runs for\n    :type task_id: int\n    :param task_runs_json: task runs from json.load(open('task_run.json'))\n    :type task_runs_json: list\n\n    :return: all task runs where task_run.json['task_id'] == :param task_id:\n    :rtype: list\n    \"\"\"\n\n    output_list = []\n    for tr in task_runs_json:\n        if task_id == tr['task_id']:\n            output_list.append(tr)\n\n    return output_list",
        "sha1": "a35d3084d9d6b4c1e8644aa302176f026d6209a6",
        "id": 211762
    },
    {
        "content": "def is_word(c):\n  \"\"\"\n  :param c character to check\n  :returns True if c is alphanumeric\n  \"\"\"\n  return c.isalnum()",
        "sha1": "01618e0bbee1ac4c0756a35f69539824faa9c577",
        "id": 497095
    },
    {
        "content": "def concat_dataset(data, bdate, edate):\n    \"\"\"Concatenate pandas DataFrame with DateTimeIndex to the specified time\n    period (bdate, edate).\n\n    Args:\n        data (pandas DataFrame): Air sensor dataset to concatenate.\n        bdate (str): The beginning timestamp for the concatenated dataset.\n        edate (str): The ending timestamp for the concatenated dataset.\n\n    Returns:\n        data (pandas DataFrame): The concatenated sensor dataset.\n\n    \"\"\"\n    if bdate is not None:\n        data = data.loc[bdate:, :]\n    if edate is not None:\n        data = data.loc[:edate, :]\n    return data",
        "sha1": "0f9109b013ed145b9ea8eea6830902aadac156ad",
        "id": 575038
    },
    {
        "content": "def factorial(digit: int) -> int:\n    \"\"\"\n    >>> factorial(3)\n    6\n    >>> factorial(0)\n    1\n    >>> factorial(5)\n    120\n    \"\"\"\n\n    return 1 if digit in (0, 1) else (digit * factorial(digit - 1))",
        "sha1": "7f81bd9c11995b796ccfb568b2635d4e4355dcc4",
        "id": 287688
    },
    {
        "content": "from datetime import datetime\n\n\ndef fromtimestamp_ms(timestamp: int) -> datetime:\n    \"\"\"Construct datetime object from UTC timestamp in milliseconds.\n\n    Args:\n        timestamp (int): UTC time in milliseconds\n\n    Returns:\n        datetime\n\n    \"\"\"\n    return datetime.fromtimestamp(timestamp / 1000)",
        "sha1": "58d31ca8c50051b2494bdfcc150ea9889316eb74",
        "id": 332102
    },
    {
        "content": "import re\n\n\ndef wildcards2patterns(wildcards):\n    \"\"\"\n    Compute a list of regular expression patterns from a list of\n    wildcard strings. A wildcard string uses '*' as a wildcard character\n    matching anything.\n\n    :param wildcards: List of wildcard strings to compute regular expression patterns for.\n    :type wildcards: list of str\n\n    :returns: Computed regular expressions.\n    :rtype: list of obj\n    \"\"\"\n    # note that we add the ^ and $ so that the *entire* string must\n    # match. Without this, e.g. a prefix will match:\n    # re.match('.*good\\\\.com', 'good.com.evil.com')  # match!\n    # re.match('.*good\\\\.com$', 'good.com.evil.com') # no match!\n    return [re.compile('^' + wc.replace('.', r'\\.').replace('*', '.*') + '$') for wc in wildcards]",
        "sha1": "626db001e04cd09d6fbad1896ca36ea81f1fefda",
        "id": 468257
    },
    {
        "content": "def xorstrings(str1, str2):\n    \"\"\"Xor 'str1' [bytes] with 'str2' [bytes].\"\"\"\n    res = []\n    for i in range(min(len(str1), len(str2))):\n        res.append(str1[i] ^ str2[i])\n    return bytes(res)",
        "sha1": "ab93a60f524b0518c8d52c54c2b1f7e8736d6d33",
        "id": 339179
    },
    {
        "content": "def is_other_or_unknown(df):\n    \"\"\"Return if vehicle is other/unknown, per NHTSA convention.\"\"\"\n    yr = df['YEAR']\n    body = df['BODY_TYP']\n    tow_veh = df['TOW_VEH']\n\n    return ((yr.between(1975, 1981) & (body.between(35, 42) |\n                                       body.isin([44, 45, 99]))) |\n            (yr.between(1982, 1990) & (body.isin([13, 14, 42, 52, 73, 77,\n                                                  80, 81, 82, 83, 88, 89,\n                                                  90, 99]))) |\n            ((1991 <= yr) & (body.isin([12, 13, 23, 42, 65, 73, 90,\n                                        91, 92, 93, 94, 95, 96, 97,\n                                        99, 98]) |\n                             ((body == 79) & tow_veh.isin([5, 6])))))",
        "sha1": "a14ce7414adf17e46410c0ad4a7681b57f32f7f4",
        "id": 96334
    },
    {
        "content": "def linalg_matrix_get_elem(a, m, n):\n    \"\"\"\n    Return matrix element m, n\n\n    Parameters:\n\n    a (array): The vector\n    m (int): The row index\n    n (int): The column index\n\n    Return (number): a[m,n]    \n    \"\"\"\n\n    return a[m,n]",
        "sha1": "3bfcbc54fe06a3af4d14c470f518791e42409631",
        "id": 327858
    },
    {
        "content": "def _sample_distribution_over_matrix(rng):\n  \"\"\"Samples the config for a distribution over a matrix.\n\n  Args:\n    rng: np.random.RandomState\n\n  Returns:\n    A distribution over matrix config (containing a tuple with the name and\n    extra args needed to create the distribution).\n  \"\"\"\n  # At this point, the choices here are arbitrary. We know some things about\n  # the spectrum of the hessian for various problems we care about but certainly\n  # not all. For example see: https://arxiv.org/abs/1901.10159.\n  # These selections define a distribution over spectrum. This distribution is\n  # NOT like the ones found in prior work, but tries to be broader, and\n  # (hopefully) will contain spectrum from real problems.\n  choice = rng.choice([\"normal\", \"linspace_eigen\", \"uniform\", \"logspace_eigen\"])\n  if choice == \"normal\":\n    mean = rng.normal(0, 1)\n    std = rng.uniform(0, 2)\n    return choice, {\"mean\": mean, \"std\": std}\n  elif choice == \"uniform\":\n    min_v = rng.uniform(-3, 1)\n    max_v = min_v + rng.uniform(0, 5)\n    return choice, {\"min\": min_v, \"max\": max_v}\n  elif choice == \"linspace_eigen\":\n    min_v = rng.uniform(0.01, 50)\n    max_v = min_v + rng.uniform(0, 100)\n    return choice, {\"min\": min_v, \"max\": max_v}\n  elif choice == \"logspace_eigen\":\n    min_v = 1e-5\n    max_v = rng.uniform(1, 1000)\n    return choice, {\"min\": min_v, \"max\": max_v}",
        "sha1": "20c39319f217e62bd33e06f4f556030f133baff7",
        "id": 660063
    },
    {
        "content": "def add_suffix(event, fields, suffix, separator='_'):\n    \"\"\" Adds a suffix to a field or list of fields\n\n    :param event: A dict with the entire event\n    :param field_or_field_list: A single field or list of fields for which to add a suffix\n    :param suffix: The suffix to add to the fields\n    :param separator: The character to place between the name and the suffix\n    :return: An altered event with the altered field names\n\n    Examples:\n\n    .. code-block:: python\n\n        # Example #1\n        event = {'a_field': 'a_value'}\n        event = add_suffix(event, fields='a_field', suffix='an_ending')\n        event = {'a_field_an_ending': 'a_value'}\n\n        # Example #2\n        event = {'a_field': 'a_value'}\n        event = add_suffix(event, fields='a_field', suffix='an_ending', separator='---')\n        event = {'a_field---an_ending': 'a_value'}\n\n        # Example #3\n        event = {'a_field': 'a_value',\n                'another_field': 'another_value'}\n        event = add_suffix(event, fields=['a_field', 'another_field'], suffix='an_ending')\n        event = {'a_field_an_ending': 'a_value',\n                'another_field_an_ending': 'another_value'}\n\n    \"\"\"\n\n    if type(fields) is not list:\n        fields = [fields]\n\n    for k, v in event.items():\n        if k in fields:\n            event[k + separator + suffix] = event[k]\n            del event[k]\n\n    return event",
        "sha1": "ae5bfa64d2fc5dc13ad1f0c961964cdaee479b4c",
        "id": 512972
    },
    {
        "content": "def SecondOrderDamping(U, DampCoeff):\n    \"\"\"Return second-order damping term.\n\n    Calculated using Equation 6-80 in CFD Vol. 1 by Hoffmann.\n\n    Call signature:\n\n        SecondOrderDamping(U, DampCoeff)\n\n    Parameters\n    ----------\n    U : 1D or 2D array\n\n        The dependent variable from time level (n) within the domain.\n\n    DampCoeff: float\n\n        Damping coefficient.\n\n    Returns\n    -------\n    D : 1D or 2D array\n\n        The second-order damping term within the entire domain.\n    \"\"\"\n    D = U.copy()  # Initialize D\n    D[1:-1] = -DampCoeff*(U[0:-2] - 2.0*U[1:-1] + U[2:])\n    return D",
        "sha1": "74732518ea74b33fd3cc57a3369aca92412af0ca",
        "id": 212854
    },
    {
        "content": "from typing import Tuple\n\n\ndef add_pixels_mod(one: Tuple[int], two: Tuple[int]):\n    \"\"\"Adds two pixels' values together and keeps them in range of [0,255].\n    \"\"\"\n    return tuple(\n        (one[i] + two[i]) % 256 for i in range(0, 3)\n    )",
        "sha1": "d324a0c7b580f9181a8353a8caeac545797abe47",
        "id": 323229
    },
    {
        "content": "def get_items(dict):\n   \"\"\" extract the key/value pairs from a dictionary \"\"\"\n   return zip (dict.keys(), dict.values())",
        "sha1": "dc57acc6eab575b0f7321381496996ad68cfa705",
        "id": 211891
    },
    {
        "content": "def Schmidt(**kwargs):\n    \"\"\"\n    Calculates Schmidt number based upon either of two input variable sets.\n\n    First method:\n       Sc(nu = kinematic viscosity,\n          alpha = thermal diffusivity)\n\n    Second method:\n        Sc(D = mass diffusivity,\n           mu = dynamic viscosity,\n           rho = fluid density)\n    \"\"\"\n\n    if ('mu' in kwargs) and ('D' in kwargs) and ('rho' in kwargs):\n        schmidt_number = kwargs['mu'] / (kwargs['rho'] * kwargs['D'])\n    elif ('nu' in kwargs) and ('D' in kwargs):\n        schmidt_number = kwargs['nu'] / kwargs['D']\n    else:\n        raise KeyError('Incorrect variable assignment')\n\n    return schmidt_number",
        "sha1": "1141ee06876a875fc830f7333ba225ad2cd44562",
        "id": 451161
    },
    {
        "content": "import torch\n\n\ndef construct_K(fx, fy, cx, cy, dtype=torch.float, device=None):\n    \"\"\"Construct a [3,3] camera intrinsics from pinhole parameters\"\"\"\n    return torch.tensor([[fx, 0, cx],\n                         [0, fy, cy],\n                         [0, 0, 1]], dtype=dtype, device=device)",
        "sha1": "e92130e997846cd5bee475c17f88649121e2fcb4",
        "id": 501844
    },
    {
        "content": "def time_vst(velocity, displacement):\n    \"\"\"Usage: Calculate time taken using velocity and displacement.\"\"\"\n\n    return displacement/velocity",
        "sha1": "11aebe9eb51b58d09153342ee1ad9fc911b2f362",
        "id": 58356
    },
    {
        "content": "def next_token(mention):\n    \"\"\" Compute the token following a mention.\n\n    Args:\n        mention (Mention): A mention.\n\n    Returns:\n        The tuple ('next', TOKEN), where TOKEN is the (lowercased) token\n        following the mention. If no such token exists, set TOKEN to 'NONE'.\n    \"\"\"\n    next_t = mention.get_context(1)\n\n    if next_t:\n        return \"next\", next_t[0].lower()\n    else:\n        return \"next\", \"NONE\"",
        "sha1": "fc3d6e3bada571edc959e07889d34a11e5a5068f",
        "id": 375083
    },
    {
        "content": "import math\n\n\ndef gaussian(X, height, x_position, standard_deviation):\n    \"\"\"Return standard gaussian function\n\n    This is the unnormalised gaussian function\n\n        f(x)=height*exp(-(x-x_position)^2/(2*standard_deviation^2))\n\n    Parameters\n    ----------\n    height:\n        This is the maximum of the gaussian peak.\n\n        This function does not normalise to constant area, the caller\n        must do this if this is what they want.\n\n    x_position:\n        This is the x position of the centre of the gaussian.  If the\n        guassian is being used to apply the impulse response of an\n        instrument applied to an XRD reflection, then this will be the\n        two-theta position of the peak.\n\n    standard_deviation:\n        The standard deviation of the guassian curve.\n\n        If this function is being applied in spectroscopy, optics or\n        electrical engineering, it is common for gaussians to be\n        defined in terms of Full Width at Half Maximum (FWHM), which\n        is the width of the peak when the height drops to half\n        of the peak height, specified by the height parameter.  If\n        the x-axis represents frequency, and the function height\n        is proportional to energy or power, then this will be the\n        gaussian's bandwidth, that is, the width between the -3db points.\n\n        To convert from FWHM to standard deviation use the relationship:\n            FWHM = 2*sqrt(2*log(2)) * standard_deviation\n\n    Returns\n    -------\n    double:\n        Evaluated gaussian function.\n\n    \"\"\"\n\n    return height * math.e**(-(X - x_position)**2 / 2 / standard_deviation**2)",
        "sha1": "1c2b6cbbcaed07eb99ea392fd55f61a0d6e8b62e",
        "id": 166487
    },
    {
        "content": "import calendar\n\n\ndef timestamp(datetime):\n    \"\"\" Returns UTC timestamp, this is included in python3 but not 2\"\"\"\n    return calendar.timegm(datetime.timetuple())",
        "sha1": "f94bfd142a1021771b0d32899efc1e0877d0ac63",
        "id": 208633
    },
    {
        "content": "def __repr_indicators(f):\n    \"\"\" String representation of an indicator (replace spaces with '#'). \"\"\"\n    return \"\".join([\"#\" if i == \" \" else i for i in f[1:3]])",
        "sha1": "fe1c04053fbd79c701791a2219b903f9c3d50bf1",
        "id": 133785
    },
    {
        "content": "def _rst_underline(text, markup):\n    \"\"\"Add and underline RsT string matching the length of the given string.\n    \"\"\"\n    return text + '\\n' + markup * len(text)",
        "sha1": "55cc28c0b7bcbcbb197716580b6e8f4a81f927c8",
        "id": 212781
    },
    {
        "content": "def split_text(txt, trunc=None):\n    \"\"\"Split text into sentences/words\n\n    Args:\n        txt(str): text, as a single str\n        trunc(int): if not None, stop splitting text after `trunc` words\n                    and ignore sentence containing `trunc`-th word\n                    (i.e. each sentence has len <= trunc)\n    Returns:\n        sentences(list): list of sentences (= list of lists of words)\n    \"\"\"\n    n_words = 0\n    sentence_break = [\".\", \"?\", \"!\"]\n    sentences = []\n    cur_sentence = []\n    for word in txt.split():\n        if word in sentence_break:\n            if len(cur_sentence) != 0:\n                cur_sentence.append(word)\n                sentences.append(cur_sentence)\n                cur_sentence = []\n            else:\n                pass\n        else:\n            n_words += 1\n            cur_sentence.append(word)\n\n        if trunc is not None and n_words > trunc:\n            cur_sentence = []\n            break\n\n    if len(cur_sentence) != 0:\n        sentences.append(cur_sentence)\n    return sentences",
        "sha1": "f900d3b75dc541e1aed1326674939f65fc3b0900",
        "id": 679121
    },
    {
        "content": "def is_daily_file_cadence(file_cadence):\n    \"\"\" Evaluate file cadence to see if it is daily or greater than daily\n\n    Parameters\n    ----------\n    file_cadence : dt.timedelta or pds.DateOffset\n        pysat assumes a daily file cadence, but some instrument data file\n        contain longer periods of time.  This parameter allows the specification\n        of regular file cadences greater than or equal to a day (e.g., weekly,\n        monthly, or yearly). (default=dt.timedelta(days=1))\n\n    Returns\n    -------\n    is_daily : bool\n        True if the cadence is daily or less, False if the cadence is greater\n        than daily\n\n    \"\"\"\n    is_daily = True\n\n    if hasattr(file_cadence, 'days'):\n        if file_cadence.days > 1:\n            is_daily = False\n    else:\n        if not (hasattr(file_cadence, 'microseconds')\n                or hasattr(file_cadence, 'seconds')\n                or hasattr(file_cadence, 'minutes')\n                or hasattr(file_cadence, 'hours')):\n            is_daily = False\n\n    return is_daily",
        "sha1": "e69f57b413b15aca7ffa4be73b8872254deb78be",
        "id": 334115
    },
    {
        "content": "def _get_detail_value(var, attr):\n    \"\"\"\n    Given a variable and one of its attributes that are available inside of\n    a template, return its 'method' if it is a callable, its class name if it\n    is a model manager, otherwise return its value\n    \"\"\"\n    value = getattr(var, attr)\n    # Rename common Django class names\n    kls = getattr(getattr(value, '__class__', ''), '__name__', '')\n    if kls in ('ManyRelatedManager', 'RelatedManager', 'EmptyManager'):\n        return kls\n    if callable(value):\n        return 'routine'\n    return value",
        "sha1": "8a7e15e7c1f0dd5355485b0da383964ae9f4fe37",
        "id": 567464
    },
    {
        "content": "from typing import List\n\n\ndef chunk_string_by_len(string: str, n: int = 3) -> List[str]:\n    \"\"\"This function returns chunks of strings of length n\n    Args:\n        string (str): Any String\n        n (int): Length of chunk\n    Returns:\n        List[str]: List of all possible chunks\n    \"\"\"\n    return [string[i:i+n] for i in range(0, len(string), n)]",
        "sha1": "45a31a0019b2984f36089b5c181490f978cd1758",
        "id": 313457
    },
    {
        "content": "def collatz_sequence(initial_word, deletion_number = 2, production_rules = {'a': 'bc', 'b': 'a', 'c': 'aaa'}):\n    \"\"\" Computes a collatz sequence, wherein the words are determined (until length < deletion numbers) by:\n    1) deleting the first m (deletion number) symbols\n    2) appending production word P(x) calculated using the first symbol x.\n    2) appending production word P(x) calculated using the first symbol x.\n\n    Args:\n        initial_word (string): The starting word.\n        deletion_number (int): The positive integer used to determine deletion and halting.\n        production_rules (dict): Production rules associating symbols with production words.\n    \"\"\"\n    word = initial_word\n    sequence = [initial_word]\n    while len(word) >= deletion_number:\n        word = word[deletion_number:] + production_rules[word[0]]\n        sequence.append(word)\n    return sequence",
        "sha1": "8979fcfa918fd2accfe26d9d3661dc1fef080e14",
        "id": 40561
    },
    {
        "content": "def get_width(img):\n    \"\"\"\n    Returns the number of columns in the image\n    \"\"\"\n    return len(img[0])",
        "sha1": "2e70d54c78c9bf4b53f92f5233aae3f554baf3e9",
        "id": 189171
    },
    {
        "content": "import socket\n\n\ndef validate_ip_address(ip_address):\n    \"\"\"Validates an IPv4 address.\"\"\"\n\n    try:\n        socket.inet_aton(ip_address)\n        return True\n    except socket.error:\n        return False",
        "sha1": "07e0e9dd697204471dd35fa2273363c9fbd1f181",
        "id": 136248
    },
    {
        "content": "import importlib\n\n\ndef load_services(services):\n    \"\"\"Loads the requested services, returns the set actually loaded.\"\"\"\n    loaded = set()\n    for service in services:\n        name = 'fixie_' + service\n        try:\n            importlib.import_module(name)\n            loaded.add(service)\n        except ImportError as e:\n            msg = 'failed to load fixie service: ' + name\n    return loaded",
        "sha1": "0e6249d306240fd674062e8d04ab2c775d57c549",
        "id": 457783
    },
    {
        "content": "def eval_poly(vec, poly):\n    \"\"\"\n    Evaluates value of a polynomial at a given point\n    :param vec: The given point :float\n    :param poly: The polynomial :ndarray[3,]\n    :return: value of polynomial at given point :float\n    \"\"\"\n    return vec ** 2 * poly[0] + vec * poly[1] + poly[2]",
        "sha1": "98c230f1548871df83e4ae9a0493705e7903d649",
        "id": 319350
    },
    {
        "content": "from typing import Optional\n\n\ndef parse_image_tag(image: str) -> Optional[tuple[str, str]]:\n    \"\"\"\n    Split a full image ref, e.g. `\"python:3\"` into its repo and tag, `('python', '3')`.\n    If there is no tag, `\":latest\"` is assumed.\n    If the image ref cannot be parsed, `None` is returned.\n    \"\"\"\n    if not isinstance(image, str):\n        return None\n    if image.count(\":\") > 1:\n        return None\n    if \":\" in image:\n        repo, tag = image.split(\":\")\n        return repo, tag\n    return image, \"latest\"",
        "sha1": "878a34a1296caca8855d0d960f8ac06ccbcec069",
        "id": 154092
    },
    {
        "content": "def get_int(string):\n    \"\"\"return the int value of a binary string\"\"\"\n    return int(string, 2)",
        "sha1": "8bc253df00acfa3a5a1918e3f31ed1a4c0d8f75f",
        "id": 132782
    },
    {
        "content": "def indent(level):\n    \"\"\"For use as a mako filter.\n\n    Returns a function that indents a block of text to the provided level.\n    \"\"\"\n\n    def indent_text_to_level(text, level):\n        result = \"\"\n        indentation = level * \"  \"\n        for line in text.splitlines(True):\n            result = result + indentation + line\n        return result\n\n    return lambda text: indent_text_to_level(text, level)",
        "sha1": "942785a20693a5bada69f775c3ecb6ce82decaf4",
        "id": 695115
    },
    {
        "content": "def A000120(n: int) -> int:\n    \"\"\"1's-counting sequence.\n\n    number of 1's in binary expansion of n (or the binary weight of\n    n).\n    \"\"\"\n    return f\"{n:b}\".count(\"1\")",
        "sha1": "295d0c41e5d28233d5cd14a1a12921040eb51c32",
        "id": 252702
    },
    {
        "content": "def steps(execution_id, query_params, client):\n    \"\"\"Get execution steps for a certain execution.\"\"\"\n\n    return client.get_execution_steps(execution_id, **query_params)",
        "sha1": "4b2a29ec32db80cbe02980d5f2585c48e8284338",
        "id": 661371
    },
    {
        "content": "def convert_to_dec_hrs(time):    \n    \"\"\" Returns the decimal Hour for a time string of form HH:MM:SS.xxx \"\"\"\n    h= time.hour*1.0\n    h += time.minute/60.0\n    h += time.second/3600.0\n    return h",
        "sha1": "a944e73f974e2c8578506a9f69e143ed763c462c",
        "id": 394284
    },
    {
        "content": "def round_list(data, n_digits):\n    \"\"\"\n    Recursively round all floats in a list to n digits.\n\n    :param data: the list to round\n    :type data: list\n    :param n_digits: amount of digits to round to\n    :type n_digits: int\n    :return: the list with values rounded\n    :rtype: list\n    \"\"\"\n\n    result = data.copy()\n\n    for i, value in enumerate(data):\n        if isinstance(value, float):\n            result[i] = round(value, n_digits)\n        elif isinstance(value, list):\n            result[i] = round_list(value, n_digits)\n\n    return result",
        "sha1": "e4768db622a65a06ef443dbdc3a30c76e7678f43",
        "id": 565138
    },
    {
        "content": "def create_method_callback(method_name, **kwargs):\n    \"\"\"Creates a callback function which executes model method.\n\n    Args:\n        method_name: The name (str) of the method to execute.\n        kwargs: Optional keyword arguments.\n\n    Returns:\n        Callback to be used with run_model. For example, to call the 'test'\n        method of a DaliModule:\n\n        cb = create_method_callback('test')\n    \"\"\"\n    def callback_method(model):\n        return getattr(model, method_name)(**kwargs)\n    return callback_method",
        "sha1": "b12602d91639bddb5318278f6e5ab24bd5342ac9",
        "id": 215727
    },
    {
        "content": "def _round_to_n(sequence, n=100):\n    \"\"\" Resolves issue when dividing by N results in a set of numbers that don't add up to N again\n\n        E.g. 100 / 3 = 33\n             33 * 3 = 99\n\n        For the above example, this function returns [33, 33, 34]\n     \"\"\"\n    if sum(sequence) != n:\n        sequence[-1] = n - sum(sequence[:-1])\n    return sequence",
        "sha1": "9f39256b0ebac3287226ba2edb84957e91e2274d",
        "id": 425281
    },
    {
        "content": "def to_pandas_edgelist(G, source='src', destination='dst'):\n    \"\"\"\n    Returns the graph edge list as a Pandas DataFrame.\n\n    Parameters\n    ----------\n    G : cugraph.Graph\n        Graph containing the edgelist.\n\n    source : str or array-like, optional (default='source')\n        source column name or array of column names\n\n    destination : str or array-like, optional (default='destination')\n        destination column name or array of column names\n\n    Returns\n    -------\n    df : pandas.DataFrame\n        pandas dataframe containing the edgelist as source and\n        destination columns.\n    \"\"\"\n    pdf = G.to_pandas_edgelist(source=source, destination=destination)\n    return pdf",
        "sha1": "3977ce5058a2564560ef3bae0f344b78fcf69e3e",
        "id": 453016
    },
    {
        "content": "def is_start_byte(b: int) -> bool:\n    \"\"\"Check if b is a start character byte in utf8\n       See https://en.wikipedia.org/wiki/UTF-8\n       for encoding details\n    Args:\n        b (int): a utf8 byte\n\n    Returns:\n        bool: whether or not b is a valid starting byte\n    \"\"\"\n\n    # a non-start char has encoding 10xxxxxx\n    return (b >> 6) != 2",
        "sha1": "8d27198671436c4e9accd80198dfe934c43f679b",
        "id": 17427
    },
    {
        "content": "def chebyshev_distance(pos1: tuple, pos2: tuple):\n    \"\"\"\n    Compute Chebyshev distance between two points\n    :param pos1: Coordinate of first point\n    :param pos2: Coordinate of second point\n    :return: Chebyshev distance between two points\n    \"\"\"\n    distance = 0\n    for ind in range(len(pos1)):\n        distance = max(distance, abs(pos1[ind] - pos2[ind]))\n    return distance",
        "sha1": "3595a7f72b4fc9d73785785f9dd8760609d2ea74",
        "id": 474272
    },
    {
        "content": "def format_persec_column_label(label):\n    \"\"\" Return a re-formatted PerSecData column label\n\n    This function converts column labels to snake case (all lower-case\n    and spaces converted to underscores).\n\n    Parameters\n    ----------\n    label: str\n        The original column label to be formatted\n\n   Returns\n   -------\n   formatted_label: str\n        The formatted label\n    \"\"\"\n    assert isinstance(label, str)\n\n    formatted_label = label.lower().replace(' ', '_')\n\n    assert formatted_label.islower() and ' ' not in formatted_label\n\n    return formatted_label",
        "sha1": "b882ea9e0255d8cebec40f6d99c8c01df81959dc",
        "id": 612811
    },
    {
        "content": "import random\n\n\ndef random_data(size):\n    \"\"\"\n    Generates a list of integers from 0 to size-1, randomly shuffled.\n    \"\"\"\n    data = list(range(size))\n    random.shuffle(data)\n    return data",
        "sha1": "f0ff3d25a9c6f0256203ed997c39fc617a897509",
        "id": 196156
    },
    {
        "content": "def count_fq(zz):\n    \"\"\" x is a sorted list with repeats. returns a list of [count,value]\n    where count is the number of times value is repeated in the list\"\"\"\n    res = []\n    s = 0\n    z = list(zz)\n    z.append(-100000)\n    for i in range(len(z)-1):\n        if not z[i] == z[i+1]:\n            v = [s+1,z[i]]\n            res.append(v)\n            s = 0\n        else:\n            s=s+1\n    return res",
        "sha1": "242274b2722ea657f83c881399c0d2f31560d59b",
        "id": 75527
    },
    {
        "content": "def force_list(obj):\n    \"\"\"Force object to be a list.\n\n    If ``obj`` is a scalar value then a list with that value as\n    sole element is returned, or\n    if ``obj`` is a tuple then it is coerced into a list.\n\n    \"\"\"\n    if isinstance(obj, tuple):\n        return list(obj)\n    elif not isinstance(obj, list):\n        return [obj]\n    return obj",
        "sha1": "25eecea6f5aee1dbd1f6ce23643079c16871f2a2",
        "id": 151996
    },
    {
        "content": "def make_enum_pair(ele, till, delimiter=\", \"):\n    \"\"\"\n    Create list of pair of strings with consecutive numbering as follows:\n    make_enumerated_pair('P%d p%d', 3) -> 'P1 p1, P2 p2, P3 p3'\n    \"\"\"\n    return delimiter.join(ele % (i + 1, i + 1) for i in range(till))",
        "sha1": "66b9410f307027ef151dd81edb7b29998ef65628",
        "id": 29829
    },
    {
        "content": "def _string_to_list(string_list):\n    \"\"\" Take a string with comma separated elements a return a list of\n    those elements.\n\n    :param string_list: String with comma separated elements.\n    :type string_list: str\n    :return: List with elements.\n    :rtype: list\n    \"\"\"\n    string_list_no_spaces = string_list.replace(' ', '')\n    element_list = list(string_list_no_spaces.split(','))\n    return element_list",
        "sha1": "692803447c11c69c6eadc70c821ff171c858f454",
        "id": 539833
    },
    {
        "content": "import socket\nimport logging\n\n\ndef _BeautifyError(err):\n  \"\"\"Try to format an error better.\n\n  Since we're dealing with daemon startup errors, in many cases this\n  will be due to socket error and such, so we try to format these cases better.\n\n  @param err: an exception object\n  @rtype: string\n  @return: the formatted error description\n\n  \"\"\"\n  try:\n    if isinstance(err, socket.error):\n      return \"Socket-related error: %s (errno=%s)\" % (err.args[1], err.args[0])\n    elif isinstance(err, EnvironmentError):\n      if err.filename is None:\n        return \"%s (errno=%s)\" % (err.strerror, err.errno)\n      else:\n        return \"%s (file %s) (errno=%s)\" % (err.strerror, err.filename,\n                                            err.errno)\n    else:\n      return str(err)\n  except Exception: # pylint: disable=W0703\n    logging.exception(\"Error while handling existing error %s\", err)\n    return \"%s\" % str(err)",
        "sha1": "ba0be8b681a731cf12a7d0c7b341c9547d80b5bc",
        "id": 132158
    },
    {
        "content": "def remove_first_word(s):\n    \"\"\"Strips the first word from a string\"\"\"\n    return s.split(' ', 1)[1]",
        "sha1": "49e2617a627fa0fd3c00e5ed1dd1519392f5834c",
        "id": 402173
    },
    {
        "content": "def message_for_ts(message_history, ts):\n    \"\"\"\n        Given list of messages and timestamp of desired message, return (message, is_last),\n        or (None, None) if not found.\n    \"\"\"\n    return next((\n        (m, i==len(message_history)-1)\n        for i, m in enumerate(message_history)\n        if m['id'] == ts\n    ), (None, None))",
        "sha1": "fd3e990431000ed271ebc5fe8416249e5aa24cc1",
        "id": 323085
    },
    {
        "content": "import random\n\n\ndef shuffle_data_and_labels(data, labels):\n    \"\"\"\n    Shuffles data and labels the same way\n    :param data: list: input data\n    :param labels: list: corresponding labels\n    :return: tuple, shuffled data and labels\n    \"\"\"\n    temp = list(zip(data, labels))\n    random.shuffle(temp)\n    return zip(*temp)",
        "sha1": "1ac352df2e95a815613dbf5cb8031dd74576c795",
        "id": 267878
    },
    {
        "content": "def rivers_with_station(stations):\n    \"\"\"\n    For a list of MonitoringStation() objects, returns a set of rivers that those stations lie on\n    \"\"\"\n    rivers = set()\n    for s in stations:\n        rivers.add(s.river)\n    return rivers",
        "sha1": "75a9fe833d4d69b7262e86fce97eebffbe89bdb6",
        "id": 406506
    },
    {
        "content": "import asyncio\n\n\nasync def run(cmd):\n    \"\"\"asynchronously run a subprocess, useful for running tncexporter in tests\"\"\"\n    proc = await asyncio.create_subprocess_shell(\n        cmd,\n        stdout=asyncio.subprocess.PIPE,\n        stderr=asyncio.subprocess.PIPE)\n    stdout, stderr = await proc.communicate()\n    if stdout:\n        return stdout.decode()\n    if stderr:\n        return stderr.decode()",
        "sha1": "4aee14481d72445528694ac9bde69428f60dcf8e",
        "id": 600292
    },
    {
        "content": "def create_context_list(simple_context=False, full_context=False):\n    \"\"\"Creates list with words that could be recognized by speech-to-text \n    service\n\n    Args:\n        simple_context: generate simple list with common start of a voice command\n        full_context: generate list with all available commands \n\n    Returns:\n        context: list with expectable words, by default returns empty list\n    \"\"\"\n\n    if full_context:\n        return [ \"Hej\", \"Rico\", \"przynie\u015b\", \"herbat\u0119\", \"potwierdzam\", \"przyjed\u017a\", \"tu\", \"tutaj\" ]\n    if simple_context:\n        return [\"Hej\", \"Rico\"]\n    else:\n        return []",
        "sha1": "e1e7d695a270fc4ffeebd21ba8957cbb085c6e92",
        "id": 282358
    },
    {
        "content": "import time\n\n\ndef check_instance_drained(ecs_c, cluster_name, instance_id, context):\n\n    \"\"\"\n    Checks and waits until an ECS instance has drained all its running tasks.\n\n    Returns True if the instance drains.\n\n    Returns False if there is less than 40 seconds left in the Lambda\n    functions execution and we need to re-invoke to wait longer.\n    \"\"\"\n\n    while True:\n\n        response = ecs_c.describe_container_instances(\n            cluster=cluster_name,\n            containerInstances=[\n                instance_id\n            ]\n        )\n\n        print(\"- Instance has {} running tasks and {} pending tasks\".format(\n            response[\"containerInstances\"][0][\"runningTasksCount\"],\n            response[\"containerInstances\"][0][\"pendingTasksCount\"]\n        ))\n\n        if response[\"containerInstances\"][0][\"runningTasksCount\"] == 0 and \\\n                response[\"containerInstances\"][0][\"pendingTasksCount\"] == 0:\n            return(True)\n\n        if context.get_remaining_time_in_millis() <= 40000:\n            return(False)\n\n        time.sleep(30)",
        "sha1": "8995dc53a244f040dbfa830ae2550e05b0c82b81",
        "id": 452329
    },
    {
        "content": "def convert_to_signed_int_16_bit(hex_str):\n    \"\"\"\n    Utility function to convert a hex string into a 16 bit signed hex integer value\n    :param hex_str: hex String\n    :return: signed 16 bit integer\n    \"\"\"\n    val = int(hex_str, 16)\n    if val > 0x7FFF:\n        val = ((val+0x8000) & 0xFFFF) - 0x8000\n    return val",
        "sha1": "a4a4fc539be7f3429853b2508b9c6bb7331112e6",
        "id": 471396
    },
    {
        "content": "def to_binary(int_value: int):\n    \"\"\"\n    This method converts an int to its binary representation.\n\n    :param int_value: The int value to convert.\n    :return: The binary representation as str.\n    \"\"\"\n    # The implementation does not build on to_numeral\n    # because the native Python conversion is faster\n    result = bin(int_value)\n\n    if len(result) > 2:\n        result = result[2:len(result)]\n\n    return result",
        "sha1": "dfbf4ce0511e814aace29f7a8d1e8331a428e9a3",
        "id": 186026
    },
    {
        "content": "def is_info_hash_valid(data: bytes, info_hash: bytes) -> bool:\n    \"\"\"\n    Checks if the info_hash sent by another peer matches ours.\n    \"\"\"\n    return data[28:48] == info_hash",
        "sha1": "05443f7ede4c6bb85eb7d86d68239a3ef778e629",
        "id": 676982
    },
    {
        "content": "from typing import Optional\nfrom pathlib import Path\nfrom typing import Dict\nfrom typing import Any\nimport yaml\nfrom typing import cast\n\n\ndef read_variables_from_yaml(yaml_file: Optional[Path] = None) -> Dict[str, Any]:\n    \"\"\"\n    Reads a YAML file, that is expected to contain an entry 'variables'. Returns the dictionary for the 'variables'\n    section of the file. If the file name is not given, an empty dictionary will be returned.\n    :param yaml_file: The yaml file to read.\n    :return: A dictionary with the variables from the yaml file.\n    \"\"\"\n    if yaml_file is None:\n        return dict()\n    yaml_contents = yaml.load(yaml_file.open('r'), Loader=yaml.Loader)\n    v = \"variables\"\n    if v in yaml_contents:\n        return cast(Dict[str, Any], yaml_contents[v])\n    else:\n        raise KeyError(f\"The Yaml file was expected to contain a section '{v}'\")",
        "sha1": "cc2e9a0611a8ef9afa25edec2c65f10c4c30f48c",
        "id": 286619
    },
    {
        "content": "def generate_grid(obj, size):\n    \"\"\"Returns an array of x,y pairs representing all the coordinates in a\n    square (size * size) grid centered around a RoomPosition object\"\"\"\n\n    pos = obj.pos or obj\n\n    # set boundaries to respect room position limits\n    left, right = max(0, pos.x - size), min(49, pos.x + size) + 1\n    top, bottom = max(0, pos.y - size), min(49, pos.y + size) + 1\n\n    return [[x, y] for x in range(left, right) for y in range(top, bottom)]",
        "sha1": "50024640d5b59320d594ec4bfccb241657294e01",
        "id": 114336
    },
    {
        "content": "def get_parent_resource(project):\n    \"\"\"Returns the parent resource.\"\"\"\n    return f'projects/{project}'",
        "sha1": "933d13526d48e0a98bcac57933e5c6507cba8209",
        "id": 98670
    },
    {
        "content": "def output_type(to_check: str) -> str:\n    \"\"\"Determine whether the output value of a file is a digit or a string.\n\n    Args:\n        to_check (str): The string variant of the value\n\n    Returns:\n        str: 'dig' if parsing to float succeeded, 'str' otherwise\n\n    \"\"\"\n    try:\n        checked = float(to_check)\n        return 'dig'\n    except:\n        return 'str'",
        "sha1": "389a769c1517c25a0c85ebd71c027f95ac5a0345",
        "id": 226944
    },
    {
        "content": "def convert_string(inpt):\n    \"\"\"Return string value from input lit_input\n\n    >>> convert_string(1)\n    '1'\n    \"\"\"\n    return str(inpt)",
        "sha1": "ebc6052f834cb63464c1e0b40d8724547a4c572e",
        "id": 502077
    },
    {
        "content": "def _remove_overlapping_vars(list_to_check, includes_list):\n    \"\"\" Checks, and removes, any variable in list_to_check that is also in\n    includes_list, returning list_to_check without overlapping variable\n    names.\n    \"\"\"\n    return [x for x in list_to_check if x not in includes_list]",
        "sha1": "72ac9dfa15ab923e4c39ac743918253ad0097595",
        "id": 386373
    },
    {
        "content": "import fnmatch\n\n\ndef get_bucket_inventory(client, bucket, inventory_id):\n    \"\"\"Check a bucket for a named inventory, and return the destination.\"\"\"\n    inventories = client.list_bucket_inventory_configurations(\n        Bucket=bucket).get('InventoryConfigurationList', [])\n    inventories = {i['Id']: i for i in inventories}\n    found = fnmatch.filter(inventories, inventory_id)\n    if not found:\n        return None\n\n    i = inventories[found.pop()]\n    s3_info = i['Destination']['S3BucketDestination']\n    return {'bucket': s3_info['Bucket'].rsplit(':')[-1],\n            'prefix': \"%s/%s/%s\" % (s3_info['Prefix'], bucket, i['Id'])}",
        "sha1": "bb334f5e702e8351b6d3fb1d3865486a7732eaff",
        "id": 439532
    },
    {
        "content": "import getopt\n\n\ndef get_command_line_args(argv):\n    \"\"\"\n    Gets the command line arguments which are:\n\n    Returns\n    -------\n    pm_model (str) - The PowerModels.jl power model e.g. \"DCPPowerModel\"\n    pm_solver (str) - The solver to use, e.g. \"juniper\", \"gurobi\"\n    grid_name (str) - optional if you only want to calculate one grid, e.g. \"rts\"\n    lc (str) - the loadcases to calculate. Either \"ts\" or \"scaled\"\n    kind (str) - the optimizations to run, e.g. \"tnep,ots,repl\". Can only be a part of these like \"tnep,repl\"\n\n    \"\"\"\n    pm_model, pm_solver, grid_name = None, None, None\n    kind = \"tnep,repl,ots\"\n    lc = \"scaled\"\n    try:\n        opts, args = getopt.getopt(argv, \":m:s:g:k:\",\n                                   [\"model=\", \"solver=\", \"grid=\", \"kind=\"])\n    except getopt.GetoptError:\n        raise getopt.GetoptError(\"error reading input\")\n    for opt, arg in opts:\n        if opt in (\"-m\", \"--model\"):\n            pm_model = arg\n        if opt in (\"-s\", \"--solver\"):\n            pm_solver = arg\n        if opt in (\"-g\", \"--grid\"):\n            grid_name = arg\n        if opt in (\"-k\", \"--kind\"):\n            kind = arg\n    if pm_solver is None:\n        UserWarning(\"pm_solver is None. You must specify a solver with '--solver='\")\n    if pm_model is None:\n        UserWarning(\"pm_model is None. You must specify a model with '--model='\")\n    return pm_model, pm_solver, grid_name, lc, kind",
        "sha1": "126de7ba0aa6267ef9052b70ca4273ef1af97601",
        "id": 31642
    },
    {
        "content": "def extract_ccs_path(file_path):\n    \"\"\"Extracts and returns the ccs path from file path.\n\n    Args:\n        file_path (str): full path of file\n\n    Returns:\n        str: ccs path (contained in 'file_path')\n    \"\"\"\n    file_path = file_path.replace('\\\\', '/')\n    index = file_path.index('ccsv')\n    end = file_path.index('/', index)\n\n    ccs_path = file_path[:end]\n\n    return ccs_path",
        "sha1": "9e27cb56f6a272c089b3d4e75d6511dbcf2116a9",
        "id": 583278
    },
    {
        "content": "def extract_sub_peptide(polymer_name, helm_dict, polymer_data):\n    \"\"\"\n    Extracts the specified subpeptide and adds it to the dictionary of HELM strings\n\n    :param polymer_name: name of the polymer to extract\n    :param helm_dict: dictionary of the HELM strings, where keys are the names of the polymers\n    :param polymer_data: HELM sequence of the given subpeptide\n    :return: updated dictionary of the HELM strings\n    \"\"\"\n    if polymer_name not in helm_dict:\n        helm_dict[polymer_name] = \"\"\n    v = helm_dict[polymer_name]\n    if len(v) > 0:\n        v += \"\\n\"\n    v += polymer_name + \"{\" + polymer_data + \"}$$$$\"\n    helm_dict[polymer_name] = v\n\n    return helm_dict",
        "sha1": "0a0daf37c0f63511d2bd03d08c575d48fb617e38",
        "id": 218053
    },
    {
        "content": "def _wave_add_grad_dfx_cc(op, grad):\n  \"\"\"Gradient for Add.\"\"\"\n  return grad, grad",
        "sha1": "91c6eb4f2257ed451140110d767dcfd1c363872a",
        "id": 446443
    },
    {
        "content": "def pfbranch_to_dict(pf,key): \n    \"\"\"\n    Recursive function to convert a single branch in an AntelopePf to a python dict.\n    \n    This function utilizes recursion to follow a chain of arbitrary length of \n    branches defined in an AntelopePf object.   Result is a dict with a chain of \n    dicts of the same length.  i.e. if AntelopePf has 3 levels of branches\n    the dict will have a 3 levels of associative arrays keyed by the same\n    branch names as the Arr items in the original Pf file.  Note this should\n    be called from the top level one branch at a time.  i.e. for the parent\n    AntelopePf this function should be called once for each returned key by \n    pf.arr_keys().\n    \n    Note that at each level Tbl& sections of the original pf are parsed to \n    be converted to lists of strings with each line of the Tbl section being\n    one string in the list.  \n    \n    :param pf: is an AntelopePf.  Recursive calls use get_branch outputs \n       that return one of these.\n    :param key: key used to access the branch requested\n    :type key: string\n    :return:  python dict translation of AntelopePf branch structure\n    :raise:  RunTime errors are possible from the ccore methods that are called.\n    \"\"\"\n    brkeys=pf.arr_keys()\n    if(len(brkeys)>0):\n        # This loads simple parameters at this level\n        allbrdata=pf.todict()\n        # This loads any Tbl& data at this level of the hierarchy\n        tblkeys=pf.tbl_keys()\n        for k in tblkeys:\n            tvals=pf.get_tbl(k)\n            allbrdata[k]=tvals\n        for k in brkeys:\n            pfbranch=pf.get_branch(k)\n            bk=pfbranch_to_dict(pfbranch,k)\n            allbrdata[k]=bk\n        return allbrdata\n    else:\n        brdata=pf.todict()\n        return brdata",
        "sha1": "bba219374bdf6186ef9d9c3845915b5b82eeae07",
        "id": 526014
    },
    {
        "content": "import click\nfrom typing import Union\nfrom typing import Any\nfrom typing import List\nfrom typing import Tuple\nfrom pathlib import Path\n\n\ndef validate_path_pair(\n    ctx: click.core.Context,\n    param: Union[click.core.Option, click.core.Parameter],\n    value: Any,\n) -> List[Tuple[Path, Path]]:\n    \"\"\"\n    Validate a pair of paths expected to be in the format:\n    /absolute/local/path:/remote/path.\n    \"\"\"\n    # We \"use\" variables to satisfy linting tools.\n    for _ in (ctx, param):\n        pass\n\n    result = []  # type: List[Tuple[Path, Path]]\n\n    if value is None:\n        return result\n\n    for path_pair in value:\n        try:\n            [local_path, remote_path] = list(map(Path, path_pair.split(':')))\n        except ValueError:\n            message = (\n                '\"{path_pair}\" is not in the format '\n                '/absolute/local/path:/remote/path.'\n            ).format(path_pair=path_pair)\n            raise click.BadParameter(message=message)\n\n        if not local_path.exists():\n            message = '\"{local_path}\" does not exist.'.format(\n                local_path=local_path,\n            )\n            raise click.BadParameter(message=message)\n\n        if not remote_path.is_absolute():\n            message = '\"{remote_path} is not an absolute path.'.format(\n                remote_path=remote_path,\n            )\n            raise click.BadParameter(message=message)\n\n        result.append((local_path, remote_path))\n\n    return result",
        "sha1": "d6267479828546e395e6691b2495b807ca24e79a",
        "id": 663524
    },
    {
        "content": "def make_video_path(task_path, itr):\n    \"\"\"\n    Generate a video name based on a task name and iteration number\n    :param task_path: the name of the task\n    :param itr: the iteration number\n    \"\"\"\n    return \"{}_tune_iter{}.mp4\".format(task_path.replace(\"_center\", \"\"), itr)",
        "sha1": "ff245ad4890863fd6cfec600071f8b10e4330e63",
        "id": 643115
    },
    {
        "content": "import pathlib\nimport inspect\n\n\ndef get_module_basename(path):\n    \"\"\"\n    Get the module name of the module defined in source ``path``.\n    \"\"\"\n    path = pathlib.Path(path)\n    module_name = inspect.getmodulename(str(path))\n    # This is either garbage or a package\n    if module_name is None:\n        module_name = path.name\n    return module_name",
        "sha1": "c630aa0a8cbb0370c6dd9c968c2dcbe55f87013b",
        "id": 171512
    },
    {
        "content": "import csv\nimport random\n\n\ndef fetchVign(config):\n\t\"\"\"This functions fetch an image, randomly, chosen from a markov tree defined in ram.csv\n\tram.csv definition is: 1st column the name of the image (without extension), subsequent columns, possible outcomes chosen randomly\n\tIt returns an array with the file names\"\"\"\n\tstarts = []\n\tstartdest = []\n\tnvign = 0\n\tcurrVign = \"000\"\n\tstory = []\n\twith open(config[\"csvLocation\"]+\"/\"+config[\"csvTree\"]) as ram:\n\t\tcsvReader = csv.reader(ram)\n\t\tfor row in csvReader:\n\t\t\tstarts.append(row[0])\n\t\t\tstartdest.append(row)\n\twhile nvign <100:\n\t\tstory.append(startdest[starts.index(currVign)][random.randint(1,len(startdest[starts.index(currVign)])-1)])\n\t\tcurrVign = story[nvign]\n\t\tif currVign == \"END\":\n\t\t\treturn story\n\t\tstory[nvign]+=\".png\"\n\t\tnvign +=1\n\tprint(\"tree with no END\")\n\tquit()",
        "sha1": "dbf95656b15a7159052fa9e7bb6c51206ecc7b6f",
        "id": 269802
    },
    {
        "content": "import itertools\n\n\ndef create_discrete_actions(num_buttons, max_buttons_down):\n    \"\"\"\n    Return list of available actions, when we have\n    num_buttons buttons available and we are allowed\n    to press at most max_buttons_down, as\n    a discrete action space.\n\n    Parameters:\n        num_buttons (int): Number of buttons available\n        max_buttons_down (int): How many buttons can be pressed\n                                down at once.\n\n    Returns:\n        actions (List of Lists): A list of available actions\n    \"\"\"\n    # Remove no-op action\n    actions = [\n        list(action) for action in itertools.product((0, 1), repeat=num_buttons)\n        if (sum(action) <= max_buttons_down and sum(action) > 0)\n    ]\n    return actions",
        "sha1": "12b6ccd0efa327fddb52b8b45b663d9abb7a2457",
        "id": 69407
    },
    {
        "content": "def get_fsed(num):\n    \"\"\" Generate random value of f_sed based on distribution provided by Mark Marley:\n    f_sed             Frequency\n    0.000000         0.099\n    0.010000         0.001\n    0.030000         0.005\n    0.100000         0.010\n    0.300000         0.025\n    1.000000         0.280\n    3.000000         0.300\n    6.000000         0.280\n\n    Input num is a uniform random value between 0 and 1\n    \"\"\"\n\n    if num < .099:\n        r = 0\n    elif num < .1:\n        r = .01\n    elif num < .105:\n        r = .03\n    elif num < .115:\n        r = .1\n    elif num < .14:\n        r = .3\n    elif num < .42:\n        r = 1\n    elif num < .72:\n        r = 3\n    else:\n        r = 6\n    return float(r)",
        "sha1": "2597624916f51d6835af8e0e3c71af437ace936b",
        "id": 307063
    },
    {
        "content": "def rem_comment(line):\n    \"\"\"Remove a comment from a line.\"\"\"\n    return line.split(\"#\", 1)[0].rstrip()",
        "sha1": "86907c71b7e285e98345b28be39c2646401eeccc",
        "id": 60989
    },
    {
        "content": "def get_interfaces(client):\n    \"\"\"Display current interface list\n\n    Returns:\n        List of current interface\n    \"\"\"\n    return client.call('get_interfaces')",
        "sha1": "a6923e69d8d1e460d8a5ce49f3fe86af7b0bd0db",
        "id": 438626
    },
    {
        "content": "def to_percent_max100(value):\n    \"\"\"\n    gibt Anteil in Prozent zur\u00fcck; max 100\n    :param value: float\n    :return: float\n    \"\"\"\n    return min(value * 100, 100)",
        "sha1": "bae7efa4d12dcdf0abea824518fc1376e13b6abd",
        "id": 352799
    },
    {
        "content": "def paginated_list(full_list, max_results, next_token):\n    \"\"\"\n    Returns a tuple containing a slice of the full list\n    starting at next_token and ending with at most the\n    max_results number of elements, and the new\n    next_token which can be passed back in for the next\n    segment of the full list.\n    \"\"\"\n    sorted_list = sorted(full_list)\n    list_len = len(sorted_list)\n\n    start = sorted_list.index(next_token) if next_token else 0\n    end = min(start + max_results, list_len)\n    new_next = None if end == list_len else sorted_list[end]\n\n    return sorted_list[start:end], new_next",
        "sha1": "99a20e3af4ff64d3de52eccbc00aea9fda735e00",
        "id": 12455
    },
    {
        "content": "def filter_out_empty_dict_entries(dict_to_filter):\n    \"\"\"\n    Filter out entries in a given dict that correspond to empty values.\n    At the moment this is empty lists, dicts and None\n\n    :param dict_to_filter: dict to filter\n\n    :returns: dict without empty entries\n    \"\"\"\n\n    EMPTY_VALUES = (None, [], {})\n\n    return {key: val for key, val in dict_to_filter.items() if val not in EMPTY_VALUES}",
        "sha1": "872c3fc71f495c3c85e4a7a68e67eff164ce1486",
        "id": 459771
    },
    {
        "content": "import random\nimport string\n\n\ndef random_schema_name() -> str:\n    \"\"\"Generate a random PostgreSQL schema name for testing.\"\"\"\n    return 'temp_{name}'.format(\n        name=''.join(\n            (random.choice(string.ascii_lowercase) for _ in range(10)),  # noqa:S311\n        ),\n    )",
        "sha1": "350ea729e34d1f8262f39f4a1985ab75b421cf26",
        "id": 19112
    },
    {
        "content": "def ra_as_hours(ra_degrees, seconds_decimal_places=2):\n    \"\"\" Takes Right Ascension degrees as float, returns RA string. TESTS OK 2020-10-24.\n    :param ra_degrees: Right Ascension in degrees, limited to 0 through 360. [float]\n    :param seconds_decimal_places: number of places at end of RA string (no period if zero). [int]\n    :return: RA in hours/hex format. [string, or None if outside RA range]\n    \"\"\"\n    if (ra_degrees < 0) | (ra_degrees > 360):\n        return None\n    seconds_decimal_places = int(max(0, seconds_decimal_places))  # ensure int and non-negative.\n    total_ra_seconds = ra_degrees * (3600 / 15)\n    int_hours = int(total_ra_seconds // 3600)\n    remaining_seconds = total_ra_seconds - 3600 * int_hours\n    int_minutes = int(remaining_seconds // 60)\n    remaining_seconds -= 60 * int_minutes\n    if seconds_decimal_places > 0:\n        seconds, fract_seconds = divmod(remaining_seconds, 1)\n        int_fract_seconds = int(round(fract_seconds * 10 ** seconds_decimal_places))\n    else:\n        seconds, fract_seconds, int_fract_seconds = round(remaining_seconds), 0, 0\n    int_seconds = int(seconds)\n    if seconds_decimal_places > 0:\n        if int_fract_seconds >= 10 ** seconds_decimal_places:\n            int_fract_seconds -= 10 ** seconds_decimal_places\n            int_seconds += 1\n    if int_seconds >= 60:\n        int_seconds -= 60\n        int_minutes += 1\n    if int_minutes >= 60:\n        int_minutes -= 60\n        int_hours += 1\n    if int_hours >= 24:\n        int_hours -= 24\n    if seconds_decimal_places > 0:\n        format_string = '{0:02d}:{1:02d}:{2:02d}.{3:0' + str(int(seconds_decimal_places)) + 'd}'\n    else:\n        format_string = '{0:02d}:{1:02d}:{2:02d}'\n    ra_string = format_string.format(int_hours, int_minutes, int_seconds, int_fract_seconds)\n    return ra_string",
        "sha1": "e23cce78633cdb4d182babe095cf13b351ddf68f",
        "id": 25747
    },
    {
        "content": "def uniquify_header(line):\n    \"\"\"Make names unique in a CSV header line.\"\"\"\n    fields = line.strip().split(\",\")\n    for i, field in enumerate(fields):\n        if field in fields[i + 1:]:\n            count = 1\n            for j in range(i, len(fields)):\n                if fields[j] == field:\n                    fields[j] = \"{} [{}]\".format(\n                        field,\n                        count\n                    )\n                    count += 1\n\n    return \",\".join(fields) + \"\\n\"",
        "sha1": "2477b5bfe0c38bcc8a501b9835740f73571d3982",
        "id": 384813
    },
    {
        "content": "def determine_issue_types(warnings):\n    \"\"\"\n    Get a list of issue types.\n\n    :rtype: list\n    \"\"\"\n\n    issue_types = warnings[\"Report\"][\"IssueTypes\"][\"IssueType\"]\n    if not isinstance(issue_types, list):\n        return [issue_types]\n    return issue_types",
        "sha1": "55f96852de474c757fba3a494a4789e67552f88d",
        "id": 458073
    },
    {
        "content": "import hashlib\n\n\ndef hash_file(filepath: str, chunk_size: int = 65536) -> bytes:\n    \"\"\"Calculate the SHA512 of a file\n\n    :param filepath: Path of the file to hash\n    :param chunk_size: Number of bytes per chunk of file to hash\n    :return: File hash in a :class:`bytes` object\n    \"\"\"\n    sha512 = hashlib.sha512()\n    with open(filepath, 'rb') as src:\n        for chunk in iter(lambda: src.read(chunk_size), b''):\n            sha512.update(chunk)\n    return sha512.digest()",
        "sha1": "cc03aaf71b68e3a7d85a69a5bef9610cc12e9dec",
        "id": 70169
    },
    {
        "content": "def jordan_cell_sizes(J):\n    \"\"\"Return a tuple of Jordan cell sizes from a matrix J in Jordan\n    normal form.\n\n    Examples:\n    >>> jordan_cell_sizes(matrix([[1,1,0,0],[0,1,0,0],[0,0,1,1],[0,0,0,1]]))\n    (2, 2)\n    >>> jordan_cell_sizes(matrix([[1,1,0,0],[0,1,1,0],[0,0,1,0],[0,0,0,1]]))\n    (3, 1)\n    >>> jordan_cell_sizes(zero_matrix(5,5))\n    (1, 1, 1, 1, 1)\n    \"\"\"\n    assert J.is_square()\n    sizes = []\n    n = 1\n    for i in range(J.nrows() - 1):\n        if J[i, i+1].is_zero():\n            sizes.append(n)\n            n = 1\n        else:\n            n += 1\n    sizes.append(n)\n    assert sum(sizes) == J.nrows()\n    return tuple(sizes)",
        "sha1": "a9442af8299f294126ec562a22adf19820df6c8f",
        "id": 239164
    },
    {
        "content": "def dtc_converter(result):\n    \"\"\"\n    Converts Diagnostics Trouble Codes (DTCs) result into a cloud friendly format.\n    \"\"\"\n\n    if result.get(\"_type\", None) != \"get_dtc\":\n        raise Exception(\"Unable to convert as DTC result: {:}\".format(result))\n\n    result[\"_type\"] = \"dtc\"\n    result[\"values\"] = [{\"code\": r[0], \"text\": r[1]} for r in result.pop(\"value\", None) or []]\n\n    return result",
        "sha1": "8c6af7454454a28ccc61600a17cb638cf5e88512",
        "id": 559380
    },
    {
        "content": "def odd(n):\n    \"\"\"\n    Checks whether a number is odd.\n    :param n: Number.\n    :return: True if n is odd, False otherwise.\n    \"\"\"\n    return n % 2 == 1",
        "sha1": "18aa1763b1ecb78cc17c441be3b374af6cd95876",
        "id": 153766
    },
    {
        "content": "from typing import Tuple\n\n\ndef rect_area(a: Tuple[int, int, int, int]) -> int:\n    \"\"\"helper function for calculate rectangle area\n\n    Args:\n        a: <List[int]> rectangle\n\n    Returns:\n        <int> calculated rectangle area\n    \"\"\"\n    return a[2] * a[3]",
        "sha1": "1f59be9abbbe58ecd66798e498f742048c9a4dd9",
        "id": 109862
    },
    {
        "content": "def _dedent(text):\n    \"\"\"Like `textwrap.dedent()` but only supports space indents.\"\"\"\n    indents = []\n    for line in text.splitlines():\n        stripped = line.lstrip(\" \")\n        if stripped:\n            indents.append(len(line) - len(stripped))\n    if not indents:\n        return text\n    indent = min(indents)\n    result = []\n    for line in text.splitlines(True):\n        if line.startswith(\" \" * indent):\n            line = line[indent:]\n        result.append(line)\n    return \"\".join(result)",
        "sha1": "e3fe522e6044b41985c90e3f122e7f83b8a39a6e",
        "id": 467627
    },
    {
        "content": "def strip(l):\n    \"\"\"Strip strings from a list.\"\"\"\n\n    return list(map(lambda x: x.strip(), l))",
        "sha1": "a492fd32586ce2328b19f64b339bcc669b427e13",
        "id": 211605
    },
    {
        "content": "import re\nimport string\n\n\ndef rm_word_all_punct(dfcol):\n    \"\"\" Remove words that are entirely punctuation \"\"\"\n    punct = re.escape(string.punctuation) \n    ss = f\"((?<=\\s)|^)([{punct}]+)((?=\\s)|$)\"\n    return dfcol.str.replace(ss, r'', regex=True)",
        "sha1": "2ad82922b01da11466e2b77ed9980ce83de117fe",
        "id": 544405
    },
    {
        "content": "from typing import Counter\n\n\ndef classe_liste(points):\n    \"\"\"\n    Renvoie la classe la plus repr\u00e9sent\u00e9 dans la liste points.\n\n    Parameters\n    ----------\n    points : list\n        Liste de point pour laquelle il faut d\u00e9terminer la classe.\n\n    Returns\n    -------\n    classe : int\n        classe la plus repr\u00e9senter dans la liste points.\n\n    \"\"\"\n    classes = []\n    for point in points:\n        classes.append(point[-1])\n    classe = Counter(classes).most_common(1)\n    classe = classe[0][0]\n    return classe",
        "sha1": "18c1e83e9c58fce9b428d60647eadd6a973baf4c",
        "id": 367630
    },
    {
        "content": "def prediction_to_vad_label(\n    prediction,\n    frame_size: float = 0.032,\n    frame_shift: float = 0.008,\n    threshold: float = 0.5,\n):\n    \"\"\"Convert model prediction to VAD labels.\n\n    Args:\n        prediction (List[float]): predicted speech activity of each **frame** in one sample\n            e.g. [0.01, 0.03, 0.48, 0.66, 0.89, 0.87, ..., 0.72, 0.55, 0.20, 0.18, 0.07]\n        frame_size (float): frame size (in seconds) that is used when\n                            extarcting spectral features\n        frame_shift (float): frame shift / hop length (in seconds) that\n                            is used when extarcting spectral features\n        threshold (float): prediction values that are higher than `threshold` are set to 1,\n                            and those lower than or equal to `threshold` are set to 0\n    Returns:\n        vad_label (str): converted VAD label\n            e.g. \"0.31,2.56 2.6,3.89 4.62,7.99 8.85,11.06\"\n\n    NOTE: Each frame is converted to the timestamp according to its center time point.\n    Thus the converted labels may not exactly coincide with the original VAD label, depending\n    on the specified `frame_size` and `frame_shift`.\n    See the following exmaple for more detailed explanation.\n\n    Examples:\n        >>> label = parse_vad_label(\"0.31,0.52 0.75,0.92\")\n        >>> prediction_to_vad_label(label)\n        '0.31,0.53 0.75,0.92'\n    \"\"\"\n    frame2time = lambda n: n * frame_shift + frame_size / 2\n    speech_frames = []\n    prev_state = False\n    start, end = 0, 0\n    end_prediction = len(prediction) - 1\n    for i, pred in enumerate(prediction):\n        state = pred > threshold\n        if not prev_state and state:\n            # 0 -> 1\n            start = i\n        elif not state and prev_state:\n            # 1 -> 0\n            end = i\n            speech_frames.append(\n                \"{:.2f},{:.2f}\".format(frame2time(start), frame2time(end))\n            )\n        elif i == end_prediction and state:\n            # 1 -> 1 (end)\n            end = i\n            speech_frames.append(\n                \"{:.2f},{:.2f}\".format(frame2time(start), frame2time(end))\n            )\n        prev_state = state\n    return \" \".join(speech_frames)",
        "sha1": "993491314fd1b92402701996ce94c10f8dcf59e2",
        "id": 72059
    },
    {
        "content": "import inspect\n\n\ndef format_type(value):\n    \"\"\"\n    Return a full name of the value's type.\n\n    >>> import abusehelper.core.events\n    >>> format_type(abusehelper.core.events.Event())\n    'abusehelper.core.events.Event'\n\n    The package path prefix for builtin types gets omitted.\n\n    >>> format_type(abusehelper.core.events)\n    'module'\n    >>> format_type(abusehelper.core.events.Event)\n    'type'\n    >>> format_type(1)\n    'int'\n    \"\"\"\n\n    type_ = type(value)\n    name = type_.__name__\n    module = inspect.getmodule(type_)\n    if module is not None and module.__name__ != \"__builtin__\":\n        name = module.__name__ + \".\" + name\n    return name",
        "sha1": "f150ba3e38fc3623843490e24967ee04b9609915",
        "id": 204880
    },
    {
        "content": "def fetch_ref_codon(ref_pos, curr_gene, curr_seq):\n    \"\"\" Fetch codon within gene for given site \"\"\"\n    # position of site in gene\n    within_gene_pos = ref_pos - curr_gene['start'] if curr_gene['strand'] == '+' else curr_gene['end'] - ref_pos\n    # position of site in codon\n    within_codon_pos = within_gene_pos % 3\n    # gene sequence (oriented start to stop)\n    ref_codon = curr_seq[within_gene_pos-within_codon_pos:within_gene_pos-within_codon_pos+3]\n    return ref_codon, within_codon_pos",
        "sha1": "0ee6454a76ade51950cacbd5f7f8859677b607a3",
        "id": 683312
    },
    {
        "content": "from typing import Optional\nfrom typing import List\n\n\ndef str2list(x: str) -> Optional[List]:\n    \"\"\"Convert string to list base on , delimiter.\"\"\"\n    if x:\n        return x.split(\",\")",
        "sha1": "62cd7f087f72981e22f2f8fdc1871a394e00501b",
        "id": 41836
    },
    {
        "content": "def language_interpreter(file_ending):\n    \"\"\"\n    Takes a file extension and converts it to the class\n    name required by the syntax highlighter.\n\n    :param file_ending: The file extension to convert\n                        excluding the '.' (py, java, js, sh, xml)\n    :return: The HTML class name telling the syntax highlighter which preset to apply.\n    \"\"\"\n    mappings = {\n        \"py\": \"python\",\n        \"java\": \"java\",\n        \"js\": \"javascript\",\n        \"sh\": \"shell\",\n        \"xml\": \"xml\"\n    }\n\n    return mappings[file_ending]",
        "sha1": "3f33438af032918e727b9b5431de813a23515381",
        "id": 161941
    },
    {
        "content": "def get_books_liked_by_user(user_id, user_book_rating, min_rating):\n    \"\"\"Get books which the user liked.\n\n    :param user_id: User for which the closest neighbor will be find\n    :type: int\n    :param user_book_rating: Ratings of each user for each book read\n    :type: pandas dataframe with cols [\"user_id\", \"book_id\", \"rating\"]\n    :param min_rating: Minimal rating that defines if a user liked a book\n    :type: int\n    :return: Books which got at least the minimal rating by the user\n    :rtype: List of ints\n    \"\"\"\n    user2_book_rating = user_book_rating[user_book_rating[\"user_id\"]==user_id]\n    books_liked = user2_book_rating[user2_book_rating[\"rating\"] >= min_rating][\"book_id\"].to_list()\n    return books_liked",
        "sha1": "8b9e69d976459da07400dc6db463c8a5bf06a2af",
        "id": 245581
    },
    {
        "content": "def clamp(x, a, b):\n    \"\"\"clamp x to range [a, b]\"\"\"\n    return max(a, min(x, b))",
        "sha1": "624e4484b4cce0b1df6c1f8b73e1202a03c120d5",
        "id": 147390
    },
    {
        "content": "def update_newlist(ques, new, user_input):\n    \"\"\"Edit the string with user's input.\"\"\"\n    new = new.replace(ques, user_input, 1)\n    return(new)",
        "sha1": "d38f9510e24a61c57011f0a65eaee4787ee850f2",
        "id": 333747
    },
    {
        "content": "def _max_pool2d_out_shape(\n    imgshape, ws, stride, pad, ndim,\n):\n    \"\"\"Return the shape of max_pool2d output.\n\n    Adapted from Theano (2020/11/08):\n    https://github.com/Theano/Theano/blob/master/theano/tensor/signal/pool.py\n\n    Parameters\n    ----------\n    imgshape : tuple, list, or similar of integer or scalar Theano variable\n        The shape of a tensor of images. The last N elements are\n        interpreted as the number of rows, and the number of cols.\n    ws : list or tuple of N ints\n        Downsample factor over rows and column.\n        ws indicates the pool region size.\n    stride : list or tuple of N ints\n        Stride size, which is the number of shifts over rows/cols/slices to get the\n        next pool region.\n    pad : tuple of N ints\n        For each downsampling dimension, this specifies the number of zeros to\n        add as padding on both sides. For 2D and (pad_h, pad_w), pad_h specifies the\n        size of the top and bottom margins, pad_w specifies the size of the left and\n        right margins.\n    ndim : int\n        The number of pooling dimensions N.\n        The default is 2.\n\n    Returns\n    -------\n    list\n        The shape of the output from this op, for input of given shape.\n    \"\"\"\n\n    assert ndim > 0\n    assert (\n        len(imgshape) >= ndim\n    ), \"imgshape must have at least {} dimensions\".format(ndim)\n\n    # Compute output shape based on formula on Torch page (2020/11/16):\n    # https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html#torch.nn.MaxPool2d\n    h_in, w_in = imgshape[-ndim:]\n    h_out = int((h_in + 2 * pad[0] - (ws[0] - 1) - 1) // stride[0] + 1)\n    w_out = int((w_in + 2 * pad[1] - (ws[1] - 1) - 1) // stride[1] + 1)\n\n    rval = list(imgshape[:-ndim]) + [h_out, w_out]\n    return rval",
        "sha1": "56ce915e41870603c80e2041c7a22e18b595e3e7",
        "id": 198501
    },
    {
        "content": "def leading_whitespace(line):\n  \"\"\"Returns the number of leading whitespace in the line.\"\"\"\n  return len(line) - len(line.lstrip())",
        "sha1": "189e743831c13b46dd1f7b56ad02c3d6b704b2db",
        "id": 422671
    },
    {
        "content": "def combine_list(list1, list2):\n    \"\"\" combine two lists without creating duplicate entries) \"\"\"\n    return list1 + list(set(list2) - set(list1))",
        "sha1": "4aac147c61ad4b10b7d81a276cd988e8e6a59a71",
        "id": 254048
    },
    {
        "content": "import torch\n\n\ndef pcorrect(out, tilt):\n  \"\"\"Compute proportion of stimuli classified correctly from network output\n\n  Args:\n    out (torch.Tensor): output of network for each stimulus, i.e. the\n      predicted probability of each stimulus being tilted right\n    tilt (torch.Tensor): true tilt of each stimulus (1. for tilt right,\n      0. for tilt left)\n  \n  Returns:\n    float: proportion of stimuli classified correctly\n  \n  \"\"\"\n  out_tilt = (out > 0.5).type(torch.float)  # predicted tilt label: 1. for tilt right, 0. for tilt left (make sure to convert to float!)\n  return (tilt == out_tilt).type(torch.float).mean().item()",
        "sha1": "7d051c2ebb2e4850553e6846e828d57f3c5890c9",
        "id": 212147
    },
    {
        "content": "def check_point(cur_x, cur_y, minx, miny, maxx, maxy):\n    \"\"\"\n    check whether the point is in the area\n    \"\"\"\n    return minx < cur_x < maxx and miny < cur_y < maxy",
        "sha1": "ba09834f235a3c7984486d0a72089d3c7bfc0a98",
        "id": 459406
    },
    {
        "content": "def capitalize_first_letter(text):\n    \"\"\"\n    Given a string, capitalize the first letter.\n    \"\"\"\n    chars = list(text.strip())\n    chars[0] = chars[0].upper()\n    return \"\".join(chars)",
        "sha1": "c690a4e8392d7eedbc539c800c69e9addb60d0ef",
        "id": 219205
    },
    {
        "content": "def _drop_unnecessary_columns(data):\n    \"\"\" Drop all columns with nan values, with constant values or defined as irrelevant during the data analysis.\n\n    :param data: Data to drop the columns from\n    :return: The given data without the columns\n    \"\"\"\n    cols_nan = ['sensor_measurement_22', 'sensor_measurement_23']\n    cols_const = [\n        'operational_setting_3',\n        'sensor_measurement_1',\n        'sensor_measurement_5',\n        'sensor_measurement_6',\n        'sensor_measurement_10',\n        'sensor_measurement_16',\n        'sensor_measurement_18',\n        'sensor_measurement_19',\n        'sensor_measurement_22',\n        'sensor_measurement_23'\n    ]\n    cols_irrelevant = [\n        'operational_setting_1',\n        'operational_setting_2',\n        'sensor_measurement_11',\n        'sensor_measurement_12',\n        'sensor_measurement_13'\n    ]\n\n    return data.drop(columns=cols_const + cols_nan + cols_irrelevant)",
        "sha1": "5f40736261bc1162eae557aaa63d1eae54542e61",
        "id": 491390
    },
    {
        "content": "def get_float(value, allow_sign=False) -> float:\n    \"\"\"Convert a value to a float.\n\n    Args:\n        value: String value to convert.\n        allow_sign: If True, negative values are allowed.\n    Return:\n        float(value) if possible.\n    \"\"\"\n    try:\n        float_val = float(value)\n    except ValueError:\n        error = \"Could not convert '%s' to float\" % (value)\n        raise ValueError(error)\n    if float_val < 0 and not allow_sign:\n        raise ValueError(\"Negative numbers are not supported.\")\n    return float_val",
        "sha1": "53d2cf82c6cf2919accf4d2b880b412fd588e317",
        "id": 274267
    },
    {
        "content": "def get_bit_values(number, size=32):\n    \"\"\"\n    Get bit values as a list for a given number\n\n    >>> get_bit_values(1) == [0]*31 + [1]\n    True\n\n    >>> get_bit_values(0xDEADBEEF)\n    [1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, \\\n1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1]\n\n    You may override the default word size of 32-bits to match your actual\n    application.\n\n    >>> get_bit_values(0x3, 2)\n    [1, 1]\n\n    >>> get_bit_values(0x3, 4)\n    [0, 0, 1, 1]\n    \"\"\"\n    number += 2 ** size\n    return list(map(int, bin(number)[-size:]))",
        "sha1": "dddcf6169315f0bacb2b95c684315b2ce7dabc0c",
        "id": 418820
    },
    {
        "content": "def add_named_item(item_dict, item, item_name):\n    \"\"\"\n    Adds a named item to the given dict if not already there. If it is there\n    then returns the one from the dict.\n    \"\"\"\n\n    if item_name not in item_dict:\n        item_dict[item_name] = item\n\n    return item_dict[item_name]",
        "sha1": "8c701d2804affd045d12bca2a64823fea11f8f0f",
        "id": 255752
    },
    {
        "content": "def _get_reference_bodyreference(referencebody_element):\n    \"\"\"Parse ReferenceInput BodyReference element\n    \"\"\"\n    return referencebody_element.attrib.get(\n        '{http://www.w3.org/1999/xlink}href', '')",
        "sha1": "c0baeec99e3d9d4a54f17a721d00202e214defcc",
        "id": 60038
    },
    {
        "content": "def build_call(*args):\n    \"\"\"Create a URL for a call to the OEC API\"\"\"\n    call = 'http://atlas.media.mit.edu/'\n    for val in args:\n        call += str(val) + '/'\n    return call",
        "sha1": "aa0ea5e8e8473e8e7e51e5d85c6dcc7644f2d796",
        "id": 366145
    },
    {
        "content": "import math\n\n\ndef acos2(x: float, y: float) -> float:\n    \"\"\"Function to return the inverse cos function across the range (-pi, pi], rather than (0, pi]\n\n    Parameters\n    ----------\n    x : float\n        x coordinate of the point in 2D space\n    y : float\n        y coordinate of the point in 2D space\n\n    Returns\n    -------\n    theta : float\n        Angle corresponding to point in 2D space in radial coordinates, within range (-pi, pi]\n    \"\"\"\n    r = math.sqrt(x**2+y**2)\n    if y >= 0:\n        return math.acos(x/r)\n    else:\n        return -math.acos(x/r)",
        "sha1": "485186a9257d5158d150fee42337c20396e74426",
        "id": 174699
    },
    {
        "content": "def calculate_RR_for_motif(ct):\n    \"\"\"\n    Motif is treatment\n    No motif is placebo\n\n    :param ct: mutually exclusive counts of mutated matching motifs, matching mutations, matching motifs, and matching bases\n    :return: enrichment or risk ratio\n    \"\"\"\n    try:\n        RR = ((ct.loc['mutation', 'motif'] / (ct.loc['mutation', 'motif'] + ct.loc['no mutation', 'motif'])) /\n              (ct.loc['mutation', 'no motif'] / (ct.loc['mutation', 'no motif'] + ct.loc['no mutation', 'no motif'])))\n    except ZeroDivisionError:\n        RR = 0.0\n    return RR",
        "sha1": "4abeacbe785bde47ecae4c980589dfb26b873ae0",
        "id": 435246
    },
    {
        "content": "def get_pdb_atom_info(line):\n    \"\"\"Split and read pdb-file line (QPREP-OUT).\n\n     Extract and return:\n        atomnumber, atomname, resname, resid)\n    \"\"\"\n    # UNIT: check if the correct numbers are extracted\n    atomnum = int(line[6:11])\n    atmname = line[13:17].strip()\n    resname = line[17:20].strip()\n    resinum = int(line[22:27])\n\n    return (atomnum, atmname, resname, resinum)",
        "sha1": "e0c05d1d543c7b08be3c404427c19b42a1b93a15",
        "id": 640940
    },
    {
        "content": "def hparams_frames_per_second(hparams):\n    \"\"\"Compute frames per second as a function of HParams.\"\"\"\n    return hparams.sample_rate / hparams.spec_hop_length",
        "sha1": "a46feb8438363d425ac6f7dcb9d80f11d27a5816",
        "id": 667578
    },
    {
        "content": "def draw_triangle(t):\n    \"\"\"Draws a triangle t = (v0, v1, v2)\"\"\"\n    def func(ctx):\n        v0, v1, v2 = t\n        ctx.move_to(v0.x, v0.y)\n        ctx.line_to(v1.x, v1.y)\n        ctx.line_to(v2.x, v2.y)\n        ctx.close_path()\n    return func",
        "sha1": "a648c431b5992a975365e2c042487bb3906c555b",
        "id": 140518
    },
    {
        "content": "def _remove_trailing_chars(text: str) -> str:\n    \"\"\" Removes trailing characters from the beginning or end of a string. \"\"\"\n    chars = ['.', '@', '/', '&', '-', \"'\"]\n    for char in chars:\n        text = text.strip(char)\n    return text",
        "sha1": "b19e1d369fc858a2a269fdfc14a128907ecf0bc3",
        "id": 91089
    },
    {
        "content": "def get_chunks(l, n, max_chunks=None):\n    \"\"\"\n    Returns a chunked version of list l with a maximum of n items in each chunk\n\n    :param iterable[T] l: list of items of type T\n    :param int n: max size of each chunk\n    :param int max_chunks: maximum number of chunks that can be returned. Pass none (the default) for unbounded\n    :return: list of chunks\n    :rtype: list[T]\n    \"\"\"\n    if n is None:\n        return [l]\n\n    if n <= 0:\n        raise ValueError('get_chunk: n must be a positive value. Received {}'.format(n))\n\n    if max_chunks is not None and max_chunks > 0:\n        n = max(max_chunks, int(float(len(l)) / float(n)) + 1)\n\n    return [l[i:i+n] for i in range(0, len(l), n)]",
        "sha1": "42c890363b9e8fa1740e612c4757936eb8358e1e",
        "id": 326047
    },
    {
        "content": "import json\n\n\ndef load_data(data):\n    \"\"\" Wrapper to load json data, to be compatible with Python3.\n    Returns: JSON data\n\n    Keyword arguments:\n    data: might be bytes or str\n    \"\"\"\n    if type(data) == bytes:\n        return json.loads(data.decode(\"utf-8\"))\n    else:\n        return json.loads(data)",
        "sha1": "c33c661c2a42d162d06c3e17487e072908fd0bf4",
        "id": 7170
    },
    {
        "content": "def get_envs_in_group(group_name, nova_creds):\n    \"\"\"\n    Takes a group_name and finds any environments that have a SUPERNOVA_GROUP\n    configuration line that matches the group_name.\n    \"\"\"\n    envs = []\n    for section in nova_creds.keys():\n        if ('SUPERNOVA_GROUP' in nova_creds[section] and\n                nova_creds[section]['SUPERNOVA_GROUP'] == group_name):\n            envs.append(section)\n    return envs",
        "sha1": "e63bbde1ae4ea98ed5e5d0b19da42d4985796748",
        "id": 450410
    },
    {
        "content": "def check_if_pack_on_hand(hand):\n    \"\"\"\n    Function used to check if player have a pack of cards on hand.\n    :param hand: list of cards on player hand\n    :return: list of cards values which can be played as pack\n    \"\"\"\n    tmp = {}\n    for card in hand:\n        if card[1] in tmp.keys():\n            tmp[card[1]] += 1\n        else:\n            tmp[card[1]] = 1\n\n    keys = tmp.keys()\n    packs = []\n    for key in keys:\n        if tmp[key] >= 3:\n            packs.append(key)\n\n    return packs",
        "sha1": "4302b36395cb229119af0e279f4d9180b25e9d3d",
        "id": 594535
    },
    {
        "content": "def _splitList(data, n):\n    \"\"\"Split data list to n sized sub lists.\"\"\"\n    return [data[i : i + n] for i in range(0, len(data), n)]",
        "sha1": "6f976fea84c26269693f1c5d7594c70351a2f1fb",
        "id": 622989
    },
    {
        "content": "import torch\n\n\ndef digitize(tensor, bin_boundaries):\n    \"\"\"Implement numpy digitize using torch.\"\"\"\n    result = torch.zeros(tensor.shape).long()\n    for boundary in bin_boundaries:\n        result += (tensor > boundary).long()\n    return result",
        "sha1": "19982da7fdc3be7e4043ea1c26c086408fdab50a",
        "id": 331897
    },
    {
        "content": "import string\n\n\ndef is_pangram(s):\n    \"\"\"Detect if a sentence contains every letter of the alphabet.\"\"\"\n    all_letters = set(string.ascii_lowercase)\n    if set(s.lower()) >= all_letters:\n        return True\n    else:\n        return False",
        "sha1": "0acad0e64396e1328482856f58e73f10686d201c",
        "id": 435599
    },
    {
        "content": "import logging\n\n\ndef key(name):\n    \"\"\"A key for sorting property names like function.property.index\"\"\"\n\n    try:\n        dot = name.rindex('.')\n        return (name[:dot].lower(), int(name[dot+1:]))\n    except ValueError:\n        logging.warning(\"Property name not of the form STRING.INTEGER: %s\",\n                        name)\n        return (name, 0)",
        "sha1": "1073040e4cff9fa93bbb0dc6f33d0cb93d6dee50",
        "id": 510802
    },
    {
        "content": "def CCC(y_true, y_pred):\n    \"\"\"\n    Calculate the CCC for two numpy arrays.\n    \"\"\"\n    x = y_true\n    y = y_pred\n    xMean = x.mean()\n    yMean = y.mean()\n    xyCov = (x * y).mean() - (xMean * yMean)\n    xVar = x.var()\n    yVar = y.var()\n    return 2 * xyCov / (xVar + yVar + (xMean - yMean) ** 2)",
        "sha1": "62cbadb4101da707ee9f82bc3eea4521657f95ce",
        "id": 518745
    },
    {
        "content": "def obtenerSecuencia(variable, senal):\n    \"\"\"\n    Obtiene la secuencia de una se\u00f1al dada\n    Parameters:\n        variable (str): La letra de la variable de la se\u00f1al\n        senal (SenialDiscreta): La se\u00f1al discreta\n    Returns:\n        str: Secuencia de una se\u00f1al en formato {...,#,...}\n    \"\"\"\n    secuencia = variable + \"(n) = [\"\n    for e in senal.obtener_datos():\n        if e != \"\":\n            secuencia = secuencia + str(e) + \",\"\n        else:\n            secuencia = secuencia + str(e)\n    secuencia = secuencia + \"]\"\n\n    return secuencia",
        "sha1": "1b786c133c8f8825cc803dd6cda7f00d2ada0a1f",
        "id": 451979
    },
    {
        "content": "def FiLM_complex(x, gammas, betas):\n    \"\"\"\n    :param x: an output feature map of a CNN layer [*, ch, T, F]\n    :param gamma: [*, ch]\n    :param beta: [*, ch]\n    :return: gamma * x + beta\n    \"\"\"\n    gamma_ = gammas.unsqueeze(-1).unsqueeze(-1)\n    beta_ = betas.unsqueeze(-1).unsqueeze(-1)\n    return gamma_ * x + beta_",
        "sha1": "46be9b1304b4ddc6e88e9c8d74a90231c4156987",
        "id": 559485
    },
    {
        "content": "def ckstr(checksum):\n    \"\"\"Return a value as e.g. 'FC8E', i.e. an uppercase hex string with no leading\n    '0x' or trailing 'L'.\n    \"\"\"\n    return hex(checksum)[2:].rstrip('L').upper()",
        "sha1": "f9e3abde35fb609a252ea72804a1d32b05eff04e",
        "id": 377199
    },
    {
        "content": "def _parse_name(name):\n    \"\"\" Parse the name of an ensemble prediction \n    \"\"\"\n    components = name.split('_')\n    assert len(components) == 2, 'Name does not follow the convention of type_userdefined, such as point_alice or distribution_bob123'\n    assert components[0] in ['point', 'distribution', 'interval', 'quantile', 'particle'], 'Prediction type %s not valid' % components[0]\n    return components[0], components[1]",
        "sha1": "70ec56e06897683f71a97e861781b11f20980829",
        "id": 260923
    },
    {
        "content": "def converter(L):\n    \"\"\"Takes picobot code, as a list, and returns a picobot dictionary\"\"\"\n    picobotDict = {}\n    for item in L:\n        key = (int(item[0]), str(item[2:6]))\n        value = (str(item[10]), int(item[12]))\n        picobotDict[key] = value\n    return picobotDict",
        "sha1": "22d442b09fb533241c532e76e1bcb19587ed4d4f",
        "id": 384609
    },
    {
        "content": "def listify_plottable_item(item):\n  \"\"\"\n  plottable is a list of strings:\n  'FBti0020401\\t78\\t-1.0\\tR'\n  split on tab and return gene, coordinate, count and orientation\n  \"\"\"\n  gene, coordinate, count, orientation = item.split(\"\\t\")\n  return gene, coordinate, count, orientation",
        "sha1": "6eda4458943c37742d944ffa35628509f171973c",
        "id": 486634
    },
    {
        "content": "import requests\nfrom bs4 import BeautifulSoup\n\n\ndef main_page_setup(search_page, location, sort_type, language):\n    \"\"\"Create soup object for main jobs page\"\"\"\n    url = f'https://de.indeed.com/Jobs?l={location}&sort={sort_type}' \\\n                                f'&lang={language}&start={search_page}'\n    main_jobs_page = requests.get(url).text\n    soup = BeautifulSoup(main_jobs_page, 'lxml')\n    return soup",
        "sha1": "a95be15d409f52e1fcca72d54fd6afe3ff69fac7",
        "id": 466287
    },
    {
        "content": "def initialize_back_prop(AL, Y, AL_prime, Y_prime):\n    \"\"\"\n    Initialize backward propagation\n\n    Arguments:\n    :param AL -- output of the forward propagation L_model_forward()... i.e. neural net predictions   (if regression)\n                                                                 i.e. neural net probabilities (if classification)\n          >> a numpy array of shape (n_y, m) where n_y = no. outputs, m = no. examples\n\n    :param Y -- true \"label\" (classification) or \"value\" (regression)\n         >> a numpy array of shape (n_y, m) where n_y = no. outputs, m = no. examples\n\n    :param AL_prime -- the derivative of the last layer's activation output(s) w.r.t. the inputs x: AL' = d(AL)/dX\n                >> a numpy array of size (n_y, n_x, m) where n_y = no. outputs, n_x = no. inputs, m = no. examples\n\n    :param Y_prime -- the true derivative of the output(s) w.r.t. the inputs x: Y' = d(Y)/dX\n               >> a numpy array of shape (n_y, n_x, m) where n_y = no. outputs, n_x = no. inputs, m = no. examples\n\n    Returns:\n    :return dAL -- gradient of the loss function w.r.t. last layer activations: d(L)/dAL\n           >> a numpy array of shape (n_y, m)\n\n    :return dAL_prime -- gradient of the loss function w.r.t. last layer activations derivatives: d(L)/dAL' where AL' = d(AL)/dX\n                 >> a numpy array of shape (n_y, n_x, m)\n    \"\"\"\n    n_y, _ = AL.shape  # number layers, number examples\n    Y = Y.reshape(AL.shape)\n    dAL = AL - Y  # derivative of loss function w.r.t. to activations: dAL = d(L)/dAL\n    dAL_prime = AL_prime - Y_prime  # derivative of loss function w.r.t. to partials: dAL_prime = d(L)/d(AL_prime)\n\n    return dAL, dAL_prime",
        "sha1": "135a442cb6258bf1e7f24c1edd8cf535e57e514f",
        "id": 676766
    },
    {
        "content": "import time\n\n\ndef get_elapsed_time_ms(start_time_in_seconds):\n    \"\"\"\n    Returns the elapsed time in millis from the given start time.\n    \"\"\"\n    end_time = time.time()\n    return int((end_time - start_time_in_seconds) * 1000)",
        "sha1": "bc71a20fddf62a1cfa4ab51724b826946d171b66",
        "id": 42622
    },
    {
        "content": "from functools import reduce\n\n\ndef replace(token: str) -> str:\n    \"\"\"\n    Receives a word which could be a number or a word or a word\n    with symbols and if the string has any af the specified\n    symbols they will be removed.\n\n    Attributes\n    ----------\n    symbols_toRemove : [str]\n        The symbols we want to remove in the string\n\n    Parameters\n    ----------\n    token : str\n        The word which is going to be cleanned.\n\n    Returns\n    -------\n    token : str\n        A string without symbols like \".\", \",\", \"'\", \"(\", \")\", \"/\".\n    \"\"\"\n\n    symbols_toRemove = [\".\", \",\", \"'\", \"(\", \")\", \"/\"]\n\n    try:\n        float(token)\n    except ValueError:\n        # Use of the function reduce to apply the replace to each symbol\n        token = reduce(lambda x, y : x.replace(y, \"\"), symbols_toRemove, token)\n\n    return token",
        "sha1": "c314f3317ee61cf0be5566796c5f26c746ca8d3d",
        "id": 513798
    },
    {
        "content": "def pp_num(num):\n    \"\"\"\n    pretty prints number with commas\n    \"\"\"\n    s = '%d' % num\n    groups = []\n    while s and s[-1].isdigit():\n        groups.append(s[-3:])\n        s = s[:-3]\n    return s + ','.join(reversed(groups))",
        "sha1": "d249e973b52114c72fa684471d441b870b1fc6c4",
        "id": 55383
    },
    {
        "content": "def fix_alpha(with_alpha, without_alpha):\n    \"\"\"\n    Given two of the same image, one with an alpha channel and one without, add the alpha\n    channel back into the image without, and return it.\n\n    :param with_alpha:\n    :param without_alpha:\n    :return:\n    \"\"\"\n    alpha_added = with_alpha.copy()\n    alpha_added[:, :, :3] = without_alpha\n    return alpha_added",
        "sha1": "abd2c81d5569983ce8a30233112e1d5b5bf48b89",
        "id": 662566
    },
    {
        "content": "import math\n\n\ndef calculate_bucket_key(seq, radix, exp, min_element, index):\n    \"\"\"\n    \u0412\u044b\u0447\u0438\u0441\u043b\u044f\u0435\u0442 \u0438\u043d\u0434\u0435\u043a\u0441 \u043a\u043e\u0440\u0437\u0438\u043d\u044b \u0434\u043b\u044f \u0434\u0430\u043d\u043d\u043e\u0433\u043e \u0447\u0438\u0441\u043b\u0430.\n    :param seq: \u043a\u043e\u043b\u043b\u0435\u043a\u0446\u0438\u044f \u0447\u0438\u0441\u0435\u043b.\n    :param radix: \u043e\u0441\u043d\u043e\u0432\u0430 \u0441\u0438\u0441\u0442\u0435\u043c\u044b \u0441\u0447\u0438\u0441\u043b\u0435\u043d\u0438\u044f (\u043e\u0431\u044b\u0447\u043d\u043e \u0440\u0430\u0432\u043d\u0430 10).\n    :param exp: \u0440\u0430\u0437\u0440\u044f\u0434 \u0446\u0438\u0444\u0440\u044b \u0432 \u0447\u0438\u0441\u043b\u0435 (10, 100, 1000).\n    :param min_element: \u043c\u0438\u043d\u0438\u043c\u0430\u043b\u044c\u043d\u043e\u0435 \u0447\u0438\u0441\u043b\u043e \u0432 \u043a\u043e\u043b\u043b\u0435\u043a\u0446\u0438\u0438.\n    :param index: \u0438\u043d\u0434\u0435\u043a\u0441 \u0447\u0438\u0441\u043b\u0430 \u0438\u0437 \u043a\u043e\u043b\u043b\u0435\u043a\u0446\u0438\u0438, \u0434\u043b\u044f \u043a\u043e\u0442\u043e\u0440\u043e\u0433\u043e \u0431\u0443\u0434\u0435\u0442 \u0432\u044b\u0447\u0438\u0441\u043b\u044f\u0442\u044c\u0441\u044f\n    \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435.\n    :return: \u0438\u043d\u0434\u0435\u043a\u0441 \u043a\u043e\u0440\u0437\u0438\u043d\u044b \u0434\u043b\u044f \u0434\u0430\u043d\u043d\u043e\u0433\u043e \u0447\u0438\u0441\u043b\u0430.\n    \"\"\"\n    return math.floor(((seq[index] - min_element) / exp) % radix)",
        "sha1": "4cb8bfb070ce243c608658f3c6cb06b97ad2cb45",
        "id": 478075
    },
    {
        "content": "def tokenize_en(text, spacy_en):\n    \"\"\"\n    Tokenizes English text from a string into a list of strings\n    \"\"\"\n    return [tok.text for tok in spacy_en.tokenizer(text)]",
        "sha1": "7a2b88772ec8729993bd950fe191f512cd76f7d8",
        "id": 415930
    },
    {
        "content": "def merge_intervals(data):\n    \"\"\"\n    data = [(10,20), (15,30), (100, 200)]\n    out = [(10,30), (100,200)]\n    \"\"\"\n    if len(data)==0:\n        return data\n    result = []\n    saved = list(data[0])\n    for st, en in sorted([sorted(t) for t in data]):\n        if st <= saved[1]:\n            saved[1] = max(saved[1], en)\n        else:\n            result.append((saved[0], saved[1]))\n            saved[0] = st\n            saved[1] = en\n    result.append(saved)\n    return result",
        "sha1": "e83b458890433d9367b9810aee2d68f85773af73",
        "id": 60559
    },
    {
        "content": "def binary_search(arr, l, r, val):\n    \"\"\"\n    # Returns index of x in arr if present, else -1\n\n    Time Complexity: O(Logn)\n    Space Complexity: O(1)\n\n    :param arr: List[int]\n    :param val: int\n    :return: int\n    \"\"\"\n    if r >= l:\n\n        mid = 1 + (r - l) // 2\n\n        if arr[mid] == val:\n            return mid\n\n        elif arr[mid] > val:\n            return binary_search(arr, l, mid-1, val)\n\n        else:\n            return binary_search(arr, mid+1, r, val)\n\n    else:\n        return -1",
        "sha1": "8810120ced403d44edfc26dcb605152a5d8a4e2a",
        "id": 201131
    },
    {
        "content": "def encode_labelme_shape(point_list):\n    \"\"\"\n    Encode the labelme shape (usually a list of points)\n    Return a serializable object (usually a string) for json dumping\n    \"\"\"\n    code = list()\n    for point in point_list:\n        assert len(point) == 2\n        code.append('{:.6f}+{:.6f}'.format(point[0], point[1]))\n    code = ','.join(code)\n    return code",
        "sha1": "00309dab2d8fe864cfa82762f5327133fe842a4b",
        "id": 141050
    },
    {
        "content": "def slice_to_flow(arr, i, n):\n    \"\"\" slice to the ith flow value given a total of n possible flow values\"\"\" \n    assert(arr.shape[0] % n == 0)\n    incr = round(arr.shape[0]/n)\n    i_lower = i * incr\n    i_upper = (i+1) * incr\n    return arr[i_lower:i_upper, :]",
        "sha1": "7772fc0684327cf771169c766ce733ef8e757359",
        "id": 63911
    },
    {
        "content": "from typing import Union\nfrom typing import NoReturn\n\n\ndef _get_file_str(file: int) -> Union[str, NoReturn]:\n\t\"\"\"Convert a given integer to a file letter on a chessboard.\"\"\"\n\tfile_dict = {  # Used for int to str lookup for files\n\t\t0: 'a', 1: 'b', 2: 'c',\n\t\t3: 'd', 4: 'e', 5: 'f',\n\t\t6: 'g', 7: 'h'\n\t}\n\n\treturn file_dict[file]",
        "sha1": "6c057a780792b26551825be72aa1553a2b80497d",
        "id": 632915
    },
    {
        "content": "import json\n\n\ndef get_top_python_packages(top_n=100):\n    \"\"\"\n    Generate list of most downloaded python packages\n\n    Args:\n        top_n: the number of most downloaded packages to return\n\n    Returns:\n        (list) Names of most downloaded packages\n    \"\"\"\n    # JSON file containing top 4000 packages\n    # found here: https://hugovk.github.io/top-pypi-packages/\n    top_python_pkgs = \"top_python_pkg_downloads.json\"\n    with open(top_python_pkgs, \"r\") as j:\n        contents = json.loads(j.read())\n\n        # store names of top packges\n        top_pkgs = []\n        cnt = 0\n        for pkg in contents[\"rows\"]:\n            # only append TOP_N number of packages\n            if cnt == top_n:\n                break\n            top_pkgs.append(pkg[\"project\"])\n            cnt += 1\n\n        return top_pkgs",
        "sha1": "d19136a5ee21fe65c33ef4a5336f46863bf2c6b0",
        "id": 28459
    },
    {
        "content": "import re\n\n\ndef validate_maintenance_window(window):\n    \"\"\"Validate PreferredMaintenanceWindow for DBInstance\"\"\"\n\n    days = (\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\")\n    day_re = r'[A-Z]{1}[a-z]{2}'\n    hour = r'[01]?[0-9]|2[0-3]'\n    minute = r'[0-5][0-9]'\n    r = (\"(?P<start_day>%s):(?P<start_hour>%s):(?P<start_minute>%s)-\"\n         \"(?P<end_day>%s):(?P<end_hour>%s):(?P<end_minute>%s)\") % (day_re,\n                                                                   hour,\n                                                                   minute,\n                                                                   day_re,\n                                                                   hour,\n                                                                   minute)\n    range_regex = re.compile(r)\n    m = range_regex.match(window)\n    if not m:\n        raise ValueError(\"DBInstance PreferredMaintenanceWindow must be in \"\n                         \"the format: ddd:hh24:mi-ddd:hh24:mi\")\n    if m.group('start_day') not in days or m.group('end_day') not in days:\n        raise ValueError(\"DBInstance PreferredMaintenanceWindow day part of \"\n                         \"ranges must be one of: %s\" % \", \".join(days))\n    start_ts = (days.index(m.group('start_day')) * 24 * 60) + \\\n        (int(m.group('start_hour')) * 60) + int(m.group('start_minute'))\n    end_ts = (days.index(m.group('end_day')) * 24 * 60) + \\\n        (int(m.group('end_hour')) * 60) + int(m.group('end_minute'))\n    if abs(end_ts - start_ts) < 30:\n        raise ValueError(\"DBInstance PreferredMaintenanceWindow must be at \"\n                         \"least 30 minutes long.\")\n    return window",
        "sha1": "ced6140fa250e157aaa532f7c531abf64e41768e",
        "id": 637860
    },
    {
        "content": "import re\nfrom bs4 import BeautifulSoup\n\n\ndef remove_html_tags(text):\n    \"\"\" Removes HTML tags from text. \"\"\"\n\n    # prevent missing white space in further deletion of html symbols\n    text = re.sub('<p>', ' ', text)\n\n    # add punctuation after headings\n    text = re.sub('</h1>', '. ', text)\n    text = re.sub('</h2>', '. ', text)\n\n    soup = BeautifulSoup(text, features=\"html.parser\")\n    text = soup.get_text()\n\n    return text",
        "sha1": "34d2b3e1f0e27457d7296149b89bf5eded1b62b3",
        "id": 646063
    },
    {
        "content": "def tf(seconds):\n    \"\"\"\n    Formats time in seconds to days, hours, minutes, and seconds.\n\n    Parameters\n    ----------\n    seconds : float\n        The time in seconds.\n\n    Returns\n    -------\n    str\n        The formatted time.\n    \"\"\"\n    days = seconds // (60*60*24)\n    seconds -= days * 60*60*24\n\n    hours = seconds // (60*60)\n    seconds -= hours * 60*60\n\n    minutes = seconds // 60\n    seconds -= minutes * 60\n\n    tf = []\n\n    if days > 0:\n        tf.append(\"%s days\" % int(days))\n\n    if hours > 0:\n        tf.append(\"%s hours\" % int(hours))\n\n    if minutes > 0:\n        tf.append(\"%s minutes\" % int(minutes))\n\n    tf.append(\"%s seconds\" % round(seconds, 2))\n\n    return \", \".join(tf)",
        "sha1": "22649dd1bd74cc08d73c5cd43b1b96658a3bcb3a",
        "id": 701067
    },
    {
        "content": "def escape_node_name(node_name: str) -> str:\n    \"\"\"Escapes any special characters in an ontology node name\"\"\"\n    return node_name.replace(r\"|\", r\"\\|\").replace(r\".\", r\"\\.\")",
        "sha1": "899501b6b92db2d169ff07d850bbe89ea626f0ac",
        "id": 369741
    },
    {
        "content": "import torch\n\n\ndef add_const_col(mat: torch.Tensor):\n    \"\"\"\n\n    Parameters\n    ----------\n    mat : torch.Tensor[n_data, n_col]\n\n    Returns\n    -------\n    res : torch.Tensor[n_data, n_col+1]\n        add one column only contains 1.\n\n    \"\"\"\n    assert mat.dim() == 2\n    n_data = mat.size()[0]\n    device = mat.device\n    return torch.cat([mat, torch.ones((n_data, 1), device=device)], dim=1)",
        "sha1": "1cb8b21ac7874e27fd7f0842bfa9a05c5bd2b389",
        "id": 554753
    },
    {
        "content": "def analyze_run_events(requestId, events, context):\n    \"\"\"\n    Collect information about request execution from log events.\n    \"\"\"\n    assert len(events) > 0, \"No events found for request %s\" % requestId\n    errors = 0\n    warnings = 0\n    startts = 0\n    endts = 0\n    for event in events:\n        if 'message' in event:\n            message = event['message']\n            if message.startswith('START'):\n                startts = event['timestamp']\n            elif message.startswith('END'):\n                endts = event['timestamp']\n            elif message.startswith('[ERROR]'):\n                errors = errors + 1\n            elif message.startswith('[WARNING]'):\n                warnings = warnings + 1\n    assert startts > 0, \"No START event found in request log trace %s\" % requestId\n    assert endts > 0, \"No END event found in request log trace %s\" % requestId\n    duration = endts - startts\n    return { 'start': startts, 'end': endts, 'duration': duration, 'errors': errors, 'warnings': warnings }",
        "sha1": "3e630a3d39e42ad90a594ca9124879a394a3eaf4",
        "id": 523378
    },
    {
        "content": "def translate_vehicle_layout_to_distance(layout: str):\n    \"\"\"\n    :param layout: str ex: -O-O-------O--\n    :return: tuple with axles distance (m) and vehicle lenght (m)\n\n    \"\"\"\n    axles_distance = []\n    _distance = 0\n\n    layout = layout.upper()\n\n    for c in layout:\n        _distance += 50\n        if c == 'O':\n            axles_distance.append(_distance/100)\n            _distance = 0\n\n    return axles_distance[1:], len(layout)*50/100",
        "sha1": "454cbbc8d42671733e5554d4e19b8bab9823edca",
        "id": 291230
    },
    {
        "content": "import random\n\n\ndef give_some_candies_or_not(tot):\n    \"\"\"\n    tot : string\n        trick or treat!\n    return:\n        # of candies: int (1~50) if treat\n        None: None if trick\n    \"\"\"\n    num_of_candies = random.randint(1, 50)\n    if tot == 'treat':\n        return num_of_candies\n    else:\n        return None",
        "sha1": "87653efb401b13fad90b5ad4adfd27cda4750598",
        "id": 367792
    },
    {
        "content": "def get_message_id(timestamp, topic):\n    \"\"\"Unify the way to get a unique identifier for the given message\"\"\"\n    return '{}{}'.format(timestamp, topic.replace('/', '_'))",
        "sha1": "072d03347ce44d30ebbe28ad6ff2559c021784db",
        "id": 637608
    },
    {
        "content": "import pathlib\n\n\ndef resolve_output_path(file_path):\n    \"\"\"Resolves and creates the output path for a given examples artifacts\n\n    Args:\n        file_path (str): file path string\n\n    Returns:\n        [str]: path to the examples output\n    \"\"\"\n\n    output_path = f\"{file_path}/output\"\n\n    pathlib.Path(output_path).mkdir(parents=True, exist_ok=True)\n\n    return output_path",
        "sha1": "9960ef91d2e83a8884be470ddf0d70b22279cc69",
        "id": 70005
    },
    {
        "content": "def check_experiment_existence(topic_model):\n    \"\"\"\n    Checks if topic_model has experiment.\n\n    Parameters\n    ----------\n    topic_model : TopicModel\n        topic model\n\n    Returns\n    -------\n    bool\n        True if experiment exists, in other case False.\n\n    \"\"\"\n    is_experiment = topic_model.experiment is not None\n\n    return is_experiment",
        "sha1": "98d16718f3106b5f07d7449feff45d6b19bac18a",
        "id": 325447
    },
    {
        "content": "def edge_boundary(G, nbunch1, nbunch2=None):\n    \"\"\"Return the edge boundary.\n\n    Edge boundaries are edges that have only one end\n    in the given set of nodes.\n\n    Parameters\n    ----------\n    G : graph\n      A networkx graph\n\n    nbunch1 : list, container\n       Interior node set\n\n    nbunch2 : list, container\n       Exterior node set.  If None then it is set to all of the\n       nodes in G not in nbunch1.\n\n    Returns\n    -------\n    elist : list\n       List of edges\n\n    Notes\n    -----\n    Nodes in nbunch1 and nbunch2 that are not in G are ignored.\n\n    nbunch1 and nbunch2 are usually meant to be disjoint,\n    but in the interest of speed and generality, that is\n    not required here.\n\n    \"\"\"\n    if nbunch2 is None:     # Then nbunch2 is complement of nbunch1\n        nset1=set((n for n in nbunch1 if n in G))\n        return [(n1,n2) for n1 in nset1 for n2 in G[n1] \\\n                if n2 not in nset1]\n\n    nset2=set(nbunch2)\n    return [(n1,n2) for n1 in nbunch1 if n1 in G for n2 in G[n1] \\\n            if n2 in nset2]",
        "sha1": "627533e6c5aece000c201a809f2e30f1f02ace87",
        "id": 139396
    },
    {
        "content": "def lowest_common_ancestor(taxa1, taxa2, taxa_tree):\n    \"\"\"Return a string giving the lowest common ancestor of taxa{1, 2}.\n\n    Compute the most specific taxonomic rank shared by taxa1 and taxa2.\n    e.g. if taxa1 is 'Staphylococcus aureus' and taxa2 is 'Staphylococcus\n    lugdunensis' this function should return 'Staphylococcus'.\n\n    Use `taxa_tree.ancestors(<taxon>)` to get a list of ancestors for taxon.\n    \"\"\"\n    ancestors1 = set(taxa_tree.ancestors(taxa1))\n    for ancestor in taxa_tree.ancestors(taxa2):\n        if ancestor in ancestors1:\n            return ancestor",
        "sha1": "f0ae50c3a3cbda0b22ff09bc044423c2bcc3515f",
        "id": 99868
    },
    {
        "content": "def to_secs(delta):\n    \"\"\"Convert a :py:class:`datetime.timedelta` to a number of seconds.\n\n    (This is basically a backport of\n    :py:meth:`datetime.timedelta.total_seconds`.)\n    \"\"\"\n    return (delta.days * 86400.0 +\n            delta.seconds +\n            delta.microseconds / 1000000.0)",
        "sha1": "1ff9c8b2a857f713d7bda2691b0b2cfe8a1611dc",
        "id": 647801
    },
    {
        "content": "def divmult(n: int, m: int = 2) -> int:\n    \"\"\"\n    Checks how many times n is divisible by m. Returns -1 if n=0\n\n    @author = Joel\n\n    :param n: Numerator\n    :param m: Denominator\n    :return: Multiplicity\n    \"\"\"\n    if n < 0:\n        raise ValueError('Only non-negative integers are supported for n.')\n    elif n == 0:\n        return -1\n    q, r = divmod(n, m)\n    count = 0\n    while not r:\n        count += 1\n        q, r = divmod(q, m)\n    return count",
        "sha1": "76fcc5c89a643da8ce2f5ad1e011afa85715d35f",
        "id": 96910
    },
    {
        "content": "def human_bytes(byte: int, precision: int = 2) -> str:\n    \"\"\"Return a human readable version of the byte amount\"\"\"\n    kilo_byte = 10 ** 3\n    mega_byte = 10 ** 6\n    giga_byte = 10 ** 9\n    tera_byte = 10 ** 12\n    if byte >= tera_byte:\n        return f\"{round(byte / tera_byte, precision)} TB\"\n    elif byte >= giga_byte:\n        return f\"{round(byte / giga_byte, precision)} GB\"\n    elif byte >= mega_byte:\n        return f\"{round(byte / mega_byte, precision)} MB\"\n    elif byte >= kilo_byte:\n        return f\"{round(byte / kilo_byte, precision)} kB\"\n    return f\"{byte} B\"",
        "sha1": "7e715e7406ae724be702a0acf8cc73dd36d78a83",
        "id": 114853
    },
    {
        "content": "import re\n\n\ndef parse_date(deadline_date):\n    \"\"\"\n    Given a date in the form MM/DD/YY or MM/DD/YYYY, returns\n    the integers MM, DD, and YYYY (or YY) in this order.\n    \"\"\"\n    deadline_split = re.split('\\\\/|\\\\-', deadline_date)\n    return int(deadline_split[0]), int(deadline_split[1]), int(deadline_split[2])",
        "sha1": "0ded6bccce8437aad61cfa5ff121c5ed0595849b",
        "id": 1199
    },
    {
        "content": "def get_permission_codename(action, opts):\n    \"\"\"\n    Returns the codename of the permission for the specified action.\n    \"\"\"\n    return '%s_%s' % (action, opts.module_name)",
        "sha1": "c3f0004e9ec32fff7c8f55aef08961806b9dfc6f",
        "id": 543514
    },
    {
        "content": "def WILLR(data, period=14):\n    \"\"\"\n    Williams %R\n\n    Williams %R, or just %R, is a technical analysis oscillator showing the current closing price in relation to\n    the high and low of the past N days (for a given N).\n\n    It was developed by a publisher and promoter of trading materials, Larry Williams.\n\n    It's purpose is to tell whether a stock or commodity market is trading near the high or the low, or somewhere in\n    between, of its recent trading range.\n\n    The oscillator is on a negative scale, from \u2212100 (lowest) up to 0 (highest).\n\n    :param pd.DataFrame data: pandas DataFrame with open, high, low, close data\n    :param int period: period used for indicator calculation period used for indicator calculation\n    :return pd.Series: with indicator data calculation results\n    \"\"\"\n    willr_list = []\n    i = 0\n    close, high, low = data['close'], data['high'], data['low']\n    while i < len(close):\n        if i + 1 < period:\n            willr = float(' NaN')\n        else:\n            start = i + 1 - period\n            end = i + 1\n            willr = (max(high[start:end]) - close[i]) / (max(high[start:end]) - min(low[start:end])) * 100\n        #            willr *= -1\n        willr_list.append(willr)\n        i += 1\n    return willr_list",
        "sha1": "d6a921a4f177c2f1a4e908876752908f33f299f2",
        "id": 472186
    },
    {
        "content": "def byte_align(size, alignment):\n    \"\"\"Returns the int larger than ``size`` aligned to ``alginment`` bytes.\"\"\"\n    mask = alignment - 1\n    if size & mask == 0:\n        return size\n    else:\n        return (size | mask) + 1",
        "sha1": "8f0cc11b562e562c5d0f7ec5922ed1a8b79a1ee7",
        "id": 149706
    },
    {
        "content": "def convert_lut_init_to_hex(val: str) -> str:\n    \"\"\"Converts EDIF decimal and hexadecimal notation to hexadecimal for SpDE.\n\n    Args:\n        val (str): value in decimal or hexadecimal notation (i.e. ``16'hABCD``)\n\n    Returns:\n        str: string containing only hexadecimal number, without ``0x`` prefix\n            (i.e. \"ABCD\")\n    \"\"\"\n    if \"'\" not in val:\n        return str(format(int(val), 'x')).upper()\n    else:\n        return str(val.split(\"'h\")[1]).upper()",
        "sha1": "b4a8e2ecfb186ec7383f2902b7462bff1d6acc43",
        "id": 390861
    },
    {
        "content": "def make_header_id_unique(header_id, used_ids):\n    \"\"\"Make the given ID unique given a dictionary of used_ids\n\n    Arguments:\n    header_id - Slugified header ID\n    used_id - Dictionary associating each header ID without suffixes to \n    the number of times that such ID has been used.\n    \"\"\"\n    if header_id in used_ids:\n        unique_header_id = header_id + '-' + str(used_ids[header_id])\n        used_ids[header_id] += 1\n    else:\n        unique_header_id = header_id\n        used_ids[header_id] = 1\n\n    return unique_header_id",
        "sha1": "03aecc6a529364150836cface521b3937b67463c",
        "id": 197141
    },
    {
        "content": "def list_i2str(ilist):\n    \"\"\"\n    Convert an integer list into a string list.\n    \"\"\"\n    slist = []\n    for el in ilist:\n        slist.append(str(el))\n    return slist",
        "sha1": "e77bbeef7baba03ca7726b636e87c8346f699a3b",
        "id": 267064
    },
    {
        "content": "from typing import List\n\n\ndef unique(ll: List) -> List:\n    \"\"\"Return unique elements in a list preserving order.\"\"\"\n    ret = []\n    for x in ll:\n        if x not in ret:\n            ret.append(x)\n    return ret",
        "sha1": "1b5b7a5e9368326a01e9d4bbc0f284bb2dbb7365",
        "id": 537840
    },
    {
        "content": "def strip_slashes(name):\n    \"\"\"Remove slashes from the beginning and end of a part/URL.\"\"\"\n    if name.startswith('/'):\n        name = name[1:]\n    if name.endswith('/'):\n        name = name[:-1]\n    return name",
        "sha1": "1d095868262036e84371bd77e29088d58cb135d5",
        "id": 235267
    },
    {
        "content": "def nillable_dict(func):\n    \"\"\"Decorator that retuns empty dictionary if input is None\"\"\"\n\n    def wrapper(cls, element):\n        if element is None:\n            return {}\n        else:\n            return func(cls, element)\n    return wrapper",
        "sha1": "80e424bf84ab64db5ec99d8c9a91298d67e790b1",
        "id": 633928
    },
    {
        "content": "def get_midi_phrases(text, character_table):\n    \"\"\"\n    \u6b4c\u8a5e\u3092\u30dd\u30b1\u30c3\u30c8\u30df\u30af\u304c\u6271\u304616\u9032\u6570\u6587\u5b57\u5217\u306b\u5909\u63db\u3059\u308b\u95a2\u6570\uff0e\n    \n    Inputs\n    ----------\n    text : str\n        \u6b4c\u8a5e(\u534a\u89d2\u30b9\u30da\u30fc\u30b9\u3067\u533a\u5207\u3089\u308c\u3066\u3044\u3066\u3082\u3088\u3044)\n    character_table : dict(str:str)\n        \u30dd\u30b1\u30c3\u30c8\u30df\u30af\u306e\u30c7\u30d5\u30a9\u30eb\u30c8\u306e\u6587\u5b57\u30c6\u30fc\u30d6\u30eb\u3092\u8868\u3059\u8f9e\u66f8\n    \n    Returns\n    ----------\n    midi_phrases : list(str)\n        \u30dd\u30b1\u30c3\u30c8\u30df\u30af\u304c\u6271\u304616\u9032\u6570\u6587\u5b57\u5217\u306e\u30ea\u30b9\u30c8\n    \"\"\"\n    \n    midi_phrases = []\n    for t in text.replace(' ', ''):\n        midi_phrases.append(character_table[t])\n    \n    return midi_phrases",
        "sha1": "03d578278366a94e847a36fc3cd863b04c50a8d6",
        "id": 522613
    },
    {
        "content": "def is_field(token):\n    \"\"\"Checks if the token is a valid ogc type field\n    \"\"\"\n\n    return token in [\"name\", \"description\", \"encodingType\", \"location\", \"properties\", \"metadata\",\n                     \"definition\", \"phenomenonTime\", \"resultTime\", \"observedArea\", \"result\", \"id\", \"@iot.id\",\n                     \"resultQuality\", \"validTime\", \"time\", \"parameters\", \"feature\"]",
        "sha1": "04cf3ac8777dd7a4605121eb02433d9f4c195d32",
        "id": 368276
    },
    {
        "content": "def datetime_to_daysElapsed(cur_datetime, base_datetime):\n    \"\"\"\n    Description:\n        Computes the number of days elapsed since 'base' date.\n\n    Args:\n        cur_datetime (datetime): Current date and time\n        base_datetime (datetime): Base date and time\n    Returns:\n        Datetime object containing number of days elapsed\n    \"\"\"\n    time_delta = cur_datetime - base_datetime\n    time_to_days = (time_delta.seconds)/(24 * 60 * 60)\n    return time_delta.days + time_to_days",
        "sha1": "59833301b51da1a5d98afce060146ec9b13c20ae",
        "id": 438670
    },
    {
        "content": "def shortest_path_from_list(file_name, name_list):\n  \"\"\" Determines the shortest path to a file in a list of candidates.\n\n  Args:\n    file_name: A string specifying the name of the matching candidates.\n    name_list: A list of strings specifying paths.\n  Returns:\n    A string specifying the candidate with the shortest path or None.\n  \"\"\"\n  candidates = [path for path in name_list if path.split('/')[-1] == file_name]\n  if not candidates:\n    return None\n\n  shortest_path = candidates[0]\n  for candidate in candidates:\n    if len(candidate.split('/')) < len(shortest_path.split('/')):\n      shortest_path = candidate\n\n  return shortest_path",
        "sha1": "c827fc1a8b1af8a3082f2a33db1365b611d1c61b",
        "id": 309158
    },
    {
        "content": "import random\nimport torch\n\n\ndef expand(image, boxes, filler):\n    \"\"\"\n    Perform a zooming out operation by placing the image in a larger canvas of filter material.\n\n    This helps to detect smaller objects.\n\n    Args:\n        image: an image tensor of dim (3, original_h, original_w)\n        boxes: bounding boxes in boundary coordinates - a tensor of dim (n_objs, 4)\n        filler: RGB values of the filter material - list like [R, G, B]\n\n    Returns:\n        expanded image and updated bouding box coords that match the transform.\n    \"\"\"\n\n    org_h = image.size(1)\n    org_w = image.size(2)\n    max_scale = 4\n    scale = random.uniform(1, max_scale)\n    new_h = int(scale * org_h)\n    new_w = int(scale * org_w)\n\n    # Create image filler (canvas).\n    filler = torch.FloatTensor(filler)\n    new_img = torch.ones((3, new_h, new_w), dtype=torch.float) * filler.unsqueeze(\n        1\n    ).unsqueeze(1)\n\n    # Place original image at random coords in canvas.\n    left = random.randint(0, new_w - org_w)\n    right = left + org_w\n    top = random.randint(0, new_h - org_h)\n    bot = top + org_h\n    new_img[:, top:bot, left:right] = image\n\n    # Adjust bounding box coords accordingly.\n    new_boxes = boxes + torch.FloatTensor([left, top, left, top]).unsqueeze(0)\n\n    return new_img, new_boxes",
        "sha1": "a3ef729ddc588ebe7c54ca9fc4c22fd0861b327e",
        "id": 453710
    },
    {
        "content": "import hashlib\n\n\ndef hash_function(str):\n    \"\"\"Returns SHA1 hash of string.\"\"\"\n    return hashlib.sha1(str.encode('utf-8')).hexdigest()",
        "sha1": "278406260112196d541d0aab908b04116877dd0d",
        "id": 245946
    },
    {
        "content": "def apply_mask(mask, binary):\n    \"\"\"Apply a bitmask to a binary number\"\"\"\n    applied = []\n    for i, j in zip(mask, binary):\n        if i == 'X':\n            applied.append(j)\n        else:\n            applied.append(i)\n    return ''.join(applied)",
        "sha1": "f3f6c63119ed7b1c7a8c9e67e1a1a729cb87e64a",
        "id": 501979
    },
    {
        "content": "import math\n\n\ndef zscore_denominator(expected, N):\n    \"\"\"\n    Computes denominator for the zscore computation, denominator is fixed for expected, N\n    :param expected:\n    :param N:\n    :return:\n    \"\"\"\n    expected = float(expected) / float(N)\n    return math.sqrt((expected*(1.0-expected))/float(N))",
        "sha1": "3425874c81beb9de723186a5c6da1c75f3f4dcf6",
        "id": 440546
    },
    {
        "content": "import random\n\n\ndef reservoir_sample(filename, n_samples=1, random_seed=None):\n    \"\"\"Return a random subset of lines from a file\n\n    Parameters\n    ----------\n    filename: path\n        File to be loaded\n    n_samples: int\n        number of lines to return\n    random_seed: int or None\n        If set, use this as the random seed\n    \"\"\"\n    if random_seed is not None:\n        random.seed(random_seed)\n    sample = []\n    with open(filename) as f:\n        for n, line in enumerate(f):\n            if n < n_samples:\n                sample.append(line.rstrip())\n            else:\n                r = random.randint(0, n_samples)\n                if r < n_samples:\n                    sample[r] = line.rstrip()\n    return sample",
        "sha1": "6b6f174a0fc1b383b55ddcd4ef624d749808edf8",
        "id": 616148
    },
    {
        "content": "def get_folder_from_root(drive, file_title):\n    \"\"\"\n    Returns the file/directory as a GoogleDriveFile with the given title.\n    :param drive: GoogleDrive object to use to pull file from\n    :param file_title: Title to return file for\n    :return: First GoogleDriveFile matching file_title found\n    \"\"\"\n    file_list = drive.ListFile({'q': \"'root' in parents and trashed=false\"}).GetList()\n    for file1 in file_list:\n        if file1['title'] == file_title:\n            return file1",
        "sha1": "eae21c29e9ea095ad93f2a70341ee58681e890be",
        "id": 550387
    },
    {
        "content": "import random\n\n\ndef construct_drink(_preference, _ingredients):\n    \"\"\"This function creates and returns drink based on a pirate's drink preferences\"\"\"\n    drink = []\n    for key, value in _preference.items():\n        if value is True:\n            drink.append(random.choice(_ingredients[key]))\n\n    return drink",
        "sha1": "8a443470c3f5e5c893d2e7fb210f653282b3bf88",
        "id": 139150
    },
    {
        "content": "def goal_must_be_positive(cls, value):\n    \"\"\"Validate that goal is a positive number.\"\"\"\n    assert value >= 0, f'goal == {value}, must be >= 0'\n    return value",
        "sha1": "cef66f0faeabea1761bf9af5d001a1b4ee430576",
        "id": 515890
    },
    {
        "content": "def is_external_plugin(module_path):\n    \"\"\"\n    Returns true when the given module is an external plugin.\n\n    Implementation note: does a simple check on the name to see if it's\n    not prefixed with \"kolibri.\". If so, we know it's not an internal plugin.\n    \"\"\"\n\n    return not module_path.startswith(\"kolibri.\")",
        "sha1": "b3058bf76882ce7f52518c993e9350d7b4fa9b24",
        "id": 674835
    },
    {
        "content": "def nest_dict(flat_dict, sep='-'):\n    \"\"\"Return nested dict by splitting the keys on a delimiter.\n\n    Flask-wtf returns embedded document fields as a flattened dict, with\n    embedded document names embedded in the key. Any keys with empty values\n    will be removed.\n\n    For example, a document User may have an embedded document Comment.\n    Flask-wtf will return this in a form as \"user-comment\". This function\n    returns a nested dictionary like d[\"user\"][\"comment\"].\n\n    Args:\n        flat_dict (dict): Flattened dict of embedded document fields.\n        sep (str): Seperator between nested keys.\n\n    Returns:\n        dict: Nested dictionary which better represents the embedded documents.\n    \"\"\"\n\n    # Start a new dict to hold top level keys and take values for these top level keys\n    new_dict = {}\n    hyphen_dict = {}\n    eds = set()\n    for k, v in flat_dict.items():\n        if not v:\n            pass\n        elif '-' not in k:\n            new_dict[k] = v\n        else:\n            hyphen_dict[k] = v\n            eds.add(k.split(sep)[0])\n\n    # Create a new nested dict for each embedded document\n    # And add these dicts to the correct top level key\n    ed_dict = {}\n    for ed in eds:\n        ed_dict = {}\n        for k, v in hyphen_dict.items():\n            if ed == k.split(sep)[0]:\n                ed_dict[k.split(sep)[1]] = v\n        new_dict[ed] = ed_dict\n\n    return new_dict",
        "sha1": "e8a9a38c06db49e50c1b0a1f0a5216c46bf0df9c",
        "id": 393010
    },
    {
        "content": "import inspect\n\n\ndef _is_code_module(module):\n    \"\"\"Determine if a module comes from python code\"\"\"\n    # getsourcefile will not return \"bare\" pyc modules. we can reload those?\n    try:\n        return inspect.getsourcefile(module) or \"\"\n    except TypeError:\n        return \"\"",
        "sha1": "74e4cba7eafd6998eb74ec4ea7dfb32f4467c65f",
        "id": 244918
    },
    {
        "content": "def time_into_milliseconds(time_string: str) -> int:\n    \"\"\"Utility function to turn time string into milliseconds from H:M:S.f format.\"\"\"\n    hours = int(time_string[:2])\n    mins = int(time_string[3:5])\n    seconds = float(time_string[6:])\n    return int(hours * 3600000 + mins * 60000 + seconds * 1000)",
        "sha1": "7dc93f762d5ea8d4bf8dadc25b202c6277c4730d",
        "id": 106644
    },
    {
        "content": "from typing import Any\nfrom typing import Type\n\n\ndef _is_measurement_device(instrument_handle: Any, class_type: Type) -> bool:\n    \"\"\" Returns True if the instrument handle is of the given type, else False.\n\n        This function checks whether the given handle is of the correct instrument type.\n        All error's are catched related to importing of not installed drivers or instruments\n        which are not connected.\n\n        Args:\n            instrument_handle: An measurement device instance.\n            class_type: The type of the measurement class.\n\n        Returns:\n            True if of the given class_type, else False.\n    \"\"\"\n    try:\n        is_present = isinstance(instrument_handle, class_type)\n    except Exception:\n        is_present = False\n\n    return is_present",
        "sha1": "7a0d9ba51a36df8c800f35e20b4a3ae690522502",
        "id": 118029
    },
    {
        "content": "def has_numbers(input_str: str):\n    \"\"\" Check if a string has a number character \"\"\"\n    return any(char.isdigit() for char in input_str)",
        "sha1": "5038cb737cdcfbad3a7bd6ac89f435559b67cebc",
        "id": 707028
    },
    {
        "content": "import io\n\n\ndef file_to_png(fp):\n\t\"\"\"Convert an image to PNG format with Pillow.\n\t\n\t:arg file-like fp: The image file.\n\t:rtype: bytes\n\t\"\"\"\n\timport PIL.Image # pylint: disable=import-error\n\twith io.BytesIO() as dest:\n\t\tPIL.Image.open(fp).save(dest, \"PNG\", optimize=True)\n\t\treturn dest.getvalue()",
        "sha1": "213378ff88bc93c7c399e1e0ff4f2168e680b194",
        "id": 573770
    },
    {
        "content": "def floor_minute(timestamp):\n    \"\"\"\u628a\u65f6\u95f4\u6233\u5bf9\u9f50\u5230\u6574\u5206\u949f\n\n    :param timestamp: \u65f6\u95f4\u6233\n    \"\"\"\n    return int(timestamp) // 60 * 60",
        "sha1": "c1aa674d10fc92cc92033e8e8246f5debd70eb98",
        "id": 543720
    },
    {
        "content": "def _toArchitectureDir(architecture):\n  \"\"\"Re-map 'x64' to 'amd64' to match MSVC directory names.\n  \"\"\"\n  return {'x64':'amd64'}.get(architecture, architecture)",
        "sha1": "53f630829ea4c91c032401ba16f28d900c16e98a",
        "id": 122452
    },
    {
        "content": "def is_equal(a, b):\n    \"\"\"This function checks if the two variables are equal to each other.\n\n        >>> is_equal(2, 2)\n        True\n        >>> is_equal(2, 3)\n        False\n        >>> is_equal(\"Dog\", \"Dog\")\n        True\n        >>> is_equal(\"Cat\", \"Dog\")\n        False\n    \"\"\"\n    return(a == b)",
        "sha1": "397117cdc553a39fb7f756f751dfdcd157a11730",
        "id": 481232
    },
    {
        "content": "from math import fabs\nfrom datetime import datetime\n\n\ndef pretty_date(d):\n    \"\"\"\n    Format the time delta between d and now human-friendly. E.g. 'in 2 hours', '20 seconds ago'\n    :param d:\n    :return:\n    \"\"\"\n\n    now = datetime.now()\n    diff = now - d\n    total_seconds = diff.seconds + diff.days * 24 * 60 * 60\n    sec = int(fabs(total_seconds))\n\n    if sec < 60:\n        v = sec\n        unit = 'second' + ('s' if v != 1 else '')\n    elif sec < 60 * 60:\n        v = sec / 60\n        unit = 'minute' + ('s' if v != 1 else '')\n    elif sec < 60 * 60 * 24:\n        v = sec / 60 / 60\n        unit = 'hour' + ('s' if v != 1 else '')\n    else:\n        v = sec / 60 / 60 / 24\n        unit = 'day' + ('s' if v != 1 else '')\n\n    if total_seconds < 0:\n        return 'in %i %s' % (v, unit)  # future\n    else:\n        return '%i %s ago' % (v, unit)",
        "sha1": "153bf459c55d2d69c8d9638be1f91f66adfcac47",
        "id": 357022
    },
    {
        "content": "def PDMS2Dec (decst, sep=\":\"):\n    \"\"\" Convert a declination string to degrees\n\n    Returns dec in deg\n    decst     Dec string as \"dd:mm:ss.s\"\n    sep       sympol to use to separate components instead of \":\"\n    \"\"\"\n    ################################################################\n    pp = decst.split(sep)\n    if len(pp)>0:\n        deg = int(pp[0])\n    else:\n        deg = 0\n    if len(pp)>1:\n        min = int(pp[1])\n    else:\n        min = 0\n    if len(pp)>2:\n        ssec = float(pp[2])\n    else:\n        ssec = 0.0\n    dec =  abs(deg) + min/60.0 + ssec/3600.0\n    if pp[0].find(\"-\") >=0:\n        dec = -dec\n    return dec\n    # end PDec2DMS",
        "sha1": "ba6a82db98699c20ee2ef7d7941397f20c527693",
        "id": 274665
    },
    {
        "content": "def get_md5(item):\n    \"\"\"calculate MD5 hash of a value\"\"\"\n    # this is slow but expected to not have a lot of use\n    import hashlib  # delay the import - it's rarely needed\n\n    return hashlib.md5(str(item).encode()).hexdigest()",
        "sha1": "076b7db1d6bba34f7769a67edc85eef92d81a4e3",
        "id": 163459
    },
    {
        "content": "import hashlib\n\n\ndef encrypt_password(password, salt):\n    \"\"\"Encrypt a provided password and return a hash.\n\n    Keyword arguments:\n    password -- The plaintext password to be encrypted\n    salt -- The salt to be used for padding\n\n    Returns:\n        String\n    \"\"\"\n\n    passwd_hash = hashlib.pbkdf2_hmac('sha256', password.encode('utf-8'), salt, 10000).hex()\n    return passwd_hash",
        "sha1": "c3890db947196d3f7b5227e91137a5ec15413fa5",
        "id": 494710
    },
    {
        "content": "def json_serial(obj):\n    \"\"\"JSON serializer for objects not serializable by default json code\n    >>> from datetime import date, datetime\n    >>> json_serial(date(2018, 12, 12))\n    '2018-12-12'\n    >>> json_serial(datetime(2018, 12, 12, 12, 12))\n    '2018-12-12T12:12:00'\n    >>> json_serial(\"\")\n    Traceback (most recent call last):\n        ...\n    TypeError: Type <class 'str'> not serializable\n    \"\"\"\n    try:\n        return obj.isoformat()\n    except AttributeError:\n        raise TypeError(\"Type %s not serializable\" % type(obj))",
        "sha1": "801a545cbe2e0ec8d263d57882cd1d8c07ea1707",
        "id": 105487
    },
    {
        "content": "def get_adder(summand1):\n    \"\"\"Returns a function that adds numbers to a given number.\"\"\"\n    def adder(summand2):\n        return summand1 + summand2\n    return adder",
        "sha1": "ddd1c81eb177cd992294c06d9edf2f7fb8d2efb6",
        "id": 285525
    },
    {
        "content": "from typing import Any\nfrom pydantic import BaseModel  # noqa: E0611\nfrom enum import Enum\n\n\nasync def clean_python_types(data: \"Any\") -> \"Any\":\n    \"\"\"Turn any types into MongoDB-friendly Python types.\n\n    Use `dict()` method for Pydantic models.\n    Use `value` property for Enums.\n    Turn tuples and sets into lists.\n    \"\"\"\n    res: \"Any\" = None\n    if isinstance(data, (list, tuple, set)):\n        res = []\n        for datum in data:\n            res.append(await clean_python_types(datum))\n    elif isinstance(data, dict):\n        res = {}\n        for key in list(data.keys()):\n            res[key] = await clean_python_types(data[key])\n    elif isinstance(data, BaseModel):\n        # Pydantic model\n        res = await clean_python_types(data.dict())\n    elif isinstance(data, Enum):\n        res = await clean_python_types(data.value)\n    elif isinstance(data, type):\n        res = await clean_python_types(f\"{data.__module__}.{data.__name__}\")\n    else:\n        # Unknown or other basic type, e.g., str, int, etc.\n        res = data\n    return res",
        "sha1": "f67c2be6ada2b528122093c2879f40055dd0f135",
        "id": 662299
    },
    {
        "content": "def checkDiscardBackButtonClicked(game, clickX, clickY):\n    \"\"\"\n    Returns true if back button was clicked, else false\n    \"\"\"\n    return (game.discardBackButtonObj is not None and \n        game.discardBackButtonObj.collidepoint(clickX, clickY))",
        "sha1": "968da7fbe953cc5168bb90abdc37d48b9e041302",
        "id": 172103
    },
    {
        "content": "def get_submodule_by_path(module, path):\n    \"\"\"\n    Return a sub-module by its name.\n    The path may refer to any nested node using '/' as the path delimiter.\n    \"\"\"\n    # Separate out the next module's name\n    parts = path.split('/', 1)\n    name = parts[0]\n    sub_path = parts[1] if len(parts) == 2 else None\n\n    if hasattr(module, name):\n        submodule = getattr(module, name)\n        if sub_path is None:\n            return submodule\n        return get_submodule_by_path(module=submodule, path=sub_path)\n\n    raise KeyError(f'No module named \"{name}\" found.')",
        "sha1": "9e115fe2c1992474a771a53bf65bc43aaf93b72a",
        "id": 504167
    },
    {
        "content": "from pathlib import Path\n\n\ndef golden_snippet(filename: str) -> str:\n    \"\"\"Load the golden snippet with the name provided\"\"\"\n    snippet_path = Path(__file__).parent / \"golden_snippets\" / filename\n    return snippet_path.read_text()",
        "sha1": "f700e5928b48a881db299a139d5a1f616dcc65d6",
        "id": 358608
    },
    {
        "content": "def make_name_map(names):\n    \"\"\"\n    Create a dictionary mapping lowercase names to capitalized names.\n\n    Parameters\n    ----------\n    names : sequence\n\n    Returns\n    -------\n    dict\n\n    \"\"\"\n    return dict((name.lower(), name) for name in names)",
        "sha1": "a35a0ae90105b2ec53c3d9b5d2665d11b1c95d8e",
        "id": 394960
    },
    {
        "content": "def case_determination(b, c):\n    \"\"\"Determine the Current Case in Terms of b and c.\"\"\"\n\n    # Case 1\n    if b.min() >= 0 and c.min() >= 0:\n        return 1\n    # Case 2\n    if b.min() >= 0 and c.min() < 0:\n        return 2\n    # Case 3\n    if b.min() < 0:\n        return 3",
        "sha1": "14ffbeef9b4de67094d4a6962319b343b44cf500",
        "id": 313867
    },
    {
        "content": "def extract_preterminals(tree):\n    \"\"\"Extract the preterminals from the given tree.\"\"\"\n    return [node for node in tree.subtrees() if node.height() == 2]",
        "sha1": "41386b2165fa5394c5a5cdc7c09c928681ba0b11",
        "id": 317667
    },
    {
        "content": "import uuid\n\n\ndef shard_uuid(path):\n    \"\"\"Compute a UUID for a shard path.\"\"\"\n    return str(uuid.uuid3(uuid.NAMESPACE_URL, path))",
        "sha1": "ba8d46365ce041997eb5040d94a57c70c8fd32cc",
        "id": 229761
    },
    {
        "content": "def calc_starting_row(page_num, rows_per_age=10):\n    \"\"\"\n    Calculate a starting row for the Solr search results. We only retrieve one page at a time\n    :param page_num: Current page number\n    :param rows_per_age: number of rows per page\n    :return: starting row\n    \"\"\"\n    page = 1\n    try:\n        page = int(page_num)\n    except ValueError:\n        pass\n    if page < 1:\n        page = 1\n    elif page > 100000:  # @magic_number: arbitrary upper range\n        page = 100000\n    return rows_per_age * (page - 1), page",
        "sha1": "e9e467f152c0daad2cd9f5431f6abf916893955b",
        "id": 542121
    },
    {
        "content": "import torch\n\n\ndef get_number_of_voxels_per_class(labels: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Computes the number of voxels for each class in a one-hot label map.\n    :param labels: one-hot label map in shape Batches x Classes x Z x Y x X or Classes x Z x Y x X\n    :return: A tensor of shape [Batches x Classes] containing the number of non-zero voxels along Z, Y, X\n    \"\"\"\n    if not len(labels.shape) in [5, 4]:\n        raise Exception(\"labels must have either 4 (Classes x Z x Y x X) \"\n                        \"or 5 dimensions (Batches x Classes x Z x Y x X), found:{}\"\n                        .format(len(labels.shape)))\n\n    if len(labels.shape) == 4:\n        labels = labels[None, ...]\n\n    return torch.count_nonzero(labels, dim=(2, 3, 4))",
        "sha1": "568a91639a42cf3cd3debe365c5a963512d95dfc",
        "id": 706030
    },
    {
        "content": "import time\n\n\ndef generate_filename() -> str:\n    \"\"\"Create unique filename\"\"\"\n    return time.strftime(\"%Y%m%d-%H%M%S\")",
        "sha1": "a8499566aa42e60b1bcc89d48ba2c78d594940f4",
        "id": 285050
    },
    {
        "content": "def add_zero(number):\n    \"\"\"\n    Add zero before number if number is smaller than 10.\n    :param number: <string> -> number\n    :return: <string> -> number\n    \"\"\"\n    if int(number) < 10:\n        number = '0' + str(number)\n    return number",
        "sha1": "978b5bd2d6bbd0a8cb66206fd80f3dfc2f0d2e3a",
        "id": 418493
    },
    {
        "content": "import collections\n\n\ndef make_ordered_dict(d):\n    \"\"\"\n    Turn a {k:v} dictionary into an ordered dictionary ordered from largest\n    v to smallest\n    \"\"\"\n    k = list(d.keys())\n    k.sort(key=lambda x: d[x])\n    k.reverse()\n    nd = collections.OrderedDict()\n    for i in k:\n        nd[i] = d[i]\n    return nd",
        "sha1": "470a07ffb4a72346fadf59beef21ce38b59ad59f",
        "id": 220095
    },
    {
        "content": "def ip_to_int(ip):\n    \"\"\"\n    >>> ip_to_int(None)\n    0\n    >>> ip_to_int('0.0.0.0')\n    0\n    >>> ip_to_int('1.2.3.4')\n    16909060\n    \"\"\"\n    if ip is None:\n        return 0\n    result = 0\n    for part in ip.split('.'):\n        result = (result << 8) + int(part)\n    return result",
        "sha1": "030fad68684cde8a132df6f5e5891f11a6bf6eb5",
        "id": 630942
    },
    {
        "content": "def my_isclose(a, b, rel_tol=1e-09, abs_tol=0.0):\n    \"\"\"\n    Test if a and b are close enough to consider equal.\n    \n    This function is essentially the same as the math.isclose function which\n    was added in Python 3.5 but is created here to increase compatibility across\n    all Python 3 versions.\n    \"\"\"\n    return abs(a-b) <= max(rel_tol * max(abs(a), abs(b)), abs_tol)",
        "sha1": "f7799e6cd7df7500439de51ed0c9f1d8ecf1c94b",
        "id": 622452
    },
    {
        "content": "def _get_summary_name(tensor, name=None, prefix=None, postfix=None):\n  \"\"\"Produces the summary name given.\n\n  Args:\n    tensor: A variable or op `Tensor`.\n    name: The optional name for the summary.\n    prefix: An optional prefix for the summary name.\n    postfix: An optional postfix for the summary name.\n\n  Returns:\n    a summary name.\n  \"\"\"\n  if not name:\n    name = tensor.op.name\n  if prefix:\n    name = prefix + '/' + name\n  if postfix:\n    name = name + '/' + postfix\n  return name",
        "sha1": "8d7350a777621b240ec2f93b0110de54fae9e411",
        "id": 181638
    },
    {
        "content": "def parse_text(elem):\n    \"\"\"Parse text from an element.\n\n    >>> parse_text(ET.XML('<Property>Hello World</Property>'))\n    'Hello World'\n    >>> parse_text(ET.XML('<Property>True  </Property>'))\n    True\n    >>> parse_text(ET.XML('<Property>123</Property>'))\n    123\n    >>> print(parse_text(ET.XML('<Property />')))\n    None\n    \"\"\"\n    if elem.text is None:\n        return None\n    try:\n        return int(elem.text)\n    except ValueError:\n        pass\n    text = elem.text.strip()\n    if text == 'True':\n        return True\n    if text == 'False':\n        return False\n    return elem.text",
        "sha1": "87ca6780659d51c4057bb0cc3fb460b99b2a920c",
        "id": 307470
    },
    {
        "content": "def format_list(list1, fmt = '%16s', delimiter = \",\"):\n    \"\"\"\n    format list of numbers to string.\n    delimiter defaults = ',' \n    \"\"\"\n    string1 = delimiter.join(fmt % h for h in list1) + '\\n'\n    return string1",
        "sha1": "65a341b59f27867c7da069314b37dbd2cd95fa1b",
        "id": 639512
    },
    {
        "content": "import csv\n\n\ndef indexGroups(samplefile, groupvar):\n    \"\"\"\n    Build dictionary with sample ID as key, group ID as value\n\n    \"\"\"\n\n    sg_dict = {}\n\n    f = open(samplefile, 'r', encoding=\"utf-8\")\n    reader = csv.DictReader(f, delimiter='\\t')\n\n    for row in reader:\n        sg_dict[row['ID']] = row[groupvar]\n\n    return sg_dict",
        "sha1": "882ab5f0aacc675bd73c745e913127b3b7ba935d",
        "id": 414955
    },
    {
        "content": "def get_coverage(router_features, rows, possible_args, extra_features=lambda feature: False):\n    \"\"\"\n    Generates pairwise coverage information for a given set of router feature triplets as well as a list of rows\n    containing parameter values\n    :param router_features: List of router feature triplets\n    :param rows: Rows which each map each router feature triplet to a parameter value\n    :param possible_args: A dict mapping containing all possible parameter values for a given feature\n    :param extra_features: function which returns True if the given feature should have an extra value representing\n        \"invalid\" configurations. This is used only for the random baseline, in particular to represent for example\n        using a route-map which is not actually defined on the router\n    :return: The total number of unique pairs in router_features as well as the number of unique pairs covered by rows\n    \"\"\"\n    covered_pairs = {(f1, f2): set() for f1 in router_features for f2 in router_features if f1 != f2}\n\n    def num_feature_args(feature):\n        if extra_features(feature):\n            return len(possible_args[feature]) + 2\n        else:\n            return len(possible_args[feature]) + 1\n\n    num_pairs = {((r1, f1, arg1), (r2, f2, arg2)): num_feature_args(f1) * num_feature_args(f2)\n                 for (r1, f1, arg1) in router_features for (r2, f2, arg2) in router_features if\n                 (r1, f1, arg1) != (r2, f2, arg2)}\n\n    coverage = []\n\n    for row in rows:\n        for f1 in row:\n            for f2 in row:\n                if f1 != f2:\n                    covered_pairs[f1, f2].add((row[f1], row[f2]))\n\n        covered = 0\n        for p in covered_pairs:\n            covered = covered + len(covered_pairs[p])\n\n        coverage.append(covered)\n\n    total = 0\n    for p in num_pairs:\n        total = total + num_pairs[p]\n\n    return total, coverage",
        "sha1": "f2b9922c09349faea62a07195bdf2221a1a695ca",
        "id": 463086
    },
    {
        "content": "def _bytes(packet):\n    \"\"\"\n    Returns a human-friendly representation of the bytes in a bytestring.\n\n    >>> _bytes('\\x12\\x34\\x56')\n    '123456'\n    \"\"\"\n    return ''.join('%02x' % ord(c) for c in packet)",
        "sha1": "c3877c4081e191bcdcb95dd958beb0fd8356699e",
        "id": 304651
    },
    {
        "content": "def _band_shortname(long_name):\n    \"\"\"Get short band name, e.g. `Near Infrared (NIR)` becomes `nir` and\n    `Red` becomes `red`.\n    \"\"\"\n    if '(' in long_name:\n        start = long_name.find('(') + 1\n        end = long_name.find(')')\n        short_name = long_name[start:end]\n    else:\n        short_name = long_name.replace(' ', '_').replace('-', '_')\n    return short_name.lower()",
        "sha1": "87762f305cc622b2fb743a8fb9d5c1af41ac67f1",
        "id": 89084
    },
    {
        "content": "def extended_gcd(a, b):\n    \"\"\"Find the solution of ax + by = gcd(x, y), returns (x, y, gcd(x, y))\"\"\"\n    if b == 0:\n        return 1, 0, a\n    x, y, gcd = extended_gcd(b, a % b)\n    return y, x - (a // b) * y, gcd",
        "sha1": "f3dbc95e3b13d89079916948bf76ddba5184d6c6",
        "id": 185565
    },
    {
        "content": "from typing import List\n\n\ndef repeat_prev(prev_vals: List[float]):\n    \"\"\"\n    Repeats the previous seen value again\n    \"\"\"\n    return prev_vals[-1]",
        "sha1": "026b89ad9f46e20b051f075cde56132fcbf69f7d",
        "id": 523213
    },
    {
        "content": "def center_crop_tensor(t, crop_height, crop_width):\n    \"\"\" Given the output size (crop_height, crop_width), center crop the target tensor \"\"\"\n\n    n, c, h, w = t.size()\n    if h == crop_height and w == crop_width:\n        return t\n    sy = (h - crop_height) // 2\n    ty = sy + crop_height\n    sx = (w - crop_width) // 2\n    tx = sx + crop_width\n    crop_tensor = t[:, :, sy:ty, sx:tx].contiguous()\n    return crop_tensor",
        "sha1": "5260ff129e19b5ab99dcab072ca8c6fefe8c623c",
        "id": 315229
    },
    {
        "content": "import calendar\n\n\ndef derive_time_features(df, source_date_column = 'index'):\n    \"\"\"\n    Creates time series features from datetime index\n\n    :param df:                  Data frame representing time series\n    :param source_date_column:  Column in df representing time\n    :return:                    Data frame with derived features only\n    \"\"\"\n    if (source_date_column == 'index'):\n        df['date'] = df.index\n        date_col = 'date'\n    else:\n        date_col = source_date_column\n\n    #df['hour'] = df[date_col].dt.hour\n    #df['hour'] = df[date_col].dt.minute\n    df['dayofweek'] = df[date_col].dt.dayofweek\n    df['dayname'] = df['dayofweek'].apply(lambda x: calendar.day_name[x])\n    df['quarter'] = df[date_col].dt.quarter\n    df['month'] = df[date_col].dt.month\n    df['year'] = df[date_col].dt.year\n    df['dayofyear'] = df[date_col].dt.dayofyear\n    df['dayofmonth'] = df[date_col].dt.day\n    df['weekofyear'] = df[date_col].dt.weekofyear\n\n    X = df[[\n            #'hour',\n            #'minute',\n            'dayofweek',\n            'quarter',\n            'month',\n            'year',\n            'dayofyear',\n            'dayofmonth',\n            'weekofyear'\n           ]]\n    return X",
        "sha1": "61c8e428a9064a79537cac4f2447fed397e8e81c",
        "id": 392537
    },
    {
        "content": "def ma(df, ma_ranges=[10, 21, 50]):\n    \"\"\"\n    Simple Moving Average\n\n    Parameters\n    ----------\n    df : pandas.DataFrame, must include columns ['Close']\n     Dataframe where the ma is extracted from\n\n    ma_ranges: list, default [10, 21, 50]\n     List of periods of Simple Moving Average to be extracted\n\n    Return\n    ------\n    df : pandas.DataFrame\n     DataFrame with Simple Moving Average\n    \"\"\"\n\n    df = df.copy()\n    for period in ma_ranges:\n        df[f\"MA{period}\"] = df['Close'].rolling(window=period).mean()\n\n    return df",
        "sha1": "ddd88c75f17e0dcac6a9ce5991ee263ba92c6e83",
        "id": 653105
    },
    {
        "content": "def code() -> str:\n    \"\"\"\n    Example G-code module, a 40mm circle.\n\n    Please simulate first, before milling.\n    \"\"\"\n    return \"\"\"\n        G90\n        G17\n        G2 X0 Y40 I0 J20\n        G2 X0 Y0 I0 J-20\n    \"\"\"",
        "sha1": "a565c57e9773c9c511cbfc489dd7cb085a69bb8b",
        "id": 345364
    },
    {
        "content": "from typing import Callable\nfrom typing import Any\nfrom typing import Dict\n\n\ndef call(f: Callable[..., Any], *args: Any, **kwargs: Any) -> Any:\n    \"\"\"\n    For each parameter that f takes, it is looked up in the kwargs,\n    if present, then passed to f, otherwise the following argument is\n    consumed starting from the first one.\n    Example:\n    Take the following function\n    def add(x, y, z):\n        return x + 2*y + 10*z\n    Then:\n    >>> call(add, 5, 3, y = 1)\n    37\n    In this example, call first looks for x in kwargs. x is not found,\n    so the first positional argument, 5, is assigned to x. Then y is\n    assigned to 1 as it was passed as a keyword argument. Finally, z\n    is not a keyword argument, so it is consumed from the following\n    positional argument, in this case 3. Then, the call to add looks like\n    add(5, 1, 3) which returns 37.\n    \"\"\"\n    params: Dict[str, type] = f.__annotations__.copy()\n    # return is not a parameter so we take it out\n    params.pop(\"return\", None)\n\n    if len(args) + len(kwargs) != len(params):\n        raise TypeError(\n            \"Incorrect number of arguments passed. \"\n            f\"Required number of arguments is {len(params)}, \"\n            f\"but {len(args) + len(kwargs)} were passed\"\n        )\n\n    for key in kwargs:\n        # If an incorrect keyword argument was passed,\n        # an error is raised\n        if key not in params:\n            raise TypeError(\n                f\"Keyword argument {key} was passed but not used in the function.\"\n            )\n\n    for pname, ptype in params.items():\n        # If the parameter wasnt passed as keyword, then\n        # it is consumed from args and correctly casted\n        # according to the function signature\n        if kwargs.get(pname, None) is None:\n            kwargs[pname] = ptype(args[0])\n            args = args[1:]\n\n    return f(**kwargs)",
        "sha1": "dbf0071e3237a5e2aed134fe9888163f09e4c08b",
        "id": 325208
    },
    {
        "content": "def reverse_complement(nuc_sequence):\n    \"\"\"\n    Returns the reverse complement of a nucleotide sequence.\n    >>> reverse_complement('ACGT')\n    'ACGT'\n    >>> reverse_complement('ATCGTGCTGCTGTCGTCAAGAC')\n    'GTCTTGACGACAGCAGCACGAT'\n    >>> reverse_complement('TGCTAGCATCGAGTCGATCGATATATTTAGCATCAGCATT')\n    'AATGCTGATGCTAAATATATCGATCGACTCGATGCTAGCA'\n     \"\"\"\n    complements = {\n    \t\"A\": \"T\",\n    \t\"C\": \"G\",\n    \t\"G\": \"C\",\n    \t\"T\": \"A\"\n    }\n    rev_seq = \"\".join([complements[s] for s in nuc_sequence[::-1]])\n    return rev_seq",
        "sha1": "664482c6f621fa128982ca5b55e4c1d5160137e4",
        "id": 316427
    },
    {
        "content": "def input_format(line):\n    \"\"\" Converts a string into a list of vertices and a point\n\n    Receive an entry with this format with \n    this format '1 1, 3 2, 1 4, 3 4 | 3 3 '\n    Returns:\n    list = [(1,1), (3,2), (1,4), (3,4)] and a point = (3,3)\n\n    :param line: \n    :return: list, point\n    \"\"\"\n\n    line_strip = line.rstrip('\\n').split(' | ')\n    point = line_strip[1].split(' ')\n    coord = (float(point[0]), float(point[1]))\n    vertices = []\n    for vertex in line_strip[0].split(', '):\n        vertices.append(\n            (float(vertex.split(' ')[0]),\n             float(vertex.split(' ')[1])))\n\n    return vertices, coord",
        "sha1": "61c6ec5bc6e655ee193f63de286e64727220d2cf",
        "id": 620840
    },
    {
        "content": "def solution(A):\n    \"\"\"\n    A function that given a non-empty array A containing an odd number (N) of elements - all integers, and each element of the array can be paired with another element that has the same value, except for one element that is left unpaired, returns the value of the unpaired element.\n    For example, given array A such that:\n    A[0] = 9  A[1] = 3  A[2] = 9\n    A[3] = 3  A[4] = 9  A[5] = 7\n    A[6] = 9\n    the function should return 7\n    \"\"\"\n    # Define a dictionary to keep value-count pairs\n    value_dict = {}\n    # Define a variable to store the value with an odd count\n    odd_value = None\n\n    # Loop through A taking note of the current element's count\n    for el in A:\n        if el in value_dict:\n            value_dict[el] += 1\n        else:\n            value_dict[el] = 1\n        odd_value = el if value_dict[el] % 2 == 1 else odd_value\n    return odd_value",
        "sha1": "54fbef0954b5f08fae7494eedbfec49539d40253",
        "id": 60453
    },
    {
        "content": "import hashlib\n\n\ndef hash_all(strs, digest=None):\n  \"\"\"Returns a hash of the concatenation of all the strings in strs.\n\n  If a hashlib message digest is not supplied a new sha1 message digest is used.\n  \"\"\"\n  digest = digest or hashlib.sha1()\n  for s in strs:\n    digest.update(s)\n  return digest.hexdigest()",
        "sha1": "585496aaae534d24cba512482765aeb9250ef6b8",
        "id": 38156
    },
    {
        "content": "from pathlib import Path\n\n\ndef ensure_unique(path: Path, extension: str = \".tif\", ndigits: int = 3):\n    \"\"\"\n    Get next suitable filepath (extension = \".tif\") or\n    folderpath (extension = \"\"), appended with a counter of ndigits.\n    \"\"\"\n    p = path\n    stem = p.stem\n    # check if provided path already has an ndigit number in it\n    cur_num = stem.rsplit(\"_\")[-1]\n    if cur_num.isdigit() and len(cur_num) == ndigits:\n        stem = stem[: -ndigits - 1]\n        current_max = int(cur_num) - 1\n    else:\n        current_max = -1\n\n    # # find the highest existing path (if dir)\n    paths = (\n        p.parent.glob(f\"*{extension}\")\n        if extension\n        else (f for f in p.parent.iterdir() if f.is_dir())\n    )\n    for fn in paths:\n        try:\n            current_max = max(current_max, int(fn.stem.rsplit(\"_\")[-1]))\n        except ValueError:\n            continue\n\n    # build new path name\n    number = f\"_{current_max+1:0{ndigits}d}\"\n    return path.parent / f\"{stem}{number}{extension}\"",
        "sha1": "6345f983a11d6fb0f16ffbeb9d3f1be315c51a49",
        "id": 304963
    },
    {
        "content": "import hashlib\n\n\ndef md5(string):\n    \"\"\"\n    Encrypt a string with MD5, used for password and Gravatar.\n\n        >>> md5('')\n        'd41d8cd98f00b204e9800998ecf8427e'\n        >>> md5('admin')\n        '21232f297a57a5a743894a0e4a801fc3'\n    \"\"\"\n    string = string.encode('UTF-8')\n    return hashlib.md5(string).hexdigest()",
        "sha1": "9a2b1d391b8a89dc60f5129d0988e83abeba3241",
        "id": 496647
    },
    {
        "content": "def Flink(n, Tb, Tc):\n    \"\"\"\n    Link factor for the bolo to the bath\n\n    Args:\n    n (float): thermal carrier index\n    Tb (float): bath temperature [K]\n    Tc (float): transition temperature [K]\n    \"\"\"\n    return (((n + 1)/(2 * n + 3)) * (1 - (Tb / Tc)**(2 * n + 3)) /\n            (1 - (Tb / Tc)**(n + 1)))",
        "sha1": "b6394202af002e2f2a052d5950c06114043d7958",
        "id": 394072
    },
    {
        "content": "import unicodedata\n\n\ndef iswbound(char):\n    \"\"\"Returns whether the given character is a word boundary.\"\"\"\n    category = unicodedata.category(char)\n    # If it's a space separator or punctuation\n    return 'Zs' == category or 'Sk' == category or 'P' == category[0]",
        "sha1": "9ce01a085878320855dea3b6a90ad7ccb646d044",
        "id": 504220
    },
    {
        "content": "import click\n\n\ndef validate_relay(context, param, value):\n    \"\"\"\n    Validates presence of relay by name.\n    \"\"\"\n    relays = context.obj.api.relays()\n    relay = next((r for r in relays\n                  if r[\"name\"] == value), None)\n    if relay is None:\n        error = \"Relay \\\"%s\\\" does not exist\" % value\n        raise click.BadParameter(error)\n\n    return relay",
        "sha1": "644e12abed2e308209aa4d756b9e189a3b287d97",
        "id": 627419
    },
    {
        "content": "def _accumulated_cost_matrix(ac):\n    \"\"\"Fast computation of accumulated cost matrix using cost matrix.\n\n    Parameters\n    ----------\n    ac: nd-array\n        Given cost matrix c, ac = acc_initialization(...), ac[1:, 1:] = c.\n\n    Returns\n    -------\n        The accumulated cost matrix.\n    \"\"\"\n    for i in range(ac.shape[0] - 1):\n        for j in range(ac.shape[1] - 1):\n            ac[i + 1, j + 1] += min(ac[i, j + 1], ac[i + 1, j], ac[i, j])\n    return ac",
        "sha1": "2c2be451e891ed092bb75ebf61152ccbfce23f17",
        "id": 522625
    },
    {
        "content": "from pathlib import Path\nfrom typing import Optional\n\n\ndef load_latest_model(model_folder: Path) -> Optional[Path]:\n    \"\"\"Finds the latest model inside the folder where\n    all the models are saved\n\n    Args:\n        model_folder (Path): folder containing the models\n\n    Returns:\n        Path: file path of the most recent model\n    \"\"\"\n\n    model_cps = [\n        filename\n        for filename in model_folder.parent.iterdir()\n        if filename.name.startswith(model_folder.name)\n    ]\n\n    if len(model_cps) == 0:\n        return None\n\n    return sorted(model_cps)[-1]",
        "sha1": "62818ca57b6e2e4ae97890c30eca1e21b6fd0d45",
        "id": 639432
    },
    {
        "content": "import torch\n\n\ndef my_listperm2matperm(listperm):\n    \"\"\"Converts a batch of permutations to its matricial form.\n\n    Args:\n    listperm: 2D tensor of permutations of shape [batch_size, n_objects] so that\n      listperm[n] is a permutation of range(n_objects).\n\n    Returns:\n    a 3D tensor of permutations matperm of\n      shape = [batch_size, n_objects, n_objects] so that matperm[n, :, :] is a\n      permutation of the identity matrix, with matperm[n, i, listperm[n,i]] = 1\n    \"\"\"\n    n_objects = listperm.size()[1]\n    eye = torch.eye(n_objects, dtype=torch.int, device=listperm.device)[listperm]\n    # eye= torch.tensor(eye, dtype=torch.int32)\n    return eye",
        "sha1": "4a7d17ca8dafe5752251a56651aeb02a7b0aac6d",
        "id": 420232
    },
    {
        "content": "def is_cyclic(graph, cur_vertex, parent, visited) -> bool:\n    \"\"\"\n    Detect cycle in an undirected graph using DFS.\n    \"\"\"\n    visited.add(cur_vertex)\n\n    for neighbor, connected in enumerate(graph[cur_vertex]):\n        if connected and neighbor != parent:\n            if neighbor in visited:\n                return True\n            else:\n                has_cycle = is_cyclic(graph, neighbor, cur_vertex, visited)\n                if has_cycle:\n                    return True\n\n    return False",
        "sha1": "b08cddf3c7a51c9f0d9552c637696bcaa6ee4402",
        "id": 451471
    },
    {
        "content": "import random\n\n\ndef weighted_choice(population, weights):\n    \"\"\"Randomly choose an element from a population, according to a given\n       probability distribution.\n    \"\"\"\n    weight_sum = sum(weights)\n    choice = random.random()\n    for i, element in enumerate(population):\n        if sum(weights[:i + 1]) / weight_sum >= choice:\n            return element\n    return None",
        "sha1": "a05d1ae4ced4f6c5c7eaafd09593f1bcb590d7ea",
        "id": 526356
    },
    {
        "content": "def adjacent(g, x, y):\n    \"\"\" Returns whether nodes x and y are adjacent \"\"\"\n    return g.has_edge(x, y) or g.has_edge(y, x)",
        "sha1": "c7149fe559c8fa6e533f1003b424147e22fba1c6",
        "id": 456959
    },
    {
        "content": "def create_n_ride_data(size):\n    \"\"\"Create n ride request and return as a list\"\"\"\n    ride_data = []\n    for i in range(size):\n        ride_data.append({\n            \"start_location\": \"Start from here%d\" % i,\n            \"end_location\": \"End here%d\" % i,\n            })\n    return ride_data",
        "sha1": "c2891052385fee2b93190ab3791bb9b87c44ba23",
        "id": 661037
    },
    {
        "content": "def check_attr(attr, attributes):\n    \"\"\"\n    Check if the attr string case-insensitively corresponds to a\n    Gurobi attribute.\n    \"\"\"\n    for a in attributes:\n\n        if attr == a:\n            return True\n        if attr.lower() == a.lower():\n            return True\n\n    return False",
        "sha1": "288e7336b827884757f8cacb011444c43431939b",
        "id": 202635
    },
    {
        "content": "import json\nimport base64\n\n\ndef shell_decode(args):\n    \"\"\"Parse serialized shell-compatible args string\n\n    Parse a string produced by shell_encode().\n    \"\"\"\n\n    args = args.replace('x', '=')\n    argv = json.loads(base64.b32decode(args).decode('ascii'))\n    return [str(arg) for arg in argv]",
        "sha1": "a94030e5133d47f33fbd52eb48764fd1a5d5db21",
        "id": 193703
    },
    {
        "content": "def get_proteinnet_data(d, id):\n    \"\"\"\n    Search for and return the data for the protein with ID in dataset d.\n    \"\"\"\n    for subset_name, subset in zip([\"train\", \"valid\", \"test\"], (d[\"train\"], d[\"valid\"][70], d[\"test\"])):\n        for idx, pn_id in enumerate(subset[\"ids\"]):\n            if id.upper() in pn_id:\n                return subset[\"ang\"][idx], subset[\"ids\"][idx], subset[\"crd\"][idx], subset[\"seq\"][idx]\n    print(f\"Could not find {id}.\")\n    return None, None, None, None",
        "sha1": "a28d5070ce171a1732368fea07b009edcfcfdb06",
        "id": 370675
    },
    {
        "content": "def fill_fields_template(fields, templates):\n    \"\"\"\n        Substitute the name, type and value for all the fields\n    \"\"\"\n\n    final_template = \"\"\n    for field in fields:\n        final_template += (templates[3]\n                           .replace(\"{name}\", field['name'])\n                           .replace(\"{type}\", field['type'])\n                           .replace(\"{value}\", field['value']))\n\n    return final_template",
        "sha1": "aa3aaa570769e722fb7f84f28756ab628e1abda2",
        "id": 217280
    },
    {
        "content": "def short_to_decimal(code):\n    \"\"\"\n    Convert an ICD9 code from short format to decimal format.\n    \"\"\"\n\n    if len(code) <= 3:\n        return code.lstrip(\"0\")\n    else:\n        return code[:3].lstrip(\"0\") + \".\" + code[3:]",
        "sha1": "900ab58dcb59a49b5475d71d08e5720657e03f9e",
        "id": 69002
    },
    {
        "content": "def armstrongNumber(num):\n    \"\"\"assumes num is an int\n    returns a boolean, True if num is an armstrong number, else False.\n    An armstrong number = sum of digits ** number of digits\n    e.g. 371 = 3**3 + 7**3 + 1**3\n    \"\"\"\n    digit_sum = 0\n    for digit in str(num):\n        digit_sum += int(digit) ** 3\n    return num == digit_sum",
        "sha1": "519812a5dd4612c1127225c66be0790e4576eb45",
        "id": 65009
    },
    {
        "content": "def compute_log_likelihoods(estimator,\n                            input_fn,\n                            checkpoint_path=None):\n  \"\"\"Decode from an input_fn.\n\n  Args:\n    estimator: a TPUEstimator\n    input_fn: function that returns a tf.Dataset\n    checkpoint_path: an optional string\n\n  Returns:\n    list of floats\n  \"\"\"\n  result_iter = estimator.predict(\n      input_fn, checkpoint_path=checkpoint_path)\n  return [float(f) for f in result_iter]",
        "sha1": "cdf97af7052c35f13baa121ee8937d42929349ef",
        "id": 520471
    },
    {
        "content": "import zipfile\n\n\ndef _read_zipentry(zip_file, entry):\n    \"\"\"\n    Read a zip entry from a given zip file.\n    \"\"\"\n    with zipfile.ZipFile(zip_file, 'r') as myzip:\n        return myzip.read(entry)",
        "sha1": "78de493ac1b8e662845550850f62bc852cfef527",
        "id": 523793
    },
    {
        "content": "from typing import Sequence\n\n\ndef combination_sums(total: int, terms: Sequence[int]) -> int:\n    \"\"\"Counts the number of combinations that sum to a given value.\n\n    Args:\n        total: A positive integer, representing the target sum.\n        terms: A sequence of positive integer values that can be combined.\n\n    Returns:\n        The number of distinct combinations (with replacement) of values in\n        ``terms`` that sum to give ``total``.\n\n    Raises:\n        ValueError: If ``total`` or any term in ``terms`` is negative or 0.\n    \"\"\"\n\n    if total <= 0:\n        raise ValueError(\"Argument 'total' must be a positive integer\")\n    for term in terms:\n        if term <= 0:\n            raise ValueError(\"Each term in 'terms' must be a positive integer\")\n\n    # initialize the combination array\n    combos = [0] * (total + 1)\n    combos[0] = 1\n\n    # dynamically compute combinations by summing combination dependencies\n    for i, term in enumerate(terms):\n        for j in range(term, total + 1):\n            combos[j] += combos[j - terms[i]]\n\n    return combos[total]",
        "sha1": "a4d4ad659411fab14f41061cb3fd98a045bb092e",
        "id": 571895
    },
    {
        "content": "def player_rank(player):\n    \"\"\"Calculate a rank value for a player\"\"\"\n\n    rank = float(((player['WIN'] * 25.0) + (player['CTF_SCORE'] * 5.0) + (player['SLAYER_SCORE'] / 2.0)) - (player['LOSS'] * 30.0))\n    if rank < 0:\n        rank = 0\n    return rank",
        "sha1": "6be6b54182e75a6468bb9597e3954ad3db503c0b",
        "id": 269064
    },
    {
        "content": "def coleman_liau_index(n_chars, n_words, n_sents):\n    \"\"\"\n    Readability score whose value estimates the number of years of education\n    required to understand a text, similar to :func:`flesch_kincaid_grade_level()`\n    and :func:`smog_index()`, but using characters instead of syllables.\n    Higher value => more difficult text.\n\n    References:\n        https://en.wikipedia.org/wiki/Coleman%E2%80%93Liau_index\"\"\"\n    return (5.879851 * n_chars / n_words) - (29.587280 * n_sents / n_words) - 15.800804",
        "sha1": "991af9c14555beaf4886e667661bb45f0f72017d",
        "id": 574640
    },
    {
        "content": "def reg_n_correct(prediction, y, significance=None):\n    \"\"\"Calculates the number of correct predictions made by a conformal\n    regression model.\n    \"\"\"\n    if significance is not None:\n        idx = int(significance * 100 - 1)\n        prediction = prediction[:, :, idx]\n\n    low = y >= prediction[:, 0]\n    high = y <= prediction[:, 1]\n    correct = low * high\n\n    return y[correct].size",
        "sha1": "190eec0754ef61bf2bac3b93d184233a6e2c3316",
        "id": 39740
    },
    {
        "content": "import random\n\n\ndef generate_random_offset(background_shape: tuple, object_shape: tuple) -> tuple:\n    \"\"\"\n    Generate a safe random offset for the background.\n    :param background_shape: tuple\n    :param object_shape: tuple\n    :return: tuple - offset in x, y\n    \"\"\"\n    b_height, b_width = background_shape\n    o_height, o_width = object_shape\n\n    random_x = random.randrange(0, b_width - o_width, 1)\n    random_y = random.randrange(0, b_height - o_height, 1)\n    return random_x, random_y",
        "sha1": "c3d015f3be7add5ee1a472e8c73f0a32abca898e",
        "id": 16485
    },
    {
        "content": "def distribute_atoms(atoms, n):\n    \"\"\" split a 1D list atoms into n nearly-even-sized chunks.\n    \"\"\"\n    k, m = divmod(len(atoms), n)\n    return [atoms[i*k+min(i,m) : (i+1)*k+min(i+1,m)] for i in range(n)]",
        "sha1": "12a21245f2e1cb412bdb35aadf3c7d130d11107f",
        "id": 692409
    },
    {
        "content": "def moveeffect_000(score, move, user, target, battle) -> int:\n    \"\"\"\n    Move Effect Name: Default\n\n    When a move has no effect, or the id is unknown, this effect will be used.\n    \"\"\"\n\n    return score",
        "sha1": "273230378d9fe6bcbfaaf49258aa5b0d5dcdc88c",
        "id": 505545
    },
    {
        "content": "def sans_ns(tag):\n    \"\"\"Remove the namespace prefix from a tag.\"\"\"\n    return tag.split('}')[-1]",
        "sha1": "06c7b7ff965af75bbff3d7273bfa2f74196f0270",
        "id": 479587
    },
    {
        "content": "def get_user_id(conn, user_name):\n    \"\"\"Get ID of a user based on user name.\n\n    Must be an exact match. Case sensitive.\n\n    Parameters\n    ----------\n    conn : ``omero.gateway.BlitzGateway`` object\n        OMERO connection.\n    user_name : str\n        Name of the user for which an ID is to be returned.\n\n    Returns\n    -------\n    user_id : int\n        ID of the OMERO user. Returns `None` if group cannot be found.\n\n    Examples\n    --------\n    >>> get_user_id(conn, \"jaxl\")\n    35\n    \"\"\"\n    if type(user_name) is not str:\n        raise TypeError('OMERO user name must be a string')\n\n    for u in conn.containedExperimenters(1):\n        if u.getName() == user_name:\n            return u.getId()\n    return None",
        "sha1": "4c29585502477d123ec5a8df315de374ebf9badc",
        "id": 618122
    },
    {
        "content": "def get_mean_param(params):\n    \"\"\"Return the parameter used to show reconstructions or generations.\n    For example, the mean for Normal, or probs for Bernoulli.\n    For Bernoulli, skip first parameter, as that's (scalar) temperature\n    \"\"\"\n    if params[0].dim() == 0:\n        return params[1]\n    elif len(params) == 3:\n        return params[1]\n    else:\n        return params[0]",
        "sha1": "6887b3f867d96f83d70a97074d5799d09b81b1f2",
        "id": 618268
    },
    {
        "content": "def check_dict_of_arrays(doa, columns):\n    \"\"\"\n    Checks the data-structure that dict of arrays has at least the keys in\n    columns and that each entry's length is the same as the others.\n    - doa: (Dict String (Array String)), dictionary with string keys to arrays of string\n    - columns: (Array String), the columns of doa\n    RETURN: (Array String), array of issues\n    \"\"\"\n    issues = []\n    length = None\n    for col in columns:\n        if col not in doa:\n            issues.append(f\"{col} not in doa\")\n            continue\n        if length is None:\n            try:\n                length = len(doa[col])\n            except TypeError:\n                issues.append(f\"could not take length of doa['{col}']\")\n        elif len(doa[col]) != length:\n            issues.append(\n                    f\"len(doa['{col}']) = {len(doa[col])} != {length},\" +\n                    \" the length of other columns\")\n    return issues",
        "sha1": "d070482e8d12f02f833e21ff8adcf1bac28b69d8",
        "id": 388897
    },
    {
        "content": "def get_last_id(list_of_id, width):\n    \"\"\" Gets the last identifier given a list of identifier.\n\n    :param list_of_id: list of identifier\n    :param width: the width of the identifier.\n    :return: the last identifier.\n\n    \"\"\"\n    last_number = 0\n    for identifier in list_of_id:\n        if identifier == \"\":\n            last_number = 0\n        else:\n            last_number = max(last_number, int(identifier.lstrip('0')))\n    last = (width - len(str(last_number))) * \"0\" + str(last_number)\n    return last",
        "sha1": "22bd3c7cc4fd5aaae1005d7317bbe29f407091b4",
        "id": 684899
    },
    {
        "content": "import imghdr\n\n\ndef validate_image_header(filename: str):\n    \"\"\"Validate that an image has a valid header.\n\n    Returns True if valid, False if invalid.\n\n    :param filename: name of file to analyze\n    :type filename: str\n    :return: flag indicating whether header is valid (by using `imghdr.what()`)\n    :rtype: bool\n    \"\"\"\n\n    status = imghdr.what(filename)\n    if status is not None:\n        return True\n    else:\n        return False",
        "sha1": "56f3c20c1f4bf8a50ff1961e3c95d6e70f2f835e",
        "id": 269937
    },
    {
        "content": "from typing import Iterable\n\n\ndef flatten(nested):\n    \"\"\"Flatten a nested sequence where the sub-items can be sequences or\n    primitives. This differs slightly from itertools chain methods because\n    those require all sub-items to be sequences. Here, items can be primitives,\n    sequences, nested sequences, or any combination of these. Any iterable\n    items aside from strings will be completely un-nested, so use with caution\n    (e.g. a torch Dataset would be unpacked into separate items for each\n    index). This also returns a list rather than a generator.\n\n    Parameters\n    ----------\n    nested: sequence (list, tuple, set)\n        Sequence where some or all of the items are also sequences.\n\n    Returns\n    -------\n    list: Flattened version of `nested`.\n    \"\"\"\n    def _walk(nested):\n        for group in nested:\n            if isinstance(group, Iterable) and not isinstance(group, str):\n                yield from _walk(group)\n            else:\n                yield group\n    return list(_walk(nested))",
        "sha1": "464cd221dfaf6f842bf6da0d96ad8322d1c84e71",
        "id": 13415
    },
    {
        "content": "def get_filenames_of_set(train_set):\n    \"\"\"\n    This function reads the names of the files that we are going to use as our training test\n    :param train_set - the file which contains the names of the training set\n    :return: content - an array containing the names of each filew\n    \"\"\"\n    # read the names of the files that we are going to use as our training test\n    with open(train_set) as f:\n        content = f.readlines()\n    content = [x.strip() for x in content]\n    return content",
        "sha1": "0da5e7321606b1a863b081cace61ecb5f92cd83f",
        "id": 37039
    },
    {
        "content": "def as_number(as_number_val: str) -> int:\n    \"\"\"Convert AS Number to standardized asplain notation as an integer.\"\"\"\n    as_number_str = str(as_number_val)\n    if \".\" in as_number_str:\n        big, little = as_number_str.split(\".\")\n        return (int(big) << 16) + int(little)\n    else:\n        return int(as_number_str)",
        "sha1": "083171f89b06259e849e49cf0b89b0459bb75ac9",
        "id": 616632
    },
    {
        "content": "def convert_to_text(batch, lengths, dico, lang_id, params):\n    \"\"\"\n    Convert a batch of sentences to a list of text sentences.\n    \"\"\"\n    batch = batch.cpu().numpy()\n    lengths = lengths.cpu().numpy()\n    bos_index = params.bos_index[lang_id]\n\n    slen, bs = batch.shape\n    assert lengths.max() == slen and lengths.shape[0] == bs\n    assert (batch[0] == bos_index).sum() == bs\n    assert (batch == params.eos_index).sum() == bs\n    sentences = []\n\n    for j in range(bs):\n        words = []\n        for k in range(1, lengths[j]):\n            if batch[k, j] == params.eos_index:\n                break\n            words.append(dico[batch[k, j]])\n        sentences.append(\" \".join(words))\n    return sentences",
        "sha1": "46b505af1d73c080dce3a0ab2712f27a175a498b",
        "id": 588923
    },
    {
        "content": "import click\n\n\ndef click_prompt_custom_var(var_name, default):\n    \"\"\"\n    Click prompt to ask the user for the value of a given variable.\n\n    http://click.pocoo.org/4/api/#click.prompt\n\n    :param str var_name: Variable that gets assigned by the user.\n    :param default: Default value which will be returned if the user doesn't input anything.\n    :returns: The given value or the default.\n    \"\"\"\n    return click.prompt(var_name, default=default)",
        "sha1": "28aac4c7a7b51ccd3c0635f3956dafd13c76f8db",
        "id": 358489
    },
    {
        "content": "def convert_to_num(version_str):\n    \"\"\"Convert the version string to a number\n\n    >>> convert_to_num(None)\n    0\n\n    >>> convert_to_num(10)\n    10\n\n    >>> convert_to_num('1.0.0')\n    100\n\n    >>> convert_to_num('1.0.0-alpha')\n    100\n    \"\"\"\n    if not version_str:\n        return 0\n    if str(version_str).isdigit():\n        return version_str\n    version_str = version_str.replace(\".\", \"\")\n    if \"-\" in version_str:\n        version_str = version_str.split(\"-\")[0]\n    return int(version_str) if version_str.isdigit() else 0",
        "sha1": "d069399e8650ad8c9fbcf443aa110fad801fd09a",
        "id": 548948
    },
    {
        "content": "def to_pandas(modin_obj):\n    \"\"\"\n    Convert a Modin DataFrame/Series to a pandas DataFrame/Series.\n\n    Parameters\n    ----------\n    modin_obj : modin.DataFrame, modin.Series\n        The Modin DataFrame/Series to convert.\n\n    Returns\n    -------\n    pandas.DataFrame or pandas.Series\n        Converted object with type depending on input.\n    \"\"\"\n    return modin_obj._to_pandas()",
        "sha1": "a5f0bfe3f0e5129561a04b42db01be76ae9855b4",
        "id": 412500
    },
    {
        "content": "def get_vacant_binding_index(num_agents, bindings, lowest_binding_index,\n                             force_scheduling=False):\n    \"\"\"Return a vacant binding_index to use and whether or not it exists.\n\n    This method can be used with DHCP and L3 agent schedulers. It will return\n    the lowest vacant index for one of those agents.\n    :param num_agents: (int) number of agents (DHCP, L3) already scheduled\n    :param bindings: (NetworkDhcpAgentBinding, RouterL3AgentBinding) agent\n                     binding object, must have \"binding_index\" field.\n    :param lowest_binding_index: (int) lowest index number to be scheduled.\n    :param force_scheduling: (optional)(boolean) if enabled, the method will\n                             always return an index, even if this number\n                             exceeds the maximum configured number of agents.\n    \"\"\"\n    binding_indices = [b.binding_index for b in bindings]\n    all_indices = set(range(lowest_binding_index, num_agents + 1))\n    open_slots = sorted(list(all_indices - set(binding_indices)))\n\n    if open_slots:\n        return open_slots[0]\n\n    if not force_scheduling:\n        return -1\n\n    # Last chance: if this is a manual scheduling, we're gonna allow\n    # creation of a binding_index even if it will exceed\n    # dhcp_agents_per_network.\n    if max(binding_indices) == len(binding_indices):\n        return max(binding_indices) + 1\n    else:\n        # Find binding index set gaps and return first free one.\n        all_indices = set(range(lowest_binding_index,\n                                max(binding_indices) + 1))\n        open_slots = sorted(list(all_indices - set(binding_indices)))\n        return open_slots[0]",
        "sha1": "a3842da804ffb2f53d3089e5c4696fcec911aaa6",
        "id": 75063
    },
    {
        "content": "import re\n\n\ndef _quote(match: re.Match) -> str:\n    \"\"\"Add quotes to '=attribute'\"\"\"\n    attr, end = match.groups()\n    return f'=\"{attr}\"{end}'",
        "sha1": "8fbc79984d453338fc57069c4653b2ce57512203",
        "id": 426406
    },
    {
        "content": "def is_df(\n        df):\n    \"\"\"is_df\n\n    Test if ``df`` is a valid ``pandas.DataFrame``\n\n    :param df: ``pandas.DataFrame``\n    \"\"\"\n    return (\n        hasattr(df, 'to_json'))",
        "sha1": "fe5c111e8883ff64e3b63602e57aaa793ef710fa",
        "id": 42721
    },
    {
        "content": "def initial_positions(walls):\n    \"\"\"Calculate initial positions.\n\n    Given the list of walls, returns the free positions that are closest to the\n    bottom left and top right corner. The algorithm starts searching from\n    (1, height-2) and (width-2, 1) respectively and uses the Manhattan distance\n    for judging what is closest. On equal distances, a smaller distance in the\n    x value is preferred.\n    \"\"\"\n    width = max(walls)[0] + 1\n    height = max(walls)[1] + 1\n\n    left_start = (1, height - 2)\n    left = []\n    right_start = (width - 2, 1)\n    right = []\n\n    dist = 0\n    while len(left) < 2:\n        # iterate through all possible x distances (inclusive)\n        for x_dist in range(dist + 1):\n            y_dist = dist - x_dist\n            pos = (left_start[0] + x_dist, left_start[1] - y_dist)\n            # if both coordinates are out of bounds, we stop\n            if not (0 <= pos[0] < width) and not (0 <= pos[1] < height):\n                raise ValueError(\"Not enough free initial positions.\")\n            # if one coordinate is out of bounds, we just continue\n            if not (0 <= pos[0] < width) or not (0 <= pos[1] < height):\n                continue\n            # check if the new value is free\n            if pos not in walls:\n                left.append(pos)\n\n            if len(left) == 2:\n                break\n\n        dist += 1\n\n    dist = 0\n    while len(right) < 2:\n        # iterate through all possible x distances (inclusive)\n        for x_dist in range(dist + 1):\n            y_dist = dist - x_dist\n            pos = (right_start[0] - x_dist, right_start[1] + y_dist)\n            # if both coordinates are out of bounds, we stop\n            if not (0 <= pos[0] < width) and not (0 <= pos[1] < height):\n                raise ValueError(\"Not enough free initial positions.\")\n            # if one coordinate is out of bounds, we just continue\n            if not (0 <= pos[0] < width) or not (0 <= pos[1] < height):\n                continue\n            # check if the new value is free\n            if pos not in walls:\n                right.append(pos)\n\n            if len(right) == 2:\n                break\n\n        dist += 1\n\n    # lower indices start further away\n    left.reverse()\n    right.reverse()\n    return [left[0], right[0], left[1], right[1]]",
        "sha1": "ce673714a434891701fd29601027e7b42b8920f3",
        "id": 407214
    },
    {
        "content": "from typing import Tuple\nimport torch\n\n\ndef _sobel_kernel(spatial_size: int) -> Tuple[torch.Tensor, ...]:\n    \"\"\"\n        Constructs the sobel kernel for 2D or 3D kernel sizes.\n\n        Args:\n            spatial_size (int): The spatial size of the sobel kernel.\n\n        Raises:\n            ValueError when spatial_size is not 2 or 3.\n    \"\"\"\n    if spatial_size == 2:\n        g_x = (\n            torch.Tensor([[+1, 0, -1], [+2, 0, -2], [+1, 0, -1]])\n            .unsqueeze(0)\n            .unsqueeze(0)\n        )\n\n        g_y = (\n            torch.Tensor([[+1, +2, +1], [0, 0, 0], [-1, -2, 1]])\n            .unsqueeze(0)\n            .unsqueeze(0)\n        )\n\n        return g_x, g_y\n    elif spatial_size == 3:\n        g_x = (\n            torch.Tensor(\n                [\n                    [[1, 0, -1], [1, 0, -1], [1, 0, -1]],\n                    [[1, 0, -1], [2, 0, -2], [1, 0, -1]],\n                    [[1, 0, -1], [1, 0, -1], [1, 0, -1]],\n                ]\n            )\n            .unsqueeze(0)\n            .unsqueeze(0)\n        )\n\n        g_y = (\n            torch.Tensor(\n                [\n                    [[1, 1, 1], [0, 0, 0], [-1, -1, -1]],\n                    [[1, 2, 1], [0, 0, 0], [-1, -2, -1]],\n                    [[1, 1, 1], [0, 0, 0], [-1, -1, -1]],\n                ]\n            )\n            .unsqueeze(0)\n            .unsqueeze(0)\n        )\n\n        g_z = (\n            torch.Tensor(\n                [\n                    [[1, 1, 1], [1, 2, 1], [1, 1, 1]],\n                    [[0, 0, 0], [0, 0, 0], [0, 0, 0]],\n                    [[-1, -1, -1], [-1, -2, -1], [-1, -1, -1]],\n                ]\n            )\n            .unsqueeze(0)\n            .unsqueeze(0)\n        )\n\n        return g_x, g_y, g_z\n    else:\n        raise ValueError(\n            f\"Sobel operator is defined only for spatial sizes 2 and 3, but given {spatial_size}.\"\n        )",
        "sha1": "e476d155f4b2fa10d23404be2995439202df8de4",
        "id": 185528
    },
    {
        "content": "def check_related_lot_status(tender, award):\n    \"\"\"Check if related lot not in status cancelled\"\"\"\n    lot_id = award.get('lotID')\n    if lot_id:\n        if [l['status'] for l in tender.get('lots', []) if l['id'] == lot_id][0] != 'active':\n            return False\n    return True",
        "sha1": "6b4aeb76ee938de434f646913a196162e903df8e",
        "id": 670445
    },
    {
        "content": "def emu_to_amps_m2(emu):\n    \"\"\"Converts \\(emu\\) to \\(Am^{2}\\)\"\"\"\n    return emu*1e-3",
        "sha1": "20c81c0e7da2212f2a5f331d03ed44bd91c97ab3",
        "id": 176946
    },
    {
        "content": "import torch\nimport warnings\n\n\ndef whiten(obs, check_finite=True):\n    \"\"\"\n    Normalize a group of observations on a per feature basis.\n\n    Before running k-means, it is beneficial to rescale each feature\n    dimension of the observation set with whitening. Each feature is\n    divided by its standard deviation across all observations to give\n    it unit variance.\n\n    Parameters\n    ----------\n    obs : ndarray N x M x D\n        Each row of the array is an observation.  The\n        columns are the features seen during each observation.\n    check_finite : bool, optional\n        Whether to check that the input matrices contain only finite numbers.\n        Disabling may give a performance gain, but may result in problems\n        (crashes, non-termination) if the inputs do contain infinities or NaNs.\n        Default: True\n\n    Returns\n    -------\n    result : ndarray\n        Contains the values in `obs` scaled by the standard deviation of each column.\n    \"\"\"\n    std_dev = torch.std(obs, dim=1, keepdim=True) # (N, 1, D)\n    zero_std_mask = std_dev == 0\n    if zero_std_mask.any():\n        std_dev[zero_std_mask] = 1.0\n        warnings.warn(\"Some columns have standard deviation zero. \"\n                      \"The values of these columns will not change.\",\n                      RuntimeWarning)\n    return obs / std_dev",
        "sha1": "31842b7a2446da1eabb6a4bc6acc7625bb466f95",
        "id": 59345
    },
    {
        "content": "def get_columns_to_compare(test_folder):\n    \"\"\"\n    Helper function to test the files in SampleData. Gets\n    indices of angles that can be compared for equality, depending\n    on which file is being compared.\n\n    There are 57 angle coordinates to be compared, with 3 coordinates\n    for each of 19 angles.\n\n    If the global coordinate system is unknown for a given file,\n    angles affected by the GCS are ignored.\n    Ignored angles are Pelvis, R Foot, L Foot, Head, Thorax, with corresponding\n    indices 0, 1, 2 and 21 - 32.\n\n    The files in Test_Files also ignore the Neck X coordinate, at \n    index 33.\n    \"\"\"\n    gcs_ignore = [i for i in range(21, 33)]\n    gcs_ignore.extend([0,1,2])\n    columns = [i for i in range(57)]\n    if (test_folder == 'ROM'):\n        return columns\n    if (test_folder == '59993_Frame'):\n        for i in gcs_ignore:\n            columns.remove(i)\n        return columns\n    if (test_folder == 'Test_Files'):\n        for i in gcs_ignore:\n            columns.remove(i)\n        columns.remove(33)\n        return columns",
        "sha1": "0b23cbaf00d844cbeef92fc3c0d461a26600ed5a",
        "id": 476643
    },
    {
        "content": "def leave_only_keys(*keys):\n    \"\"\"\n    A function to leave only the specified keys in the dictionary.\n    \"\"\"\n\n    def filtered(dict_):\n        \"\"\"\n        Filter a dictionary by keys.\n        \"\"\"\n        return {\n            key: dict_[key]\n            for key in keys\n            if key in dict_\n        }\n\n    return filtered",
        "sha1": "6f15b12c6e1c655a430b9d32ecee74f08144bcfd",
        "id": 588314
    },
    {
        "content": "import torch\n\n\ndef single_forward(model, inp):\n    \"\"\"PyTorch model forward (single test), it is just a simple warpper\n    Args:\n        model (PyTorch model)\n        inp (Tensor): inputs defined by the model\n    Returns:\n        output (Tensor): outputs of the model. float, in CPU\n    \"\"\"\n    with torch.no_grad():\n        model_output = model(inp)\n        if isinstance(model_output, list) or isinstance(model_output, tuple):\n            output = model_output[0]\n        else:\n            output = model_output\n    output = output.data.float().cpu()\n    return output",
        "sha1": "567c64f6d122b788e3419a5343bd51dab74ee62d",
        "id": 410797
    },
    {
        "content": "def remove_resource(admin_mc, request):\n    \"\"\"Remove a resource after a test finishes even if the test fails.\"\"\"\n    client = admin_mc.client\n\n    def _cleanup(resource):\n        request.addfinalizer(lambda: client.delete(resource))\n\n    return _cleanup",
        "sha1": "6fd635e0906bbad96094cdba3d4cdb2dc4b97a2c",
        "id": 646739
    },
    {
        "content": "def repr_tags(taglist, max_bytes=10):\n    \"\"\"Returns a printable representation of a :class:`Gst.TagList`.\n\n    Tag values of type bytes are truncated to the specified length to avoid\n    large amounts of output when logging.\n\n    :param taglist: A GStreamer taglist to be represented.\n    :type taglist: :class:`Gst.TagList`\n    :param max_bytes: The maximum number of bytes to show for bytes tag values.\n    :type max_bytes: int\n    :rtype: string\n    \"\"\"\n    result = dict(taglist)\n    for tag_values in result.values():\n        for i, val in enumerate(tag_values):\n            if type(val) is bytes and len(val) > max_bytes:\n                tag_values[i] = val[:max_bytes] + b\"...\"\n    return repr(result)",
        "sha1": "ec23cfc705c062246a629b0ecc7dd7d3b135b711",
        "id": 377522
    },
    {
        "content": "def bytify(n=0, size=1, reverse=False, strict=False):\n    \"\"\"\n    Returns bytearray of at least size bytes equivalent of integer n that is\n    left zero padded to size bytes. For n positive, if the bytearray\n    equivalent of n is longer than size  and strict is False the bytearray\n    is extended to the length needed to fully represent n. Otherwise if strict\n    is True or n is negative it is truncated to the least significant size bytes.\n\n    Big endian is the default.\n    If reverse is true then it reverses the order of the bytes in the resultant\n    bytearray for little endian with right zero padding.\n    \"\"\"\n    if n < 0 or strict:\n        n =  n & (2 ** (size * 8) - 1)\n    b = bytearray()\n    count = 0\n    while n:\n        b.insert(0, n & 0xFF)\n        count += 1\n        n >>=  8\n    if (count < size):\n        b = bytearray([0]*(size-count)) + b\n    if reverse:\n        b.reverse()\n    return b",
        "sha1": "7a7db59f20029ef4e0d00c766175680244a5665c",
        "id": 132206
    },
    {
        "content": "import torch\n\n\ndef _cross_squared_distance_matrix(x:torch.Tensor, y:torch.Tensor):\n    \"\"\"Pairwise squared distance between two (batch) matrices' rows (2nd dim).\n\n    Computes the pairwise distances between rows of x and rows of y.\n\n    Args:\n        x: torch.Tensor [shape=(b, n, d), float]\n        y: torch.Tensor [shape=(b, m, d), float]\n    Returns:\n        squared_dists: torch.Tensor [shape=(b, n, m), float]\n            `squared_dists[b,i,j] = ||x[b,i,:] - y[b,j,:]||^2`.\n    \"\"\"\n    x_norm_squared = torch.sum(x**2, dim=2).unsqueeze(2)  # shape=(b, n, 1)\n    y_norm_squared = torch.sum(y**2, dim=2).unsqueeze(1)  # shape=(b, 1, m)\n    x_y_transpose = torch.einsum('bnd, bmd -> bnm', x, y)  # shape=(b, n, m)\n    # squared_dists[b,i,j] = ||x_bi - y_bj||^2 = x_bi'x_bi- 2x_bi'x_bj + x_bj'x_bj\n    squared_dists = x_norm_squared - 2*x_y_transpose + y_norm_squared\n    return squared_dists.float()",
        "sha1": "7201df6c8abf6f7a4021675d2b9b1ebd2604fc19",
        "id": 160941
    },
    {
        "content": "def grazing(vs, eaten, zooplankton):\n    \"\"\"Zooplankton grows by amount digested, eaten decreases by amount grazed\"\"\"\n    return {eaten: - vs.grazing[eaten], zooplankton: vs.grazing[eaten]}",
        "sha1": "9e955c3cc16e00f8fffd9ef210093ecd82ac11df",
        "id": 649138
    },
    {
        "content": "def _namespace_data(data: dict, namespace: str) -> dict:\n    \"\"\"Prefix configuration key with a namespace.\n\n    Namespace is ConfigSource wide, and is resolved at source build time.\n\n    It should be one of:\n    - \"target\"\n    - \"app\"\n    - library name (where \"mbed_lib.json\" comes from)\n\n    If given key is already namespaced, return it as is - this is going to be the case for\n    keys from \"target_overrides\" entries. Keys from \"config\" usually need namespacing.\n    \"\"\"\n    namespaced = {}\n    for key, value in data.items():\n        if \".\" not in key:\n            key = f\"{namespace}.{key}\"\n        namespaced[key] = value\n    return namespaced",
        "sha1": "caf52e230465cb4f3d02b21187507d5fe7c123f2",
        "id": 637217
    },
    {
        "content": "def landsat_clean_mask_invalid(dataset):\n    \"\"\"\n    Masks out invalid data according to the LANDSAT\n    surface reflectance specifications. See this document:\n    https://landsat.usgs.gov/sites/default/files/documents/ledaps_product_guide.pdf pages 19-20.\n\n    Parameters\n    ----------\n    dataset: xarray.Dataset\n        An `xarray.Dataset` containing bands such as 'red', 'green', or 'blue'.\n\n    Returns\n    -------\n    invalid_mask: xarray.DataArray\n        An `xarray.DataArray` with the same number and order of coordinates as in `dataset`.\n        The `True` values specify what pixels are valid.\n    \"\"\"\n    invalid_mask = None\n    data_arr_names = [arr_name for arr_name in list(dataset.data_vars)\n                      if arr_name not in ['pixel_qa', 'radsat_qa', 'cloud_qa']]\n    # Only keep data where all bands are in the valid range.\n    for i, data_arr_name in enumerate(data_arr_names):\n        invalid_mask_arr = (0 < dataset[data_arr_name]) & (dataset[data_arr_name] < 10000)\n        invalid_mask = invalid_mask_arr if i == 0 else (invalid_mask & invalid_mask_arr)\n    return invalid_mask",
        "sha1": "8d32beeb3cc9addc400d90ceb61207f9e259954e",
        "id": 444095
    },
    {
        "content": "def weight_name_to_layer_name(weight_name):\n    \"\"\" Convert the name of weights to the layer name \"\"\"\n    tokens = weight_name.split('_')\n    type_name = tokens[-1]\n\n    # modern naming convention\n    if type_name == 'weights' or type_name == 'bias':\n        if len(tokens) >= 3 and tokens[-3] == 'input':\n            return weight_name[:weight_name.rfind('input')-1]            \n        return weight_name[:weight_name.rfind(type_name)-1]\n    # legacy\n    if type_name == 'im':\n        return weight_name[:-4]\n    if type_name == 'pose':\n        return weight_name[:-6]\n    return weight_name[:-1]",
        "sha1": "f6f80aeb392d3913eaa907a43d4064d8bd72db3d",
        "id": 333593
    },
    {
        "content": "def get_device_str(device_id, num_gpus):\n    \"\"\"Return a device string for multi-GPU setup.\"\"\"\n    if num_gpus == 0:\n        return \"/cpu:0\"\n    device_str_output = \"/gpu:%d\" % (device_id % num_gpus)\n    return device_str_output",
        "sha1": "dec2fd1b61916020c2b118714fe4997d00f753a3",
        "id": 83417
    },
    {
        "content": "import re\n\n\ndef is_image_name_id(name):\n    \"\"\"Check whether the given image name is in fact an image ID (hash).\"\"\"\n    if re.match('^sha256:[0-9a-fA-F]{64}$', name):\n        return True\n    return False",
        "sha1": "4871dfda1aabbdbbf17f84b59079ed664ff33598",
        "id": 429417
    },
    {
        "content": "from pathlib import Path\nfrom typing import List\n\n\ndef list_joltage_steps(file: Path) -> List[int]:\n    \"\"\"\n    List joltages from the given file\n\n    :param file: file containing the input values\n    :return: list of joltage steps\n    \"\"\"\n\n    adapter_output_joltages = sorted([int(v) for v in open(file)])\n    device_input_joltage = 3 + max(adapter_output_joltages)\n    joltages = [0] + adapter_output_joltages + [device_input_joltage]\n    pairs = list(zip(joltages[:-1], joltages[1:]))\n    joltage_steps = [p[1] - p[0] for p in pairs]\n\n    return joltage_steps",
        "sha1": "b88c13dbb26f8228b38830d3a3ca408fbd54bc3f",
        "id": 109563
    },
    {
        "content": "import uuid\n\n\ndef validate_uuid4(uuid_string: str) -> bool:\n    \"\"\"\n    Determine if the uuid supplied is valid.\n    :param uuid_string: the uuid which needs to to checked\n    :return: bool: if the uuid is valid or not\n    \"\"\"\n    try:\n        val = uuid.UUID(uuid_string, version=4)\n    except ValueError:\n        return False\n    return str(val) == uuid_string",
        "sha1": "8ceee1b3bc284a91f50a1ef95eb011adea0b8532",
        "id": 337344
    },
    {
        "content": "def time_order(time1, time2):\n    \"\"\"\n    Make sure that the times are the correct way round\n    where start time is before end time.\n\n    :param time1: First time input\n    :param time2: Second time input\n\n    :return: start_time, end_time\n    \"\"\"\n    start_time = time1 if time1 < time2 else time2\n    end_time = time2 if time2 > time1 else time1\n\n    return start_time, end_time",
        "sha1": "314d0d7187222744d3eb67521f5991b1c488325c",
        "id": 340879
    },
    {
        "content": "import hashlib\n\n\ndef md5(v1, *values):\n    \"\"\"Create a hash over a sequence of values.\"\"\"\n\n    result = hashlib.md5(v1)\n    for value in values:\n        result.update(value)\n    return result",
        "sha1": "e3b5da0f813445e45adee162a66f3a5a819971e0",
        "id": 392987
    },
    {
        "content": "from datetime import datetime\n\n\ndef _date_to_str(date: datetime) -> str:\n    \"\"\"\n    Internal utility that stores the correct text format for dates.\n\n    :param date: Date to be converted.\n    :return: Date as string with the format yyyy-mm-dd.\n    \"\"\"\n    return date.strftime('%Y-%m-%d')",
        "sha1": "420b64c76e502d29b4921182ad3c7299ba0623ec",
        "id": 522183
    },
    {
        "content": "def _hex_to_int(hexstring: str) -> int:\n    \"\"\"Converts a hex string representation of an integer to an integer.\n    \"\"\"\n    return int(hexstring, 16)",
        "sha1": "9a165244be0b6c05f72c01243785b643baa76a81",
        "id": 174943
    },
    {
        "content": "def get_query_params(query):\n    \"\"\"\n    Extract (key, value) pairs from the given GET / POST query. Pairs\n    can be split by '&' or ';'.\n    \"\"\"\n    params = {}\n    if query:\n        delim = \"&\"\n        if \"&\" not in query and \";\" in query:\n            delim = \";\"\n        for k_v in query.split(delim):\n            k, v = k_v, \"\"\n            if \"=\" in k_v:\n                k, v = k_v.split(\"=\")\n            params[k] = v\n    return params",
        "sha1": "2078b77b217918dbfb071599acb2059151a5f74a",
        "id": 383528
    },
    {
        "content": "from typing import List\n\n\ndef sum_cpu_resources_unformatted(cpu_resources: List[str]):\n    \"\"\" Sum cpu resources given in k8s format and return the sum in the same format. \"\"\"\n    cpu_sum = 0\n    for cpu_resource in cpu_resources:\n        if not cpu_resource:\n            continue\n        # If CPU resources are gives as for example 100m, we simply strip last character and sum leftover numbers.\n        elif cpu_resource[-1] == \"m\":\n            cpu_sum += int(cpu_resource[:-1])\n        # Else we assume that cpu resources are given as float value of normal CPUs instead of miliCPUs.\n        else:\n            cpu_sum += int(float(cpu_resource) * 1000)\n\n    return cpu_sum",
        "sha1": "a0baaf8d79f4a4cca240003b84e77d12f083d985",
        "id": 485080
    },
    {
        "content": "def stress_positions(stress: str, scansion: str) -> list:\n    \"\"\"Given a stress value and a scansion line, return the index positions of the stresses.\n    >>> stress_positions(\"-\", \"    -  U   U - UU    - U U\")\n    [0, 3, 6]\n    \"\"\"\n    line = scansion.replace(\" \", \"\")\n    stresses = []\n    for idx, char in enumerate(line):\n        if char == stress:\n            stresses.append(idx)\n    return stresses",
        "sha1": "4bec806ae52c78f8d41df8537c4a5bfc29b47587",
        "id": 468398
    },
    {
        "content": "def is_empty_tile(tile):\n    \"\"\"Return true for an empty tile. An empty tile is completely transparent.\"\"\"\n    alpha = sum([i for i in tile.tostring('raw', 'A')])\n    if alpha == 0:\n        return True\n    return False",
        "sha1": "b24cc5d0a05714c7ccbf43dc517b4b4ebc6bca3b",
        "id": 342908
    },
    {
        "content": "def str_or_none(value):\n    \"\"\"Returns string casted value if given value is not None\"\"\"\n    return str(value) if value is not None else value",
        "sha1": "f92c0399c62ce2355a09c5d346e4053d669c6cae",
        "id": 631575
    },
    {
        "content": "def extract_measured_states(record, measured_list):\n    \"\"\"\n    Given the following input:\n        record: a list of all final states where the index of the list corresponds to the index in the quantum board\n        measured_list: a list of index for qubits that have been measured by the player\n    Return a dictionary of that maps the position that has been measured to the correspond state from record (This will be used to update the classic board)\n    \"\"\"\n    result = {}\n    for index in measured_list:\n        result[str(index)] = record[index]\n    return result",
        "sha1": "5b728579ee98e258dbdefdc3267f4d9dfccc793e",
        "id": 344479
    },
    {
        "content": "def is_numeric(value):\n    \"\"\"\n    check whether the given value is a numeric value. Numbers with decimal point or thousand separator can be properly determined.\n    :param value: the given value to be verified\n    :return: True if the given value is numeric\n    \"\"\"\n    value_str = str(value).replace(',', '')\n    try:\n        float(value_str)\n        return True\n    except:\n        return False",
        "sha1": "76b5ef8cafe5ae6484a97d63c6afdcb27fbea8b9",
        "id": 293745
    },
    {
        "content": "def send_request(service, rpc, msg):\n    \"\"\"\n    Send a CBSDRequest gRPC message to gRPC endpoint\n\n    Parameters:\n        service: DBService\n        rpc: the gRPC method of the service\n        msg: CBSDRequest message\n\n    Returns:\n        CBSDStateResult\n    \"\"\"\n    resp = rpc(msg)\n    return resp",
        "sha1": "9ec14e9f32e6a757290082050031eeee062390f9",
        "id": 254916
    },
    {
        "content": "def get_path_array(node):\n    \"\"\"\n    Takes an end node and gives you every node (in order) for the shortest path to it.\n\n    PARAMS:\n        node (node): end node\n\n    RETURNS:\n        array[nodes]: every note you need to visit (in order)\n    \"\"\"\n    if node.shortest_path_via == None:\n        return [node]\n    else:\n        return get_path_array(node.shortest_path_via) + [node]",
        "sha1": "0633fc9bb7e043e57a8356a23807495c7c975d14",
        "id": 679067
    },
    {
        "content": "def fn2(x):\n    \"\"\"A linear function for testing the dimension reduction\"\"\"\n    return 10*(x[0] + x[1]) + (x[1] - x[0]) + x[2] + 0.1*x[3]",
        "sha1": "1da68c227e0fd0215aecb2729253f075c57b19a2",
        "id": 461252
    },
    {
        "content": "def _build_task_dependency(tasks):\n    \"\"\"\n    Fill the task list with all the needed modules.\n\n    Parameters\n    ----------\n    tasks : list\n        list of strings, containing initially only the last module required.\n        For instance, to recover all the modules, the input should be ``['fourier']``.\n\n    Returns\n    -------\n    tasks : list\n        Complete task list.\n    \"\"\"\n    if not isinstance(tasks, (tuple, list)):\n        tasks = [tasks]\n    tasks = set(tasks)\n    if 'thermodynamics' in tasks:\n        tasks.discard('background')\n    # if 'lensing' in tasks:\n    #    tasks.add('harmonic')\n    if 'harmonic' in tasks:\n        tasks.add('fourier')\n    if 'fourier' in tasks:\n        tasks.add('transfer')\n    return list(tasks)",
        "sha1": "6da74f01badfc395c3be7daf8ae30df9e2837c63",
        "id": 76300
    },
    {
        "content": "def ensure_support_staging_jobs_have_correct_keys(\n    support_and_staging_matrix_jobs: list, prod_hub_matrix_jobs: list\n) -> list:\n    \"\"\"This function ensures that all entries in support_and_staging_matrix_jobs have\n    the expected upgrade_staging and eason_for_staging_redeploy keys, even if they are\n    set to false/empty.\n\n    Args:\n        support_and_staging_matrix_jobs (list[dict]): A list of dictionaries\n            representing jobs to upgrade the support chart and staging hub on clusters\n            that require it.\n        prod_hub_matrix_jobs (list[dict]): A list of dictionaries representing jobs to\n            upgrade production hubs that require it.\n\n    Returns:\n        support_and_staging_matrix_jobs (list[dict]): Updated to ensure each entry has\n            the upgrade_staging and reason_for_staging_redeploy keys, even if they are\n            false/empty.\n    \"\"\"\n    # For each job listed in support_and_staging_matrix_jobs, ensure it has the\n    # upgrade_staging key present, even if we just set it to False\n    for job in support_and_staging_matrix_jobs:\n        if \"upgrade_staging\" not in job.keys():\n            # Get a list of prod hubs running on the same cluster this staging job will\n            # run on\n            hubs_on_this_cluster = [\n                hub[\"hub_name\"]\n                for hub in prod_hub_matrix_jobs\n                if hub[\"cluster_name\"] == job[\"cluster_name\"]\n            ]\n            if hubs_on_this_cluster:\n                # There are prod hubs on this cluster that require an upgrade, and so we\n                # also upgrade staging\n                job[\"upgrade_staging\"] = \"true\"\n                job[\n                    \"reason_for_staging_redeploy\"\n                ] = \"Following prod hubs require redeploy: \" + \", \".join(\n                    hubs_on_this_cluster\n                )\n            else:\n                # There are no prod hubs on this cluster that require an upgrade, so we\n                # do not upgrade staging\n                job[\"upgrade_staging\"] = \"false\"\n                job[\"reason_for_staging_redeploy\"] = \"\"\n\n    return support_and_staging_matrix_jobs",
        "sha1": "3852500882732ac0f92dfc5d526c0fe23d9a9388",
        "id": 676695
    },
    {
        "content": "from typing import List\n\n\ndef get_label_list(labels: List[List[int]]) -> List[int]:\n    \"\"\"Gets a sorted list of all the unique labels from `labels`.\n\n    Args:\n        labels: A list of lists, each corresponding to the label-sequence of a text.\n\n    Returns:\n        All the unique labels the ever appear in `labels`, given in a sorted list.\n\n    Example:\n        Given `labels=[[0, 0, 3, 2, 5], [4, 0], [5, 2, 3]]`, returns `[0, 2, 3, 4, 5]`.\n    \"\"\"\n    unique_labels = set()\n    for label in labels:\n        unique_labels = unique_labels | set(label)\n    label_list = list(unique_labels)\n    label_list.sort()\n    return label_list",
        "sha1": "be795ff63c1eaccd221289708551a8ddd02b2cc5",
        "id": 14088
    },
    {
        "content": "def short_description(description, **kwargs):\n    \"\"\"\n    This decorator adds the django short_description attribute to the\n    given function.\n\n    It also adds every keyword argument as extra attribute to the\n    decorated function.\n\n    :param description: the value for the short description attribute\n    :type description: str or unicode\n\n    :return: the decorator function\n    :rtype: function\n    \"\"\"\n    def _wrapper(func):\n        \"\"\"\n        Internal wrapper function.\n\n        It only adds some attributes to the function object.\n        \"\"\"\n        func.short_description = description\n        for key in kwargs:\n            setattr(func, key, kwargs[key])\n        return func\n    return _wrapper",
        "sha1": "4c75cda3bd4a1f5674a527a02a6f27754bed6177",
        "id": 108418
    },
    {
        "content": "def adt_object_to_element_name(adt_object):\n    \"\"\"Returns XML element name for the given adt_object\"\"\"\n\n    objtype = adt_object.objtype\n    return f'{objtype.xmlnamespace.name}:{objtype.xmlname}'",
        "sha1": "9b2c9490268ad5dd9ea387255d60d9440e1beff1",
        "id": 639581
    },
    {
        "content": "def map_div_js(context, center_latitude, center_longitude, zoom_level, map_div_id):\n    \"\"\"\n    Standardize map display\n    \n    zoom_level should be 14 or higher for individual trees.\n    map_div_id may be: map or tree-map\n    \"\"\"\n    return {\n        'geojson': context['geojson'],\n        'zoom_level': zoom_level,\n        'center_latitude': center_latitude,\n        'center_longitude': center_longitude,\n        'map_div_id': map_div_id\n    }",
        "sha1": "ae0261c243e910596c73be9b4704531900f10795",
        "id": 576877
    },
    {
        "content": "import logging\n\n\ndef rescale_log_data(log_data, acceleration_scale = 256.):\n\t\"\"\"\n\tRescale raw acceleration data to g values\n\n\tParameters\n\t----------\n\tlog_data : np.array()\n\t\tarray with YXZ acceleration data (in integers otherwise no scaling required)\n\tacceleration_scale : float (optional)\n\t\tvalue to scale the acceleration\n\n\tReturns\n\t-------\n\tscaled_log_data : np.array()\n\t\tlog_data scaled by acceleration scale\n\t\"\"\"\n\n\ttry:\n\n\t\t# calculate the scaling factor\n\t\tscale_factor = 1. / float(acceleration_scale)\n\n\t\t# apply scaling and return\n\t\treturn log_data * scale_factor\n\texcept Exception as e:\n\t\tlogging.error('Error rescaling log data: {}'.format(e))\n\t\texit(1)",
        "sha1": "aed1ac6c6f3a4e224f4c772b5eba08834661f2f0",
        "id": 202726
    },
    {
        "content": "def create_lookup_dicts(unique_chars, specials=None):\n    \"\"\"\n\n    Args:\n        unique_chars: Set of unique chars appearning in texts.\n        specials: Special characters we want to add to the dict,\n                  such as <PAD>, <SOS> or <EOS>\n\n    Returns:\n        char2ind: look updict from character to index\n        ind2char: lookup dict from index to character\n\n    \"\"\"\n    char2ind = {}\n    ind2char = {}\n    i = 0\n\n    if specials is not None:\n        for sp in specials:\n            char2ind[sp] = i\n            ind2char[i] = sp\n            i += 1\n    for ch in unique_chars:\n        char2ind[ch] = i\n        ind2char[i] = ch\n        i += 1\n    return char2ind, ind2char",
        "sha1": "39cf7fff8c1409c24c91b75c356ad620784d950e",
        "id": 245436
    },
    {
        "content": "def get_filename_pair(filename):\n    \"\"\"\n    Given one *.spar/*.sdat filename, returns tuple with both filenames\n    It doesn't matter if the filename is a fully qualified path or not.\n    - one assumption, the extension are either all caps or all lower\n\n    \"\"\"\n    spar_filename = sdat_filename = filename[:-3]\n    if filename[-1:].isupper():\n        sdat_filename += 'DAT'\n        spar_filename += 'PAR'\n    else:\n        sdat_filename += 'dat'\n        spar_filename += 'par'\n\n    return (spar_filename, sdat_filename)",
        "sha1": "1d3a33ba68e14d9217e5e2a7833258d9c94f1667",
        "id": 100026
    },
    {
        "content": "def at_joint_target(current, desired, goal_tolerance):\n    \"\"\"This function can be used to check if the move_group is already at the desired\n    joint target.\n\n    Parameters\n    ----------\n    current : :py:obj:`list`\n        The current joint configuration.\n    desired : :py:obj:`list`\n        The desired joint target.\n    goal_tolerance : :py:obj:`float`\n        The planner target goal tolerance.\n\n    Returns\n    -------\n    :py:obj:`bool`\n        Bool specifying if move_group is already at the goal target.\n    \"\"\"\n\n    # Round to the goal tolerance\n    accuracy = str(goal_tolerance)[::-1].find(\".\")\n    current = [round(item, accuracy) for item in current]\n    desired = [round(item, accuracy) for item in desired]\n\n    # Check if move_group is at joint target\n    if current == desired:\n        return True  # Already at goal\n    else:\n        return False",
        "sha1": "a88a13518141a76a3325b0b77d5e4a939d0db8d9",
        "id": 385500
    },
    {
        "content": "def _get_calls(data, cnv_only=False):\n    \"\"\"Retrieve calls, organized by name, to use for heterogeneity analysis.\n    \"\"\"\n    cnvs_supported = set([\"cnvkit\", \"battenberg\"])\n    out = {}\n    for sv in data.get(\"sv\", []):\n        if not cnv_only or sv[\"variantcaller\"] in cnvs_supported:\n            out[sv[\"variantcaller\"]] = sv\n    return out",
        "sha1": "a03a6214cc3630a0cbfd58a447afa7d48fa9b576",
        "id": 306873
    },
    {
        "content": "def merge_list(*_list: list):\n    \"\"\"Combines multiple lists into a new one.\n\n    Parameters\n    ----------\n    *_list : list\n        It contains all of the lists inside it.\n\n    Returns\n    -------\n    merged_list : list\n        Returns the Merged list.\n    \"\"\"\n    merged_list = []\n\n    for list in _list:\n        merged_list.extend(iter(list))\n\n    return merged_list",
        "sha1": "78284ea382fb74a1eb21f40fdec4e881d1a9f05f",
        "id": 518507
    },
    {
        "content": "import base64\n\n\ndef convert_hex_to_base64(hex):\n    \"\"\"\n    Converts hex string to base64 encoding\n    :param hex: hex encoded string\n    :return: base64 encoded string\n    \"\"\"\n    # Convert hex to byte string\n    decoded_hex = bytearray.fromhex(hex)\n\n    # Convert byte string to base64 encoded string; then convert to string\n    encoded_base64_str = bytes.decode(base64.b64encode(decoded_hex))\n\n    return encoded_base64_str",
        "sha1": "e437e98887ff5b9097e6e11d2a4bc016c1af965a",
        "id": 398859
    },
    {
        "content": "def get_pids(search):\n    \"\"\"Get a list of pids of the search results.\"\"\"\n    pids = []\n    for hit in search().scan():\n        pids.append(hit[\"pid\"])\n\n    return pids",
        "sha1": "280ab7996bc1daf8230762dd40a28a10153b31c7",
        "id": 621377
    },
    {
        "content": "def _rgb_to_linear(c: float) -> float:\n    \"\"\"Converts RGB to linear sRGB\n\n    :param c: (float) RGB value\n    :return: (float) linear sRGB value\n    \"\"\"\n    if c > (0.0031308 * 12.92):\n        return pow(c * (1.0 / 1.055) + (0.055 / 1.055), 2.4)\n    return c * (1.0 / 12.92)",
        "sha1": "73fe77e7636a3569abefb122fa2d3f44dc97f2c1",
        "id": 525352
    },
    {
        "content": "def is_tandem(seq):\n    \"\"\"Check if `seq` is tandemly repetitive or not, assuming it is error-free.\"\"\"\n    L = len(seq)\n    for i in range(1, int(L / 2) + 1):\n        if L % i != 0:\n            continue\n        if seq == seq[:i] * int(L / i):\n            return True\n    return False",
        "sha1": "7693b713e931974e519062d5a7c308d6d8bd1003",
        "id": 237680
    },
    {
        "content": "def youngs(v=None, u=None, K=None, L=None, Vp=None, Vs=None, rho=None):\n    \"\"\"\n    Compute the Young's modulus of a material given sufficient other moduli.\n\n    :param: v: Poisson's ratio (combine with u, K, or L)\n    :param u: Shear modulus (combine with v, K, or L)\n    :param K: Bulk modulus (combine with v, u, or L)\n    :param L: First Lame parameter (combine with v, u, or K)\n    :param Vp: Compressional velocity (combine with Vs and rho)\n    :param Vs: Shear velocity (combine with Vp and rho)\n    :param rho: Density (combine with Vp and Vs)\n    \"\"\"\n    if v and u:\n        E = 2*u*(1+v)\n    elif v and K:\n        E = 3*K*(1-2*v)\n    elif v and L:\n        E = (L*(1+v)*(1-2*v))/(v)\n    elif u and K:\n        E = (9*K*u)/(3*K+u)\n    elif u and L:\n        E = u*(3*L+2*u)/(L+u)\n    elif K and L:\n        E = 9*K*(K-L)/(3*K-L)\n    elif Vp and Vs and rho:\n        E = rho*Vs**2*(3*Vp**2-4*Vs**2)/(Vp**2-Vs**2)\n    else:\n        E = None\n    return(E)",
        "sha1": "a48de2839f4ba4d2bb20dddf01070a2ac078a811",
        "id": 424809
    },
    {
        "content": "def proc_to_seconds(val, entry):\n    \"\"\"Process a value in minutes to seconds\"\"\"\n    return 60 * int(val)",
        "sha1": "aacfa600b065e89443ede1b8eafc771ed695c191",
        "id": 613384
    },
    {
        "content": "def is_terminal(x, y, world_size):\n    \"\"\"Identify whether a state is terminal or not (top-left and bottom-right corner)\"\"\"\n    return (x == 0 and y == 0) or (x == world_size-1 and y == world_size-1)",
        "sha1": "0ee925f7b0ce006ab5964994d1e8feeb37b94aee",
        "id": 137017
    },
    {
        "content": "def bool_2_int(value):\n    \"\"\"Convert boolean to 0 or 1\n    Required for eg. /json.htm?type=command&param=makefavorite&idx=IDX&isfavorite=FAVORITE\n\n    Args:\n        value (bool)\n\n    Returns:\n        1 if True, else 0\n    \"\"\"\n    if isinstance(value, bool):\n        return int(value)\n    else:\n        return 0",
        "sha1": "8b37a0b8590eaf3b717fe7392f34e980c08fbd52",
        "id": 162877
    },
    {
        "content": "def astore_url(package, uid, instance = \"https://astore.corp.enfabrica.net\"):\n    \"\"\"Returns a URL for a particular package version from astore.\"\"\"\n    if not package.startswith(\"/\"):\n        package = \"/\" + package\n    return \"{}/d{}?u={}\".format(\n        instance,\n        package,\n        uid,\n    )",
        "sha1": "089d905eb37337b133e299b0d9d9ed4313ad62d3",
        "id": 275612
    },
    {
        "content": "from pathlib import Path\nimport re\nimport random\n\n\ndef _get_files_and_labels_list(dataset_dir):\n    \"\"\"Get a list of filenames and labels from the dataset directory\n    \"\"\"\n    file_list = []\n    lib_names = sorted(f.name for f in Path(dataset_dir).iterdir() if re.match(r\"^n[0-9]+$\", f.name))\n    class_id = {v: i for i, v in enumerate(lib_names)}\n    for lib in Path(dataset_dir).iterdir():\n        for img in lib.iterdir():\n            file_list.append([str(img), class_id[lib.name]])\n    random.seed(0)\n    random.shuffle(file_list)\n    return file_list",
        "sha1": "5c9cb6d0e8327ae847f0b8f1cb8c43cd312ddba9",
        "id": 586889
    },
    {
        "content": "def short_id_from_s3_key(input):\n    \"\"\"\n        Transform s3 keys to case or page short IDs used by volume XML:\n            32044142600386_redacted/alto/32044142600386_redacted_ALTO_00009_0.xml  ->  alto_00009_0\n            32044142600386_redacted/casemets/32044142600386_redacted_CASEMETS_0001.xml  -> casemets_0001\n    \"\"\"\n    if ('CASEMETS' in input or 'ALTO' in input) and input.endswith(\"xml\"):\n        return input.split('/')[-1].split('.')[0].split('redacted_')[1].lower()\n    raise Exception(\"Not an ALTO or CASEMETS s3_key\")",
        "sha1": "b36528319b5d1187bf462d2820ffbf413ffb45c1",
        "id": 218132
    },
    {
        "content": "def get_preresolution(file: str) -> int:\n    \"\"\"\n    Read pre file and returns number of unknowns\n\n    :param file: pre file\n    :return: number of unknowns\n    \"\"\"\n    with open(file) as f:\n        content = f.readlines()\n    ind = [idx for idx, s in enumerate(content) if '$DofData' in s][0]\n    tmp = content[ind + 5].split()[-1]\n\n    return int(tmp)",
        "sha1": "8084c37792246bef5f6fc9da43e30da9f4902cbc",
        "id": 30623
    },
    {
        "content": "def get_file_text(path):\n    \"\"\" Returns file text by path\"\"\"\n    file_io = open(path, \"r\")\n    text = file_io.read()\n    file_io.close()\n    return text",
        "sha1": "e7d165b8b62c24b8a34ef375350a67be17b8e69a",
        "id": 58256
    },
    {
        "content": "import base64\n\n\ndef decode_attachment_data(attachment) -> bytes:\n    \"\"\"\n    Decode a raw attachment into usable bytes.\n\n    :param attachment:\n    :return: the bytes containing file data\n    \"\"\"\n    if attachment['content_transfer_encoding'] == 'base64':\n        return base64.b64decode(attachment['payload'])\n    return attachment['payload'].encode('utf-8')",
        "sha1": "460ddc9000b9d85311d3e46d1cb46aaeec18957c",
        "id": 180158
    },
    {
        "content": "import torch\n\n\ndef sym_normalize(A):\n    \"\"\"\n    Symmetrical normalization: computes D^-1/2 * A * D^-1/2.\n\n    Parameters\n    ----------\n    A: torch.Tensor\n        The matrix to normalize.\n\n    Returns\n    -------\n    A_norm: torch.FloatTensor\n        The normalized adjacency matrix.\n    \"\"\"\n    degs = A.sum(dim=1)\n    norm = torch.pow(degs, -0.5)\n    norm[torch.isinf(norm)] = 1\n    return A * norm[:, None] * norm[None, :]",
        "sha1": "72e8293d72bed57660b4a5daa559ac5d35a3747e",
        "id": 220176
    },
    {
        "content": "def get_E_Elc_rfrg_annual(E_Elc_rfrg_annual_JIS, JIS_year):\n    \"\"\"\u6642\u523b\u5225\u6d88\u8cbb\u96fb\u529b\u91cf\u3092\u8a08\u7b97\u3059\u308b\n\n    Parameters\n    ----------\n    E_Elc_rfrg_annual_JIS : float\n        JIS\u306b\u6e96\u62e0\u3057\u3066\u6e2c\u5b9a\u3055\u308c\u305f\u5e74\u9593\u6d88\u8cbb\u96fb\u91cf[kWh]\n\n    JIS_year : Int\n        \u5e74\u9593\u6d88\u8cbb\u96fb\u529b\u91cf\u306e\u6e2c\u5b9a\u6642\u306b\u6e96\u62e0\u3057\u305fJIS\u898f\u683c\u306e\u5236\u5b9a\u5e74[\u5e74]\n\n    Returns\n    ----------\n    E_Elc_rfrg_annual : float\n        \u5e74\u9593\u6d88\u8cbb\u96fb\u91cf[kWh]\n    \"\"\"\n\n    if JIS_year == 1999:\n        E_Elc_rfrg_annual = E_Elc_rfrg_annual_JIS\n    elif JIS_year == 2006:\n        E_Elc_rfrg_annual = E_Elc_rfrg_annual_JIS / 3.48\n    elif JIS_year == 2015:\n        E_Elc_rfrg_annual = E_Elc_rfrg_annual_JIS / 3.48 * 0.2891\n    else:\n        raise ValueError(JIS_year)\n\n    return E_Elc_rfrg_annual",
        "sha1": "59d74157775874e2666874128eae8f0632d2b4af",
        "id": 174864
    },
    {
        "content": "def get_element_phases_string(element):\n    \"\"\"Utility function for obtaining the OpenDSS phases string for given element data.\"\"\"\n\n    # Obtain string of connected phases.\n    phases_string = \"\"\n    if element['is_phase_0_connected'] == 1:\n        phases_string += \".0\"\n    if element['is_phase_1_connected'] == 1:\n        phases_string += \".1\"\n    if element['is_phase_2_connected'] == 1:\n        phases_string += \".2\"\n    if element['is_phase_3_connected'] == 1:\n        phases_string += \".3\"\n\n    return phases_string",
        "sha1": "6b7b4800338cea3076dbdc4aaabcc932f8188e58",
        "id": 360346
    },
    {
        "content": "def get_pronouns(gender: str):\n    \"\"\"Gets a list of pronouns based on the gender given. Only binary genders supported, sorry.\"\"\"\n    gender = gender.lower()\n    if gender == \"female\":\n        pronoun = [\"she\", \"her\", \"her\"]\n    elif gender == \"male\":\n        pronoun = [\"he\", \"his\", \"him\"]\n    else:\n        pronoun = [\"it\", \"its\", \"it\"]\n    return pronoun",
        "sha1": "c97bc0a6c783320f951650ed4f92d2f06cce8172",
        "id": 614555
    },
    {
        "content": "import re\n\n\ndef seems_like_section_name(line):\n    \"\"\"Check whether `line` starts with 'Para' or ends with ':', ignoring case and whitespace.\"\"\"\n    return bool(\n        re.search(r'(^[^a-z\u00e1\u00e9\u00ed\u00f3\u00fa\u00fc0-9]*para\\b|:\\s*$)', line, re.IGNORECASE)\n    )",
        "sha1": "d1c10116319c39cb5e0b5c57a2007703578da75c",
        "id": 22019
    },
    {
        "content": "def HyphenateWord(original_word, max_length, join_str='-'):\n  \"\"\"Breaks a word at max_length with hyphens.\n\n  If the word is still too long (i.e., length > 2*max_length), the word will\n  be split again. The word will be split at word breaks defined by a\n  switch from lowercase to uppercase. The function will attempt to break at\n  the closest switch before the max_length number of letters. If there is no\n  upper case letter before the max_length, the original word will be returned.\n  If max_length is 0 the original word will be returned.\n\n  Args:\n    original_word: the word to break.\n    max_length: maximum length in chars for any piece of the word.\n    join_str: characters to join the hyphenated parts with, defaults to '-'\n  Returns:\n    the hyphenated word.\n  \"\"\"\n  word_list = []\n  while len(original_word) > max_length:\n    for i in range(max_length, -1, -1):\n      if original_word[i].isupper():\n        word_list += [original_word[:i]]\n        original_word = original_word[i:]\n        break\n      if i == 0:\n        # There was no uppercase letter found so we don't break and just\n        # return the original word\n        return original_word\n\n  word_list += [original_word]\n\n  return join_str.join(word_list)",
        "sha1": "91068b73f77a2d0b8155df338a9ceeb09dcc0efe",
        "id": 513869
    },
    {
        "content": "def list_to_dict(list_to_convert: dict, key_name: str):\n    \"\"\"\n    USed to convert a list of dictionaries to a dictionary using some common properties (i.e. name)\n    Careful as data will be lost for duplicate entries, this assumes the list is a \"set\".\n    :param list_to_convert: A list of dictionaries\n    :param key_name: A value from each dict to use as the key.\n    :return: A dictionary.\n    \"\"\"\n    converted_dict = dict()\n    if list_to_convert:\n        for item in list_to_convert:\n            converted_dict[item[key_name]] = item\n    return converted_dict",
        "sha1": "edd998a0aac0a989e296694546c63781b03ea392",
        "id": 561288
    },
    {
        "content": "def accumulation_distribution(close, low, high, volume):\n    \"\"\"\n    Cumulative indicator that makes us of price and volume to assess\n    whether an asset is being accumulated or distributed.\n    :param close: closing price\n    :param low: lowest price\n    :param high: highest price\n    :param volume: daily volume\n    :return: ADI: Accumulation/Distribution Indicator\n    \"\"\"\n\n    # Calculate current money flow volume\n    cmfv = (((close - low) - (high - close)) / (high - low)) * volume\n\n    ADI = cmfv.cumsum()\n\n    return ADI",
        "sha1": "f193d256322898bf9dd871fc9b6e8ec238d52f86",
        "id": 94432
    },
    {
        "content": "def save_page_content(pdfContent, page_id, page_data):\n    \"\"\"Appends the content of a scanned page, line by line, to a pandas DataFrame.\"\"\"\n    if page_data:\n        for idx, line in enumerate(page_data, 1):\n            line = ' '.join(line)\n            pdfContent = pdfContent.append(\n                {'page': page_id, 'line_id': idx, 'line': line}, ignore_index=True\n            )\n    return pdfContent",
        "sha1": "f39cfb1c1af092b324b3c58bfa20187840f33deb",
        "id": 419806
    },
    {
        "content": "import re\n\n\ndef convert_iobes_to_bio(seq):\n    \"\"\"Convert a sequence of IOBES tags to BIO tags\n\n    :param seq: `List[str]` The list of IOBES tags.\n\n    :returns: `List[str]` The list of BIO tags.\n    \"\"\"\n    # Use re over `.replace` to make sure it only acts on the beginning of the token.\n    return list(map(lambda x: re.sub(r'^S-', 'B-', re.sub(r'^E-', 'I-', x)), seq))",
        "sha1": "4f79c63c894ef03604d994f7ed95571a676c7da3",
        "id": 324132
    },
    {
        "content": "import hashlib\n\n\ndef md5sum(infile):\n    \"\"\"Calculate the md5sum of a file\n    \"\"\"\n    # Implementation taken from: http://stackoverflow.com/a/4213255\n    md5 = hashlib.md5()\n    with open(infile,'rb') as f:\n        for chunk in iter(lambda: f.read(128*md5.block_size), b''):\n            md5.update(chunk)\n    return md5.hexdigest()",
        "sha1": "475ad58c71d361a6dfec9127dbbddc53fb60cd21",
        "id": 678298
    },
    {
        "content": "def isFloat(string):\n    \"\"\" is the given string a float? \"\"\"\n    try: float(string)\n    except ValueError: return 0\n    else: return 1",
        "sha1": "4605d2975523dab25ec598ee7ed0324c0c200b05",
        "id": 677567
    },
    {
        "content": "from typing import Optional\n\n\ndef parse_options(dict_in: Optional[dict], defaults: Optional[dict] = None):\n    \"\"\"\n    Utility function to be used for e.g. kwargs\n    1) creates a copy of dict_in, such that it is safe to change its entries\n    2) converts None to an empty dictionary (this is useful, since empty dictionaries cant be argument defaults)\n    3) optionally, sets defaults, if keys are not present\n\n    Parameters\n    ----------\n    dict_in\n    defaults\n\n    Returns\n    -------\n\n    \"\"\"\n    if dict_in is None:\n        dict_in = {}\n    else:\n        dict_in = dict_in.copy()\n    if defaults:\n        for key in defaults:\n            dict_in.setdefault(key, defaults[key])\n    return dict_in",
        "sha1": "d679539ba29f4acab11f5db59c324473a2e24cc6",
        "id": 705452
    },
    {
        "content": "def read_file_precision(file,precision):\n    \"\"\" Read in the file converting floats to the given precision \"\"\"\n    file_data=[]\n    with open(file) as file_handle:\n        for line in file_handle:\n            formatted_data=[]\n            for data in line.rstrip().split(\"\\t\"):\n                try:\n                    data_float=float(data)\n                    data=\"{:.{digits}f}\".format(data_float, digits=precision)\n                except ValueError:\n                    pass\n                formatted_data.append(data)\n            file_data.append(\"\\t\".join(formatted_data))\n    return file_data",
        "sha1": "8d1901b7651f0eae04424121a885c48519f05e8d",
        "id": 203465
    },
    {
        "content": "from typing import Sequence\n\n\ndef get_problem_type_input_args() -> Sequence[str]:\n    \"\"\"Return ``typing.get_args(ProblemTypeInput)``.\"\"\"\n    return (\"trivial-gcd\", \"nontrivial-gcd\", \"trivial-factor\", \"nontrivial-factor\")",
        "sha1": "937759fde8ebeb0cf0666f83e7e3741720eac324",
        "id": 31955
    },
    {
        "content": "def split_x0(func):\n    \"\"\"\n    Wrapper that splits array x0 of the minimization routine into two arrays.\n\n    Splits the the first argument x0 into two arrays alpha_i and tau_i and \n    forwards both arrays to the called function. A single array x0 is necessary \n    to optimize both alpha_i and tau_i at the same time. However, typically, \n    only alpha_i is optimized and tau_i is kept constant. This wrapper allows \n    to use the same function in both scenarios.\n\n    Parameters\n    ----------\n    func : function\n        Function that calculates least squares residual.\n\n    Returns\n    -------\n    split : function\n\n    See also\n    --------\n    prony.ls_res : Function to be wrapped during minimization of Prony terms.\n    \"\"\"\n    def split(*args):\n        alpha_i = args[0][0:int(args[0].shape[0]/2)]\n        tau_i = args[0][int(args[0].shape[0]/2):]\n        return func(alpha_i, tau_i, args[1], args[2])\n    return split",
        "sha1": "ed6a6dbee52e2f1d14d715052827d3dcd9d208f2",
        "id": 248240
    },
    {
        "content": "def porosity_total(phie, vclay, phiclay):\n    \"\"\"\n    Converts effective porosity to total porosity\n\n    Parameters\n    ----------\n    phie : float\n        Effective porosity (decimal)\n    vclay : float\n        Volume of clay (decimal)\n    phiclay : float\n        Clay porosity - taken from a shale interval (decimal)\n\n    Returns\n    -------\n    float\n        Returns total porosity (decimal)\n    \"\"\"\n    return phie + vclay * phiclay",
        "sha1": "f0db54915f5605f6a7cdcf9cf9c1da3da3278607",
        "id": 149139
    },
    {
        "content": "def inheritdoc(cls, gap=\"\\n\"):\n    \"\"\"\n    Decorator to automatize the inheritance of documentation from a class method.\n\n    Example\n    -------\n\n    >>> from skhep.utils.decorators import inheritdoc\n    >>> class ADerivedClass(ABaseClass):                 # doctest: +SKIP\n    ...    @inheritdoc(ABaseClass)                       # doctest: +SKIP\n    ...    def amethod(self): pass                       # doctest: +SKIP\n    \"\"\"\n\n    def _fn(fn):\n        if fn.__name__ in cls.__dict__:\n            if fn.__doc__ is None:\n                fn.__doc__ = cls.__dict__[fn.__name__].__doc__\n            else:\n                fn.__doc__ = (\n                    cls.__dict__[fn.__name__].__doc__.strip() + gap + fn.__doc__.strip()\n                )\n        return fn\n\n    return _fn",
        "sha1": "df2a3878c1ef77700e77a35d5d4eb0e21b32691f",
        "id": 578025
    },
    {
        "content": "def dec_2_deg(coord):\n    \"\"\"\n    Converts a coordinate from decimal minutes to decimal degrees\n\n    Parameters\n    ------------\n    coord : str\n        Coordinate to be converted from decimal minutes to decimal degrees\n\n\n    Returns\n    ------------\n    deg : str\n        Coordinate transformed into decimal degrees\n    \"\"\"\n\n    splt = coord.split('.')\n\n    dec = float(splt[1]) / 60\n    deg = float(splt[0]) + dec\n\n    if (len(coord) > 5):    # Implies we're dealing with a longitude coordinate\n        deg_str = '-' + str(deg)\n        return deg_str[:7]\n    else:\n        return str(deg)[:6]\n\n    #return deg",
        "sha1": "937819dd31048970d1cea28d4682a0eef160ce83",
        "id": 506442
    },
    {
        "content": "def get_dict_optional_value(d,keys_to_try_in_order, default_value=None):\n    \"\"\"\n    Tries each key in order, if not value is found, returns default_value\n    \"\"\"\n    \n    for key in keys_to_try_in_order:\n        if key in d and d.get(key):\n            return d[key]\n        \n    return default_value",
        "sha1": "bac50e1d92807577d8df914f9c5d6ba8d3144147",
        "id": 288953
    },
    {
        "content": "import torch\n\n\ndef unit_to_midi(unit, midi_min, midi_max = 90.0, clip = False):\n    \"\"\"Map the unit interval [0, 1] to MIDI notes.\"\"\"\n    unit = torch.clamp(unit, 0.0, 1.0) if clip else unit\n    return midi_min + (midi_max - midi_min) * unit",
        "sha1": "1525f95e18206041b73cf5a4cf00745fb3a7e241",
        "id": 601976
    },
    {
        "content": "def get_config_interfaces_name_by_type(config_interfaces, type_str):\n    \"\"\"\n    Extract interface names matching specific type from Interface config\n    :param config_interfaces: Interfaces config dict\n    :param type_str: Type string to match\n    :return: List of Interface names matching type_str\n    \"\"\"\n    interface_name_list = []\n    for name, config_interface in config_interfaces.items():\n        if config_interface.get('type') == type_str:\n            interface_name_list.append(name)\n\n    return interface_name_list",
        "sha1": "a511b9e78206300914ad6e3ce6178a9a1c4b5d65",
        "id": 483042
    },
    {
        "content": "def get_event_id_missing_feature(con, table, missing_feature):\n    \"\"\"\n    Get the a list of event ids in a given table where data is missing in a specific column (=feature\n    :param con:\n    :param table:\n    :param missing_feature:\n    :return:\n    \"\"\"\n\n    query_missing_data = f\"\"\"SELECT event_id FROM {table} WHERE {missing_feature} is NULL\"\"\"\n\n    with con.cursor() as cursor:\n        cursor.execute(query_missing_data)\n        list_event_ids = [dictionary.get('event_id') for dictionary in cursor.fetchall()]\n\n    return list_event_ids",
        "sha1": "53415f79e0a7afe756868220311489c7409c6585",
        "id": 304500
    },
    {
        "content": "def hsv_to_hex(h, s, v):\n    \"\"\"\n    >>> print(hsv_to_hex(0, 0, 0))\n    #000000\n    >>> print(hsv_to_hex(0, 0, 1))\n    #FFFFFF\n    >>> print(hsv_to_hex(0, 1, 1))\n    #FF0000\n    >>> print(hsv_to_hex(120, 1, 1))\n    #00FF00\n    >>> print(hsv_to_hex(240, 1, 1))\n    #0000FF\n    >>> print(hsv_to_hex(60, 1, 1))\n    #FFFF00\n    >>> print(hsv_to_hex(180, 1, 1))\n    #00FFFF\n    >>> print(hsv_to_hex(300, 1, 1))\n    #FF00FF\n    >>> print(hsv_to_hex(0, 0, .75))\n    #C0C0C0\n    >>> print(hsv_to_hex(0, 0, .5))\n    #808080\n    >>> print(hsv_to_hex(0, 1, .5))\n    #800000\n    >>> print(hsv_to_hex(60, 1, .5))\n    #808000\n    >>> print(hsv_to_hex(120, 1, .5))\n    #008000\n    >>> print(hsv_to_hex(300, 1, .5))\n    #800080\n    >>> print(hsv_to_hex(180, 1, .5))\n    #008080\n    >>> print(hsv_to_hex(240, 1, .5))\n    #000080\n    >>> print(hsv_to_hex(220, .95, 1))\n    #0C5DFF\n    \"\"\"\n    # Implementation of http://www.rapidtables.com/convert/color/hsv-to-rgb.htm\n    h %= 360\n\n    if not (0 <= s <= 1 and 0 <= v <= 1):\n        raise ValueError('`s` and `v` must be between 0 and 1.')\n\n    c = v * s\n    x = c * (1 - abs(((h / 60) % 2) - 1))\n    m = v - c\n    if 0 <= h < 60:\n        t = c, x, 0\n    elif 60 <= h < 120:\n        t = x, c, 0\n    elif 120 <= h < 180:\n        t = 0, c, x\n    elif 180 <= h < 240:\n        t = 0, x, c\n    elif 240 <= h < 300:\n        t = x, 0, c\n    elif 300 <= h < 360:\n        t = c, 0, x\n    else:\n        raise ValueError('`h` should be between 0 and 359.')\n    r2, g2, b2 = t\n    r, g, b = r2 + m, g2 + m, b2 + m\n\n    def to_hex(color):\n        return hex(int(min(255, color * 256)))[2:].upper().zfill(2)\n\n    return '#' + to_hex(r) + to_hex(g) + to_hex(b)",
        "sha1": "8f33987dcd4d4d8f48c4087906e732e212c57cec",
        "id": 607431
    },
    {
        "content": "def clean(dictionary, keep):\n    \"\"\"Remove attributes that are not in the 'keep' list\"\"\"\n    for key in dictionary.copy().keys():\n        if key not in keep:\n            dictionary.pop(key)\n    return dictionary",
        "sha1": "73f2543323c1cddae67c5263b154d23a5ca9ad24",
        "id": 327838
    },
    {
        "content": "def train_and_test_partition(inputs, targets, train_part, test_part):\n    \"\"\"\n    Splits a data matrix (or design matrix) and associated targets into train\n    and test parts.\n\n    parameters\n    ----------\n    inputs - a 2d numpy array whose rows are the data points, or can be a design\n        matrix, where rows are the feature vectors for data points.\n    targets - a 1d numpy array whose elements are the targets.\n    train_part - A list (or 1d array) of N booleans, where N is the number of\n        data points. If the ith element is true, then the ith data point will be\n        added to the training data.\n    test_part - (like train_part), but specifying the test points.\n\n    returns\n    -------     \n    train_inputs - the training input matrix\n    train_targets - the training targets\n    test_inputs - the test input matrix\n    test_targets - the test targets\n    \"\"\"\n\n    # getting the indices of the train and test portion\n    if len(inputs.shape) == 1:\n        # if inputs is a sequence of scalars, we should reshape into a matrix\n        inputs = inputs.reshape((inputs.size, 1))\n\n    # getting the indices of the train and test portion\n    train_inputs = inputs[train_part, :]\n    test_inputs = inputs[test_part, :]\n    train_targets = targets[train_part]\n    test_targets = targets[test_part]\n\n    return train_inputs, train_targets, test_inputs, test_targets",
        "sha1": "a41829faecf34256ef2ae6d80c0fe72713832cb4",
        "id": 655402
    },
    {
        "content": "def Btu_hftR2kJ_hmK(x):\n    \"\"\"Btu/(hr-ft-R) -> kJ/(h-m-K)\"\"\"\n    return 6.231*x",
        "sha1": "a60b398292e5d883737339e6d7343656295b556c",
        "id": 90321
    },
    {
        "content": "def group_group_name(group_value):\n    \"\"\"Compute name for an app group.\n\n    :returns: Name of the group\n    :rtype: str\n    \"\"\"\n    return 'group_{}'.format(group_value)",
        "sha1": "bdaa23a7fe1978848fd38c1d3fc3fca0bbcf33a6",
        "id": 268010
    },
    {
        "content": "def default(event, **kwargs):\n    \"\"\"A Hacktoolkit-flavored default event handler for Slack webhook events\n\n    Returns a payload if applicable, or None\n    \"\"\"\n    text = kwargs.get('text')\n    command = kwargs.get('command')\n    args = kwargs.get('args')\n\n    # for example, we could...\n    # make another webhook call in response\n    channel = event['channel_id']\n    slack_text = 'You said:\\n>%s\\n Roger that.' % text\n    #webhook_call(text=slack_text, channel=channel, username=username)\n\n    payload = {\n        'text' : slack_text,\n    }\n    return payload",
        "sha1": "4496fd9698dc331cc70abb173b8cff4d8a06d994",
        "id": 110211
    },
    {
        "content": "import torch\n\n\ndef ut_mask(seq_len):\n    \"\"\" Upper Triangular Mask\n    \"\"\"\n    return torch.triu(torch.ones(seq_len, seq_len), diagonal=1).to(dtype=torch.bool)",
        "sha1": "565a6a1b4d85be76308e3bc0859a548e3926cfad",
        "id": 337093
    },
    {
        "content": "def _binop_precision(l_dtype, r_dtype, op):\n    \"\"\"\n    Returns the result precision when performing the\n    binary operation `op` for the given dtypes.\n\n    See: https://docs.microsoft.com/en-us/sql/t-sql/data-types/precision-scale-and-length-transact-sql\n    \"\"\"  # noqa: E501\n    p1, p2 = l_dtype.precision, r_dtype.precision\n    s1, s2 = l_dtype.scale, r_dtype.scale\n    if op in (\"add\", \"sub\"):\n        return max(s1, s2) + max(p1 - s1, p2 - s2) + 1\n    elif op == \"mul\":\n        return p1 + p2 + 1\n    else:\n        raise NotImplementedError()",
        "sha1": "a172b4aece2d062a23b1cbbcda0ab9586d9c1b90",
        "id": 622178
    },
    {
        "content": "def replace(orig, position, char):\n    \"\"\"\n    Replace a character in string by position\n    :param orig: origin string\n    :param position: position to be replaced\n    :param char: character to replace\n    :return:\n    \"\"\"\n    str_len = len(orig)\n    if position < 0 or position > str_len - 1:\n        return orig\n\n    # Method 1\n    # orig = list(orig)\n    # orig[position] = char\n    # return ''.join(orig)\n\n    # Method 2 - Faster\n    return orig[:position] + char + orig[position + 1:]",
        "sha1": "07cca07ed904d34cab1845b7636912c7be0fa2a1",
        "id": 605928
    },
    {
        "content": "def parse_type(msg_type):\n    \"\"\"\n    Parse ROS message field type\n    :param msg_type: ROS field type, ``str``\n    :returns: base_type, is_array, array_length, ``(str, bool, int)``\n    :raises: :exc:`ValueError` If *msg_type* cannot be parsed\n    \"\"\"\n    if not msg_type:\n        raise ValueError(\"Invalid empty type\")\n    if '[' in msg_type:\n        var_length = msg_type.endswith('[]')\n        splits = msg_type.split('[')\n        if len(splits) > 2:\n            raise ValueError(\"Currently only support 1-dimensional array types: %s\"%msg_type)\n        if var_length:\n            return msg_type[:-2], True, None\n        else:\n            try:\n                length = int(splits[1][:-1])\n                return splits[0], True, length\n            except ValueError:\n                raise ValueError(\"Invalid array dimension: [%s]\"%splits[1][:-1])\n    else:\n        return msg_type, False, None",
        "sha1": "1dfe4f3abb7b69bed17b60ee2666279081666dc6",
        "id": 709626
    },
    {
        "content": "import torch\n\n\ndef sfm_perspective_project_naive(points, fx=1.0, fy=1.0, p0x=0.0, p0y=0.0):\n    \"\"\"\n    Compute perspective projection using focal length and principal point.\n\n    Args:\n        points: (N, V, 3) representing the padded points.\n        fx: world units\n        fy: world units\n        p0x: pixels\n        p0y: pixels\n    Returns:\n        (N, V, 3) tensor of projected points.\n    \"\"\"\n    z = points[:, :, 2]\n    x = (points[:, :, 0] * fx) / z + p0x\n    y = (points[:, :, 1] * fy) / z + p0y\n    points = torch.stack((x, y, 1.0 / z), dim=2)\n    return points",
        "sha1": "974d6019fdd4d52ef6b43246dc72c351e25bffb9",
        "id": 552426
    },
    {
        "content": "def _escape_percent_sign(string):\n    \"\"\"Return a string within which all percent signs are escaped\"\"\"\n    return string.replace('%', \"%%\")",
        "sha1": "eb2239395f32325a78d4fa32258b553c58b2486a",
        "id": 107837
    },
    {
        "content": "import json\n\n\ndef retrieve_password_email(request):\n    \"\"\" Helper function to unpack the username and the password from the\n    request\n    Raises an exception if the request wasn't made via POST\n    \"\"\"\n    info = json.loads(request.POST.get('user'))\n    email = info['email']\n    password = info['password']\n    return password, email",
        "sha1": "c3983f89844146f75c9175b0eab071f357bd7529",
        "id": 519368
    },
    {
        "content": "def process_additional_exclude_paths(raw_value):\n    \"\"\"Process additional list of exclude paths and return a list.\"\"\"\n    assert isinstance(raw_value, str), \"The value should be a string, got: {}\".format(type(raw_value))\n\n    pathlist = []\n    raw_pathlist = raw_value.split(',')\n    for raw_path in raw_pathlist:\n        pathlist.append(raw_path.strip())\n    return pathlist",
        "sha1": "e5e457f950cb1c116f6509b71f23438fdcc105d1",
        "id": 366309
    },
    {
        "content": "def hit_count(result):\n    \"\"\" Parse the hit count from an ElasticSearch response\n        ES7: result['hits']['total']['value'], ES6: result['hits']['total']\"\"\"\n\n    return result['hits']['total']['value'] \\\n        if type(result['hits']['total']) is dict \\\n        else result['hits']['total']",
        "sha1": "254241975f5e5198cd96e423c175d728f81ef4b2",
        "id": 182429
    },
    {
        "content": "def read_string(puzzle_string: str):\n    \"\"\"\n    Read string describing puzzle,\n    converting it into a form the solver can understand.\n    puzzle_string: string specifying puzzle\n    return: array of numbers and operations for each row and column\n    \"\"\"\n    puzzle = [\n        part.split(' ')\n        for part\n        in puzzle_string.split('\\n')\n    ]\n\n    puzzle = [\n        row[:2] + [float(row[2])]\n        for row\n        in puzzle\n        if len(row) == 3\n    ]\n\n    return puzzle",
        "sha1": "1e6e25b6a58aa16b7a9d23bcc632fc0622b2800a",
        "id": 109368
    },
    {
        "content": "def invertMask(mask):\n\t\"\"\"\n\t\tInverts a numpy binary mask.\n\t\"\"\"\n\treturn mask == False",
        "sha1": "f6a668a9b2f0928e2a71dc7e4de4d1e04cf307de",
        "id": 23978
    },
    {
        "content": "def get_or_else(hashmap, key, default_value=None):\n    \"\"\" Get value or default value\n    Args:\n        hashmap (dict): target\n        key (Any): key\n        default_value (Any): default value\n    Returns:\n        value of key or default_value\n    \"\"\"\n    value = hashmap.get(key)\n    if value is None:\n        return default_value\n    else:\n        if 0 < len(value.strip()):\n            return value\n        else:\n            return default_value",
        "sha1": "6713541f8075636e92c2d2c2a2c444a9cf822375",
        "id": 667369
    },
    {
        "content": "def mapdict(itemfunc, dictionary):\n    \"\"\"\n    Much like the builtin function 'map', but works on dictionaries.\n    *itemfunc* should be a function which takes one parameter, a (key,\n    value) pair, and returns a new (or same) (key, value) pair to go in\n    the dictionary.\n    \"\"\"\n    return dict(map(itemfunc, dictionary.items()))",
        "sha1": "1f0573410f82acb1f3c06029cf4bfaccd295e1ac",
        "id": 1110
    },
    {
        "content": "def _is_array(value):\n    \"\"\" Is the provided value an array of some sorts (list, tuple, set)? \"\"\"\n    return isinstance(value, (list, tuple, set, frozenset))",
        "sha1": "c3ad0440555da15affac304a9788f75a9727e93f",
        "id": 230866
    },
    {
        "content": "def _get_part(pointlist, strokes):\n    \"\"\"Get some strokes of pointlist\n\n    Parameters\n    ----------\n    pointlist : list of lists of dicts\n    strokes : list of integers\n\n    Returns\n    -------\n    list of lists of dicts\n    \"\"\"\n    result = []\n    strokes = sorted(strokes)\n    for stroke_index in strokes:\n        result.append(pointlist[stroke_index])\n    return result",
        "sha1": "7f2c96945d39b7704ddefed746c358ee6b41a667",
        "id": 454883
    },
    {
        "content": "def scale_gradient(tensor, scale, clone_input=True):\n    \"\"\"Scales the gradient of `tensor` for the backward pass.\n    Args:\n        tensor (Tensor): a tensor which requires gradient.\n        scale (float): a scalar factor to be multiplied to the gradient\n            of `tensor`.\n        clone_input (bool): If True, clone the input tensor before applying\n            gradient scaling. This option is useful when there are multiple\n            computational branches originated from `tensor` and we want to\n            apply gradient scaling to part of them without impacting the rest.\n            If False, apply gradient scaling to the input tensor directly.\n    Returns:\n        The (cloned) tensor with gradient scaling hook registered.\n    \"\"\"\n    if clone_input:\n        output = tensor.clone()\n    else:\n        output = tensor\n    output.register_hook(lambda grad: grad * scale)\n    return output",
        "sha1": "9c969307980a19b29767af82ff001ed47c9f36d0",
        "id": 426954
    },
    {
        "content": "def create_payload(title, body):\n    \"\"\"Create payload mapping for the github api request.\"\"\"\n    return {\n        'title': title,\n        'body': body\n    }",
        "sha1": "a7005dcaef75a9ec5294cd8efd1cc23642389c86",
        "id": 559470
    },
    {
        "content": "import math\n\n\ndef liters_to_bars(liters, bottle_size, round=False):\n    \"\"\"returns the bars corresponding to a given volume in a given bottle_size in liters\"\"\"\n    if round:\n        return math.ceil(liters / bottle_size)\n    else:\n        return liters / bottle_size",
        "sha1": "feaec7a7249d0bcacb0a96cda1078e60ef2e2456",
        "id": 586269
    },
    {
        "content": "def DNI(seed_set_cascades):\n    \"\"\"\n    Measure the number of distinct nodes in the test cascades started of the seed set\n    \"\"\"\n    combined = set()    \n    for i in seed_set_cascades.keys():\n        for j in seed_set_cascades[i]:    \n            combined = combined.union(j)\n    return len(combined)",
        "sha1": "3e8ebd6318bd788e4a7c0b7d53e24522b5a865b6",
        "id": 470386
    },
    {
        "content": "def __get_correction_factor(temperature, humidity):\n    \"\"\"Calculates the correction factor for ambient air temperature and relative humidity\n    Based on the linearization of the temperature dependency curve\n    under and above 20 degrees Celsius, assuming a linear dependency on humidity,\n    provided by Balk77 https://github.com/GeorgK/MQ135/pull/6/files\n    0.00035  - 'CORA' Parameters to model temperature and humidity dependence\n    0.02718  - 'CORB'\n    1.39538  - 'CORC'\n    0.0018   - 'CORD'\n    -0.003333333  - 'CORE'\n    -0.001923077  - 'CORF'\n    1.130128205   - 'CORG'\n    \"\"\"\n    if temperature < 20:\n        return 0.00035 * temperature * temperature - 0.02718 \\\n               * temperature + 1.39538 - (humidity - 33.) * 0.0018\n    return -0.003333333 * temperature + -0.001923077 * humidity + 1.130128205",
        "sha1": "81452785f3424fdcb98e69be6151a872607b4ece",
        "id": 195107
    },
    {
        "content": "def check_nocachekey(option):\n    \"\"\"\n    checks if an option is a NoCacheKey option or Etag\n\n    :param option:\n    :return:\n    \"\"\"\n    return ((option.number & 0x1E) == 0x1C) | (option.number == 4)",
        "sha1": "dc277e3ffc3a5dc6228eba72ec732d1e2f465c08",
        "id": 593902
    },
    {
        "content": "def _expSum(x, iterations=1000):\n    \"\"\"Calculate e^x using power series.\n\n    exp x := Sum_{k = 0}^{inf} x^k/k! = 1 + x + x^2/2! + x^3/3! + x^4/4! + ...\n \n    Which can be rewritten as:\n                                    = 1 + ((x/1)(1 + (x/2)(1 + (x/4)(...) ) ) )\n\n    This second way of writting it is easier to calculate than the first one\n    as it does not need to directly calculate the factorial of each term.\n\n    Arguments:\n        iterations: Times to iterate over the exponential power series.\n                    The minimum valid number is 1 (one), and it'll return the\n                    equivalent to perform 1 + x.\n                    Default is 1000 as it does not take much time to complete\n                    (even for big numbers such as e^500) and going beyond that\n                    does not make a significative difference.\n                    e^500, e^2, e^50, and some  other tried examples get the \n                    same number up to 14 decimal places using 1000 (the \n                    default) and  1000000 (the default value squared) \n                    iterations.\n\n    Returns:\n        Floating point number.\n\n    Raises:\n        ArithmeticError: When trying to iterate less than one time.\n\n    \"\"\"\n    \n    if type(x) is (not int or not float):\n        raise ArithmeticError('Please provide an int or float.')\n    \n    if (iterations < 1):\n        raise ArithmeticError('At least one iteration needed to calculate e^x')\n\n    # e^0 = 1\n    if (x == 0):\n        return float(1.0)\n\n    isNegative = False\n\n    # The algorithm always calculates e^x (x > 0) and then divides 1 by the\n    # result if x < 0. This avoids missing extra precission due to floating \n    # point.\n    if (x < 0):\n        isNegative = True\n        x *= -1\n\n    result = float(1.0)\n\n    for num in range(iterations, 0, -1):\n        # In the power series: = 1 + ((x/1)(1 + (x/2) (1 + (x/4) (...) ) ) )\n        # x/num is the same as (x/4), or (x/2), (x/1); result is the rightmost\n        # part of the series, which has been already calculated.\n        result = 1 + ((x * result)/num)\n\n    if isNegative:\n        result = float(1/result)\n\n    return float(result)",
        "sha1": "42b51ed8ac9e2092553f9390b4035958b7e3c39d",
        "id": 74507
    },
    {
        "content": "def import_object(name):\n    \"\"\"\n    Import an object from a module, by name.\n\n    :param name: The object name, in the ``package.module:name`` format.\n    :return: The imported object\n    \"\"\"\n\n    if name.count(':') != 1:\n        raise ValueError(\"Invalid object name: {0!r}. \"\n                         \"Expected format: '<module>:<name>'.\"\n                         .format(name))\n\n    module_name, class_name = name.split(':')\n    module = __import__(module_name, fromlist=[class_name])\n    return getattr(module, class_name)",
        "sha1": "7822570779519954f2e06c5451c704fd905eb48a",
        "id": 46738
    },
    {
        "content": "def rm_prefix(name):\n    \"\"\"\n    Removes nova_ os_ novaclient_ prefix from string.\n    \"\"\"\n    if name.startswith('nova_'):\n        return name[5:]\n    elif name.startswith('novaclient_'):\n        return name[11:]\n    elif name.startswith('os_'):\n        return name[3:]\n    else:\n        return name",
        "sha1": "c03a2e97cce14358ba289d5796047db12ea6f8a0",
        "id": 196166
    },
    {
        "content": "from typing import Counter\n\n\ndef build_vocab(train_corpus):\n    \"\"\"\n    Build a vocabulary with word frequencies for an entire corpus.\n    Returns: {word : (ID, frequency)}\n    \"\"\"\n\n    vocab = Counter()\n    for line in train_corpus:\n        for tokens in line:\n            vocab.update([tokens])\n\n    return {word: (i, freq) for i, (word, freq) in enumerate(vocab.items())}",
        "sha1": "1404a5219df07999a1d40045eb02ac7fd651b354",
        "id": 412591
    },
    {
        "content": "import copy\n\n\ndef _prune_keys(in_dict, *keys):\n    \"\"\"remove key(s) and their values if they exist in the dict.\"\"\"\n    dict_ = copy.deepcopy(in_dict)\n\n    for key in keys:\n        if dict_.get(key):\n            dict_.pop(key)\n    return dict_",
        "sha1": "cd759f7aba8c3305c1d66dafbb448223e6a23835",
        "id": 81286
    },
    {
        "content": "def info_to_dict(infos):\n    \"\"\"\n    Convert INFO field to dict\n    \"\"\"\n    ret = {}\n    for info in infos:\n        s = info.split(\"=\", 1)\n        if len(s) == 1:\n            ret[info] = None\n        else:\n            ret[s[0]] = s[1]\n    return ret",
        "sha1": "b6af4e2196850de5d751af2ea9e43d071e4ab79d",
        "id": 477739
    },
    {
        "content": "def detect_type(content):\n    \"\"\"\n    >>> detect_type([1, 2]) == list\n    True\n\n    >>> detect_type({\"a\": 1}) == dict\n    True\n\n    >>> detect_type(1) == \"object\"\n    True\n    \"\"\"\n    if isinstance(content, list):\n        return list\n    elif isinstance(content, dict):\n        return dict\n    else:\n        return \"object\"",
        "sha1": "945130265a2c361bbfe36dcc23cccb57c3dec659",
        "id": 373320
    },
    {
        "content": "def is_within_distance(number, target_number, distance):\n    \"\"\" returns true if the number is within 'distance' from the 'target_number' \"\"\"\n    actual_distance = abs(target_number - number)\n    return actual_distance <= distance",
        "sha1": "16f2fe738c7ff84e0503f5c38cbdefdad27bcaf5",
        "id": 342421
    },
    {
        "content": "def bound_size(b):\n    \"\"\"Length of the given bound, represented as a tuple.\"\"\"\n    return b[1]-b[0]",
        "sha1": "21f81b1a3cd889d4e2f5051d5d6b11e21b4aea90",
        "id": 546605
    },
    {
        "content": "def utility(game, state, player):\n    \"\"\"Return the value to player; 1 for win, -1 for loss, 0 otherwise.\"\"\"\n    return state.utility if player == 'W' else -state.utility",
        "sha1": "7e718af20e86967b4f7fff072e579c3acf9b2b5b",
        "id": 63916
    },
    {
        "content": "def convert_to_supported_type(variable) :\n\t\"\"\"\n\tConverts variables into types which are\n\tsupported by MySQL.\n\t\n\tTypes such as float64 are not supported\n\tby SQL. \n\t\n\tInspired by:\n\thttps://stackoverflow.com/questions/17053435/mysql-connector-python-insert-python-variable-to-mysql-table\n\t\n\t\"\"\"\n\t\n\tif type(variable).__name__ == 'float64' :\n\t\treturn float(variable)\n\t\t\n\telif type(variable).__name__ == 'int64' :\n\t\treturn int(variable)\n\t\t\n\telse :\n\t\treturn variable",
        "sha1": "cd40c55314ed98ed21e6cf5b127df4fd7bd4e031",
        "id": 185191
    },
    {
        "content": "def read_plink_ped(plink_ped_fp):\n    \"\"\"Read in plink .ped file.\"\"\"\n    parents_dict = {}\n    ped_dict = {}\n    with open(plink_ped_fp, \"rt\") as handle:\n        for record in handle:\n            if record.startswith(\"-9\"):\n                parents_dict[record.split()[1]] = record.split()\n            else:\n                ped_dict[record.split()[1]] = record.split()\n    return(parents_dict, ped_dict)",
        "sha1": "4367d48d0f5c39bf6e806c6330294c61fd86552a",
        "id": 125147
    },
    {
        "content": "def window_function(u):\n    \"\"\"\n    params:\n    - u: an iterable\n\n    return:\n    - 1 if u_i < 1/2 for all i \u2208 {1,2, ... , d}, and 0 otherwise\n    \"\"\"\n    for u_i in u:\n        if abs(u_i) >= 0.5:\n            return 0\n\n    return 1",
        "sha1": "4dfa6a9220aa1b9262e703a468228cd8354c6497",
        "id": 75402
    },
    {
        "content": "def args_to_str(\n        args,\n        positional_arg=None,\n        fields=None,\n        to_flag=True,\n        arg_joiner=' ',\n        val_sep=' ',\n        list_joiner=' ',\n        ):\n    \"\"\"Convert an argparse.ArgumentParser object back into a string,\n    e.g. for running an external command.\"\"\"\n    def val_to_str(v):\n        if isinstance(v, list):\n            return list_joiner.join(map(str, v))\n        return str(v)\n\n    def arg_to_str(k, v):\n        if to_flag:\n            k = f\"--{k.replace('_', '-')}\"\n        if v is True:\n            return k\n        if v is False:\n            return \"\"\n        else:\n            v = val_to_str(v)\n        return k + val_sep + v\n\n    if positional_arg:\n        pos_args = val_to_str(args.__dict__[positional_arg]) + val_sep\n    else:\n        pos_args = \"\"\n\n    if fields is None:\n        items = args.__dict__.items()\n    else:\n        items = [(k, args.__dict__[k]) for k in fields]\n\n    return pos_args + arg_joiner.join([\n        arg_to_str(k, v)\n        for k, v in items\n        if v is not None and k != positional_arg])",
        "sha1": "701efd4dfbadde94ee80b78504db720ac52d33a2",
        "id": 675775
    },
    {
        "content": "from typing import Tuple\n\n\ndef zipped(arr: Tuple[Tuple[float, ...], ...]) -> Tuple[float, ...]:\n    \"\"\"\n    >>> zipped(((89.0, 90.0, 78.0, 93.0, 80.0), (90.0, 91.0, 85.0, 88.0, 86.0),\n    ... (91.0, 92.0, 83.0, 89.0, 90.5)))\n    (90.0, 91.0, 82.0, 90.0, 85.5)\n    \"\"\"\n    arr_zipped = zip(*arr)\n    return tuple(sum(el) / len(el) for el in arr_zipped)",
        "sha1": "1954b6973379978b09c8ada20857e3187245dabc",
        "id": 378410
    },
    {
        "content": "def to_camel_case(snake_str: str) -> str:\n    \"\"\"\n    Converts snake case to camel case.\n\n    :param snake_str: The string in snake case to be converted\n    :return: The converted string in camel case\n    \"\"\"\n    components = snake_str.split('_')\n    return ' '.join(x.title() for x in components)",
        "sha1": "9b0a121636984bca575a688bcf9b626d506cc45a",
        "id": 387150
    },
    {
        "content": "def median(x):\n    \"\"\"Finds the median value of data set x\"\"\"\n    x_sorted = sorted(x)\n    mid = len(x) // 2\n\n    if len(x) % 2 == 1:\n        return x_sorted[mid]\n\n    else:\n        l = mid - 1\n        h = mid\n        return (x_sorted[l] + x_sorted[h]) / 2",
        "sha1": "87395bcbc1bc4800318a61cf7c4d3dbdc925486a",
        "id": 62531
    },
    {
        "content": "def index(lst, i):\n    \"\"\"\n    Return a value from a list at index\n\n    :param lst: list\n    :param i: index\n    :return:\n    \"\"\"\n    return lst[int(i)]",
        "sha1": "99222d20684ca40f7941c5b6c5ac47beb16495dd",
        "id": 293851
    },
    {
        "content": "def getKerasLayerType(layer):\n  \"\"\"Get the keras layer type name as a string\n  \"\"\"\n  return str(type(layer)).split('.')[-1][:-2]",
        "sha1": "61c2f3212fbd466e796191e9faa5ff81f153da0a",
        "id": 152415
    },
    {
        "content": "def get_key(json, key, default):\n    \"\"\"\n    Gets a defined key from a json object or returns a default if the key cant be found\n    :param json: json object to search key in\n    :param key: key to search for\n    :param default: default to return if no key is found\n    :return:\n    \"\"\"\n    try:\n        return json[key]\n    except KeyError:\n        return default",
        "sha1": "d4ce2374fdf2ec175de065609cac6268d1261780",
        "id": 502421
    },
    {
        "content": "def coords_to_lat_lon_lists(coords):\n    \n    \"\"\"Converts a list of coordinates in tuple form (latitude,longitude) to a list of latitudes and a list of longitudes.\n    \n    Parameters\n    ----------\n    coords : list(tuple)\n      a list of coordinates in tuple form (latitude,longitude)\n      \n    Returns\n    -------\n    lats : list\n      list of latitudes in input order\n    lons : list\n      list of longitudes in input order\n    \"\"\"\n    \n    lats = [c[0] for c in coords]\n    lons = [c[1] for c in coords]\n    \n    return lats,lons",
        "sha1": "8da18ee4afb4b3ace26c9e847b9314e9e38a5432",
        "id": 628904
    },
    {
        "content": "def _convert_to_list(value):\n    \"\"\"Convert a value to a list if it's not already a list\"\"\"\n    if value is None:  # empty value\n        return []\n    if not isinstance(value, list):\n        return [value]  # single value\n    return value",
        "sha1": "aef5dae34c31a478d6ded868db57c4c7eb26aac4",
        "id": 588432
    },
    {
        "content": "import hashlib\n\n\ndef _hash_value(value):\n    \"\"\"Hash value to help identify what cached file to use.\"\"\"\n    return hashlib.md5(value.encode('utf-8')).hexdigest()[:9]",
        "sha1": "cd7a965cb148d6e2b5dd47e0b68907b60ba7f459",
        "id": 290431
    },
    {
        "content": "def write_select_topics(topics_dict):\n    \"\"\"Create a string for a SELECT part of an SQL query for a fact table\n\n    Given a dictionary key-value pairs, this function outputs a string to be\n    used as part of an SQL SELECT section. This is meant to be used when\n    flatenning a table, and the given dict should contain all Key-Title pairs\n    of the fact table to be used.\n\n    For more details: https://www.cbs.nl/-/media/statline/documenten/handleiding-cbs-opendata-services.pdf?la=nl-nl\n    \"\"\"\n\n    string = \"\"\n    for key, title in topics_dict.items():\n        string += f\"\\n    , fct.{key} AS {title.lower().replace(' ', '_').replace('(', '').replace(')', '').replace('%', 'per').replace(',', '')}\"\n    return string",
        "sha1": "4313834ffb709fec8f2975849ce097433e88bd64",
        "id": 158783
    },
    {
        "content": "def extract_main_domains(df):\n    \"\"\"Create column with the main domains.\n\n    The main domains were picked by looking at the data and going with what made\n    sense (there is no clear rule for defining them).\n    \"\"\"\n    main_domains = ['Epilepsy', 'Sleep', 'BCI', 'Affective', 'Cognitive', \n                    'Improvement of processing tools', 'Generation of data']\n    domains_df = df[['Domain 1', 'Domain 2', 'Domain 3', 'Domain 4']]\n    df['Main domain'] = [row[row.isin(main_domains)].values[0] \n        if any(row.isin(main_domains)) else 'Others' \n        for ind, row in domains_df.iterrows()]\n\n    return df",
        "sha1": "1c1815c8c3b13445e9ae149949bd8c346b6f529a",
        "id": 187068
    },
    {
        "content": "def allowed_file(filename: str) -> bool:\n\n    \"\"\"\n    Check if the input file has the correct format\n\n    Args:\n        filename: the filename of the uploaded image\n\n    Returns:\n        if the filename has the right extension.\n    \"\"\"\n\n    allowed_extensions = {'png', 'jpg', 'jpeg'}\n\n    return '.' in filename and \\\n           filename.rsplit('.', 1)[1].lower() in allowed_extensions",
        "sha1": "51f1c2f40b5479344645495ebf3fca44f7d3c9b4",
        "id": 396966
    },
    {
        "content": "def get_uid(item):\n    \"\"\"\n    \u83b7\u53d6item\u7684uid\n    :param item: item\u6570\u636e\uff0cdict\u578b\n    :return: item\u7684uid\n    \"\"\"\n    return item.get('uid')",
        "sha1": "574566eb100db4d06b8b05a6269df45f343671cb",
        "id": 574278
    },
    {
        "content": "from typing import Tuple\n\n\ndef hex_to_rgb(hex_value: str) -> Tuple[int, int, int]:\n    \"\"\"\n    Converts a valid hexadecimal color string given by '#XXXXXX' to a tuple representing RGB values.\n    If the string representation does not match the above description, a ValueError is raised.\n    :param hex_value: String representation of hexadecimal color.\n    :return: Tuple of RGB color values: (R,G,B)\n    \"\"\"\n    if len(hex_value) != 7 or hex_value[0] != \"#\":\n        raise ValueError(\n            \"the color has to be specified by '#XXXXXX'. Invalid value %s\" % hex_value\n        )\n    hex_value = hex_value.lstrip(\"#\")\n    try:\n        int(hex_value, 16)\n    except ValueError:\n        raise ValueError(\n            \"the color value has to be a valid hexadecimal number. Invalid value %s\"\n            % hex_value\n        )\n    return int(hex_value[0:2], 16), int(hex_value[2:4], 16), int(hex_value[4:6], 16)",
        "sha1": "2fbbfd2958c86a8e36d5e26f58ecf566873ce79b",
        "id": 467670
    },
    {
        "content": "import requests\nimport json\n\n\ndef get_TIME_SERIES_DAILY(ticker):\n    \"\"\"Calls the Alpha Vantage TIME_SERIES_DAILY API to get time series daily\n\n    Params:\n        1) ticker\n            type: string\n            desc: stock symbol\n    Return Value:\n        json response from the API call\n    \n    API Documentation:\n        https://www.alphavantage.co/documentation/\n    \"\"\"\n    apikey = 'W6Y6NLWDMM2FVX9B'\n    fullurl = \"https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol=\" + ticker + \"&apikey=\" + apikey\n    r = requests.get(fullurl)\n    r_json = json.loads(r.text)\n    return r_json",
        "sha1": "89f3729c6e8f685acb2f60818872261dfb14c9bc",
        "id": 220783
    },
    {
        "content": "def get_uniprot_secondaries(uniprot_result):\n    \"\"\"\n    Get secondary accession identifiers from UniProt text result\n    :param uniprot_result:\n    :return:\n    \"\"\"\n    accession_lines = [l for l in uniprot_result.split('\\n') if l.startswith('AC')]\n    secondaries = []\n    for ac_line in accession_lines:\n        secondaries += ['UniProt:{}'.format(uid[:-1]) for uid in ac_line.split()[1:]]\n    return secondaries",
        "sha1": "bca4cced24d2aa8f1200aee7d4bf0f05c5913bb3",
        "id": 625066
    },
    {
        "content": "def uses_only(word, letters):\n    \"\"\"return true if word only use a set letters\"\"\"\n    letters = letters.lower()\n    for letter in word:\n        if letter.lower() not in letters:\n            return False\n    return True",
        "sha1": "c0118b10e90bb61186419c68fa710353425c081c",
        "id": 53673
    },
    {
        "content": "def hit(filenames, method, *args, **kwargs):\n    \"\"\"\n    Run the given accessor method with args & kwargs; if found remove the\n    result path from filenames and return True, else return False.\n    \"\"\"\n    try:\n        medium = method(*args, **kwargs)\n        assert medium.exists\n    except ValueError:\n        return False\n    except:\n        print('Error while processing', method, args, kwargs)\n        raise\n    try:\n        filenames.remove(medium.path)\n    except KeyError:\n        pass\n    return True",
        "sha1": "dcd025fe3e299290cf43e661d934e845d5c325dd",
        "id": 278964
    },
    {
        "content": "def _page_to_title(page):\n  \"\"\"Extract the title from a page.\n\n  Args:\n    page: a unicode string\n  Returns:\n    a unicode string\n  \"\"\"\n  # print(\"page=%s\" % page)\n  start_tag = u\"<title>\"\n  end_tag = u\"</title>\"\n  start_pos = page.find(start_tag)\n  end_pos = page.find(end_tag)\n  assert start_pos != -1\n  assert end_pos != -1\n  start_pos += len(start_tag)\n  return page[start_pos:end_pos]",
        "sha1": "ce480f619f9e10c52dc5d4c03f156038919bf168",
        "id": 143498
    },
    {
        "content": "def copy_dict(source_dict, diffs):\n    \"\"\"Returns a copy of source_dict, updated with the new key-value pairs in diffs.\"\"\"\n    result = dict(source_dict)\n    result.update(diffs)\n    return result",
        "sha1": "971ea9e79d5a3b279d69d578464d767988891494",
        "id": 23013
    },
    {
        "content": "def has_prefix(x: list):\n    \"\"\"Returns \"True\" if some value in the list is a prefix for another value in this list.\"\"\"\n    for val in x:\n        if len(list(filter(val.startswith, x))) > 1:\n            return True\n\n    return False",
        "sha1": "e34bcadfb847ccd04dce3c72f2ca46d5720e5db3",
        "id": 409829
    },
    {
        "content": "import six\n\n\ndef _Net_backward(self, diffs=None, start=None, end=None, **kwargs):\n    \"\"\"\n    Backward pass: prepare diffs and run the net backward.\n\n    Parameters\n    ----------\n    diffs : list of diffs to return in addition to bottom diffs.\n    kwargs : Keys are output blob names and values are diff ndarrays.\n            If None, top diffs are taken from forward loss.\n    start : optional name of layer at which to begin the backward pass\n    end : optional name of layer at which to finish the backward pass\n        (inclusive)\n\n    Returns\n    -------\n    outs: {blob name: diff ndarray} dict.\n    \"\"\"\n    if diffs is None:\n        diffs = []\n\n    if start is not None:\n        start_ind = list(self._layer_names).index(start)\n    else:\n        start_ind = len(self.layers) - 1\n\n    if end is not None:\n        end_ind = list(self._layer_names).index(end)\n        outputs = set(self.bottom_names[end] + diffs)\n    else:\n        end_ind = 0\n        outputs = set(self.inputs + diffs)\n\n    if kwargs:\n        if set(kwargs.keys()) != set(self.outputs):\n            raise Exception('Top diff arguments do not match net outputs.')\n        # Set top diffs according to defined shapes and make arrays single and\n        # C-contiguous as Caffe expects.\n        for top, diff in six.iteritems(kwargs):\n            if diff.shape[0] != self.blobs[top].shape[0]:\n                raise Exception('Diff is not batch sized')\n            self.blobs[top].diff[...] = diff\n\n    self._backward(start_ind, end_ind)\n\n    # Unpack diffs to extract\n    return {out: self.blobs[out].diff for out in outputs}",
        "sha1": "4af46a2a55c17025d8320921c6d0a6e4ad0c5ba4",
        "id": 481429
    },
    {
        "content": "def validate(name, re=None, convert=None, doc=None, mandatory=True):\n    \"\"\"Decorator. Apply on a :class:`wsgiservice.Resource` or any of it's\n    methods to validates a parameter on input. When a parameter does not\n    validate, a :class:`wsgiservice.exceptions.ValidationException` exception\n    will be thrown.\n\n    :param name: Name of the input parameter to validate.\n    :type name: string\n    :param re: Regular expression to search for in the input parameter. If\n               this is not set, just validates if the parameter has been set.\n    :type re: regular expression\n    :param convert: Callable to convert the validated parameter value to the\n                    final data type. Ideal candidates for this are the\n                    built-ins int or float functions. If the function raises a\n                    ValueError, this is reported to the client as a 400 error.\n    :type convert: callable\n    :param doc: Parameter description for the API documentation.\n    :type doc: string\n    :param mandatory: Whether the parameter is mandatory. By default this is\n        `True`.\n    :type mandatory: bool\n    \"\"\"\n\n    def wrap(cls_or_func):\n        if not hasattr(cls_or_func, '_validations'):\n            cls_or_func._validations = {}\n        cls_or_func._validations[name] = {\n            're': re,\n            'convert': convert,\n            'doc': doc,\n            'mandatory': mandatory,\n        }\n        return cls_or_func\n    return wrap",
        "sha1": "71e9a15d6a2d34e33579ea37737b8f345a8fd6ed",
        "id": 380812
    },
    {
        "content": "from typing import Optional\nimport zipfile\n\n\ndef might_contain_dag(file_path: str, safe_mode: bool, zip_file: Optional[zipfile.ZipFile] = None):\n    \"\"\"\n    Heuristic that guesses whether a Python file contains an Airflow DAG definition.\n\n    :param file_path: Path to the file to be checked.\n    :param safe_mode: Is safe mode active?. If no, this function always returns True.\n    :param zip_file: if passed, checks the archive. Otherwise, check local filesystem.\n    :return: True, if file might contain DAGs.\n    \"\"\"\n    if not safe_mode:\n        return True\n    if zip_file:\n        with zip_file.open(file_path) as current_file:\n            content = current_file.read()\n    else:\n        if zipfile.is_zipfile(file_path):\n            return True\n        with open(file_path, 'rb') as dag_file:\n            content = dag_file.read()\n    content = content.lower()\n    return all(s in content for s in (b'dag', b'airflow'))",
        "sha1": "8650926ebd2b09324699cc563c5f01c4b0b67df7",
        "id": 345959
    },
    {
        "content": "import torch\n\n\ndef labels_and_weights(label_file_df):\n    \"\"\" \n     Get list of unique sample labels and weights of the samples using\n     the inverse of the count. Weights is a tensor to be compatible with\n     CrossEntropyLoss. \n    \"\"\"\n    labels_all = label_file_df.iloc[:,-1].astype(str).values.tolist()\n    labels_unique = set(labels_all)\n    labels = sorted(labels_unique)\n    \n    labels_count = [labels_all.count(label) for label in labels]\n    weights = 1. / torch.tensor(labels_count, dtype=torch.float) \n    \n    return labels, weights",
        "sha1": "4406b2bb1db6fb2a0f42eeae0c4ac5a093906606",
        "id": 501900
    },
    {
        "content": "def othertitles(hit):\n    \"\"\"Split a hit.Hit_def that contains multiple titles up, splitting out the hit ids from the titles.\"\"\"\n    id_titles = hit.Hit_def.text.split('>')\n\n    titles = []\n    for t in id_titles[1:]:\n        fullid, title = t.split(' ', 1)\n        hitid, id = fullid.split('|', 2)[1:3]\n        titles.append(dict(id = id,\n                           hitid = hitid,\n                           fullid = fullid,\n                           title = title))\n    return titles",
        "sha1": "fa5bbb47d26adbc61817e78e950e81cc05eca4a6",
        "id": 3974
    },
    {
        "content": "import random\n\n\ndef gen_conference_challenge(length: int):\n    \"\"\"Generate a random challenge for Zoom.\"\"\"\n    if length > 10:\n        length = 10\n    field = \"abcdefghijklmnopqrstuvwxyz01234567890ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n    return \"\".join(random.sample(field, length))",
        "sha1": "4c1f064df4300b479e708972da1d9d34abf1618a",
        "id": 253005
    },
    {
        "content": "def scanDataSet(dataset, Ck, minSupport):\n    \"\"\"\n    Function to generate Lk from Ck\n    :param D: dataset\n    :param Ck: candidates of k items\n    :param minSupport: value of support value of interest\n    :return: return Lk, dict of support values\n    \"\"\"\n    # we create a dict of value to count the amount of transaction supported by candidate\n    subsetCounter = {}\n    for transaction in dataset:\n        for candidate in Ck:\n            if candidate.issubset(transaction):\n                if not candidate in subsetCounter: subsetCounter[candidate]=1\n                else: subsetCounter[candidate] += 1\n    number_items = float(len(dataset))\n    Lk = []\n    supportData = {}\n    # candidate in Ck are in Lk if they support minimum value\n    for subsetCandidate in subsetCounter:\n        support = subsetCounter[subsetCandidate] / number_items\n        if support >= minSupport:\n            Lk.insert(0, subsetCandidate)\n        supportData[subsetCandidate] = support\n    return Lk, supportData",
        "sha1": "207b928f86982c613c9c6977c068c9cf3c30a291",
        "id": 272076
    },
    {
        "content": "def _min_or_none(itr):\n    \"\"\"\n    Return the lowest value in itr, or None if itr is empty.\n\n    In python 3, this is just min(itr, default=None)\n    \"\"\"\n    try:\n        return min(itr)\n    except ValueError:\n        return None",
        "sha1": "4d93f681aa80668886ab3cbb310a5c60349150a8",
        "id": 548303
    },
    {
        "content": "def _parse_coord(sample, variables):\n    \"\"\"\n    See if the variables in the sample are a valid encoding of a coordinate.\n\n    The encoding is defined this way:\n        For n-1 variables, the axis' range is {0, 1, ..., n}.\n        A coordinate with value x is encoded as\n            v_i = 0 for x <= i < n - 1\n            v_i = 1 for 0 <= i < x\n    \"\"\"\n    coord = 0\n    for v, v_index in enumerate(variables):\n        if sample[v_index] == 1:\n            coord = v + 1\n            if v != 0 and sample[v_index - 1] == 0:\n                # Not a valid encoding\n                return None\n    return coord",
        "sha1": "d736f8ebbfead4191abfa320627e510f8240e294",
        "id": 562729
    },
    {
        "content": "def getAveIncome(cluster):\n    \"\"\"\n    Given a Cluster object, finds the average income field over the members\n    of that cluster.\n    \n    cluster: the Cluster object to check\n    \n    Returns: a float representing the computed average income value\n    \"\"\"\n    tot = 0.0\n    numElems = 0\n    for c in cluster.getPoints():\n        tot += c.getOriginalAttrs()[1]\n\n    return float(tot) / len(cluster.getPoints())",
        "sha1": "7ae31dfbe7aa7396ec8a5ef174118785a9db163d",
        "id": 276124
    },
    {
        "content": "def getQuaternionFromDict(d):\n    \"\"\"\n    Get the quaternion from a dict describing a transform. The dict entry could be\n    one of orientation, rotation, quaternion depending on the convention\n    \"\"\"\n    quat = None\n    quatNames = ['orientation', 'rotation', 'quaternion']\n    for name in quatNames:\n        if name in d:\n            quat = d[name]\n\n\n    if quat is None:\n        raise ValueError(\"Error when trying to extract quaternion from dict, your dict doesn't contain a key in ['orientation', 'rotation', 'quaternion']\")\n\n    return quat",
        "sha1": "e2f9edef19af914df97357a53a7a4086d5d91c56",
        "id": 299240
    },
    {
        "content": "def _integer_surround(number):\n    \"\"\"Return the 2 closest integers to number, smaller integer first.\"\"\"\n    if number > 0:\n        return int(number), int(number) + 1\n    else:\n        return int(number) - 1, int(number)",
        "sha1": "a309930e3d7992008c08085a1ea121480de1396e",
        "id": 639294
    },
    {
        "content": "def namefromcycle(cyclenumber, version='1.2'):\n\n    \"\"\"Return the dataset name corresponding to the given cycle number.\n\n    SE 1.0 and 1.1 used dashes; SE 1.2 uses zero-padded integers with\n    10-digit field width.\n    \"\"\"\n\n    if version == '1.2':\n        return 'cycle%010d' % cyclenumber\n    else:\n        return 'cycle-' + str(cyclenumber)",
        "sha1": "45964c2788acfe6b19451aeb4c5e908732b3b27b",
        "id": 598045
    },
    {
        "content": "def test_lessthan(value, other):\n    \"\"\"Check if value is less than other.\"\"\"\n    return value < other",
        "sha1": "199d674e58b55a6c5ceb49136e35ace0067c5289",
        "id": 159961
    },
    {
        "content": "def split_int(i, floats):\n    \"\"\"\n    Prepare a list of integers proportional to `floats` which sum up to `i`.\n    :param i: integer to be split, equal to the sum of returned integers.\n    :type i: int\n    :param floats: floats to which individual integers will be proportional to.\n    :type floats: tuple[float]\n    :return: List of integers, each proportional to float on `floats` list.\n    :rtype: tuple[int]\n    \"\"\"\n\n    def split_int_from_sorted(j, sorted_floats):\n        if all([f <= 0 for f in sorted_floats]):\n            return tuple([0 for _ in sorted_floats])\n        elif len(sorted_floats) == 1:\n            return (j,)\n        else:\n            chip = int(round(j * sorted_floats[-1] / sum(sorted_floats)))\n            return (*split_int_from_sorted(j - chip, sorted_floats[:-1]), chip)\n\n    ints = split_int_from_sorted(i, sorted(floats))\n    float_order = sorted(range(len(floats)), key=lambda k: floats[k])\n    float_ord_indexes = sorted(range(len(floats)), key=lambda k: float_order[k])\n    return (ints[i] for i in float_ord_indexes)",
        "sha1": "17489791952570120fc3043c1c38d1c53ecf0177",
        "id": 192731
    },
    {
        "content": "def manage_addBooleanIndex(self, id, extra=None,\n                           REQUEST=None, RESPONSE=None, URL3=None):\n    \"\"\"Add a boolean index\"\"\"\n    return self.manage_addIndex(\n        id, 'BooleanIndex', extra=extra,\n        REQUEST=REQUEST, RESPONSE=RESPONSE, URL1=URL3)",
        "sha1": "e8a43ee859466bf1d8362ffad242eeeee954c199",
        "id": 671133
    },
    {
        "content": "def COUNT(*args):\n  \"\"\"\n  Builtin count aggregator for groupby\n\n  Example: Get the number of occurrences of each user.\n\n  >>> sf.groupby(\"user\",\n  ...            {'count':tc.aggregate.COUNT()})\n\n  \"\"\"\n  # arguments if any are ignored\n  return (\"__builtin__count__\", [\"\"])",
        "sha1": "0bd74cc44593fbbb06409e4b7a9b13bab3ecaca2",
        "id": 467309
    },
    {
        "content": "import torch\n\n\ndef dis_primal(\n    input_view: torch.Tensor,\n    param: torch.Tensor,\n    n_sample: int,\n) -> torch.Tensor:\n  \"\"\"Computes distortion penalty for the primal formulation.\n\n  Let n be the number of samples in the view of interest and p the number of\n  features. In the primal formulation, the 'param' matrix is the p*low_dim model\n  parameter and input_view corresponds to the input data, of shape n*p. The\n  distortion penalty can be written as\n\n  distortion = ||input_view*input_view.T\n                 - input_view*param*param.T*input_view.T||_2.\n\n  The distortion is computed as is when n < p. However, if n > p, we\n  compute the following formulation:\n\n  distortion = torch.sqrt(Tr((I - param*param.T)*input_view.T*input_view\n  *(I - param*param.T)*input_view.T*input_view))\n\n  to avoid computing terms that are O(n**2) in memory or runtime.\n\n  Arguments:\n    input_view: torch.Tensor, one of the two views.\n    param: torch.Tensor, model parameters.\n    n_sample: int, sample size of entire dataset.\n  Returns:\n    distortion_value: torch.Tensor, scalar value.\n  \"\"\"\n  n_sample, p_feature = input_view.shape\n  if n_sample < p_feature:\n    inner_prod = torch.matmul(input_view, input_view.t())\n    tmp = torch.matmul(torch.matmul(\n        torch.matmul(input_view, param), param.t()), input_view.t())\n    tmp = (inner_prod - tmp)**2\n    distortion_value = torch.sqrt(torch.sum(tmp))\n  else:\n    gram = torch.matmul(input_view.t(), input_view)\n    tmp = torch.matmul(param, torch.matmul(param.t(), gram))\n    prod = gram - tmp\n    distortion_value = torch.sqrt(torch.trace(torch.matmul(prod, prod)))\n  return distortion_value",
        "sha1": "6826994ce40799e5b83059d158bd65f50c381622",
        "id": 21130
    },
    {
        "content": "def rAsciiLine(ifile):\n    \"\"\"Returns the next non-blank line in an ASCII file.\"\"\"\n\n    _line = ifile.readline().strip()\n    while len(_line) == 0:\n        _line = ifile.readline().strip()\n    return _line",
        "sha1": "6bc4e8ea18f6a0a5252daa25b246dc00a0383458",
        "id": 418771
    },
    {
        "content": "def overlapping_community(G, community):\n    \"\"\"Return True if community partitions G into overlapping sets.\n    \"\"\"\n    community_size = sum(len(c) for c in community)\n    # community size must be larger to be overlapping\n    if not len(G) < community_size:\n        return False\n    # check that the set of nodes in the communities is the same as G\n    if not set(G) == set.union(*community):\n        return False\n    return True",
        "sha1": "da9e3465c6351df0efd19863e579c49bbc6b9d67",
        "id": 707432
    },
    {
        "content": "import typing\n\n\ndef unpack_key_value_pair(entry) -> typing.Tuple[object, object]:\n    \"\"\"ensure the given entry can be unpacked into a key/value pair\n\n    :param entry: an iterable element with exactly two elements\n    :return: a tuple containing the key value pair\n    :raises: ValueError when the entry can't be unpacked\n    \"\"\"\n    try:\n        k, v = entry\n    except (TypeError, ValueError, AttributeError):\n        raise ValueError(f\"{repr(entry)} is not a key/value pair\")\n    return k, v",
        "sha1": "8094e4d58439c6ea91fafd12b06b3c0104af003c",
        "id": 640665
    },
    {
        "content": "def all_success(mgmt_commands):\n  \"\"\"Determines if all child processes were successful.\n\n  Args:\n    mgmt_commands : A list of all Command objects\n\n  Returns:\n    True if all child processes succeeded\n  \"\"\"\n\n  for mgmt_command in mgmt_commands:\n    if mgmt_command.retcode != 0:\n      return False\n  return True",
        "sha1": "1bc0d32491711e0d20106f1f294093b30e77bd55",
        "id": 8566
    },
    {
        "content": "def probability(problem, train_ixs, obs_labels, selected_ixs, batch_size, **kwargs):\n    \"\"\"\n    Score is simply the probability of being a target under current model.\n    :param problem: dictionary that defines the problem, containing keys:\n        * points:       an (n_samples, n_dim) matrix of points in the space\n        * num_classes:  the number of different classes [0, num_classes)\n        * batch_size:   number of points to query each iteration\n        * num_queries:  the max number of queries we can make on the data\n        * model:        the sk-learn model we are training\n    :param train_ixs: index into `points` of the training examples\n    :param obs_labels: labels for the training examples\n    :param selected_ixs: indices into problem['points'] to score\n    :param kwargs: unused\n    :return: scores for each of selected_ixs\n    \"\"\"\n    points = problem['points']\n    model = problem['model']\n\n    test_X = points[selected_ixs]\n\n    p_x = model.predict_proba(test_X)\n\n    return p_x[:,1].reshape(-1)",
        "sha1": "7df47b9cc8243c14df4208a773b167a36c1619fd",
        "id": 497777
    },
    {
        "content": "def _dummy_boxstream(stream, **kwargs):\n    \"\"\"Identity boxstream, no tansformation.\"\"\"\n    return stream",
        "sha1": "7ff65f1860c2e18149c496e135c28f43ccc7a980",
        "id": 690744
    },
    {
        "content": "def IsBlankLine(line):\n  \"\"\"Returns true if the given line is blank.\n\n  We consider a line to be blank if the line is empty or consists of\n  only white spaces.\n\n  Args:\n    line: A line of a string.\n\n  Returns:\n    True, if the given line is blank.\n  \"\"\"\n  return not line or line.isspace()",
        "sha1": "2353eebce8f33890d7e2a62a3d627365ac1cbea6",
        "id": 512990
    },
    {
        "content": "def loo(X, axis=0):\n    \"\"\"Generates leave-one-out selections along a given axis.\n\n    In the first iteration, the first item is left out. In following\n    iterations, each item to leave out is replaced with the first item. This\n    allows us to only make a single copy and perform all other operations\n    in-place.\n\n    Parameters\n    ----------\n    X : ndarray\n        The array to produce leave-one-out selections of.\n    axis : int (default: 0)\n        The axis along which to perform the leave-one-out selections.\n\n    Yields\n    ------\n    X_loo : ndarray\n        A copy of X, with the n'th item left out.\n    \"\"\"\n    # Generate an index that cuts off the top along the desired axis.\n    # We use slices to avoid copying data.\n    cut_top = tuple([slice(None) if i != axis else slice(1, None)\n                    for i in range(X.ndim)])\n\n    # Generate an index that returns a slice at a given index along the desired\n    # axis, without copying data.\n    def slice_at(ind):\n        return tuple([slice(None) if i != axis else ind\n                      for i in range(X.ndim)])\n\n    X_loo = None\n    for i in range(X.shape[axis]):\n        if X_loo is None:\n            X_loo = X[cut_top].copy()\n        else:\n            if i >= 2:\n                X_loo[slice_at(i - 2)] = X[slice_at(i - 1)]\n            X_loo[slice_at(i - 1)] = X[slice_at(0)]\n        yield X_loo",
        "sha1": "8285605035e1fee51393b01c9f9e19e396ce5375",
        "id": 572073
    },
    {
        "content": "def _add_dicts(*dictionaries):\n    \"\"\"Adds a list of dictionaries into a single dictionary.\"\"\"\n\n    # If keys are repeated in multiple dictionaries, the latter one \"wins\".\n    result = {}\n    for d in dictionaries:\n        result.update(d)\n\n    return result",
        "sha1": "04319e0ffe62a40a27f24e6f3a17ca75528c5b29",
        "id": 519330
    },
    {
        "content": "import json\n\n\ndef _load_configurations(filename):\n    \"\"\"\n    Load JSON configuration file in a dict.\n        :param filename: name of the JSON file containing the configuarions.\n    \"\"\"\n    return json.loads(open(filename).read())",
        "sha1": "ce3d6aa8612149e4809df8c6375a0255c3eae5ed",
        "id": 93820
    },
    {
        "content": "def get_keys_from_value(value, _dict, case_sensitive=False):\n    \"\"\"Returns list of keys in dict with value given\"\"\"\n\n    matching_keys = []\n    for key in _dict.keys():\n        for val in _dict.get(key):\n            if not case_sensitive:\n                if value.lower() == val.lower():\n                    matching_keys.append(key)\n            else:\n                if value == val:\n                    matching_keys.append(key)\n    return matching_keys",
        "sha1": "2673214398c2cc499f7bce7d8d3ff91717e8fbd8",
        "id": 346602
    },
    {
        "content": "import pickle\n\n\ndef read_pickle(pkl_path):\n    \"\"\"Returns loaded data from a pickle file.\"\"\"\n    with open(pkl_path, 'rb') as f:\n        data = pickle.load(f)\n    return data",
        "sha1": "219eac3d35c3cc10408921467e2749a93d276d96",
        "id": 590835
    },
    {
        "content": "def int_or_float(x):\n    \"\"\"Return int or float value of x.\n\n    Args:\n        x (str): Value to convert.\n\n    Returns:\n        int or float:\n\n    Raises:\n        ValueError:\n    \"\"\"\n    try:\n        return int(x)\n    except ValueError:\n        try:\n            return float(x)\n        except ValueError:\n            raise ValueError('invalid literal for int_or_float(): \\'{}\\''.format(x))",
        "sha1": "9c00267c0ade842cc9037b59e129ceaa15690f7a",
        "id": 569880
    },
    {
        "content": "def get_datetime_str(dt):\n    \"\"\"\n    >>> dt = datetime.datetime.now()\n    >>> def get_datetime_str(dt):\n    ...     return dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n    >>> get_datetime_str(dt)\n    '2021-04-11 17:19:10'\n    \"\"\"\n    return dt.strftime(\"%Y-%m-%d %H:%M:%S\")",
        "sha1": "5c657ab1d43879cb16c80b4c89061864f6827d18",
        "id": 636838
    },
    {
        "content": "def base36decode(base36_string):\n    \"\"\"Converts base36 string into integer.\"\"\"\n    return int(base36_string, 36)",
        "sha1": "66da9d391705cd0748e0e7c0ea5c69be2366ed4e",
        "id": 22527
    },
    {
        "content": "import torch\n\n\ndef read_subgraphs(sub_f, split=True):\n    \"\"\"\n    Read subgraphs from file\n\n    Args\n       - sub_f (str): filename where subgraphs are stored\n\n    Return for each train, val, test split:\n       - sub_G (list): list of nodes belonging to each subgraph\n       - sub_G_label (list): labels for each subgraph\n    \"\"\"\n\n    # Enumerate/track labels\n    label_idx = 0\n    labels = {}\n\n    # Train/Val/Test subgraphs\n    train_sub_G = []\n    val_sub_G = []\n    test_sub_G = []\n\n    # Train/Val/Test subgraph labels\n    train_sub_G_label = []\n    val_sub_G_label = []\n    test_sub_G_label = []\n\n    # Train/Val/Test masks\n    train_mask = []\n    val_mask = []\n    test_mask = []\n\n    multilabel = False\n\n    # Parse data\n    with open(sub_f) as fin:\n        subgraph_idx = 0\n        for line in fin:\n            nodes = [int(n) for n in line.split(\"\\t\")[0].split(\"-\") if n != \"\"]\n            if len(nodes) != 0:\n                if len(nodes) == 1:\n                    print(nodes)\n                label = line.split(\"\\t\")[1].split(\"-\")\n                if len(label) > 1:\n                    multilabel = True\n                for lab in label:\n                    if lab not in labels.keys():\n                        labels[lab] = label_idx\n                        label_idx += 1\n                if line.split(\"\\t\")[2].strip() == \"train\":\n                    train_sub_G.append(nodes)\n                    train_sub_G_label.append([labels[lab] for lab in label])\n                    train_mask.append(subgraph_idx)\n                elif line.split(\"\\t\")[2].strip() == \"val\":\n                    val_sub_G.append(nodes)\n                    val_sub_G_label.append([labels[lab] for lab in label])\n                    val_mask.append(subgraph_idx)\n                elif line.split(\"\\t\")[2].strip() == \"test\":\n                    test_sub_G.append(nodes)\n                    test_sub_G_label.append([labels[lab] for lab in label])\n                    test_mask.append(subgraph_idx)\n                subgraph_idx += 1\n    if not multilabel:\n        train_sub_G_label = torch.tensor(train_sub_G_label).long().squeeze()\n        val_sub_G_label = torch.tensor(val_sub_G_label).long().squeeze()\n        test_sub_G_label = torch.tensor(test_sub_G_label).long().squeeze()\n\n    if len(val_mask) < len(test_mask):\n        return (\n            train_sub_G,\n            train_sub_G_label,\n            test_sub_G,\n            test_sub_G_label,\n            val_sub_G,\n            val_sub_G_label,\n        )\n\n    return (\n        train_sub_G,\n        train_sub_G_label,\n        val_sub_G,\n        val_sub_G_label,\n        test_sub_G,\n        test_sub_G_label,\n    )",
        "sha1": "b0c1455bf0efc813a4cffc23f8735cb3a258989d",
        "id": 609262
    },
    {
        "content": "import random\n\n\ndef get_random_mac() -> str:\n    \"\"\"Generates random MAC address\"\"\"\n    return ':'.join(f'{random.randint(0, 255):02x}' for _ in range(6))",
        "sha1": "d72ad2590ccd4e8d387748732527fab9d7587c89",
        "id": 359866
    },
    {
        "content": "from typing import List\n\n\ndef parse_sra_file(sra_file: str) -> List[str]:\n    \"\"\"Parse sra file and return a list of SRA\n\n    Args:\n        sra_file: name of sra file. contains one srr accession id per line\n\n    Returns:\n        List of sra accession\n    \"\"\"\n    sra_ids = []\n    with open(sra_file) as fh_in:\n        for line in fh_in:\n            line = line.strip()\n            if line:\n                sra_ids.append(line.strip())\n    return sra_ids",
        "sha1": "4621e6eee584cc00242b467a43de1923744a4930",
        "id": 496440
    },
    {
        "content": "def find_pos_neg(pos_matrix, threshold_matrix):\n    \"\"\"Returns the number of true positives, false positives, true negatives, and false negatives.\"\"\"\n\n    TP = 0\n    FP = 0\n    TN = 0\n    FN = 0\n\n    for x in range(len(pos_matrix)):\n        for y in range(len(pos_matrix)):\n            true_value = pos_matrix[x][y]\n            if true_value == threshold_matrix[x][y]:\n                if true_value == 1:\n                    TP += 1\n                else:\n                    TN += 1\n            else:\n                if true_value == 0:\n                    FP += 1\n                else:\n                    FN += 1\n\n    return TP, FP, TN, FN",
        "sha1": "256cfcef9f2985a7fb915a13374248e097ed90f4",
        "id": 523335
    },
    {
        "content": "import re\n\n\ndef validate_afm(afm: str, extended_result: bool = False):\n    \"\"\"Checks if the passed AFM is a valid AFM number\n\n    Parameters\n    ----------\n    afm : str\n        A string to be check if it's a valid AFM\n    extended_result : bool, optional\n        Return extended object result if True, single boolean otherwise (default is False)\n    \n    Returns\n    -------\n    str or dict\n        A boolean result or a dictionary indicating the validation of the number\n    \"\"\"\n    if len(afm) != 9:\n        return {\n            'valid': False,\n            'error': \"length\"\n        } if extended_result else False\n\n    if not re.match(r\"^\\d+$\", afm):\n        return {\n            'valid': False,\n            'error': \"nan\"\n        } if extended_result else False\n\n    if afm == \"0\" * 9:\n        return {\n            'valid': False,\n            'error': \"zero\"\n        } if extended_result else False\n\n    body = afm[:8]\n    weighted_sum = 0\n    \n    for i in range(len(body)):\n        digit = body[i]\n        weighted_sum += int(digit) << (8 - i)\n    \n    calc = weighted_sum % 11\n    d9 = int(afm[8])\n    valid = calc % 10 == d9\n\n    if extended_result:\n        return {\n            'valid': valid\n        } if valid else {\n            'valid': valid,\n            'error': 'invalid'\n        }\n    \n    return valid",
        "sha1": "9eda891b1c410f55d0af118f4d2b1e35188e9ced",
        "id": 108900
    },
    {
        "content": "import random\n\n\ndef random_crop2d(*images, min_perc=0.5, max_perc=1.):\n    \"\"\"Crop randomly but identically all images given.\n\n    Could be used to pass both mask and image at the same time. Anything else will\n    throw.\n\n    Warnings\n    --------\n    Only works for channel first images. (No channel image will not work).\n    \"\"\"\n    if len(set(tuple(image.shape) for image in images)) > 1:\n        raise ValueError(\"Image shapes do not match\")\n    shape = images[0].shape\n    new_sizes = [int(dim * random.uniform(min_perc, max_perc)) for dim in shape]\n    min_idx = [random.randint(0, ax_size - size) for ax_size, size in zip(shape, new_sizes)]\n    max_idx = [min_id + size for min_id, size in zip(min_idx, new_sizes)]\n    bbox = list(slice(min_, max(max_, 1)) for min_, max_ in zip(min_idx, max_idx))\n    # DO not crop channel axis...\n    bbox[0] = slice(0, shape[0])\n    # prevent warning\n    bbox = tuple(bbox)\n    cropped_images = [image[bbox] for image in images]\n    if len(cropped_images) == 1:\n        return cropped_images[0]\n    else:\n        return cropped_images",
        "sha1": "80d512f3a84b658ee70dde8311be0318aa89a068",
        "id": 637779
    },
    {
        "content": "def has_smach_interface(obj):\n    \"\"\"Returns True if the object has SMACH interface accessors.\"\"\"\n    return (hasattr(obj, 'get_registered_input_keys') and\n            hasattr(obj, 'get_registered_output_keys') and\n            hasattr(obj, 'get_registered_outcomes'))",
        "sha1": "585f5fe64e63e4658f22c0da9238c2179ca7db84",
        "id": 576284
    },
    {
        "content": "import torch\n\n\ndef init_method_uniform(low, high):\n    \"\"\"Init method based on Uniform(low, high).\"\"\"\n    def init_(tensor):\n        return torch.nn.init.uniform_(tensor, a=low, b=high)\n    return init_",
        "sha1": "70258a0fca913d3aa0e36effe9d0c1d164b3be28",
        "id": 277757
    },
    {
        "content": "import ast\n\n\ndef print_price_table(soup):\n    \"\"\"\n    Print the price table for \"ANA\u682a\u4e3b\u512a\u5f85\u5238\uff08\u65b0\u5238\uff09\"\n\n    Note that original_price_table[0] is ['\u9031', '\u4fa1\u683c', '\u521d\u65e5\u306e\u5e73\u5747\u4fa1\u683c', '\u6700\u7d42\u65e5\u306e\u5e73\u5747\u4fa1\u683c', '\u4fa1\u683c']\n    original_price_table[0][0]: \u9031\u306e\u59cb\u307e\u308a\u306e\u65e5\uff08\u65e5\u66dc\u65e5\uff09\n    original_price_table[0][1]: \u9031\u5168\u4f53\u306e\u5b89\u5024\uff0f\u9ad8\u5024\n    original_price_table[0][2]: \u521d\u65e5\u306e\u5e73\u5747\u4fa1\u683c\n    original_price_table[0][3]: \u6700\u7d42\u65e5\u306e\u5e73\u5747\u4fa1\u683c\n    original_price_table[0][4]: \u9031\u5168\u4f53\u306e\u5b89\u5024\uff0f\u9ad8\u5024\n\n    See this page for details.\n    https://developers.google.com/chart/interactive/docs/gallery/candlestickchart\n    \"\"\"\n\n    chart_all0_script_element = soup.select(r'#pageSection > script:nth-of-type(2)')\n    chart_all0_script_text = chart_all0_script_element[0].getText()\n    price_table_text = chart_all0_script_text               \\\n        .split('google.visualization.arrayToDataTable(')[1] \\\n        .split(')')[0]\n    original_price_table = ast.literal_eval(price_table_text)\n\n    def extract_value(item):\n        \"\"\"Extract values from original item in original_price_table\"\"\"\n        average_price = (item[1] + item[4]) / 2.0     # Take average of the row price and the high price\n        return [item[0], average_price]\n\n    price_table = list(map(extract_value, original_price_table[1:]))\n    print(price_table)",
        "sha1": "a8e9ffac55573746548a73e6be1ccec9decc0982",
        "id": 288378
    },
    {
        "content": "def wrap(a, b):\n    \"\"\" Wraps the distance a around the distance b and returns the recess.\"\"\"\n    quo, rem = divmod(abs(a), b)\n    return rem if (quo % 2 == 0) else (b - rem)",
        "sha1": "0657c88cbed0e1ef990cb6ccfb0bf30072899498",
        "id": 549480
    },
    {
        "content": "import torch\n\n\ndef getMaskFromTensor(tensor: torch.Tensor):\n    \"\"\"\n        This functions extracts the mask from the unthresholded output of a NN\n        Args:\n            tensor (Tensor): Tensor image of size (1, H, W)\n        Returns:\n            Tensor: UnNormalized image.\n    \"\"\"\n    # Find sigmoid probability and apply a probability Threshold\n\n    # tensor = F.relu(tensor)\n    tensor = torch.sigmoid(tensor)\n\n    return tensor",
        "sha1": "f7907938073de282d36a9303edcfb43ad12ce957",
        "id": 534721
    },
    {
        "content": "def numseqstr(seq, sep=',', fmt='%0.2f'):\n    \"\"\"Takes a number sequence or single number and prints it using the given separator and format.\"\"\"\n    def gets(s):\n        \"\"\"Returns a number formatting string using fmt if a number, else str() version\"\"\"\n        try: # number\n            return fmt % s\n        except TypeError: # non-number\n            return str(s)\n    try:\n        # sequence\n        seq = [s for s in seq]\n        return sep.join(gets(s) for s in seq)\n    except TypeError:\n        return gets(seq)",
        "sha1": "7061e6c0b8fb0b2c1158e58ac111d90802648a8f",
        "id": 638384
    },
    {
        "content": "import math\n\n\ndef is_prime_v2(n):\n    \"\"\"Version 2 of finding if number is prime\n    it uses\n    Runtime: O(sqrt(N))\"\"\"\n    if n == 1:\n        return False\n    max_divisor = math.floor(math.sqrt(n))\n    for divisor in range(2, 1 + max_divisor):\n        if n % divisor == 0:\n            return False\n    return True",
        "sha1": "f8cd7dc040ce5ffcbd9364e56f1d4fb25c73f2cd",
        "id": 173808
    },
    {
        "content": "def extract_body(message_dict):\n    \"\"\"Extracts the body from a message dictionary.\n\n    Parameters\n    ----------\n    message_dict : dict\n\n    Returns\n    -------\n    str\n\n    \"\"\"\n    tagged_parts_list = message_dict[\"structured_text\"][\"text\"]\n    body = \"\"\n    for part_tag_dict in tagged_parts_list:\n        part = part_tag_dict[\"part\"]\n        tag = part_tag_dict[\"tags\"]\n        if tag == \"BODY\":\n            body += part + \" \"\n        elif tag == \"GREETINGS\":\n            break\n\n    return body",
        "sha1": "9d0fb8fe5bc62329ce1e788d53d4fcdb59d56e7c",
        "id": 496250
    },
    {
        "content": "import inspect\n\n\ndef _has_first_argument(function, argument):\n    \"\"\"Checks if a function takes a named argument as the first argument\"\"\"\n    argnames = inspect.getfullargspec(function).args\n    return argnames[0] == \"no_output\"",
        "sha1": "e03ba5cfc9feb08e5cfa03aaa053ca4044b06990",
        "id": 441962
    },
    {
        "content": "import re\n\n\ndef _remove_emoticons(tweet):\n    \"\"\"finds all emoticons, removes them from the\n    tweet, and then returns the tweet with emoticons\n    removed as well as a list of emoticons\n\n    Parameters:\n    -----------\n    tweet: str\n        contents of a tweet\n\n    Returns\n    -------\n    tweet_no_emoticons:\n        string of tweet with emoticons removed\n    emoticons:\n        list of emoticons\n    \"\"\"\n    emoticons_re = r'(?:[:;])(?:[-<])?(?:[()/\\\\|<>])'\n    emoticons = re.findall(emoticons_re, tweet)\n    tweet = re.sub(emoticons_re, '', tweet)\n    return tweet.strip(), emoticons",
        "sha1": "bbe7e1abed0228ccfd4aef6f191662a9a674a6ce",
        "id": 64629
    },
    {
        "content": "def force_orders(self, **kwargs):\n    \"\"\"User's Force Orders (USER_DATA)\n\n    GET /fapi/v1/forceOrders\n\n\n    https://binance-docs.github.io/apidocs/futures/en/#user-39-s-force-orders-user_data\n\n    Keyword Args:\n        symbol (str, optional)\n        autoCloseType (str, optional): \"LIQUIDATION\" for liquidation orders, \"ADL\" for ADL orders.\n        startTime (int, optional)\n        endTime (int, optional)\n        limit (int, optional): Default 50; max 100.\n        recvWindow (int, optional)\n    Notes:\n        If \"autoCloseType\" is not sent, orders with both of the types will be returned\n        If \"startTime\" is not sent, data within 7 days before \"endTime\" can be queried\n    \"\"\"\n    payload = {**kwargs}\n    url_path = \"/fapi/v1/forceOrders\"\n    return self.sign_request(\"GET\", url_path, payload)",
        "sha1": "6e848820e17e54df0f275ec4087d9c609d4e08fa",
        "id": 709031
    },
    {
        "content": "import requests\n\n\ndef create_cdec_json_api_str(station_ids: list) -> str:\n    \"\"\"\n    Function to call an api for the station ids and get the\n    date from the start date to today.\n\n    :param station_ids: List of station ids\n    \"\"\"\n    api_str = \"https://cdec.water.ca.gov/dynamicapp/req/JSONDataServlet\"\n    \n    # seperator for these is '%2C' --> meaning a ','\n    stations_str = '%2C'.join(station_ids)\n    payload = {\n        'Stations': stations_str,\n        'SensorNums': 2,\n        'dur_code': 'D',\n        'Start': '1970-11-10',\n        'End': '2020-11-10'\n    }\n\n    res = requests.get(api_str, params=payload)\n\n    return res.json()",
        "sha1": "d9aef910b785136872e32ea60ab32339465094ae",
        "id": 531497
    },
    {
        "content": "from typing import Counter\n\n\ndef normalize_counts(counts):\n    \"\"\"Return a normalized Counter object.\"\"\"\n    normed = Counter()\n    total = float(sum(list(counts.values()), 0.0))\n    assert total > 0  # cannot normalize empty Counter\n    for key, ct in list(counts.items()):\n        normed[key] = ct / total\n    return normed",
        "sha1": "159479a53d2a0de94f4528674ecb978b692d3df0",
        "id": 676529
    },
    {
        "content": "def get_entry_ids(entry):\n    \"\"\"Creates a trakt ids dict from id fields on an entry. Prefers already populated info over lazy lookups.\"\"\"\n    ids = {}\n    for lazy in [False, True]:\n        if entry.get('trakt_movie_id', eval_lazy=lazy):\n            ids['trakt'] = entry['trakt_movie_id']\n        elif entry.get('trakt_show_id', eval_lazy=lazy):\n            ids['trakt'] = entry['trakt_show_id']\n        elif entry.get('trakt_episode_id', eval_lazy=lazy):\n            ids['trakt'] = entry['trakt_episode_id']\n        if entry.get('tmdb_id', eval_lazy=lazy):\n            ids['tmdb'] = entry['tmdb_id']\n        if entry.get('tvdb_id', eval_lazy=lazy):\n            ids['tvdb'] = entry['tvdb_id']\n        if entry.get('imdb_id', eval_lazy=lazy):\n            ids['imdb'] = entry['imdb_id']\n        if entry.get('tvrage_id', eval_lazy=lazy):\n            ids['tvrage'] = entry['tvrage_id']\n        if ids:\n            break\n    return ids",
        "sha1": "a86116cdb84154ae80938f854ccc049be5c928f2",
        "id": 677790
    },
    {
        "content": "def is_same_tree(p, q):\n    \"\"\"\n    Given two binary trees, write a function to check if they are the same or not.\n\n    Two binary trees are considered the same if they are structurally identical and the nodes have the same value.\n\n    Args:\n        p: TreeNode\n        q: TreeNode\n\n    Returns:\n        bool\n\n    \"\"\"\n\n    # Definition for a binary tree node.\n    # class TreeNode:\n    #     def __init__(self, x):\n    #         self.val = x\n    #         self.left = None\n    #         self.right = None\n\n    if p and q:\n        return p.val == q.val and is_same_tree(p.left, q.left) and is_same_tree(p.right, q.right)\n    return p is q",
        "sha1": "10d459ce2efcb2c37cbe6879d940f8e9406f6e5b",
        "id": 443897
    },
    {
        "content": "def _split_by_length(msg, size):\n  \"\"\"\n  Splits a string into a list of strings up to the given size.\n\n  ::\n\n    >>> _split_by_length('hello', 2)\n    ['he', 'll', 'o']\n\n  :param str msg: string to split\n  :param int size: number of characters to chunk into\n\n  :returns: **list** with chunked string components\n  \"\"\"\n\n  return [msg[i:i + size] for i in range(0, len(msg), size)]",
        "sha1": "1e2314c8c65d824501f2f5dd679bc56c650ab98a",
        "id": 79894
    },
    {
        "content": "def startswith(full_text: tuple, subphrase: tuple) -> int:\n    \"\"\" Check that the specified text starts with the specified subphrase without considering of punctuation.\n\n    Text and subphrase are tokenized, i.e. they are tuples of strings. Matching is realized recursively.\n\n    :param full_text: a tokenized text (tuple of strings).\n    :param subphrase: a tokenized subphrase (tuple of strings).\n    :return: a number of text's words, which coincide with all subphrase's words.\n    \"\"\"\n    n_full = len(full_text)\n    n_sub = len(subphrase)\n    if (n_sub == 0) or (n_full == 0):\n        return 0\n    if n_sub > n_full:\n        return 0\n    if ' '.join(full_text) == ' '.join(subphrase):\n        return n_full\n    if ' '.join(full_text[0:n_sub]) == ' '.join(subphrase):\n        return n_sub\n    if full_text[0].isalnum() and subphrase[0].isalnum():\n        if full_text[0] != subphrase[0]:\n            return 0\n        res = startswith(full_text[1:], subphrase[1:])\n        if res == 0:\n            return 0\n        return res + 1\n    if (not full_text[0].isalnum()) and (not subphrase[0].isalnum()):\n        if (n_full < 2) or (n_sub < 2):\n            return 0\n        res = startswith(full_text[1:], subphrase[1:])\n        if res == 0:\n            return 0\n        return res + 1\n    if full_text[0].isalnum():\n        return startswith(full_text, subphrase[1:])\n    res = startswith(full_text[1:], subphrase)\n    if res == 0:\n        return 0\n    return res + 1",
        "sha1": "66ab079ac49529e8c7093dd9e7c9dbc4892f9ee1",
        "id": 512448
    },
    {
        "content": "import torch\n\n\ndef generate_novel_views(model, img_source, azimuth_source, elevation_source,\n                         azimuth_shifts, elevation_shifts):\n    \"\"\"Generates novel views of an image by inferring its scene representation,\n    rotating it and rendering novel views. Returns a batch of images\n    corresponding to the novel views.\n\n    Args:\n        model (models.neural_renderer.NeuralRenderer): Neural rendering model.\n        img_source (torch.Tensor): Single image. Shape (channels, height, width).\n        azimuth_source (torch.Tensor): Azimuth of source image. Shape (1,).\n        elevation_source (torch.Tensor): Elevation of source image. Shape (1,).\n        azimuth_shifts (torch.Tensor): Batch of angle shifts at which to\n            generate novel views. Shape (num_views,).\n        elevation_shifts (torch.Tensor): Batch of angle shifts at which to\n            generate novel views. Shape (num_views,).\n    \"\"\"\n    # No need to calculate gradients\n    with torch.no_grad():\n        num_views = len(azimuth_shifts)\n        # Batchify image\n        img_batch = img_source.unsqueeze(0)\n        # Infer scene\n        scenes = model.inverse_render(img_batch)\n        # Copy scene for each target view\n        scenes_batch = scenes.repeat(num_views, 1, 1, 1, 1)\n        # Batchify azimuth and elevation source\n        azimuth_source_batch = azimuth_source.repeat(num_views)\n        elevation_source_batch = elevation_source.repeat(num_views)\n        # Calculate azimuth and elevation targets\n        azimuth_target = azimuth_source_batch + azimuth_shifts\n        elevation_target = elevation_source_batch + elevation_shifts\n        # Rotate scenes\n        rotated = model.rotate_source_to_target(scenes_batch, azimuth_source_batch,\n                                                elevation_source_batch,\n                                                azimuth_target, elevation_target)\n    # Render images\n    return model.render(rotated).detach()",
        "sha1": "137f2cac5aa6c0d9a80f9639b8b7dcd870386871",
        "id": 333888
    },
    {
        "content": "def _attr_key(attr):\n    \"\"\"Returns appropriate key for sorting attribute names\n\n    Attribute names are a tuple of ``(namespace, name)`` where namespace can be\n    ``None`` or a string. These can't be compared in Python 3, so we conver the\n    ``None`` to an empty string.\n\n    \"\"\"\n    key = (attr[0][0] or ''), attr[0][1]\n    return key",
        "sha1": "99caa04fe6b0101043ef0c3fed3b058574578742",
        "id": 523039
    },
    {
        "content": "from typing import List\n\n\ndef reverse(s: str) -> str:\n    \"\"\"\n    Reverse a string.\n    - Time Complexity: O(len(s))\n    - Space Complexity: O(len(s))\n\n    :param s: a string\n    :return: a reversed string\n    \"\"\"\n    length = len(s)\n    rlist: List[str] = [''] * length\n\n    for i in range(length):\n        rlist[length - i - 1] = s[i]\n\n    return ''.join(rlist)",
        "sha1": "248d2c70b819d05ce1101d08c6365831f8b1929f",
        "id": 654759
    },
    {
        "content": "def score_to_quality(qval, offset: int=32, maxval: int=126):\n    \"\"\"Convert a score to quality value.\"\"\"\n    cval = int(qval) + offset\n    if cval > maxval:\n        cval = maxval\n    return chr(cval)",
        "sha1": "c4684d15d5cea820cccc91e26b814647896279fb",
        "id": 630928
    },
    {
        "content": "import re\n\n\ndef append_diff_file(f, arch, file_name, full_file_path):\n    \"\"\" Append a single summary file to the consolidated diff file.\n\n    Args:\n        f : File we are appending to\n        arch (string): architecture we ran on\n        file_name (string): base file name of file to append (not including path components)\n        full_file_path (string): full path to file to append\n\n    Returns:\n        True if diffs were found in the file, False otherwise\n    \"\"\"\n\n    diffs_found = False\n    print(\"Appending {}\".format(full_file_path))\n\n    # What platform is this file summarizing? We parse the filename itself, which is of the form:\n    #   superpmi_diff_summary_<platform>_<arch>.md\n\n    diff_os = \"unknown\"\n    diff_arch = \"unknown\"\n    match_obj = re.search(r'^superpmi_diff_summary_(.*)_(.*).md', file_name)\n    if match_obj is not None:\n        diff_os = match_obj.group(1)\n        diff_arch = match_obj.group(2)\n\n    with open(full_file_path, \"r\") as current_superpmi_md:\n        contents = current_superpmi_md.read()\n\n        # Were there actually any diffs? We currently look to see if the file contains the text \"No diffs found\",\n        # inserted by `superpmi_asmdiffs.py`, instead of just not having a diff summary .md file.\n        # (A missing file has the same effect.)\n        match_obj = re.search(r'^No diffs found', contents)\n        if match_obj is not None:\n            # There were no diffs in this file; don't add it to the result\n            pass\n        else:\n            diffs_found = True\n            # Write a header for this summary, and create a <summary><details> ... </details> disclosure\n            # section around the file.\n            f.write(\"\"\"\\\n\n## {0} {1}\n\n<details>\n\n<summary>{0} {1} details</summary>\n\nSummary file: `{2}`\n\nTo reproduce these diffs on Windows {3}:\n```\nsuperpmi.py asmdiffs -target_os {0} -target_arch {1} -arch {3}\n```\n\n\"\"\".format(diff_os, diff_arch, file_name, arch))\n\n            # Now write the contents\n            f.write(contents)\n\n            # Write the footer (close the <details> section)\n            f.write(\"\"\"\\\n\n</details>\n\n\"\"\")\n\n    return diffs_found",
        "sha1": "67c6662477cab08815070f837dd0b0f76f17f7ba",
        "id": 331504
    },
    {
        "content": "def calculate_batch(batch_size, length):\n\t\"\"\"\n\tCalculate the batch size for the data of given length.\n\n\tParameters\n\t----------\n\tbatch_size : int, float, default=None\n\t\tBatch size for training. Must be one of:\n\t\t - int : Use `batch_size`.\n\t\t - float : Use `batch_size * n_samples`.\n\t\t - None : Use `n_samples`.\n\n\tlength : int\n\t\tLength of the data to be batched.\n\n\tReturns\n\t-------\n\tbatch : int\n\t\tActual batch size.\n\t\"\"\"\n\tif batch_size is None : return length\n\telif isinstance(batch_size, int) and batch_size > 0 and \\\n\t\t\tbatch_size <= length:\n\t\treturn batch_size\n\telif isinstance(batch_size, float) and 0 < batch_size <= 1:\n\t\treturn int(batch_size * length)\n\telse:\n\t\traise ValueError(\"Batch size must be None, an int less than %d,\" % length,\n\t\t\t\t\t\t\t\"or a float within (0,1]\")",
        "sha1": "5615c6973cff66b3ca35ababf3584075bef1db1a",
        "id": 167137
    },
    {
        "content": "import sqlite3\n\n\ndef _sqlite_json_enabled() -> bool:\n    \"\"\"Return True if this Python installation supports SQLite3 JSON1.\"\"\"\n    connection = sqlite3.connect(\":memory:\")\n    cursor = connection.cursor()\n    try:\n        cursor.execute('SELECT JSON(\\'{\"a\": \"b\"}\\')')\n    except sqlite3.OperationalError:\n        return False\n    cursor.close()\n    connection.close()\n    return True",
        "sha1": "960c04887acad16885f12ed18dc9d0c8f583849b",
        "id": 292206
    },
    {
        "content": "def horizontal_rule(length=79, style=\"_\"):\n    \"\"\"Return a horizontal rule.\n\n    Keyword arguments:\n        length -- Specifies the length of the rule (default 79, minimum 3).\n        style -- Character used for the rule (may be either \"_\" or \"*\").\n\n    If the length is too low, or the style is invalid, a ValueError is raised.\n\n    >>> horizontal_rule()\n    '_______________________________________________________________________________'\n    >>> horizontal_rule(length=5, style=\"*\")\n    '*****'\n\n    >>> horizontal_rule(style=\"=\")\n    Traceback (most recent call last):\n        ...\n    ValueError: Invalid style (choose '_' or '*')\n    >>> horizontal_rule(length=2)\n    Traceback (most recent call last):\n        ...\n    ValueError: Length must be >= 3\n\n    \"\"\"\n    if style not in (\"_\", \"*\"):\n        raise ValueError(\"Invalid style (choose '_' or '*')\")\n    if length < 3:\n        raise ValueError(\"Length must be >= 3\")\n    return style * length",
        "sha1": "7865401259f794803e869e90983baab7cba66218",
        "id": 576412
    },
    {
        "content": "from datetime import datetime\n\n\ndef format_dates_for_query(date_list):\n    \"\"\"Format list of dates as needed for query.\n\n    Date list is turned into a single string for use in\n    BigQuery query `where` statement that filters by date.\n\n    Parameters\n    ----------\n    dates_list: list\n        collection of dates of days we want to pull data for\n\n    Returns\n    -------\n    list[str]: [\"YYYY-MM-DD\"), \"YYYY-MM-DD\"]\n    \"\"\"\n    formatted_date_strings = [datetime.strftime(date, \"%Y-%m-%d\")\n                              for date in date_list]\n    return formatted_date_strings",
        "sha1": "67bbab5326e3547dffab1072f133717fc79227d9",
        "id": 84219
    },
    {
        "content": "def get_parent_key(key):\n    \"\"\"\n    Gets the key of the parent object. Assumes keys are formatted like\n\n    param1__param2__param__3__...\n\n    Parameters\n    ----------\n    key: str\n        The key whose parent we want.\n\n    Output\n    ------\n    parent_key: str\n        The parent key\n    \"\"\"\n    return '__'.join(key.split('__')[:-1])",
        "sha1": "b49f706271f50bc38f2a789dcc90b72db2fea731",
        "id": 619268
    },
    {
        "content": "def curtailment_cost_rule(mod, prj, tmp):\n    \"\"\"\n    Apply curtailment cost to scheduled and subtimepoint curtailment\n    \"\"\"\n    return (mod.GenVarStorHyb_Scheduled_Curtailment_MW[prj, tmp] +\n            mod.GenVarStorHyb_Subtimepoint_Curtailment_MW[prj, tmp]) \\\n        * mod.curtailment_cost_per_pwh[prj]",
        "sha1": "6140238b06f1cf32a858c9c1fdbbf34445c27aa5",
        "id": 367605
    },
    {
        "content": "def _deep_deannotate(element):\n    \"\"\"Deep copy the given element, removing all annotations.\"\"\"\n\n    def clone(elem):\n        elem = elem._deannotate()\n        elem._copy_internals(clone=clone)\n        return elem\n\n    if element is not None:\n        element = clone(element)\n    return element",
        "sha1": "5e7872e71c44d9178bd7c8b8d72930ace1c1ff10",
        "id": 211162
    },
    {
        "content": "def prepare_broadcast(A, B):\n    \"\"\"\n    Prepares the tensors for a broadcasting operation of A onto B\n\n    Parameters\n    ----------\n    A : Tensor\n        a (N,F,) tensor\n    B : Tensor\n        a (M,F,) tensor\n\n    Returns\n    -------\n    (Tensor, Tensor)\n        the first (N,1,F,) tensor and the second (N,M,F,) tensor\n    \"\"\"\n\n    return A.view(A.size(0), 1, A.size(1)),\\\n           B.view(-1, *B.size()).expand(A.size(0), -1, B.size(1))",
        "sha1": "572a1164672cfa3b1cdbe690489856268411b425",
        "id": 471347
    },
    {
        "content": "import random\n\n\ndef get_word() -> str:\n    \"\"\"Get a random word from data.txt\n\n    Returns:\n        str: Selected word to guess in the game.\n    \"\"\"\n    with open(\"./data.txt\", \"r\", encoding=\"utf-8\") as f:\n        words = [line.strip().lower() for line in f]\n\n    word = random.choice(words)\n    return word",
        "sha1": "454c70299d9e2de2fc84c9b6d4ff9511c5aa3aa3",
        "id": 308502
    },
    {
        "content": "def get_ratelimiter_config(global_configs, api_name):\n    \"\"\"Get rate limiter configuration.\n\n    Args:\n        global_configs (dict): Global configurations.\n        api_name (String): The name of the api.\n\n    Returns:\n        float: Max calls\n        float: quota period)\n    \"\"\"\n\n    max_calls = global_configs.get(api_name, {}).get('max_calls')\n    quota_period = global_configs.get(api_name, {}).get('period')\n    return max_calls, quota_period",
        "sha1": "5011fafe91b89704da3615ed64bc9bda73f81c33",
        "id": 187678
    },
    {
        "content": "def get_features(image, model, layers=None):\n    \"\"\"\n        Run an image forward through a model and get the features for\n        a set of layers.\n        Default layers are for VGGNet matching Gatys et al (2016)\n    \"\"\"\n\n    # Need the layers for the content and style representations of an image\n    if layers is None:\n        layers = {'0': 'conv1_1',\n                  '5': 'conv2_1',\n                  '10': 'conv3_1',\n                  '19': 'conv4_1',\n                  '21': 'conv4_2',  # content representation\n                  '28': 'conv5_1'}\n\n    features = {}\n    x = image\n    # Extracting the wanted features for a given image\n    # model._modules is a dictionary holding each module in the model\n    for name, layer in model._modules.items():\n        x = layer(x)\n        if name in layers:\n            features[layers[name]] = x\n\n    return features",
        "sha1": "1274195fd4c27139852182cacf0ae4d0474896c0",
        "id": 333082
    },
    {
        "content": "import typing\nimport enum\n\n\ndef _enum_labels(\n    value: typing.Union[int, str, enum.Enum],\n    enum_type: typing.Optional[typing.Type] = None,\n) -> typing.Dict[int, str]:\n    \"\"\" Gets the human friendly labels of a known enum and what value they map to. \"\"\"\n    def get_labels(v):\n        return getattr(v, 'native_labels', lambda: {})()\n\n    return get_labels(enum_type) if enum_type else get_labels(value)",
        "sha1": "c48dece92922044050ad35f066bf303d2b7b9ac1",
        "id": 691642
    },
    {
        "content": "import copy\n\n\ndef substitute_none_for_missing(kwargs, kwarg_list):\n    \"\"\"Utility function to plug Nones in when optional parameters are not specified in expectation kwargs.\n\n    Example:\n        Input:\n            kwargs={\"a\":1, \"b\":2},\n            kwarg_list=[\"c\", \"d\"]\n\n        Output: {\"a\":1, \"b\":2, \"c\": None, \"d\": None}\n\n    This is helpful for standardizing the input objects for rendering functions.\n    The alternative is lots of awkward `if \"some_param\" not in kwargs or kwargs[\"some_param\"] == None:` clauses in renderers.\n    \"\"\"\n\n    new_kwargs = copy.deepcopy(kwargs)\n    for kwarg in kwarg_list:\n        if kwarg not in new_kwargs:\n            new_kwargs[kwarg] = None\n    return new_kwargs",
        "sha1": "66eb3daaf3470b6fd05dc89a05c58ebbd4562be7",
        "id": 140087
    },
    {
        "content": "def is_ann_profile(profile):\n    \"\"\" Check if a profile string is for DM annihilation \"\"\"\n    tokens = profile.split('_')\n    return tokens[-1] in ['point', 'map', 'radial']",
        "sha1": "884aa10b8e3bce626fd1ed18b3b580531f347d9e",
        "id": 89266
    },
    {
        "content": "def get_chemical_properties(output):\n    \"\"\"\n    Format simulation results from \"run_chemical_model\".\n\n    Parameters\n    ----------\n    output : tuple\n        Results of function \"run_chemical_model\".\n\n    Returns\n    -------\n    partial_pressure : float\n        Sum of partial pressures [atm].\n    rho_phreeqc : float\n        Denisty in [kg/m**3].\n    t_phreeqc : float\n        Temperature in [\u00b0C]\n\n    \"\"\"\n    \n    # Indices / output format provided with the USER_PUNCH block in function \"make_chemical_model\"\n    partial_pressure = sum(output[2][0:5]) # why the hell is it not 0:4 here? Whatever it works...\n    partial_pressure = partial_pressure\n    rho_phreeqc = output[2][5]\n    t_phreeqc = output[2][6]\n    \n    return partial_pressure, rho_phreeqc, t_phreeqc",
        "sha1": "60e989ee1737e2777cc03005c8c59fb964dbf50e",
        "id": 313090
    },
    {
        "content": "def mult_scalar(v1, v2):\n    \"\"\"Computes scalar multiplication of 2 vectors.\n       Vectors should be represented as 1xN matrices (row vectors).\n    \"\"\"\n    return v1.dot(v2.T)[0, 0]",
        "sha1": "c0a1ddee57f69697a3de65538cacca8b74f441bf",
        "id": 445541
    },
    {
        "content": "def intersect_2d(x1, x2):\n    \"\"\"\n    Given two arrays [m1, n], [m2,n], returns a [m1, m2] array where each entry is True if those\n    rows match.\n    :param x1: [m1, n] numpy array\n    :param x2: [m2, n] numpy array\n    :return: [m1, m2] bool array of the intersections\n    \"\"\"\n    if x1.shape[1] != x2.shape[1]:\n        raise ValueError(\"Input arrays must have same #columns\")\n\n    # This performs a matrix multiplication-esque thing between the two arrays\n    # Instead of summing, we want the equality, so we reduce in that way\n    res = (x1[..., None] == x2.T[None, ...]).all(1)\n    return res",
        "sha1": "0be6afa1f0d1cdd9bdf41dde0d938db8e46e308b",
        "id": 658448
    },
    {
        "content": "from typing import Sequence\n\n\ndef _flatten_nested_sequence(sequence):\n  \"\"\"Returns a flattened list of sequence's elements.\"\"\"\n  if not isinstance(sequence, Sequence):\n    return [sequence]\n  result = []\n  for value in sequence:\n    result.extend(_flatten_nested_sequence(value))\n  return result",
        "sha1": "e9ba4be9a224e2501aebd2dbabb815e0e330f263",
        "id": 397869
    },
    {
        "content": "from typing import Tuple\n\n\ndef _split_address(address: str) -> Tuple[str, str]:\n    \"\"\"\n    Splits address into a module string (scheme) and an inner_address.\n    \"\"\"\n    if \"://\" not in address:\n        address = \"ray://\" + address\n    # NOTE: We use a custom splitting function instead of urllib because\n    # PEP allows \"underscores\" in a module names, while URL schemes do not\n    # allow them.\n    module_string, inner_address = address.split(\"://\", maxsplit=1)\n    return (module_string, inner_address)",
        "sha1": "b4b6a0b1efcfe6f8fc6c8592d1426e5a81a19891",
        "id": 276710
    },
    {
        "content": "def first_word(text):\n    \"\"\"\n        returns the first word in a given text.\n    \"\"\"\n    word = ''\n    for token in text.strip():\n        if word and token in ' ,.':\n            break\n        elif token not in ' ,.':\n            word += token\n\n    return word",
        "sha1": "ea3bbe9096305301236a90159bf3c29c594a7fb9",
        "id": 470190
    },
    {
        "content": "def calculate_percentile(percentile, dataframe, column_name):\n    \"\"\"\n    Calculate a percentile for dataframe column\n    :param1 percentile: percentile to calculate\n    :param2 dataframe: dataframe to process\n    :param3 column_name: column name to calculate percentile on\n    Return a percentile rounded to 2 digits\n    \"\"\"\n    return round(dataframe[column_name].quantile(percentile),2 )",
        "sha1": "e78a35ce71ba8fc02bfdb62d02ec3a34676a3932",
        "id": 446791
    },
    {
        "content": "def same_name(f, g):\n    \"\"\" Test whether functions ``f`` and ``g`` are identical or have the same name \"\"\"\n    return f == g or getattr(f, '__name__', 0) == getattr(g, '__name__', 1)",
        "sha1": "5f6c166782169eddbae9144284d14d2314e5d4f4",
        "id": 132163
    },
    {
        "content": "def get_time_string(codetime):\n    \"\"\"\n    Utility function that takes the codetime and\n    converts this to a human readable String.\n\n    Args:\n        codetime (`float`):\n            Code execution time in seconds (usually the difference of two time.time() calls)\n\n    Returns:\n        `str`: A string indicating the total execution time\n    \"\"\"\n    if codetime < 60.0:\n        retstr = 'Execution time: {0:.2f}s'.format(codetime)\n    elif codetime / 60.0 < 60.0:\n        mns = int(codetime / 60.0)\n        scs = codetime - 60.0 * mns\n        retstr = 'Execution time: {0:d}m {1:.2f}s'.format(mns, scs)\n    else:\n        hrs = int(codetime / 3600.0)\n        mns = int(60.0 * (codetime / 3600.0 - hrs))\n        scs = codetime - 60.0 * mns - 3600.0 * hrs\n        retstr = 'Execution time: {0:d}h {1:d}m {2:.2f}s'.format(hrs, mns, scs)\n    return retstr",
        "sha1": "2cdc53ba83e06297c3c09b59095553db72d41643",
        "id": 22184
    },
    {
        "content": "from typing import List\n\n\ndef recall_at_k(predictions: List[int], targets: List[int], k: int = 10) -> float:\n    \"\"\"Computes `Recall@k` from the given predictions and targets sets.\"\"\"\n    predictions_set = set(predictions[:k])\n    targets_set = set(targets)\n    result = len(targets_set & predictions_set) / float(len(targets_set))\n    return result",
        "sha1": "ef7af9f0c2ebdb710263fd46a0be7b6fac7c20c9",
        "id": 170788
    },
    {
        "content": "def _vectorise(value):\n    \"\"\" Converts singletons to length 1 lists \"\"\"\n    if not isinstance(value, list):\n        return [value]\n    return value",
        "sha1": "d8e098772bdba044e4be8f4e5243f43c0e6defe0",
        "id": 189618
    },
    {
        "content": "def batch_eye_like(tensor):\n    \"\"\"\n    Creates a sequence of identity tensors indicted by the batch size with the\n    shape of the input tensor\n\n    Parameters\n    ----------\n    tensor: Tensor (b, n, ..., n)\n            b tensors with the same shape\n\n    Returns\n    -------\n    Tensor (b, n, ..., n)\n        b identity tensors\n    \"\"\"\n\n    return tensor.new_ones(tensor.size(-1)).diag().expand_as(tensor)",
        "sha1": "ef86884a79ec3ff62c02eb3aea15ea7373a766ef",
        "id": 405047
    },
    {
        "content": "def find_split_sons(raw_nodes, parent_id, sub_ops_ids):\n  \"\"\" Find ids of sons given a parent id.\n\n  Args:\n    raw_nodes: A json object contain all the nodes of the raw mxnet json file.\n    parent_id: Id of a node.\n    sub_ops_ids: Ids of all ops in a sub graph.\n\n  Returns:\n    Ids of sons of the specified parent.\n  \"\"\"\n  split_ids = set()\n  if raw_nodes[parent_id][\"op\"] != \"SliceChannel\":\n    return split_ids\n  for op_id in sub_ops_ids:\n    for lst in raw_nodes[op_id][\"inputs\"]:\n      if lst[0] == parent_id:\n        split_ids.add(lst[1])\n  split_ids_list = list(split_ids)\n  split_ids_list.sort()\n  return split_ids_list",
        "sha1": "56169f07f57dcf8b242d5947c1d9abc2abe5e4da",
        "id": 492496
    },
    {
        "content": "def sortAndReturnQuantiles(values):\n    \"\"\"Returns minimum, 0.25-quantile, median, 0.75-quantile, maximum\"\"\"\n    \n    values.sort()\n    N = len(values)\n    return (values[0], values[N/4], values[N/2], values[(3*N)/4], values[N-1])",
        "sha1": "067aec1fc88cfcf33f9bab4201b81dfede82f61d",
        "id": 23549
    },
    {
        "content": "def find_ab(side1, side2, side3):\n    \"\"\"\n    Takes three side lengths an returns two smallest in a list\n    :param side1: int or float\n    :param side2: int or float\n    :param side3: int or float\n    :return: list of 2 ints or floats\n    \"\"\"\n    x = side1\n    y = side2\n    z = side3\n    if y > x and y > z:\n        return[x, z]\n    elif z > y and z > x:\n        return[x, y]\n    else:\n        return[y, z]",
        "sha1": "d1852abd2cfe958643474a6c0d1c57d7ad4a9a56",
        "id": 283902
    },
    {
        "content": "def process_text(text, sent_tokenize):\n    \"\"\"Process text by tokenizing sentences given a tokenizer.\n\n    :param text: Text to be processed\n    :type text: string\n    :param sent_tokenize: Tokenizer\n    :type sent_tokenize: Python callable that returns list of strings\n    :return: Tokenized sentences\n    :rtype: List of strings\n    \"\"\"\n    return sent_tokenize(text)",
        "sha1": "153f6c9d6bd62f8c5d0346ee46d1d8d49cdc8744",
        "id": 164406
    },
    {
        "content": "def _parse_fingerprint_terraform(line, host=None):\n    \"\"\"Parse SSH host fingerprint from terraform output line\"\"\"\n    fingerprint = None\n    if line.find('(remote-exec)') > 0:\n        host = line.split(' ')[0].split('.')[-1]\n        fingerprint = line.split(': ', 2)[1]\n    return host, fingerprint",
        "sha1": "ef57e8c0a505af88e583eb913c3049448bc5077e",
        "id": 22979
    },
    {
        "content": "def calc_air_density(temperature, pressure, elevation_ref=None, elevation_site=None, lapse_rate=-0.113,\n                     specific_gas_constant=286.9):\n    \"\"\"\n    Calculates air density for a given temperature and pressure and extrapolates that to the site if both reference\n    and site elevations are given.\n\n    :param temperature: Temperature values in degree Celsius\n    :type temperature: float or pandas.Series or pandas.DataFrame\n    :param pressure: Pressure values in hectopascal, hPa, (1,013.25 hPa = 101,325 Pa = 101.325 kPa =\n                    1 atm = 1013.25 mbar)\n    :type pressure: float or pandas.Series or pandas.DataFrame\n    :param elevation_ref: Elevation, in meters, of the reference temperature and pressure location.\n    :type elevation_ref: Floating point value (decimal number)\n    :param elevation_site: Elevation, in meters, of the site location to calculate air density for.\n    :type elevation_site: Floating point values (decimal number)\n    :param lapse_rate: Air density lapse rate kg/m^3/km, default is -0.113\n    :type lapse_rate: Floating point value (decimal number)\n    :param specific_gas_constant: Specific gas constant, R, for humid air J/(kg.K), default is 286.9\n    :type specific_gas_constant:  Floating point value (decimal number)\n    :return: Air density in kg/m^3\n    :rtype: float or pandas.Series depending on the input\n\n        **Example usage**\n    ::\n        import brightwind as bw\n\n        #For a series of air densities\n        data = bw.load_campbell_scientific(bw.demo_datasets.demo_campbell_scientific_site_data)\n        air_density = bw.calc_air_density(data.T2m, data.P2m)\n\n        #For a single value\n        bw.calc_air_density(15, 1013)\n\n        #For a single value with ref and site elevation\n        bw.calc_air_density(15, 1013, elevation_ref=0, elevation_site=200)\n\n    \"\"\"\n\n    temp = temperature\n    temp_kelvin = temp + 273.15     # to convert deg C to Kelvin.\n    pressure = pressure * 100       # to convert hPa to Pa\n    ref_air_density = pressure / (specific_gas_constant * temp_kelvin)\n\n    if elevation_ref is not None and elevation_site is not None:\n        site_air_density = round(ref_air_density + (((elevation_site - elevation_ref) / 1000) * lapse_rate), 3)\n        return site_air_density\n    elif elevation_site is None and elevation_ref is not None:\n        raise TypeError('elevation_site should be a number')\n    elif elevation_site is not None and elevation_ref is None:\n        raise TypeError('elevation_ref should be a number')\n    else:\n        return ref_air_density",
        "sha1": "964bff72d67354abeff9a355788d3624d7ec230c",
        "id": 11282
    },
    {
        "content": "def cell_name_to_loc(cell_name):\n    \"\"\"\n    :param cell_name: A cell name in the form 'cYXB' where Y is y-coord, X is x-coord, B is block num\n    :return: (y, x) location of the cell\n    \"\"\"\n    return int(cell_name[1]), int(cell_name[2])",
        "sha1": "71f2432821a052d3c1abbf042cabbc1c91ae68a9",
        "id": 142187
    },
    {
        "content": "from typing import Tuple\n\n\ndef make_flow_paths(\n    GCM: str,\n    SCENARIO: str,\n    TRAIN_PERIOD_START: str,\n    TRAIN_PERIOD_END: str,\n    PREDICT_PERIOD_START: str,\n    PREDICT_PERIOD_END: str,\n    VARIABLE: str,\n    workdir: str = \"az://cmip6\",\n    outdir: str = \"az://cmip6/results\",\n) -> Tuple[str, str, str, str]:\n    \"\"\"Build the paths where your outputs (both intermediate and final) will go\n\n    Parameters\n    ----------\n    GCM : str\n        From run hyperparameters\n    SCENARIO : str\n        From run hyperparameters\n    TRAIN_PERIOD_START : str\n        From run hyperparameters\n    TRAIN_PERIOD_END : str\n        From run hyperparameters\n    PREDICT_PERIOD_START : str\n        From run hyperparameters\n    PREDICT_PERIOD_END : str\n        From run hyperparameters\n    VARIABLE : str\n        From run hyperparameters\n    workdir : str, optional\n        Intermediate files for caching (and might be used by other gcms), by default \"az://cmip6\"\n    outdir : str, optional\n        Final result space, by default \"az://cmip6/results\"\n\n    Returns\n    -------\n    tuple[str, str, str, str]\n        From run hyperparameters\n    \"\"\"\n    coarse_obs_path = f\"{workdir}/intermediates/ERA5_{GCM}_{TRAIN_PERIOD_START}_{TRAIN_PERIOD_END}_{VARIABLE}.zarr\"\n    spatial_anomalies_path = f\"{workdir}/intermediates/anomalies_{GCM}_{TRAIN_PERIOD_START}_{TRAIN_PERIOD_END}_{VARIABLE}.zarr\"\n    bias_corrected_path = f\"{workdir}/intermediates/bc_{SCENARIO}_{GCM}_{TRAIN_PERIOD_START}_{TRAIN_PERIOD_END}_{VARIABLE}.zarr\"\n    final_out_path = f\"{outdir}/bcsd_{SCENARIO}_{GCM}_{PREDICT_PERIOD_START}_{PREDICT_PERIOD_END}_{VARIABLE}.zarr\"\n    return coarse_obs_path, spatial_anomalies_path, bias_corrected_path, final_out_path",
        "sha1": "42d7dda7a6d3c6f1dc806b6ab3600f3aa2b570fa",
        "id": 588446
    },
    {
        "content": "import codecs\n\n\ndef readFile(f):\n    \"\"\"\n    This helper method returns an appropriate file handle given a path f.\n    This handles UTF-8, which is itself an ASCII extension, so also ASCII.\n    \"\"\"\n    return codecs.open(f, 'r', encoding='UTF8')",
        "sha1": "94cf613a0e63ecbebb445018212a380e08edce2a",
        "id": 159638
    },
    {
        "content": "def visit(h):\n    \"\"\"Converts our bespoke linked list to a python list.\"\"\"\n    o = h\n    l = []\n    while o is not None:\n        l.append(o.value)\n        o = o.next\n    return l",
        "sha1": "2cdd9b4513a4f6179177b0fe322924337c7e4d6d",
        "id": 329808
    },
    {
        "content": "from typing import List\nfrom typing import Tuple\n\n\ndef style_close(style: List[Tuple[str, str]]) -> str:\n    \"\"\"\n    HTML tags to close a style.\n\n    >>> style = [\n    ...     (\"font\", 'color=\"red\" size=\"32\"'),\n    ...     (\"b\", \"\"),\n    ...     (\"i\", \"\"),\n    ...     (\"u\", \"\"),\n    ... ]\n    >>> style_close(style)\n    '</u></i></b></font>'\n\n    Tags will always be in reverse (of open) order, so open - close will look like::\n\n        <b><i><u>text</u></i></b>\n    \"\"\"\n    return \"\".join(\"</{}>\".format(x) for x, _ in reversed(style))",
        "sha1": "2043803e50230f139a6a60599b40c53461ed6ed8",
        "id": 702929
    },
    {
        "content": "import torch\n\n\ndef trilinear_composition(h_s, h_t, x, einsum=True):\n    \"\"\"\n    Trilinear composition as described in:\n    STAMP: Short-Term Attention/Memory Priority Model forSession-based Recommendation\n\n    Shapes:\n    b: batch\n    e: embedding\n    l: longest sequence length\n    v: vocabulary size\n\n    Parameters\n    ----------\n    h_s : torch.tensor\n        Shape: (l, b, e)\n    h_t : torch.tensor\n        Shape: (l, b, e)\n    x : torch.tensor\n        Embedding matrix.\n        Shape: (v, e)\n    einsum : bool\n     Use einsum. This is recommended as it appeared to be faster in profiling.\n     The other option was added for testing purposes.\n\n    Returns\n    -------\n    <h_s, h_t, x> : torch.tensor\n        Shape: (b, l, v)\n    \"\"\"\n    if einsum:\n        return torch.einsum(\"lbe,lbe,ve->blv\", h_s, h_t, x)\n\n    else:\n        b = h_s.shape[1]\n\n        # h_t \u2299 x\n        # l,b,e \u2299 v,1,1,e  (so b,e dimensions are broadcasted)\n        bc = h_t * x.reshape(x.shape[0], 1, 1, x.shape[1])\n\n        # aT @ (bc) = h_sT @ (h_t \u2299 x)\n        # ---------\n        # l,b,e @ v,l,e,b\n        # b,e @ e,b will be matrix multiplied. v,l are untouched.\n        # result is a v,l,b,b matrix\n        vlbb = torch.matmul(h_s, bc.transpose(-1, -2))\n\n        # As dot products should not be applied across batches, the diagonal can be selected\n        # v,l,b\n        vlb = vlbb[..., torch.arange(0, b), torch.arange(0, b)]\n        # b,l,v\n        return vlb.transpose(0, 2)",
        "sha1": "cbeeff88ad144f72d2774fb47a4a7951e2fd503c",
        "id": 649446
    },
    {
        "content": "import math\n\n\ndef normalize_angle(angle):\n    \"\"\"\n    Convert an angle to an angle belonging to the range -pi..pi\n    :param angle: angle value in radians\n    :return: angle value in radians\n    \"\"\"\n    angle %= (2 * math.pi)\n    angle = angle - 2 * math.pi if angle > math.pi else angle\n    return angle",
        "sha1": "7f1474d7f1ae03884739e5da3e265874933d1ad4",
        "id": 568342
    },
    {
        "content": "def unzip(lst):\n    \"\"\"Unzip a zipped list\"\"\"\n    return list(zip(*lst))",
        "sha1": "cafb62c48f54aba0482fbde1a24b39be7b6e68fe",
        "id": 400270
    },
    {
        "content": "from collections import defaultdict\n\n\ndef convert_idsw_to_heatmap_format(filepath, dest_file):\n    \"\"\"Convert idsw format to heatmap formats\n    idsw: <frame> <id1_gt> <id1> <bb1_left> <bb1_top> <bb1_width> <bb1_height> <id2_gt> <id2> <bb2_left> <bb2_top> <bb2_width> ...\n    heatmap:  <frame> <bb1_left> <bb1_top> <bb1_width> <bb1_height> <bb2_left> <bb2_top> <bb2_width> <bb2_height> ...\n\n    Args:\n        filepath ([str]): idsw path\n        dest_file : save file path\n    \"\"\"\n\n    ids_group = defaultdict(list)\n    obj_infos = []\n\n    with open(filepath, \"r\") as f:\n        for line in f:\n            # <frame> <id1_gt> <id1> <bb1_left> <bb1_top> <bb1_width> <bb1_height> <id2_gt> <id2> <bb2_left> <bb2_top> <bb2_width>\n            p = line.rstrip().split(\" \")\n            p = list(map(int, p))\n            # get number of objects in current frame \n            num_obj = int((len(p) - 1) / 6)\n            for idx in range(num_obj):\n                # <frame> <id1_gt> <id1> <bb1_left> <bb1_top> <bb1_width> <bb1_height>\\n <frame> <id2_gt> <id2> <bb2_left> <bb2_top> <bb2_width>\n                obj_infos.append([p[0]] + p[1 + 6 * idx: 7 + 6 * idx])\n\n    for obj in obj_infos:\n        ids_group[obj[1]].append(obj)\n\n    # print(ids_group)\n    with open(dest_file, 'w') as f:\n        for _, objs in ids_group.items():\n            objs = sorted(objs, key=lambda x: x[0])\n            if len(objs) % 2 == 0:\n                for i in range(1, len(objs), 2):\n                    tmp = list(map(str, objs[i]))\n                    # print(tmp)\n                    line = tmp[0] + \" \" + tmp[3] + \" \" + tmp[4] + \" \" + tmp[5] + \" \" + tmp[6] + \"\\n\"\n                    f.write(line)\n\n    return 1",
        "sha1": "a1654e99d581aa58f7cd5f11872083051d91fd8f",
        "id": 110082
    },
    {
        "content": "from pathlib import Path\nimport textwrap\n\n\ndef write_file(path: Path, text: str, permissions: int = 0o777) -> Path:\n    \"\"\"\n    Write the given ``text`` to a new file at the given ``path``, stomping\n    anything that might exist there.\n\n    Parameters\n    ----------\n    path\n        The path to write to.\n    text\n        The text to write.\n    permissions\n        The permissions to give the file.\n\n    Returns\n    -------\n    path : pathlib.Path\n        The path the file was written to (as an absolute path).\n    \"\"\"\n    path = Path(path).absolute()\n    path.parent.mkdir(parents=True, exist_ok=True)\n    path.write_text(textwrap.dedent(text))\n    path.chmod(permissions)\n    return path",
        "sha1": "bcf575ec573e70716aba9a00cead4cba87c80966",
        "id": 409341
    },
    {
        "content": "from datetime import datetime\n\n\ndef temporal_filter(record_date_time, time_or_period, op):\n    \"\"\"\n    Helper function to perform temporal filters on feature set\n\n    :param record_date_time: datetime field value of a feature\n    :type record_date_time: :class:`datetime.datetime`\n    :param time_or_period: the time instant or time span to use as a filter\n    :type time_or_period: :class:`datetime.datetime` or a tuple of two\n                            datetimes or a tuple of one datetime and one\n                            :class:`datetime.timedelta`\n    :param op: the comparison operation\n    :type op: str\n\n    :return: a comparison expression result\n    :rtype: bool\n    \"\"\"\n\n    d = datetime.strptime(record_date_time, \"%Y-%m-%dT%H:%M:%SZ\")\n    result = None\n\n    # perform before and after operations\n    if op in ['BEFORE', 'AFTER']:\n        query_date_time = datetime.strptime(\n            time_or_period.value, \"%Y-%m-%dT%H:%M:%SZ\")\n        if op == 'BEFORE':\n            return d <= query_date_time\n        elif op == 'AFTER':\n            return d >= query_date_time\n\n    # perform during operation\n    elif 'DURING' in op:\n        low, high = time_or_period\n        low = datetime.strptime(low.value, \"%Y-%m-%dT%H:%M:%SZ\")\n        high = datetime.strptime(high.value, \"%Y-%m-%dT%H:%M:%SZ\")\n        result = d >= low and d <= high\n        if 'BEFORE' in op:\n            result = d <= high\n        elif 'AFTER' in op:\n            result = d >= low\n        return result",
        "sha1": "9f76d6a6eb96da9359c4bbb80f6cfb1dfdcb4159",
        "id": 507
    },
    {
        "content": "import socket\n\n\ndef address_in_use(host, port):\n    \"\"\"\n    Check to see if there is a listener on host:port.\n\n    :param host: the interface to check;\n    :type host: string\n    :param port: the post to check;\n    :type port: int\n    :returns: **True** if there is a listener, else **False**.\n    \"\"\"\n    try:\n        s__ = socket.create_connection((host, port))\n        s__.close()\n        return True\n    except socket.error:\n        return False",
        "sha1": "bf3ae638095743ee8dc312ff35dbeadab219e8e8",
        "id": 181160
    },
    {
        "content": "from typing import OrderedDict\n\n\ndef get_partial_state_dict(model_state_dict, modules, \n                            init_from_decoder_asr=False,\n                            init_from_decoder_mt=False):\n    \"\"\"Create state_dict with specified modules matching input model modules.\n\n    Note that get_partial_lm_state_dict is used if a LM specified.\n\n    Args:\n        model_state_dict (OrderedDict): trained model state_dict\n        modules (list): specified module list for transfer\n\n    Return:\n        new_state_dict (OrderedDict): the updated state_dict\n\n    \"\"\"\n    new_state_dict = OrderedDict()\n\n    for key, value in model_state_dict.items():\n        if init_from_decoder_asr:\n            key = key.replace(\"_asr\", \"\")\n        elif init_from_decoder_mt:\n            key = key.replace(\"decoder_mt\", \"decoder\")\n        if any(key.startswith(m) for m in modules):\n            new_state_dict[key] = value\n\n    return new_state_dict",
        "sha1": "4d83f22124dca13852451e4016f5a52a727fa031",
        "id": 610750
    },
    {
        "content": "from typing import Iterable\nfrom typing import Callable\nfrom typing import Any\nfrom functools import reduce\n\n\ndef pipeline(fns: Iterable[Callable[[Any], Any]], init_value: Any) -> Any:\n    \"\"\"Executes given functions in a pipeline:\n    E.g.: pipeline((a, b, c), x) == c(b(a(x)))\n    \"\"\"\n    return reduce(lambda acc, el: el(acc), fns, init_value)",
        "sha1": "57ece8e0f7460b7d3f783dd6b1e650ab94d2ca08",
        "id": 500084
    },
    {
        "content": "import json\n\n\ndef load_coco_json(path):\n    \"\"\"Read json with annotations.\n\n    Args:\n        path (str): path to .json file\n\n    Raises:\n        RuntimeError if .json file has no images\n        RuntimeError if .json file has no categories\n\n    Returns:\n        images mapping and categories mapping\n    \"\"\"\n\n    with open(path, \"r\") as in_file:\n        content = json.load(in_file)\n\n    if not len(content[\"images\"]):\n        raise RuntimeError(f\"There is no image records in '{path}' file!\")\n\n    if not len(content[\"categories\"]):\n        raise RuntimeError(f\"There is no categories in '{path}' file!\")\n\n    # image_id -> {\n    #   file_name,\n    #   height,\n    #   width,\n    #   annotations([{id, iscrowd, category_id, bbox}, ...])\n    # }\n    images = {}\n    for record in content[\"images\"]:\n        images[record[\"id\"]] = {\n            \"file_name\": record[\"file_name\"],\n            \"height\": record[\"height\"],\n            \"width\": record[\"width\"],\n            \"annotations\": [],\n        }\n\n    categories = {}  # category_id -> name\n    for record in content[\"categories\"]:\n        categories[record[\"id\"]] = record[\"name\"]\n\n    for record in content[\"annotations\"]:\n        images[record[\"image_id\"]][\"annotations\"].append(\n            {\n                \"id\": record[\"id\"],\n                \"iscrowd\": record[\"iscrowd\"],\n                \"category_id\": record[\"category_id\"],\n                \"bbox\": record[\"bbox\"],\n            }\n        )\n\n    return images, categories",
        "sha1": "a34ad05fec58d8b2e7aec176f051d543eb17ba01",
        "id": 280978
    },
    {
        "content": "def get_doubling_times_for_clones_in_alleles_df(alleles_df, doubling_times_df):\n    \"\"\"Returns filtered doubling times for clones in alleles.\"\"\"\n    return doubling_times_df[\n        doubling_times_df.well.apply(\n                lambda w: w in list(\n                        alleles_df.columns))]",
        "sha1": "6b8358ad83d587728d011aee0d4cc743a0421a11",
        "id": 159383
    },
    {
        "content": "def delete_file_dellos10(ssh_conn, dest_file_system, dest_file):\n    \"\"\"Delete a remote file for a Dell OS10 device.\"\"\"\n    if not dest_file:\n        raise ValueError(\"Invalid dest file specified\")\n\n    cmd = \"delete home://{}\".format(dest_file)\n    output = ssh_conn.send_command_timing(cmd)\n    if \"Proceed to delete\" in output:\n        output = ssh_conn.send_command_timing(\"yes\")\n        return output\n\n    raise ValueError(\"An error happened deleting file on Dell OS10\")",
        "sha1": "4e5db81d423f7a2d96418928ba3f3b21fa098f8e",
        "id": 542632
    },
    {
        "content": "def filter_blacklist(genes, blacklist, field='gene_id'):\n    \"\"\"Filters potential hits against given blacklist.\"\"\"\n    return genes.loc[~genes[field].isin(blacklist)]",
        "sha1": "9af27b7f0e9e74dd0f968392707666c7aedef78a",
        "id": 620183
    },
    {
        "content": "def read_from_osr(state):\n    \"\"\"Reads the contents of the output shift register.\"\"\"\n\n    return state.output_shift_register.contents",
        "sha1": "e37ea45553754728393fc897444d4b7dc05e1ffd",
        "id": 522748
    },
    {
        "content": "import re\n\n\ndef remove_numerics(string):\n    \"\"\"Transform names like 'Tobacco products (16)' into 'Tobacco products'\"\"\"\n    return re.sub(r\" \\(\\d\\d\\)$\", \"\", string)",
        "sha1": "145662f092f4044c28ddc2cf213def337bf0b1dc",
        "id": 85756
    },
    {
        "content": "def similar_exact(a, b):\n    \"\"\"Exact comparison between `a` and `b` strings.\"\"\"\n    return a == b",
        "sha1": "04a580df90bb743626859dd4f71a5c5ec3df114e",
        "id": 677408
    },
    {
        "content": "def _dp_parse_out_single_term_ops(inputs, all_inds, ind_counts):\n    \"\"\"Take ``inputs`` and parse for single term index operations, i.e. where\n    an index appears on one tensor and nowhere else.\n\n    If a term is completely reduced to a scalar in this way it can be removed\n    to ``inputs_done``. If only some indices can be summed then add a 'single\n    term contraction' that will perform this summation.\n    \"\"\"\n    i_single = {i for i, c in enumerate(all_inds) if ind_counts[c] == 1}\n    inputs_parsed, inputs_done, inputs_contractions = [], [], []\n    for j, i in enumerate(inputs):\n        i_reduced = i - i_single\n        if not i_reduced:\n            # input reduced to scalar already - remove\n            inputs_done.append((j, ))\n        else:\n            # if the input has any index reductions, add single contraction\n            inputs_parsed.append(i_reduced)\n            inputs_contractions.append((j, ) if i_reduced != i else j)\n\n    return inputs_parsed, inputs_done, inputs_contractions",
        "sha1": "c982653e274ea98bed6ad35f35a1e4d02b96d050",
        "id": 246390
    },
    {
        "content": "def filter_dict_page(pagetext_list, keyslist):\n    \n    \"\"\"Filters webtext of a given .html page, which is parsed and in list format, to only those strings \n    within pagetext_list containing an element (word or words) of inputted keyslist. \n    Returns list filteredtext wherein each element has original case (not coerced to lower-case).\"\"\"\n    \n    filteredtext = [] # Initialize empty list to hold strings of page\n    \n    for string in pagetext_list:\n        lowercasestring = str(string).lower() # lower-case string...\n        dict_list = [key.lower() for key in list(keyslist)] # ...compared with lower-case element of keyslist\n        for key in dict_list:\n            if key in lowercasestring and key in lowercasestring.split(' '): # Check that the word is the whole word not part of another one\n                filteredtext.append(string)\n\n    return filteredtext",
        "sha1": "928f44e2e8bfb423b053d0ba4447c496900de025",
        "id": 281571
    },
    {
        "content": "def split_learning_verification(dataframe, verification_ids):\n\n    \"\"\"\n        Takes a Pandas dataframe and splits it into training and testing sets. The input IDs are the testing set, the\n        rest of the dataframe is the training set\n\n        Parameters\n        ----------\n        dataframe : Pandas dataframe\n            A Pandas dataframe of the whole set that will be split\n\n        verification_ids : list\n            The ID(s) that will be separated from the dataframe to constitute the testing set\n\n        Returns\n        -------\n        test_df\n            Returns a Pandas  dataframe with the testing set consisting of the products with the IDs in the input list\n\n        train_df\n            Returns a Pandas dataframe with the training set consisting of every product with IDs that were not\n            in the input list\n\n    \"\"\"\n\n    ids = list(map(int, dataframe.id.unique()))\n\n    learn_ids = []\n\n    for e in ids:\n        if e not in verification_ids:\n            learn_ids.append(e)\n\n\n    test_df = dataframe[dataframe['id'].isin(verification_ids)]\n\n\n    train_df = dataframe[dataframe['id'].isin(learn_ids)]\n\n    return test_df, train_df",
        "sha1": "68c9ac2c1920a8a731760dffd75673543d7968c9",
        "id": 237451
    },
    {
        "content": "def deltaify_traces(traces, final_byte_duration=9999):\n  \"\"\"Convert absolute start times in traces to durations.\n\n  Traces returned by `read_traces_csv` pair bytes with start times. This\n  function computes how long each byte remains on the bus and replaces the\n  start time with this value in its output. Note that the final duration can't\n  be calculated and will be given the duration `final_byte_duration`.\n\n  Args:\n    traces: Traces to \"deltaify\" as described.\n    final_byte_duration: Duration to assign to the final byte.\n\n  Returns:\n    \"Deltaified\" traces as described.\n  \"\"\"\n  deltaified_traces = []\n  for i in range(len(traces) - 1):\n    dt = traces[i+1][0] - traces[i][0]\n    deltaified_traces.append((dt, traces[i][1]))\n  deltaified_traces.append((final_byte_duration, traces[-1][1]))\n  return deltaified_traces",
        "sha1": "8185a9825d4706bdf8a579fcefec5e27ca8c3baa",
        "id": 701612
    },
    {
        "content": "def b_to_mb(b: int):\n    \"\"\"Convert bytes to MB.\"\"\"\n    return round(float(b) / (1024 ** 2), 2)",
        "sha1": "9546a8c2e623610c54b5dac70ee238a3632c9c82",
        "id": 78678
    },
    {
        "content": "import re\n\n\ndef parse_pubchem_ids(field: str):\n    \"\"\"\n    parse_pubchem_ids() uses regular expressions to extract the PubChem\n        compound IDs from a field in a record\n\n    Args:\n        field (str): name of a pandas.DataFrame field containing PubChem\n            compound IDs in a string\n\n    Returns:\n        str: extracted pubchem_id\n    \"\"\"\n\n    regex = \"'PubChem', \\[\\'(\\d+)\\'\\]\\)\"\n    # matches \"'PubChem', ['\" characters exactly, then captures any following\n    # digits (\\d+), before another literal \"']\" character match\n\n    ids = re.findall(regex, str(field), re.IGNORECASE)\n    if len(ids) > 0:\n        pubchem_id = ids[0]\n    else:\n        pubchem_id = ''\n\n    return pubchem_id",
        "sha1": "f10da0b913dafe2a2a488ad214b312d3208e0eb2",
        "id": 497710
    },
    {
        "content": "def convert_to_str(string):\n    \"\"\"Helper function to catch bytes as strings\"\"\"\n    if type(string) is str:\n        return string\n    else:\n        return bytes.decode(string)",
        "sha1": "1445b1e8471ab12dd142b69c3a3e74feea7cfd3f",
        "id": 494356
    },
    {
        "content": "import torch\n\n\ndef input_from_batch(batch):\n    \"\"\"\n    If the input is a tensor, return it. Otherwise, return the first element.\n\n    Parameters\n    ----------\n    batch : torch.Tensor or tuple of torch.Tensor\n        Input batch.\n\n    Returns\n    -------\n    torch.Tensor\n        A tensor, used as the input to a model.\n\n    \"\"\"\n    if isinstance(batch, torch.Tensor):\n        # unsupervised learning, no labels\n        return batch\n    # iterable\n    return batch[0]",
        "sha1": "71c8f70a27531ebd9f4d4586421d363069ea781a",
        "id": 270691
    },
    {
        "content": "def roi(capital, duration, percent):\n    \n    \"\"\"\n    roi() calculates the cumulative return on investment of a certain amount for a given time at a given interest rate.\n    \n    parameters:\n    capital: amount to be invested\n    duration: how long one is investing in months\n    percent: the interest rate per month\n    \"\"\"\n    cum = [capital]\n    percentage = percent/100\n    for num in range(duration):\n        profit = cum[-1]*percentage\n        total_amt = cum[-1] + profit\n        cum.append(total_amt)\n    investment_yield = cum[-1]\n    return investment_yield",
        "sha1": "75a448ae12d061f00c78906d5ae4f971e57f8a15",
        "id": 494511
    },
    {
        "content": "def prefix_dict_keys(dictionary, prefix, delimiter=\"/\"):\n    \"\"\"Return a copy of a dictionary with a prefix added to every key.\n\n    Parameters:\n        dictionary (dict): Input dictionary.\n        prefix (str): Prefix to add to every key.\n        delimiter (str): String used to separate prefix and original key.\n\n    Returns:\n        dict\n\n    Example:\n        >>> prefix_dict_keys({'bar': 42}, prefix='foo', delimiter='.')\n        {'foo.bar': 42}\n\n    \"\"\"\n    return {delimiter.join((prefix, key)): val for key, val in dictionary.items()}",
        "sha1": "ef682b189ed6033e11a10f16317bbaecc6aa7d4f",
        "id": 151373
    },
    {
        "content": "from typing import Callable\nfrom typing import Tuple\nimport math\n\n\ndef si_magnitude(\n    base: int, suffix: str, prefixes: str,\n) -> Callable[[int], Tuple[int, str]]:\n    \"\"\"\n    SI base converter builder.\n\n    :param base: Base to truncate values to.\n    :param suffix: Suffix used to denote the type of information.\n    :param prefixes: Prefixes before the suffix to denote magnitude.\n    :return: A function to change a value by the above parameters.\n    \"\"\"\n    prefixes = \" \".join(prefixes.split(\"|\")[::-1])\n    prefixes_ = prefixes.split(\" \")\n\n    def inner(value: int) -> Tuple[int, str]:\n        \"\"\"\n        Convert a number to a truncated base form.\n\n        :param value: Value to adjust.\n        :return: Truncated value and unit.\n        \"\"\"\n        logged = math.log(value, base)\n        if -1 < value < 1:\n            logged -= 1\n        remainder = value / base ** int(logged)\n        return remainder, prefixes_[int(logged)] + suffix\n\n    return inner",
        "sha1": "ccbb3634d0ab2f150eb54bb7f1805bb615315f16",
        "id": 377285
    },
    {
        "content": "import zlib\n\n\ndef _compress_bytes(b, level):\n    \"\"\"Compress bytes to bytes.\n    \"\"\"\n    return zlib.compress(b, level)",
        "sha1": "05d777ca8d46f7cec484cde71a6735335dd0dbe9",
        "id": 278301
    },
    {
        "content": "def R0(beta, d, nu, mu1):\n    \"\"\"\n    Basic reproduction number.\n    Parameters:\n    -----------\n    beta\n        average number of adequate contacts per unit time with infectious individuals\n    d\n        natural death rate\n    nu\n        disease induced death rate\n    mu1\n        Maximum recovery rate\n    \"\"\"\n    return beta / (d + nu + mu1)",
        "sha1": "0dac9fe5f7e1a879a1afcb469ae5a8f74eff8825",
        "id": 89415
    },
    {
        "content": "def _create_fold_path_component(edge_direction, edge_name):\n    \"\"\"Return a tuple representing a fold_path component of a FoldScopeLocation.\"\"\"\n    return ((edge_direction, edge_name),)",
        "sha1": "6c169272b4d6052b9d6f9883a17e2f5be0d49193",
        "id": 665098
    },
    {
        "content": "def triangular(n):\n    \"\"\"\n    The triangular numbers are the numbers 1, 3, 6, 10, 15, 21, ...\n    They are calculated as follows.\n    1 = 1\n    1 + 2 = 3\n    1 + 2 + 3 = 6\n    1 + 2 + 3 + 4 = 10\n    1 + 2 + 3 + 4 + 5 = 15\n    Returns nth triangular number.\n    \"\"\"\n    return sum([i for i in range(n+1)])",
        "sha1": "1dea2270de3d27abd5d7fe55d1861253bcda3920",
        "id": 97265
    },
    {
        "content": "from typing import Optional\n\n\ndef check_is_subclass(obj, cls) -> Optional[bool]:\n    \"\"\"Call issubclass without raising exceptions.\"\"\"\n    try:\n        return issubclass(obj, cls)\n    except TypeError:\n        return None",
        "sha1": "0d4cf725ce565f8ccb7490c4948bc3625631c2ea",
        "id": 467102
    },
    {
        "content": "def gamma_PML(x, gamma, PML_start, PML_thk):\n    \"\"\"\n    Polynomial stretching profile for a perfectly matched layer.\n\n    Parameters:\n        x : physical coordinate\n        gamma : average value of the profile\n        PML_start : where the PML starts\n        PML_thk : thickness of the PML\n\n    Returns:\n        the value of the profile function at x\n    \"\"\"\n    return 1 + 3*(gamma - 1)*((abs(x - PML_start))/PML_thk)**2",
        "sha1": "3d5bcc5aeb997e429a000b51957c61dd00661a45",
        "id": 73094
    },
    {
        "content": "def get_ttt(jd:float):\n    \"\"\" Get Julian centuries.\n        \n    Args:\n        jd (float): Julan day number.\n    Returns:\n           (float): Julian centuries.\n    \"\"\"\n    return (jd - 2451545.0) / 36525.0",
        "sha1": "1a934050e3303f522619f2723fdfa3d5c8dc46b6",
        "id": 508457
    },
    {
        "content": "import math\n\n\ndef within_bounds(value, lower_bound, upper_bound, inclusive=False):\n  \"\"\"Determine whether given value is within bounds.\n\n  Args:\n    value: float, value to test.\n    lower_bound: float, minimum acceptable value.\n    upper_bound: float, maximum acceptable value.\n    inclusive: optional float, whether to check if metric is close to bounds.\n\n  Returns:\n    boolean, whether metric is within the given bounds.\n  \"\"\"\n  if inclusive and (\n      math.isclose(value, lower_bound) or math.isclose(value, upper_bound)):\n    return True\n  \n  return value > lower_bound and value < upper_bound",
        "sha1": "2dedee9562edc7848cb41c1805ebea9008ba4356",
        "id": 335496
    },
    {
        "content": "def viewset_model(serializer):\n    \"\"\"Get the model of a serializer.\"\"\"\n    if hasattr(serializer, 'serializer_class'):\n        return serializer.serializer_class.Meta.model\n    else:\n        return None",
        "sha1": "6a3696cae6a3f856a6697141b1c4c862dc3593b9",
        "id": 316133
    },
    {
        "content": "def three_way_radix_quicksort(sorting: list) -> list:\n    \"\"\"\n    Three-way radix quicksort:\n    https://en.wikipedia.org/wiki/Quicksort#Three-way_radix_quicksort\n    First divide the list into three parts.\n    Then recursively sort the \"less than\" and \"greater than\" partitions.\n\n    >>> three_way_radix_quicksort([])\n    []\n    >>> three_way_radix_quicksort([1])\n    [1]\n    >>> three_way_radix_quicksort([-5, -2, 1, -2, 0, 1])\n    [-5, -2, -2, 0, 1, 1]\n    >>> three_way_radix_quicksort([1, 2, 5, 1, 2, 0, 0, 5, 2, -1])\n    [-1, 0, 0, 1, 1, 2, 2, 2, 5, 5]\n    \"\"\"\n    if len(sorting) <= 1:\n        return sorting\n    return (\n        three_way_radix_quicksort([i for i in sorting if i < sorting[0]])\n        + [i for i in sorting if i == sorting[0]]\n        + three_way_radix_quicksort([i for i in sorting if i > sorting[0]])\n    )",
        "sha1": "b44c815fa12db121811c3a18ef1903ae67707c1d",
        "id": 380680
    },
    {
        "content": "def gather_input(input_string, datatype=int, req=[], notreq=[]):\n    \"\"\"\n    Create an input and return the users input - it will catch for invalid inputs.\n\n    Parameters\n    -----------\n    input_string: :class:`str`\n        This will be passed into the builtin input() function. \n    datatype: Any\n        The data type to convert the input into - if it cannot be converted it will ask the user again.\n    req: :class:`list`\n        A list with all possible inputs and if the user input is not a match it will ask again.\n        - If [], anything is allowed.\n    notreq: :class:`list`\n        A list with all inputs that should NOT be allowed\n        - If [], nothing will happen.\n    \n    Returns\n    -------\n    Any\n        The input that was received from the user. \n    \"\"\"\n    while True:\n        try: menu = datatype(input(input_string).strip())\n        except:\n            print(\"Invalid input.\")\n            continue\n        if req != []:\n            if menu not in req:\n                print(\"Invalid input.\")\n                continue\n        elif notreq != []:\n            if menu in notreq:\n                print(\"Invalid input.\")\n                continue\n        return menu",
        "sha1": "564e7d41c2bfb1f92d89f6d0ad2b7c51c54819ff",
        "id": 96621
    },
    {
        "content": "def _parse_variable(v_xml):\n    \"\"\"Parse a variable given the xml representation of it.\n    \"\"\"\n    mem_dict = {}\n\n    # Find detailed description\n    mem_dd = v_xml.find('detaileddescription')\n    try:\n        mem_ddstr = mem_dd.find('para').text\n    except AttributeError:\n        mem_ddstr = ''\n\n    mem_dict['detaileddescription'] = mem_ddstr\n\n    mem_dict['type'] = v_xml.find('type').text\n\n    return mem_dict",
        "sha1": "6efda4ea09e60d734a911ee13d1059cf2f681189",
        "id": 480108
    },
    {
        "content": "def calo_time_wins(df, thresh = 5):\n    \"\"\" Generate a list of time windows for relevant off-trigger incidents (CaloEvents).\n    \n    Parameters:\n    ----------\n        df : a pandas DataFrame of a single run. Preferable after cleaning noisy channels.\n        threshold : integer threshold for minimum number of hits in the calorimeter per clock-cycle.\n    \n    Return:\n    ------\n        time_wins: a dictionary of time windows per event. Keys are the event ids.\n        \n    Note: the time windows are specified by the first clock-cycle of the window,\n          which is the biggest dt value.\n    \n    \"\"\"\n    \n    # Create a list of stable events\n    stable_events = df.groupby(\"eventId\").count().xpos < 1000\n    stable_events = stable_events[stable_events].index.tolist()\n    df_stable = df[df.eventId.isin(stable_events)]\n    \n    # Create hit counts per clock-cycle\n    dt_dist = df_stable.groupby(['eventId', 'dt']).count()\n    dt_dist.reset_index(inplace=True)\n\n    time_wins = {}\n    \n    # Add lists of relevant time windows event-by-event\n    for i in stable_events:\n        dt_list = dt_dist.dt[(dt_dist.eventId == i) & (dt_dist.xpos >= thresh)].tolist()\n        for t in dt_list:\n            \n            # Remove possible gitter of +/- clock-cycle or long signals double counts\n            if (t+1 in dt_list) or (t+6 in dt_list):\n                dt_list.remove(t)\n\n        time_wins[i] = dt_list.copy()\n\n    return time_wins",
        "sha1": "b17c015ece87787a71e50aa427be873665c27919",
        "id": 461385
    },
    {
        "content": "def crowded_comparison_operator(self, other, pareto):\n    \"\"\"\n    The crowded-comparison operator guides the selection process at the various\n    stages of the algorithm toward a uniformly spread-out Pareto-optimal front.\n    The operator returns True if *self* is better than *other* and\n    False otherwise.\n\n    :param self: First individual of the comparison\n    :param other: Second individual of the comparison\n    :param pareto: A ParetoInfo object with the information regarding\n                   the Pareto fronts defined by the current population\n    :return: True if *self* is better than *other* and False otherwise.\n    \"\"\"\n\n    # Between two solutions with differing nondomination ranks, we prefer the\n    # solution with the lower (better) rank. Otherwise, if both solutions\n    # belong to the same front, then we prefer the solution that is located in\n    # a lesser crowded region, i.e., with the larger crowding distance.\n    if (pareto.rank[self] < pareto.rank[other]) or \\\n            (pareto.rank[self] == pareto.rank[other] and\n             pareto.crowding_distance[self] > pareto.crowding_distance[other]):\n        return True\n\n    else:\n        return False",
        "sha1": "4e8667c1b2409a43f84b2a59cb0ea4acbebfeae3",
        "id": 59841
    },
    {
        "content": "from typing import List\nfrom typing import Optional\n\n\ndef extract_id(facility_name_alias_resource: List[dict], id_type: str) -> Optional[str]:\n    \"\"\"\n    Extract ID from complex structure FacilityNameAlias\n\n    :param facility_name_alias_resource: FacilityNameAlias structure\n    :param id_type: \"UWI\" or \"UWBI\"\n    :return: alias name\n    \"\"\"\n    for elem in facility_name_alias_resource:\n        if id_type in elem[\"AliasNameTypeID\"]:\n            return elem[\"AliasName\"]\n    return None",
        "sha1": "0fd2dfb9ae30a69caf023bdf26376e7ffc90d869",
        "id": 194876
    },
    {
        "content": "def make_train_dict(input_ids, attention_masks, labels):\n    \"\"\"\n    Put things in a dictionary. Just a small utility function.\n    \"\"\"\n    return {'input_ids': input_ids, 'attention_mask': attention_masks, 'labels': labels}",
        "sha1": "11d3e7d3ecab60ae451d479d1dc288ff75230716",
        "id": 595488
    },
    {
        "content": "def PanProjectChecks(input_api, output_api,\n                     excluded_paths=None, text_files=None,\n                     license_header=None, project_name=None,\n                     owners_check=True, maxlen=80):\n  \"\"\"Checks that ALL chromium orbit projects should use.\n\n  These are checks to be run on all Chromium orbit project, including:\n    Chromium\n    Native Client\n    V8\n  When you update this function, please take this broad scope into account.\n  Args:\n    input_api: Bag of input related interfaces.\n    output_api: Bag of output related interfaces.\n    excluded_paths: Don't include these paths in common checks.\n    text_files: Which file are to be treated as documentation text files.\n    license_header: What license header should be on files.\n    project_name: What is the name of the project as it appears in the license.\n  Returns:\n    A list of warning or error objects.\n  \"\"\"\n  excluded_paths = tuple(excluded_paths or [])\n  text_files = tuple(text_files or (\n      r'.+\\.txt$',\n      r'.+\\.json$',\n  ))\n  project_name = project_name or 'Chromium'\n\n  # Accept any year number from 2006 to the current year, or the special\n  # 2006-20xx string used on the oldest files. 2006-20xx is deprecated, but\n  # tolerated on old files.\n  current_year = int(input_api.time.strftime('%Y'))\n  allowed_years = (str(s) for s in reversed(range(2006, current_year + 1)))\n  years_re = '(' + '|'.join(allowed_years) + '|2006-2008|2006-2009|2006-2010)'\n\n  # The (c) is deprecated, but tolerate it until it's removed from all files.\n  license_header = license_header or (\n      r'.*? Copyright (\\(c\\) )?%(year)s The %(project)s Authors\\. '\n        r'All rights reserved\\.\\n'\n      r'.*? Use of this source code is governed by a BSD-style license that '\n        r'can be\\n'\n      r'.*? found in the LICENSE file\\.(?: \\*/)?\\n'\n  ) % {\n      'year': years_re,\n      'project': project_name,\n  }\n\n  results = []\n  # This code loads the default black list (e.g. third_party, experimental, etc)\n  # and add our black list (breakpad, skia and v8 are still not following\n  # google style and are not really living this repository).\n  # See presubmit_support.py InputApi.FilterSourceFile for the (simple) usage.\n  black_list = input_api.DEFAULT_BLACK_LIST + excluded_paths\n  white_list = input_api.DEFAULT_WHITE_LIST + text_files\n  sources = lambda x: input_api.FilterSourceFile(x, black_list=black_list)\n  text_files = lambda x: input_api.FilterSourceFile(\n      x, black_list=black_list, white_list=white_list)\n\n  snapshot_memory = []\n  def snapshot(msg):\n    \"\"\"Measures & prints performance warning if a rule is running slow.\"\"\"\n    if input_api.sys.version_info.major == 2:\n      dt2 = input_api.time.clock()\n    else:\n      dt2 = input_api.time.process_time()\n    if snapshot_memory:\n      delta_ms = int(1000*(dt2 - snapshot_memory[0]))\n      if delta_ms > 500:\n        print(\"  %s took a long time: %dms\" % (snapshot_memory[1], delta_ms))\n    snapshot_memory[:] = (dt2, msg)\n\n  snapshot(\"checking owners files format\")\n  results.extend(input_api.canned_checks.CheckOwnersFormat(\n      input_api, output_api))\n\n  if owners_check:\n    snapshot(\"checking owners\")\n    results.extend(input_api.canned_checks.CheckOwners(\n        input_api, output_api, source_file_filter=None))\n\n  snapshot(\"checking long lines\")\n  results.extend(input_api.canned_checks.CheckLongLines(\n      input_api, output_api, maxlen, source_file_filter=sources))\n  snapshot( \"checking tabs\")\n  results.extend(input_api.canned_checks.CheckChangeHasNoTabs(\n      input_api, output_api, source_file_filter=sources))\n  snapshot( \"checking stray whitespace\")\n  results.extend(input_api.canned_checks.CheckChangeHasNoStrayWhitespace(\n      input_api, output_api, source_file_filter=sources))\n  snapshot(\"checking license\")\n  results.extend(input_api.canned_checks.CheckLicense(\n      input_api, output_api, license_header, source_file_filter=sources))\n\n  if input_api.is_committing:\n    snapshot(\"checking was uploaded\")\n    results.extend(input_api.canned_checks.CheckChangeWasUploaded(\n        input_api, output_api))\n    snapshot(\"checking description\")\n    results.extend(input_api.canned_checks.CheckChangeHasDescription(\n        input_api, output_api))\n    results.extend(input_api.canned_checks.CheckDoNotSubmitInDescription(\n        input_api, output_api))\n    snapshot(\"checking do not submit in files\")\n    results.extend(input_api.canned_checks.CheckDoNotSubmitInFiles(\n        input_api, output_api))\n  snapshot(\"done\")\n  return results",
        "sha1": "6f4d85b7e1fcfc4c3f6165b9f935b6918ae67023",
        "id": 376116
    },
    {
        "content": "def get_grid_edges(points):\n    \"\"\"Get edges of grid containing all points.\"\"\"\n    grid_edges = []\n    for index in range(2):\n        point = []\n        for func in (min, max):\n            funciest_point = func(points, key=lambda item: item[index])\n            point.append(func(funciest_point))\n        grid_edges.append(point)\n    return grid_edges",
        "sha1": "e3f907b8f84b4ab940d85f8ed1ebca47e315a2d8",
        "id": 265979
    },
    {
        "content": "def get_owner_id(logs):\n    \"\"\"Returns the logs' owner_id.\"\"\"\n    return logs['job'][0]['owner_id']",
        "sha1": "8d9b124057a36cce842ac0b8625725fba5ab76e5",
        "id": 104495
    },
    {
        "content": "import math\n\n\ndef isnan(x):\n    \"\"\"Return True if the real or imaginary part of x is not a number (NaN).\"\"\"\n    return math.isnan(x.real) or math.isnan(x.imag)",
        "sha1": "cde3706dad63291ad2eb4183cec455120b9324ee",
        "id": 122368
    },
    {
        "content": "def _divide_and_round(a, b):\n    \"\"\"divide a by b and round result to the nearest integer\n\n    When the ratio is exactly half-way between two integers,\n    the even integer is returned.\n    \"\"\"\n    # Based on the reference implementation for divmod_near\n    # in Objects/longobject.c.\n    q, r = divmod(a, b)\n    # round up if either r / b > 0.5, or r / b == 0.5 and q is odd.\n    # The expression r / b > 0.5 is equivalent to 2 * r > b if b is\n    # positive, 2 * r < b if b negative.\n    r *= 2\n    greater_than_half = r > b if b > 0 else r < b\n    if greater_than_half or r == b and q % 2 == 1:\n        q += 1\n\n    return q",
        "sha1": "86968086fe0f647054a9e86d68de2348fb4b877d",
        "id": 525439
    },
    {
        "content": "def _contains_atom(example, atoms, get_atoms_fn):\n  \"\"\"Returns True if example contains any atom in atoms.\"\"\"\n  example_atoms = get_atoms_fn(example)\n  for example_atom in example_atoms:\n    if example_atom in atoms:\n      return True\n  return False",
        "sha1": "c9e60d956585c185f9fb62cc0d11f169e6b79f88",
        "id": 704121
    },
    {
        "content": "import hashlib\n\n\ndef hash_seqs(sequences):\n    \"\"\"\n    Generates hexdigest of Sha1 hash for each sequence in a list of sequences.\n\n    This function is useful for generating sequence specific identifiers that allow for easier comparison of features\n    from multiple sequencing runs or sequence processing runs.\n\n    \"\"\"\n\n    new_sequences = list()\n    for seq in sequences:\n        # get sequence string and encode using UTF-8 for consistency\n        seq = seq.encode('UTF-8')\n\n        # get sha1 hash of sequence\n        hash_ = hashlib.sha1()\n        hash_.update(seq)\n        hash_hex = hash_.hexdigest()\n        new_sequences.append(hash_hex)\n\n    return new_sequences",
        "sha1": "35c3291a58ebc7e053250f7234faacd0356f7df5",
        "id": 700055
    },
    {
        "content": "import re\n\n\ndef clean(filepath: str) -> str:\n    \"\"\"Clean up the content of a subtitle file (vtt) to a string\n\n    Args:\n        filepath (str): path to vtt file\n\n    Returns:\n        str: clean content\n    \"\"\"\n    # read file content\n    with open(filepath, \"r\", encoding=\"utf-8\") as fp:\n        content = fp.read()\n\n    # remove header & empty lines\n    lines = [line.strip() for line in content.split(\"\\n\") if line.strip()]\n    lines = lines[1:] if lines[0].upper() == \"WEBVTT\" else lines\n\n    # remove indexes\n    lines = [lines[i] for i in range(len(lines)) if not lines[i].isdigit()]\n\n    # remove timestamps\n    pattern = r\"^\\d{2}:\\d{2}:\\d{2}.\\d{3}.*\\d{2}:\\d{2}:\\d{2}.\\d{3}$\"\n    lines = [lines[i] for i in range(len(lines))\n             if not re.match(pattern, lines[i])]\n\n    content = \" \".join(lines)\n    # remove duplicate spaces\n    pattern = r\"\\s+\"\n    content = re.sub(pattern, r\" \", content)\n\n    # add space after punctuation marks if it doesn't exist\n    pattern = r\"([\\.!?])(\\w)\"\n    content = re.sub(pattern, r\"\\1 \\2\", content)\n\n    return content",
        "sha1": "614ec11ff35d4dcc3249bf87273fb363a4d79b32",
        "id": 90731
    },
    {
        "content": "def sec2time(seconds):\n    \"\"\"\n    Converts seconds to time format\n\n    :param float|int seconds:\n    :return str: 'h:m:s\"\n    \"\"\"\n    m, s = divmod(seconds, 60)\n    h, m = divmod(m, 60)\n    return \"%d:%02d:%02d\" % (h, m, s)",
        "sha1": "fbf1cbdf7d049f97d5b3ada7b84c3f0ed0a2eaee",
        "id": 121179
    },
    {
        "content": "from typing import Tuple\n\n\ndef is_inside_offset(inner: Tuple, outer: Tuple) -> bool:\n    \"\"\"Checks if the first offset is contained in the second offset\n\n    Args:\n        inner: inner offset tuple\n        outer: outer offset tuple\n\n    Returns: bool\"\"\"\n    return outer[0] <= inner[0] <= inner[1] <= outer[1]",
        "sha1": "3b8b7f284dac976f58b234cb794258956e6c94cb",
        "id": 464345
    },
    {
        "content": "def comp_step(self):\n    \"\"\"Compute the Step between two points of the linspace\n    \"\"\"\n\n    self.check()\n    data = self.get_data()\n    return data[1] - data[0]",
        "sha1": "0511bb4abb8d10e42a1b7af146c26563d1e3bb15",
        "id": 316919
    },
    {
        "content": "def is_container(obj):\n    \"\"\" Checks whether the object is container or not.\n\n    Container is considered an object, which includes other objects,\n    thus string is not qualified, even it implments iterator protocol.\n\n    >>> is_container(\"text\")\n    False\n\n    >>> is_container(tuple())\n    True\n    \"\"\"\n    if isinstance(obj, str):\n        return False\n\n    return hasattr(obj, '__iter__')",
        "sha1": "a6772793a24fc95f159df100c5e3ba19dce33281",
        "id": 27673
    },
    {
        "content": "def moffat(x,p0,p1,p2):\n    \"\"\"\n    Moffat profile\n    This 3 parameter formulation assumes the trace is known\n\n    Args:\n        x (float or ndarray): x values\n        p0 (float): Amplitude\n        p1 (float):\n          Width scaling\n        p2 : float\n\n    Returns:\n        float or ndarray: Evaluated Moffat\n    \"\"\"\n    return p0 / (1+(x/p1)**2)**p2",
        "sha1": "aecfec5ab6210112e63343fdfc451aaed72a926a",
        "id": 535254
    },
    {
        "content": "def clean_up_file_name(filename):\n    \"\"\"\n    If FILENAME has spaces in it, replaces them with underscores. Otherwise,\n    this function returns the filename unchanged.\n    \"\"\"\n\n    words = filename.split()\n    if len(words) > 1:\n        filename = \"_\".join(words)\n    return filename",
        "sha1": "db7e289cecc7ca1f11e6eed7153f29231b087688",
        "id": 621972
    },
    {
        "content": "def filter_attributes(resource_type):\n    \"\"\"Returns a list of attributes for a given resource type.\n\n    :param str resource_type: type of resource whose list of attributes we want\n        to extract. Valid values are 'adminTask', 'task', 'adminVApp', 'vApp'\n        and 'adminCatalogItem', 'catalogItem'.\n\n    :return: the list of attributes that are relevant for the given resource\n        type.\n\n    :rtype: list\n    \"\"\"\n    attributes = None\n    if resource_type in ['adminTask', 'task']:\n        attributes = ['id', 'name', 'objectName', 'status', 'startDate']\n    elif resource_type in ['adminVApp', 'vApp']:\n        attributes = [\n            'id', 'name', 'numberOfVMs', 'status', 'numberOfCpus',\n            'memoryAllocationMB', 'storageKB', 'ownerName', 'isDeployed',\n            'isEnabled', 'vdcName'\n        ]\n    elif resource_type in ['adminCatalogItem', 'catalogItem']:\n        attributes = [\n            'id', 'name', 'catalogName', 'storageKB', 'status', 'entityType',\n            'vdcName', 'isPublished', 'ownerName'\n        ]\n    return attributes",
        "sha1": "8fb484196c6d4aa7aabc1295797a4f853e7d0872",
        "id": 568670
    },
    {
        "content": "import pkg_resources\n\n\ndef query_activated(dist):\n    \"\"\"Return True if distribution is active.\n\n    @param dist: pkg_resources Distribution object\n\n    @returns: True or False\n\n    \"\"\"\n    working_set = pkg_resources.WorkingSet()\n    if dist in working_set:\n        return True\n    else:\n        return False",
        "sha1": "26bbce30d6d07897c45e6557f1bb6ba03fdd3d1a",
        "id": 484539
    },
    {
        "content": "import re\n\n\ndef get_teams(soup):\n    \"\"\"\n    Return the team for the TOI tables and the home team\n    \n    :param soup: souped up html\n    \n    :return: list with team and home team\n    \"\"\"\n    team = soup.find('td', class_='teamHeading + border')  # Team for shifts\n    team = team.get_text()\n\n    # Get Home Team\n    teams = soup.find_all('td', {'align': 'center', 'style': 'font-size: 10px;font-weight:bold'})\n    regex = re.compile(r'>(.*)<br/?>')\n    home_team = regex.findall(str(teams[7]))\n\n    return [team, home_team[0]]",
        "sha1": "f79e1b16c2a0acf214cd9e2f03df8cfa38b7d633",
        "id": 405931
    },
    {
        "content": "def multi_item_dict(request):\n    \"\"\"\n    Fixture that yields multi item dicts.\n    \"\"\"\n    return request.param",
        "sha1": "4842c53882fc8d676a9f01ea4bb9c2d0b78015f1",
        "id": 274942
    },
    {
        "content": "import binascii\n\n\ndef hexlify(val):\n    \"\"\"\n    This function is used to display binary data in a friendly format.\n\n        .. seealso::\n            :meth:`LinkageEntity:friendly_hash`\n\n    Note:\n        - Without the decode() the builtin `hexlify` return the bytes for\n            hexadecimal representation of the binary data.\n        - The returned string is twice as long as the length of data.\n\n    :param val: binary\n    :rtype: string\n    \"\"\"\n    return binascii.hexlify(val).decode()",
        "sha1": "e4341c42ef19cb8d22ac11290aa015f27931b164",
        "id": 342222
    },
    {
        "content": "def decode_word_zero(word, mask):\n    \"\"\"\n    Decodes the\n       * Channel\n       * Slot\n       * Crate\n       * Header Length (Base 4 words + options)\n       * Event Length (Header + Trace Length / 2)\n       * Finish Code\n    :param word: The word that we're going to decode\n    :param mask: The mask that we'll be using.\n    :return: A dictionary containing the decoded information.\n    \"\"\"\n    return {\n        'channel': (word & mask.channel()[0]) >> mask.channel()[1],\n        'slot': (word & mask.slot()[0]) >> mask.slot()[1],\n        'crate': (word & mask.crate()[0]) >> mask.crate()[1],\n        'header_length': (word & mask.header_length()[0]) >> mask.header_length()[1],\n        'event_length': (word & mask.event_length()[0]) >> mask.event_length()[1],\n        'finish_code': (word & mask.finish_code()[0]) >> mask.finish_code()[1]\n    }",
        "sha1": "a500b8a4b06fc1e0137be1a4b883576348d78f11",
        "id": 369169
    },
    {
        "content": "def batch_graph_data_dict(batched_data_dict):\n    \"\"\"\n    After running dataset.batch() on data_dict representation of GraphTuple, correct the batch dimensions.\n\n    Args:\n        batched_data_dict: dict(\n            nodes[num_graphs, n_node_per_graph, F_nodes],\n            edges[num_graphs, n_edge_per_graph, F_edges],\n            senders[num_graphs, n_edge_per_graph],\n            receivers[num_graphs, n_edge_per_graph],\n            globals[num_graphs, 1, F_globals],\n            n_node[num_graphs, 1],\n            n_edge[num_graphs, 1])\n\n    Returns:\n        batched_data_dict representing a batched GraphTuple:\n        dict(\n            nodes[num_graphs, n_node_per_graph, F_nodes],\n            edges[num_graphs, n_edge_per_graph, F_edges],\n            senders[num_graphs, n_edge_per_graph],\n            receivers[num_graphs, n_edge_per_graph],\n            globals[num_graphs, F_globals],\n            n_node[num_graphs],\n            n_edge[num_graphs])\n    \"\"\"\n    if \"globals\" in batched_data_dict.keys():\n        batched_data_dict[\"globals\"] = batched_data_dict[\"globals\"][:,0,:]\n    if \"n_node\" in batched_data_dict.keys():\n        batched_data_dict['n_node'] = batched_data_dict['n_node'][:,0]\n    if \"n_edge\" in batched_data_dict.keys():\n        batched_data_dict['n_edge'] = batched_data_dict['n_edge'][:,0]\n    return batched_data_dict",
        "sha1": "febf5c18d02242317e0257eed0f4746e90f2bbe7",
        "id": 195221
    },
    {
        "content": "def nucleotide(nucleotide_index):\n    \"\"\"\n        Convert nucleotide index to a character.\n    \"\"\"\n\n    nucleotides = ['?','A','C','G','T']\n    if 1 <= nucleotide_index and nucleotide_index <= 4:\n        return nucleotides[nucleotide_index]\n    return '?'",
        "sha1": "392d8c623abada4179d8df5605970edb3568a388",
        "id": 452069
    },
    {
        "content": "def is_better(ori_value, comparing_value, is_greater):\n    \"\"\"\n    This module compare the two values based on what we are looking for\n    (Min or Max).\n\n    :param ori_value: Original value.\n    :type ori_value: number\n    :param comparing_value: New value to compare with.\n    :type comparing_value: number\n    :param is_greater: True if you want to know b > a, else False if you want\n    to check b < a.\n    :return: If b is better than a or not.\n    :rtype: bool\n    \"\"\"\n\n    if ori_value is not None:\n        if (is_greater and comparing_value <= ori_value) or \\\n                (not is_greater and comparing_value >= ori_value):\n            return False\n    return True",
        "sha1": "753da70ed60302fd431755def5e760c0ffb85678",
        "id": 83003
    },
    {
        "content": "def get_number_of_words(words):\n    \"\"\"\n    Return number of words in text\n    \"\"\"\n    return len(words)",
        "sha1": "cedba2b603ddf8c2e3d5a517884c07309b7c85fd",
        "id": 129065
    },
    {
        "content": "def _seq_prod(seq1, seq2):\n    \"\"\"Returns the element-wise product of seq1 and seq2.\"\"\"\n    return tuple(map(lambda x, y: x*y, seq1, seq2))",
        "sha1": "1ff87c1fe7c1a1e7c36e8d79c70efc48c5634bbf",
        "id": 363042
    },
    {
        "content": "def parse_domain_label(domain_label):\n    \"\"\" Parse the list of comma-separated domains from the app label. \"\"\"\n    return domain_label.replace(',', ' ').split()",
        "sha1": "5273501ae1bea9517f5b9c0a620fdb78feb79112",
        "id": 62013
    },
    {
        "content": "def extend(value, length):\n    \"\"\" Extends a string value to an exact size \"\"\"\n    remainder = length - len(value)\n    value += b''.join([b'\\x00' for _ in range(remainder)])\n    value = value[:length]\n    return value",
        "sha1": "ecce1288fb4132f1249fa08a04df62a50cc2a6a9",
        "id": 448333
    },
    {
        "content": "def getAlias(column, alias = ''):\n    \"\"\"\n    Composes an alias for the column specified. \n    \"\"\"\n    if alias:\n        return column + ' AS ' + alias\n    return column",
        "sha1": "98ead822b60d567d1f17d401c2c9800a86f00adf",
        "id": 614034
    },
    {
        "content": "import re\nimport string\n\n\ndef gen_anchor(txt: str) -> str:\n    \"\"\"\n    Generate anchor.\n\n    Generates an anchor for a title (for internal links).\n\n    :param txt: A string object.\n    :return: A hyphen-separated string.\n    \"\"\"\n    regex = re.compile(\"[%s+| ]\" % re.escape(string.punctuation))\n    return regex.sub(\"\", txt.lower())",
        "sha1": "b3d299433392adf4bf1014d0f4953daeceef171b",
        "id": 463582
    },
    {
        "content": "def format_timespan_digits(ts):\n    \"\"\"Format a timespan namedtuple as a string resembling a digital display.\"\"\"\n    if ts.days:\n        day_or_days = \"days\" if ts.days > 1 else \"day\"\n        return (\n            f\"{ts.days} {day_or_days}, \"\n            f\"{ts.hours:02d}:{ts.minutes:02d}:{ts.seconds:02d}\"\n        )\n    if ts.seconds:\n        return f\"{ts.hours:02d}:{ts.minutes:02d}:{ts.seconds:02d}\"\n    return f\"00:00:00.{ts.total_microseconds}\"",
        "sha1": "8dbc152c1c93839007b3e74c6458c53ebda7d0c0",
        "id": 87070
    },
    {
        "content": "import torch\n\n\ndef get_bbox(image_id, target, device='cpu', pred=False):\n    \"\"\"\n        image_id (int): image id from annotation.\n        target (dict): target data from annotation or result of model inference.\n        device (string): use CUDA of CPU.\n        pred (bool): if True, result include confidence score of bbox.\n            if False, result include only bboxes and labels.\n            \n        \n    \"\"\"\n    \n    box_num = target['boxes'].shape[0]\n    \n    image_id = torch.tensor(image_id).broadcast_to(box_num, 1)\n    boxes = target['boxes'].reshape(box_num, 4).to(device)\n    labels = target['labels'].reshape(box_num, 1).to(device)\n    \n    if pred:\n        scores = target['scores'].reshape(box_num, 1).to(device)\n        bbox_list = torch.cat([image_id, labels, scores, boxes], dim=1)\n        return bbox_list\n    \n    bbox_list = torch.cat([image_id, labels, boxes], dim=1)\n    \n    return bbox_list",
        "sha1": "7fa5317bce153010d32ed7175d54cd1028a02dd2",
        "id": 403613
    },
    {
        "content": "def compute_losses(batch, res, criterion_dict, args):\n    \"\"\"Calculate the loss given the model logits and the criterion\n    :param batch:\n    :param res: dict of logits\n    :param criterion_dict: dict of the criterion should have key names same as the logits\n    :param args, argparse.Namespace\n    :return: scalar loss value\n    \"\"\"\n\n    obj_clf_loss = lang_clf_loss = referential_loss = 0.\n\n    total_loss = 0.\n\n    # Get the object language classification loss and the object classification loss\n    if args.ref_cls_alpha > 0:\n        criterion = criterion_dict['logits']\n        logits = res['logits']\n        if args.s_vs_n_weight is not None:\n            referential_loss = criterion(logits, batch)\n        else:\n            referential_loss = criterion(logits, batch['target_pos'])\n        total_loss += referential_loss * args.ref_cls_alpha\n\n    if args.obj_cls_alpha > 0:\n        criterion = criterion_dict['class_logits']\n        obj_clf_loss = criterion(res['class_logits'].transpose(2, 1), batch['class_labels'])\n        total_loss += obj_clf_loss * args.obj_cls_alpha\n\n    if args.lang_cls_alpha > 0:\n        criterion = criterion_dict['lang_logits']\n        lang_clf_loss = criterion(res['lang_logits'], batch['target_class'])\n        total_loss += lang_clf_loss * args.lang_cls_alpha\n\n    return {'total_loss': total_loss, 'referential_loss': referential_loss,\n            'obj_clf_loss': obj_clf_loss, 'lang_clf_loss': lang_clf_loss}",
        "sha1": "7765a01b75e06999efcebab6215f2dbfb2e6f0ae",
        "id": 348852
    },
    {
        "content": "def bin_mag_catalog(mag_table, b, mag_table_keys=None, bin_keys=None):\n    \"\"\" Bin an external catalogue based on a bin's provided parameters.\n\n    Args:\n        mag_table The magnitude table to rebin\n        b The tbridge.Bin object\n        mag_table_keys The magnitude table keys to use for binning.\n        bin_keys The associated bin keys (must be the same length AND order as mag_table_keys).\n\n    Returns:\n        Table of magnitudes binned by the appropriate parameters.\n    \"\"\"\n    if bin_keys is None:\n        bin_keys = [\"MASSES\", \"REDSHIFTS\"]\n    if mag_table_keys is None:\n        mag_table_keys = [\"MASS\", \"Z\"]\n\n    # Gather the necessary information from the provided bin.\n    bin_dict = b.return_param_dict()\n\n    # Iterate over all keys and only return the rows that are within the bin limits.\n    for i in range(len(bin_keys)):\n        low, high = bin_dict[bin_keys[i]]\n        mag_table = mag_table[mag_table[mag_table_keys[i]] >= low]\n        mag_table = mag_table[mag_table[mag_table_keys[i]] <= high]\n\n    return mag_table",
        "sha1": "62438f296310c1a37278cbc1eda80ee540bf77a4",
        "id": 199025
    },
    {
        "content": "import functools\nimport logging\n\n\ndef _wrap(behavior):\n  \"\"\"Wraps an arbitrary callable behavior in exception-logging.\"\"\"\n  @functools.wraps(behavior)\n  def _wrapping(*args, **kwargs):\n    try:\n      return behavior(*args, **kwargs)\n    except Exception as e:\n      logging.exception('Unexpected exception from task run in logging pool!')\n      raise\n  return _wrapping",
        "sha1": "f070f409f8c2cc7fb07697b68544e603b377bdd0",
        "id": 55639
    },
    {
        "content": "import base64\nimport pickle\n\n\ndef encode_store_data(store_data):\n    \"\"\"\n    Encode store_data dict into a JSON serializable dict\n\n    This is currently done by pickling store_data and converting to a base64 encoded\n    string. If HoloViews supports JSON serialization in the future, this method could\n    be updated to use this approach instead\n\n    Args:\n        store_data: dict potentially containing HoloViews objects\n\n    Returns:\n        dict that can be JSON serialized\n    \"\"\"\n    return {\"pickled\": base64.b64encode(pickle.dumps(store_data)).decode(\"utf-8\")}",
        "sha1": "0a576a8146c0657610b508ebc6338d3ed6790b70",
        "id": 696066
    },
    {
        "content": "def percent_delta(old_score, new_score):\n    \"\"\"Calculates the percent change between two scores.\n\n    \"\"\"\n    return float(new_score - old_score) / float(old_score)",
        "sha1": "ceaf4e0eecb436818c12a52c32fb1db7f88ca102",
        "id": 582044
    },
    {
        "content": "def remove_digits(s):\n    \"\"\"\n        Returns a string with all digits removed.\n    \"\"\"\n    return ''.join(filter(lambda x: not x.isdigit(), s))",
        "sha1": "42abd1c827f48fe9c22c50aa37944b54781df448",
        "id": 664513
    },
    {
        "content": "def txn_request_attr(attr_name):\n    \"\"\"\n    Sets a `property` on the class that looks up the attr_name off of the\n    request_data.\n    \"\"\"\n    def inner(self):\n        return self.request_data[attr_name]\n\n    inner.__name__ = attr_name\n    return property(inner)",
        "sha1": "2d152de0a089c3e4c69f24c6a3f13e32e4e788af",
        "id": 299814
    },
    {
        "content": "def readDiff(fname):\n    \"\"\"Function that returns a list of changes based on a IDA dif file.\"\"\"\n    changes = []\n    with open(fname, \"r\") as f:\n        for line in f:\n            if ':' in line:\n                offset, diff = line.split(':')\n                before, after = diff.lstrip().split(' ')\n                after = after.rstrip()\n                offset = int(offset, 16)\n                before = int(before, 16)\n                after  = int(after , 16)\n                changes.append( (offset, before, after) )\n    return changes",
        "sha1": "318c53ef8e02cff1661df9c08f89a4d3e9b6e3f1",
        "id": 609935
    },
    {
        "content": "def _apply_dims(da, new_dims):\n    \"\"\"Applies new dimension names to the last dimensions of the given DataArray.\"\"\"\n    return da.rename(dict(zip(da.dims[-len(new_dims) :], new_dims)))",
        "sha1": "6d449f717e553c7d951f35226be7c9c088933b68",
        "id": 129618
    },
    {
        "content": "def get_previous_tour_by_tourid(current_tour_person_ids,\n                                previous_tour_by_personid,\n                                alts):\n    \"\"\"\n    Matches current tours with attributes of previous tours for the same\n    person.  See the return value below for more information.\n\n    Parameters\n    ----------\n    current_tour_person_ids : Series\n        A Series of person ids for the tours we're about make the choice for\n        - index should match the tours DataFrame.\n    previous_tour_by_personid : Series\n        A Series where the index is the person id and the value is the index\n        of the alternatives of the scheduling.\n    alts : DataFrame\n        The alternatives of the scheduling.\n\n    Returns\n    -------\n    prev_alts : DataFrame\n        A DataFrame with an index matching the CURRENT tours we're making a\n        decision for, but with columns from the PREVIOUS tour of the person\n        associated with each of the CURRENT tours.  Every column of the\n        alternatives will have \"_previous\" added as a suffix to keep\n        differentiated from the current alternatives that will be part of the\n        interaction.\n    \"\"\"\n    previous_tour_by_tourid = \\\n        previous_tour_by_personid.loc[current_tour_person_ids]\n\n    previous_tour_by_tourid = alts.loc[previous_tour_by_tourid]\n\n    previous_tour_by_tourid.index = current_tour_person_ids.index\n    previous_tour_by_tourid.columns = [x+'_previous' for x in\n                                       previous_tour_by_tourid.columns]\n\n    return previous_tour_by_tourid",
        "sha1": "df1f56571abc001cdbcfee624a700cd396d21a52",
        "id": 205486
    },
    {
        "content": "def doi(record):\n    \"\"\"\n    Adds a doi URI to the record if there's a ``doi`` entry in the record\n\n    Parameters\n    ----------\n    record : dict\n        the record to update\n\n    Returns\n    -------\n    dict\n        the given `record` with any updates applied\n    \"\"\"\n    doi = record.get('doi')\n    if doi is not None:\n        if 'link' not in record:\n            record['link'] = []\n        for item in record['link']:\n            if 'doi' in item:\n                break\n        else: # no break\n            if not isinstance(doi, (list, tuple)):\n                doi = [doi]\n\n            for link in doi:\n                if link.startswith('10'):\n                    link = 'http://dx.doi.org/' + link\n                record['link'].append(link)\n    return record",
        "sha1": "89837b9058e9562f5d5081e4adbc5ff833f406cf",
        "id": 424510
    },
    {
        "content": "def has_field(field_name):\n    \"\"\"Returns a function that returns True if the obj has the field_name.\"\"\"\n    def field_checker(obj):\n        return hasattr(obj, field_name)\n    return field_checker",
        "sha1": "4242d9e08a351b5d7d92e0746320d4747ae4a7f1",
        "id": 94742
    },
    {
        "content": "def merge_dfs_by_index(df1, df2):\n    \"\"\"\n    Merge two pandas dataframe index-by-index.\n\n    The dataframes have to share the same index name. Shared indexes will be\n    merged without data loss. In case of conflicting entries a ValueError is \n    raised. The merge operation is symmetric and does not depend on the\n    order of df1 and df2.\n\n    Parameters\n    ----------\n    df1: dataframe\n        Pandas dataframe to be extended\n    df2: dataframe\n        Pandas dataframe with used for extension\n        \n    Returns\n    -------\n    dataframe:\n        The merged dataframe\n\n    Raises\n    ----------\n    ValueError\n        in case of incompatible index names or values\n    \"\"\"\n    if df1.index.name != df2.index.name:\n        raise ValueError('Dataframes have incompatible indexes: '\n                         f'{df1.index.name} != {df2.index.name}.')\n\n    # check for contradicting values by comparing A+B with B+A\n    left_combine = df1.combine_first(df2)\n    right_combine = df2.combine_first(df1)\n\n    # ignoring dtypes when checking equality\n    if not left_combine.astype(object).equals(right_combine.astype(object)):\n        raise ValueError('Dataframes have incompatible values: '\n                         f'{left_combine.compare(right_combine)}')\n\n    return right_combine",
        "sha1": "fe1558403ab2ee7a01b034788edd6ac413f77eaf",
        "id": 16674
    },
    {
        "content": "def containsIf(node):\n    \"\"\" Checks whether the given node contains another if-statement \"\"\"\n\n    if node.type == \"if\":\n        return True\n\n    for child in node:\n        if child is None:\n            pass\n\n        # Blocks reset this if-else problem so we ignore them\n        # (and their content) for our scan.\n        if child.type == \"block\":\n            pass\n\n        # Script blocks reset as well (protected by other function)\n        elif child.type == \"script\":\n            pass\n\n        elif containsIf(child):\n            return True\n\n    return False",
        "sha1": "cadb4c34a4dfd6ced338ea9cacadd91609f5a5bd",
        "id": 290502
    },
    {
        "content": "def parseBool(string: str) -> bool:\n    \"\"\"\n        This method parses a string to a boolean.\n\n        :param string: The string that needs to be parsed to a boolean.\n        :return: The boolean equivalent of the passed string.\n    \"\"\"\n    if string == \"False\":\n        return False\n    if string == \"True\":\n        return True\n    raise ValueError(\"Invalid argument passed to boolean constructor.\")",
        "sha1": "e506eba1311c39645813e9950e0d62288a568237",
        "id": 171687
    },
    {
        "content": "from typing import Dict\n\n\ndef _run_compatibility_patches(json_data: Dict) -> Dict:\n    \"\"\"Patch the incoming JSON to make it compatible.\n\n    Over time the structure of the JSON information used to dump a workflow\n    has changed. These patches are to guarantee that an old workflow is\n    compliant with the new structure. The patches applied are:\n\n    1. Change action.target_url from None to ''\n\n    :param json_data: Json object to process\n    :return: Modified json_data\n    \"\"\"\n    # Target_url field in actions should be present an empty by default\n    for action_obj in json_data['actions']:\n        if action_obj.get('target_url') is None:\n            action_obj['target_url'] = ''\n\n    return json_data",
        "sha1": "e00776cfb89499fbac71cf867830fc00631ac600",
        "id": 20575
    },
    {
        "content": "def check_if_substring_match(lines, substring):\n    \"\"\"Checks the provided lines and determines if a substring is present.\n\n    Parameters\n    ----------\n    lines : list of str\n        The lines to check for a substring match.\n    substring : str\n        The substring to match\n\n    Returns\n    -------\n    bool\n        True if the substring was found in any of the lines. False otherwise.\n    \"\"\"\n\n    return any([substring in line for line in lines])",
        "sha1": "cea5c8deb0b7643f2f1554d89a2e1207a989bb09",
        "id": 506499
    },
    {
        "content": "from functools import reduce\n\n\ndef createLookup(B):\n    \"\"\"Process B to create a reverse lookup scheme that is appropriate for\n    any of the libraries in pyncomb that allow for one or more base sets to\n    be specified.\n\n    Let rev(K) be the reverse lookup dictionary of a list K, i.e. if\n    K[i] = j, then rev(K)[j] = i.\n\n    If B is an integer, then return B.\n    If B is a flat list, then return a pair (B,rev(B)).\n    If B is a list of lists / integers, then return a list Bn with:\n       1. Bn[i] = B[i] if B[i] is an integer.\n       2. Bn[i] = (B[i], rev(B[i])) if B[i] is a list.\n\n    For example, createLookup can translate a base set specification of:\n        [4,['a','b','c'],[11,22]]\n    which represents three base sets [0,1,2,3], ['a','b','c'], [11,22].\n    The returned data will consist of base sets with their reverse lookup\n    data and can be used as the B parameter in any function.\"\"\"\n\n    # Handle integers.\n    if type(B) == int:\n        return B\n\n    # Handle flat lists.\n    if reduce(lambda a,b:a and b, [type(i) != list for i in B], True):\n        return (B, dict([(B[i], i) for i in range(len(B))]))\n\n    # Handle nested lists.\n    Bn = []\n    for D in B:\n        if type(D) == int:\n            Bn.append(D)\n        else:\n            Bn.append((D, dict([(D[i],i) for i in range(len(D))])))\n    return Bn",
        "sha1": "6bb2f907fa71cbd7fa4a2ab1482b0457acd61b36",
        "id": 630337
    },
    {
        "content": "def data(context, data):\n    \"\"\"Replace the tag's content with the current data.\n    \"\"\"\n    return context.tag.clear()[data]",
        "sha1": "caa302cda6f2a8fecc3c453375d703f9438b5412",
        "id": 192799
    },
    {
        "content": "import re\n\n\ndef PostUploadHook(cl, change, output_api):\n  \"\"\"git cl upload will call this hook after the issue is created/modified.\n\n  This hook adds extra try bots list to the CL description in order to run\n  tests on the Windows 10 try bot in addition to CQ try bots.\n  \"\"\"\n  rietveld_obj = cl.RpcServer()\n  issue = cl.issue\n  description = rietveld_obj.get_description(issue)\n  if re.search(r'^CQ_INCLUDE_TRYBOTS=.*', description, re.M | re.I):\n    return []\n\n  bots = [\n    'tryserver.chromium.win:win10_chromium_x64_rel_ng',\n  ]\n\n  results = []\n  new_description = description\n  new_description += '\\nCQ_INCLUDE_TRYBOTS=%s' % ';'.join(bots)\n  results.append(output_api.PresubmitNotifyResult(\n      'Automatically added Win10 bot to run on CQ.'))\n\n  if new_description != description:\n    rietveld_obj.update_description(issue, new_description)\n\n  return results",
        "sha1": "b909505b807c79f5a8c08b530cc82bccb9b96338",
        "id": 530070
    },
    {
        "content": "def removeAmbiguousBases(seq):\n    \"\"\" Converts ambiguous bases to - as required by PhyML\"\"\"\n    new_seq = \"\"\n    for char in seq:\n        if char not in  [\"A\", \"T\", \"C\", \"G\"]:\n            char = \"-\"\n        new_seq += char\n    return new_seq",
        "sha1": "ea8c04b804d67697e3479fe41901c5e65b2deaea",
        "id": 378680
    },
    {
        "content": "def find_feature_map_to_input_scale_and_offset(pre_processed_input_image,feature_maps):\n    \"\"\"\n        Finds the scale and offset from the feature map (output) of the CNN classifier to the pre-processed input image of the CNN\n    \"\"\"\n    # Find shapes of feature maps and input images to the classifier CNN\n    input_image_shape = pre_processed_input_image.shape\n    feature_map_shape = feature_maps.shape\n    img_height, img_width, _ = input_image_shape\n    features_height, features_width, _ = feature_map_shape\n\n    # Find mapping from features map (output of vggmodel.predict) back to the input image\n    feature_to_input_x = img_width / features_width\n    feature_to_input_y = img_height / features_height\n\n    # Put anchor points in the centre of \n    feature_to_input_x_offset = feature_to_input_x/2\n    feature_to_input_y_offset = feature_to_input_y/2\n\n    return feature_to_input_x, feature_to_input_y, feature_to_input_x_offset, feature_to_input_y_offset",
        "sha1": "7803a98bcefced7471e3b3b0890e5e35a4c5b00c",
        "id": 528819
    },
    {
        "content": "import torch\n\n\ndef center(k: torch.Tensor) -> torch.Tensor:\n    \"\"\"Center features of a kernel by pre- and post-multiplying by the centering matrix H.\n\n    In other words, if k_ij is dot(x_i, x_j), the result will be dot(x_i - mu_x, x_j - mu_x).\n\n    :param k: a n by n Gram matrix of inner products between xs\n    :return: a n by n centered matrix\n    \"\"\"\n    n = k.size()[0]\n    if k.size() != (n, n):\n        raise ValueError(\n            f\"Expected k to be nxn square matrix, but it has size {k.size()}\"\n        )\n    H = (\n        torch.eye(n, device=k.device, dtype=k.dtype)\n        - torch.ones((n, n), device=k.device, dtype=k.dtype) / n\n    )\n    return H @ k @ H",
        "sha1": "a42ee442452133966a797580aa42fe0885a5e718",
        "id": 240485
    },
    {
        "content": "def min_operations(target):\n    \"\"\"\n    Return number of steps taken to reach a target number\n    input: target number (as an integer)\n    output: number of steps (as an integer)\n    \"\"\"\n    steps = 0\n    while target != 0:\n        steps += 1\n        if target % 2 == 1:\n            target -= 1\n        else:\n            target /= 2\n    return steps",
        "sha1": "68a877e9b15721ebf6de45d22ea9291e70520928",
        "id": 470321
    },
    {
        "content": "from typing import List\n\n\ndef gen_string(i: int, base: int, digits: List[str]):\n    \"\"\"Returns a string representation of an integer given the list of digits.\n\n    Args:\n        i (int): The integer to generate\n        base (int): The base representation of the number. (based on the length of the list)\n        digits (int): The list of digits to use.\n    \"\"\"\n    num_string = \"\"\n    while i > 0:\n        # Prepend the digit\n        num_string = digits[i % base] + num_string\n        # Effectively right shifting the number (dividing by the base)\n        i //= base\n\n    return num_string",
        "sha1": "ad7fe50db1f72fdc4bc5e53e09a6abdea5ff1bd7",
        "id": 673499
    },
    {
        "content": "def dms_to_d(dms):\n    \"\"\" Degrees, minutes, seconds to degrees \"\"\"\n    return dms[0] + (dms[1] + dms[2] / 60.0) / 60.0",
        "sha1": "cfa05e26a19a42cc41d0ebe8d1f64090d146dbbd",
        "id": 214557
    },
    {
        "content": "import re\n\n\ndef matches_key(key: str, message: str):\n    \"\"\"\n    Find key in message\n    :param key: key to find in message\n    :param message: message\n    :return: matched key\n    \"\"\"\n    return len(re.findall(r'(?<!\\d)' + key, message)) > 0",
        "sha1": "c7e82627514c077a45bfe611cad822be2eda3fc1",
        "id": 577673
    },
    {
        "content": "def params_to_dict(params, dct):\n    \"\"\"\n    Updates the 'dct' dictionary with the 'params' dictionary, filtering out\n    all those whose param value is None.\n    \"\"\"\n    for param, val in params.items():\n        if val is None:\n            continue\n        dct[param] = val\n    return dct",
        "sha1": "33716a7048d4b2dc68beab5e40b6819d4cfd0ad6",
        "id": 528703
    },
    {
        "content": "def name_parser(name):\n    \"\"\"Parse image names that are in the following format:\n\n        {RANK}_{IMAGE_IDX}_{BOX_PAIR_IDX}_{LABEL}_{SCORE}.png\n    \"\"\"\n    seg = name.split(\"_\")\n    sample_type = \"Positive\" if int(seg[3]) else \"Negative\"\n\n    return \"Rank: {} \".format(seg[0]) + sample_type \\\n        + \"<br>Image: {}, Pair: {}<br>\".format(seg[1], seg[2]) \\\n        + \"Score: {}\".format(seg[4][:-4])",
        "sha1": "3fb76030fcb627d5ff2c91f41f4d6a4198673f3d",
        "id": 153218
    },
    {
        "content": "import math\n\n\ndef poisson(lmbda: float,\n            x: int) -> float:\n    \"\"\"\n    Calculates the Poisson pdf.\n\n    Extended description of function.\n\n    Parameters\n    ----------\n    lmbda: description\n    x: description\n\n    Returns\n    -------\n    The appropriate integer for that selection.\n\n    \"\"\"\n    return math.exp(-1 * lmbda) * math.pow(lmbda, x)/math.factorial(x)",
        "sha1": "b122c32bf4e35b594a5b300b28a0fe3b4126f628",
        "id": 391610
    },
    {
        "content": "def has_positions(writer):\n    \"\"\"Check if writer has positions for engine selection\"\"\"\n    if writer is None:\n        return False\n    return len(writer.positionHints) > 0 or len(writer.variablePositions) > 0 or len(writer.factorPositions) > 0",
        "sha1": "57f1cd40beb70174ea41beca2ff867cd81605673",
        "id": 564356
    },
    {
        "content": "def get_path(dct, path, default=None, raise_error=True):\n    \"\"\"return the value from a key path in a nested dictionary\"\"\"\n    subdct = dct\n    for i, key in enumerate(path):\n        if not isinstance(subdct, dict) or key not in subdct:\n            if raise_error:\n                raise KeyError(\"path does not exist in dct: {}\".format(path[0:i+1]))\n            else:\n                return default\n        subdct = subdct[key]\n    return subdct",
        "sha1": "45c67b7e4758f2dafb0edd7f4ba567743abaedac",
        "id": 363481
    },
    {
        "content": "import math\n\n\ndef rad2deg(angle):\n    \"\"\"Converts value in radians to degrees.\"\"\"\n    return angle * 180.0 / math.pi",
        "sha1": "9f2390f3f9c97a7ce5db87c7a1dbbee0a5818099",
        "id": 275550
    },
    {
        "content": "def node_roles(client, node_id):\n    \"\"\"\n    Return the list of roles assigned to the node identified by ``node_id``\n\n    :arg client: An :class:`elasticsearch.Elasticsearch` client object\n    :rtype: list\n    \"\"\"\n    return client.nodes.info()['nodes'][node_id]['roles']",
        "sha1": "6b1bb0cc3eba1791ffaf022fe00248162603299e",
        "id": 515247
    },
    {
        "content": "def unpack4bitbuf(buf: list[int]\n                   ) ->list[int]:\n    \"\"\"Unpacks a 4-bit buffer\n        into a list\n\n    Args:\n        buf: unsigned byte array\n\n    Returns:\n        list\n    \"\"\"\n    retval = []\n    for b in buf:\n        retval += [b >> 4, b & 0xf]\n    return retval",
        "sha1": "d4bddd2ca41ff463426a8f96c2f0c7add9cf4f49",
        "id": 281300
    },
    {
        "content": "import random\n\n\ndef sample_baselines(bls, seed=None):\n    \"\"\"\n    Sample a set of baselines with replacement (to be used to generate a\n    bootstrap sample).\n\n    Parameters\n    ----------\n    bls : list of either tuples or lists of tuples\n        Set of unique baselines to be sampled from. If groups of baselines\n        (contained in lists) are provided, each group will be treated as a\n        single object by the sampler; its members will not be sampled\n        separately.\n\n    seed : int, optional\n        Random seed to use if randomize=True. If None, no random seed will be\n        set. Default: None.\n\n    Returns\n    -------\n    sampled_bls : list of tuples or lists of tuples\n        Bootstrap-sampled set of baselines (will include multiple instances of\n        some baselines).\n    \"\"\"\n    if seed is not None: random.seed(seed)\n\n    # Sample with replacement; return as many baselines/groups as were input\n    return [random.choice(bls) for i in range(len(bls))]",
        "sha1": "e53c66141fa4a90b6d74d64d59ecfae112d93d67",
        "id": 577109
    },
    {
        "content": "def splitem(query):\n    \"\"\"\n    Split a query into choices\n\n    >>> splitem('dog, cat')\n    ['dog', 'cat']\n\n    Disregards trailing punctuation.\n\n    >>> splitem('dogs, cats???')\n    ['dogs', 'cats']\n    >>> splitem('cats!!!')\n    ['cats']\n\n    Allow or\n    >>> splitem('dogs, cats or prarie dogs?')\n    ['dogs', 'cats', 'prarie dogs']\n\n    Honors serial commas\n    >>> splitem('dogs, cats, or prarie dogs?')\n    ['dogs', 'cats', 'prarie dogs']\n\n    Allow choices to be prefixed by some ignored prompt.\n    >>> splitem('stuff: a, b, c')\n    ['a', 'b', 'c']\n    \"\"\"\n    prompt, sep, query = query.rstrip('?.!').rpartition(':')\n\n    choices = query.split(',')\n    choices[-1:] = choices[-1].split(' or ')\n\n    return [choice.strip() for choice in choices if choice.strip()]",
        "sha1": "cef457ed560db569ff9c8714d3b2f767c8935c85",
        "id": 88555
    },
    {
        "content": "from typing import Dict\n\n\ndef extract_params_from_string(string: str) -> Dict:\n    \"\"\"Convert completion string to dictionary of params.\n\n    Args:\n        string: Raw text version of action chain\n\n    Returns:\n        Dict: Extracted parameters\n\n    Example:\n        string: to=>mom ### body=>I Love her ### application=>???\n        out: {\"recipient\": \"mom\", \"body\":\"I love her\", \"application\": None}\n    \"\"\"\n    params = string.strip().split(\"###\")\n    out = {}\n    for param in params:\n        argument, value = param.strip().split(\"=>\")\n        out[argument.strip()] = value.strip()\n    return out",
        "sha1": "95d035c580374c39afa3bc548dc6d128592865d8",
        "id": 199166
    },
    {
        "content": "def combineImagePaths(centerImagePath, leftImagePath, rightImagePath, centerMeasurement, leftMeasurement, rightMeasurement):\n    \"\"\"\n    combines cnter/left/right images and measurements to one list\n    \"\"\"\n    # combine measurements\n    measurements = []\n    measurements.extend(centerMeasurement)\n    measurements.extend(leftMeasurement)\n    measurements.extend(rightMeasurement)\n    \n    # combine image paths \n    imagePaths = []\n    imagePaths.extend(centerImagePath)\n    imagePaths.extend(leftImagePath)\n    imagePaths.extend(rightImagePath)\n    \n    return imagePaths, measurements",
        "sha1": "3c8e1dbe16bc25d9efcd029c0c28f194f275e5f8",
        "id": 35367
    },
    {
        "content": "from typing import Iterable\n\n\ndef xy(transform, rows, cols, offset='center'):\n    \"\"\"Returns the x and y coordinates of pixels at `rows` and `cols`.\n    The pixel's center is returned by default, but a corner can be returned\n    by setting `offset` to one of `ul, ur, ll, lr`.\n\n    Parameters\n    ----------\n    transform : affine.Affine\n        Transformation from pixel coordinates to coordinate reference system.\n    rows : list or int\n        Pixel rows.\n    cols : list or int\n        Pixel columns.\n    offset : str, optional\n        Determines if the returned coordinates are for the center of the\n        pixel or for a corner.\n\n    Returns\n    -------\n    xs : list\n        x coordinates in coordinate reference system\n    ys : list\n        y coordinates in coordinate reference system\n    \"\"\"\n\n    single_col = False\n    single_row = False\n    if not isinstance(cols, Iterable):\n        cols = [cols]\n        single_col = True\n    if not isinstance(rows, Iterable):\n        rows = [rows]\n        single_row = True\n\n    if offset == 'center':\n        coff, roff = (0.5, 0.5)\n    elif offset == 'ul':\n        coff, roff = (0, 0)\n    elif offset == 'ur':\n        coff, roff = (1, 0)\n    elif offset == 'll':\n        coff, roff = (0, 1)\n    elif offset == 'lr':\n        coff, roff = (1, 1)\n    else:\n        raise ValueError(\"Invalid offset\")\n\n    xs = []\n    ys = []\n    for col, row in zip(cols, rows):\n        x, y = transform * transform.translation(coff, roff) * (col, row)\n        xs.append(x)\n        ys.append(y)\n\n    if single_row:\n        ys = ys[0]\n    if single_col:\n        xs = xs[0]\n\n    return xs, ys",
        "sha1": "1d9b75b8e2c75a2c8fec834b37039eb53b5368ed",
        "id": 522104
    },
    {
        "content": "def test_propagation(hop, vheight, dist,\n                     region_hmax={\"D\":115.0,\"E\":150.0,\"F\":900.0},\n                     region_hmin={\"D\":75.0,\"E\":115.0,\"F\":150.0}):\n    \"\"\"Test the propagation path for realism.  Use the basic properties of HF\n    radars.\n    D-region (<= 115 km) is detected at distances less than 500 km\n    E-region (115 - 150(or 200?) km) is detected at distances lass than X km\n    F-region (>= 150 km) is detected at all distances\n    Parameters\n    -------------\n    hop : (float)\n        Number of hops traveled by this radar signal\n    vheight : (float)\n        Virtual height of the peak of the propagation path\n    dist : (float)\n        Distance from the radar to the first peak of the propagation path in km\n    region_hmax : (dict)\n        Maximum virtual heights allowed in each ionospheric layer.\n        (default={\"D\":115.0,\"E\":150.0,\"F\":400.0})\n    region_hmin : (dict)\n        Minimum virtual heights allowed in each ionospheric layer.\n        (default={\"D\":75.0,\"E\":115.0,\"F\":150.0})\n    Returns\n    -----------\n    good : (boolian)\n        True if the path is realistic, False if it is not\n    \"\"\"\n    good = True\n\n    if \"D\" in region_hmax.keys() and vheight <= region_hmax[\"D\"]:\n        if hop > 0.5 or dist > 500.0:\n            # D-region backscatter is restricted to 0.5 hop ionospheric\n            # backscatter near the radar (flat earth-approximation holds,\n            # great circle distance is rounded down, to simplify things)\n            good = False\n    elif \"E\" in region_hmax.keys() and vheight <= region_hmax[\"E\"]:\n        if hop < 1.5 and dist > 900.0:\n            # 0.5E and 1.0E backscatter is restrictued to slant path distances\n            # of 1000 km or less.  1.5E backscatter is typically seen at far\n            # range gates and can be entirely E-region or F/E-region\n            good = False\n\n    return good",
        "sha1": "b675ef1bf3511bf0db98e107636a9cde072fa26c",
        "id": 269505
    },
    {
        "content": "import torch\n\n\ndef collate_fn(videos):\n    \"\"\"\n    Collate function for the PyTorch data loader.\n    Merges all batch videos in a tensor with shape (length, batch, channels, width, height) and converts their pixel\n    values to [0, 1].\n    Parameters\n    ----------\n    videos : list\n        List of uint8 NumPy arrays representing videos with shape (length, batch, width, height, channels).\n    Returns\n    -------\n    torch.*.Tensor\n        Batch of videos with shape (length, batch, channels, width, height) and float values lying in [0, 1].\n    \"\"\"\n    \n    seq_len = len(videos[0])\n    batch_size = len(videos)\n    nc = 1 if videos[0].ndim == 3 else 3\n    w = videos[0].shape[3]\n    h = videos[0].shape[2]\n    tensor = torch.zeros((seq_len, batch_size, nc, h, w))\n    for i, video in enumerate(videos):\n        if nc == 1:\n            tensor[:, i, 0] += video\n        if nc == 3:\n            tensor[:, i] += video\n    return tensor",
        "sha1": "b813228250e14ffdd382f86dda096dda09f9ae56",
        "id": 342273
    },
    {
        "content": "from typing import Dict\nfrom typing import Any\nimport random\n\n\ndef add_noise_to_dict_values(dictionary: Dict[Any, float], noise_param: float) -> Dict[Any, float]:\n    \"\"\"\n    Returns a new dictionary with noise added to every key in ``dictionary``.  The noise is\n    uniformly distributed within ``noise_param`` percent of the value for every value in the\n    dictionary.\n    \"\"\"\n    new_dict = {}\n    for key, value in dictionary.items():\n        noise_value = value * noise_param\n        noise = random.uniform(-noise_value, noise_value)\n        new_dict[key] = value + noise\n    return new_dict",
        "sha1": "a767579d475f188a02fd43ec1d95e05091e5bb20",
        "id": 568207
    },
    {
        "content": "def bool_to_returncode(success):\n  \"\"\"Return 0 if |success|. Otherwise return 1.\"\"\"\n  if success:\n    print('Success.')\n    return 0\n\n  print('Failed.')\n  return 1",
        "sha1": "4effd1340bdf46b91a2fcadd3379d78eb788d282",
        "id": 400426
    },
    {
        "content": "def string_to_weld_literal(s):\n    \"\"\"\n    Converts a string to a UTF-8 encoded Weld literal byte-vector.\n\n    Examples\n    --------\n    >>> string_to_weld_literal('hello')\n    '[104c,101c,108c,108c,111c]'\n\n    \"\"\"\n    return \"[\" + \",\".join([str(b) + 'c' for b in list(s.encode('utf-8'))]) + \"]\"",
        "sha1": "d85b016091988c9307cbed56aafdd5766c3c9be5",
        "id": 1869
    },
    {
        "content": "def binary_mask_to_str(m):\n    \"\"\"Given an iterable or list of 1s and 0s representing a mask, this returns a string\n    mask with '+'s and '-'s.\"\"\"\n    m = list(map(lambda x: \"-\" if x == 0 else \"+\", m))\n    return \"\".join(m)",
        "sha1": "32706ead1d42cb1a21a6e6552a64847941a2aede",
        "id": 532265
    },
    {
        "content": "def _get_ebs_volume_state(volume):\n    \"\"\"\n    Fetch input EBS volume's latest state from backend.\n\n    :param boto3.resources.factory.ec2.Volume: Volume that needs state update.\n\n    :returns: EBS volume with latest state known to backend.\n    :rtype: boto3.resources.factory.ec2.Volume\n\n    \"\"\"\n    volume.reload()\n    return volume",
        "sha1": "16f44962ab294c16d3de8180c7eaf9dfb3669f76",
        "id": 549672
    },
    {
        "content": "async def get_authenticated_user(*, app, logger):\n    \"\"\"Get information about the authenticated GitHub user.\n\n    This function wraps the `GET /user\n    <https://developer.github.com/v3/users/#get-the-authenticated-user>`_\n    method.\n\n    Parameters\n    ----------\n    app : `aiohttp.web.Application`\n        The app instance.\n    logger\n        A `structlog` logger instance with bound context related to the\n        Kafka event.\n\n    Returns\n    -------\n    response : `dict`\n        The parsed JSON response body from GitHub.\n    \"\"\"\n    ghclient = app[\"root\"][\"templatebot/gidgethub\"]\n    response = await ghclient.getitem(\"/user\")\n    return response",
        "sha1": "b7577fbe203b08080dd2c2e9c9f3af132095e4d7",
        "id": 79140
    },
    {
        "content": "def _weight_function(G, weight):\n  \"\"\"Returns a function that returns the weight of an edge.\n  The returned function is specifically suitable for input to\n  functions :func:`_dijkstra` and :func:`_bellman_ford_relaxation`.\n  Parameters\n  ----------\n  G : NetworkX graph.\n  weight : string or function\n      If it is callable, `weight` itself is returned. If it is a string,\n      it is assumed to be the name of the edge attribute that represents\n      the weight of an edge. In that case, a function is returned that\n      gets the edge weight according to the specified edge attribute.\n  Returns\n  -------\n  function\n      This function returns a callable that accepts exactly three inputs:\n      a node, an node adjacent to the first one, and the edge attribute\n      dictionary for the eedge joining those nodes. That function returns\n      a number representing the weight of an edge.\n  If `G` is a multigraph, and `weight` is not callable, the\n  minimum edge weight over all parallel edges is returned. If any edge\n  does not have an attribute with key `weight`, it is assumed to\n  have weight one.\n  \"\"\"\n  if callable(weight):\n    return weight\n  # If the weight keyword argument is not callable, we assume it is a\n  # string representing the edge attribute containing the weight of\n  # the edge.\n  if G.is_multigraph():\n    return lambda u, v, d: min(attr.get(weight, 1) for attr in d.values())\n  return lambda u, v, data: data.get(weight, 1)",
        "sha1": "77b3b7884ba855241abcdd4a2353ac8c0c1b77b4",
        "id": 87240
    },
    {
        "content": "import asyncio\nimport warnings\n\n\ndef get_or_create_event_loop() -> asyncio.AbstractEventLoop:\n    \"\"\"Return the currently set event loop or create a new event loop if there\n    is no set event loop.\n\n    Starting from python3.10, asyncio.get_event_loop() raises a DeprecationWarning\n    when there is no event loop set, this deprecation will be enforced starting from\n    python3.12\n\n    This function serves as a future-proof wrapper over asyncio.get_event_loop()\n    that preserves the old behaviour.\n    \"\"\"\n    with warnings.catch_warnings():\n        warnings.filterwarnings('ignore', category=DeprecationWarning)\n\n        try:\n            return asyncio.get_event_loop()\n        except RuntimeError:\n            loop = asyncio.new_event_loop()\n            asyncio.set_event_loop(loop)\n            return loop",
        "sha1": "701c5e88cef6d28a24ed9de320e3f8bb7cae4a9f",
        "id": 489041
    },
    {
        "content": "def merge_two_dicts(dict_x, dict_y):\n    \"\"\"\n    Given two dicts, merge them into a new dict.\n\n    Uses a shallow copy.\n    \"\"\"\n    new_dict = dict_x.copy()\n    new_dict.update(dict_y)\n    return new_dict",
        "sha1": "5154aa0fe81093b4bb92212639d6211a6f050d7f",
        "id": 585650
    },
    {
        "content": "def is_valid_boolstr(val):\n    \"\"\"Check if the provided string is a valid bool string or not.\"\"\"\n    val = str(val).lower()\n    return val in ('true', 'false', 'yes', 'no', 'y', 'n', '1', '0')",
        "sha1": "07dd2cfb6889191aa1645c0d5df563675269bdbe",
        "id": 494803
    },
    {
        "content": "def format_message_response(params):\n    \"\"\"\n    Format automatic response\n    |params| is None if the system can't process the user's message\n    or is not confident enough to give a response.\n    Otherwise, |params| is a triple that consists of\n    the question that the system is trying to answer,\n    the response it has for that question, and the recommended command to run.\n    Return the automatic response that will be sent back to the user's chat box.\n    \"\"\"\n    if params is None:\n        return 'Thank you for your question. Our staff will get back to you as soon as we can.'\n    else:\n        question, response, command = params\n        result = 'This is the question we are trying to answer: ' + question + '\\n'\n        result += response + '\\n'\n        result += 'You can try to run the following command: \\n'\n        result += command\n        return result",
        "sha1": "13073c711a8d5ec5031a7712822f0ae58beef84f",
        "id": 48486
    },
    {
        "content": "from typing import List\n\n\ndef route_ranks(scores: List[float]) -> List[int]:\n    \"\"\"\n    Compute the rank of route scores. Rank starts at 1\n\n    :param scores: the route scores\n    :return: a list of ranks for each route\n    \"\"\"\n    ranks = [1]\n    for idx in range(1, len(scores)):\n        if abs(scores[idx] - scores[idx - 1]) < 1e-8:\n            ranks.append(ranks[idx - 1])\n        else:\n            ranks.append(ranks[idx - 1] + 1)\n    return ranks",
        "sha1": "06828f144d3cca13b01dde9bad104a648dbe71c5",
        "id": 674809
    },
    {
        "content": "import itertools\nimport shlex\n\n\ndef combine_arg_list_opts(opt_args):\n  \"\"\"Helper for processing arguments like impalad_args. The input is a list of strings,\n  each of which is the string passed into one instance of the argument, e.g. for\n  --impalad_args=\"-foo -bar\" --impalad_args=\"-baz\", the input to this function is\n  [\"-foo -bar\", \"-baz\"]. This function combines the argument lists by tokenised each\n  string into separate arguments, if needed, e.g. to produce the output\n  [\"-foo\", \"-bar\", \"-baz\"]\"\"\"\n  return list(itertools.chain(*[shlex.split(arg) for arg in opt_args]))",
        "sha1": "77cfc6fa54201083c2cb058b8a9493b7d020273e",
        "id": 708830
    },
    {
        "content": "def get_columns_from_data_range_rows(data, start_row, end_row, columns):\n    \"\"\"\n    This function is a wrapper around the indexing of pandas data frames. This function gets some of the rows\n    from the data frame specified by the start_row and end_row.\n    :param data: Pandas data frame\n    :param start_row: int value specifying the row to start from\n    :param end_row: int value specifying the row to end at\n    :param columns: The columns which are to be selected.\n    :return: Pandas data frame\n    \"\"\"\n    return data.loc[start_row:end_row, columns]",
        "sha1": "550a2ec731d6d7ff817263fd01940f1948752001",
        "id": 67324
    },
    {
        "content": "def summation(n, term):\n    \"\"\"Sum the first N terms of a sequence.\n\n    >>> summation(5, cube)\n    225\n    \"\"\"\n    total, k = 0, 1\n    while k <= n:\n        total, k = total + term(k), k + 1\n    return total",
        "sha1": "62afffc81316b695b39a53d183bbff80c8a6889f",
        "id": 604472
    },
    {
        "content": "def marc21_to_type(self, key, value):\n    \"\"\"\n    Get document type.\n\n    Books: LDR/6-7: am\n    Journals: LDR/6-7: as\n    Articles: LDR/6-7: aa + add field 773 (journal title)\n    Scores: LDR/6: c|d\n    Videos: LDR/6: g + 007/0: m|v\n    Sounds: LDR/6: i|j\n    \"\"\"\n    type_of_record = value[6]\n    bibliographic_level = value[7]\n\n    if type_of_record == 'a' and bibliographic_level == 'm':\n        return 'book'\n    if type_of_record == 'a' and bibliographic_level == 's':\n        return 'journal'\n    if type_of_record == 'a' and bibliographic_level == 'a':\n        return 'article'\n    if type_of_record in ['i', 'j']:\n        return 'sound'\n    if type_of_record == 'g':\n        return 'video'\n    if type_of_record in ['c', 'd']:\n        return 'score'\n\n    if bibliographic_level == 'm':\n        return 'book'\n\n    return 'score'",
        "sha1": "e070e994aad2e9283986682e86a6ad3a5e9c01fa",
        "id": 287129
    },
    {
        "content": "async def delete_cache(bot, member):\n    \"\"\"\n    Delete a member's cache.\n    \"\"\"\n    try:\n        if not bot.message_cache[member.id]:\n            return False\n\n        bot.message_cache.pop(member.id)\n\n    except (TypeError, KeyError):\n        return False",
        "sha1": "2e21a017837b045dfbd0d33badd855c914177e47",
        "id": 442822
    },
    {
        "content": "def square(x):\n    \"\"\"The square of a number\n\nParameters\n----------\nx: float\n    Number to square\n\nReturns\n-------\nfloat\n    Square of x\n\"\"\"\n    return x**2.",
        "sha1": "6d25ebc3d2956f5d70157cc8312063fba62519a8",
        "id": 448310
    },
    {
        "content": "def tricks_to_result(tricks: int, level: int):\n\t\"\"\"\n\tConvert tricks made to a result, e.g. 8 tricks\n\tin a 4-level contract becomes -2\n\t\"\"\"\n\treturn tricks - (level + 6)",
        "sha1": "4671dd5bf9afe30c15e0cda39d2fb350b7608eac",
        "id": 251963
    },
    {
        "content": "def progressBar(curr, total, b_length=60, prefix='', suffix='', decimals=0):\n    \"\"\"Return the progress bar\n\n    Code from https://gist.github.com/aubricus/f91fb55dc6ba5557fbab06119420dd6a\n    \"\"\"\n    str_format = \"{0:.\" + str(decimals) + \"f}\"\n    percents = str_format.format(100 * (curr/ float(total)))\n    filled_len = int(round(b_length * curr / float(total)))\n    bar = '\u2588' * filled_len + '\u2591' * (b_length - filled_len)\n\n    return '\\r%s |%s| %s%s %s' % (prefix, bar, percents, '%', suffix)",
        "sha1": "e91e516707800fbb901b710199bc625affc69af6",
        "id": 652348
    },
    {
        "content": "from typing import Union\nfrom pathlib import Path\n\n\ndef read_as_text(filename: Union[Path, str]) -> str:\n    \"\"\"Read a file as text.\"\"\"\n    with open(filename) as file_handle:\n        txt = file_handle.read()\n    return txt",
        "sha1": "0709c56c204c536e89df8ed62bea2c94091839da",
        "id": 490022
    },
    {
        "content": "def get_mongolike(d, key):\n    \"\"\"\n    Retrieve a dict value using dot-notation like \"a.b.c\" from dict {\"a\":{\"b\":{\"c\": 3}}}\n    Args:\n        d (dict): the dictionary to search\n        key (str): the key we want to retrieve with dot notation, e.g., \"a.b.c\"\n\n    Returns:\n        value from desired dict (whatever is stored at the desired key)\n\n    \"\"\"\n    lead_key = key.split(\".\", 1)[0]\n    try:\n        lead_key = int(lead_key)  # for searching array data\n    except:\n        pass\n\n    if \".\" in key:\n        remainder = key.split(\".\", 1)[1]\n        return get_mongolike(d[lead_key], remainder)\n    return d[lead_key]",
        "sha1": "f3a68c1ee51ff3215d2adaf5dc4735190db7d153",
        "id": 158363
    },
    {
        "content": "def get_appliance_flow_bandwidth_stats(\n    self,\n    ne_id: str,\n    flow_id: int,\n    flow_seq_num: int,\n) -> list:\n    \"\"\"Get the so far accumulated bandwidth stats about the flow\n\n    .. list-table::\n        :header-rows: 1\n\n        * - Swagger Section\n          - Method\n          - Endpoint\n        * - flow\n          - GET\n          - /flow/flowBandwidthStats/{neId}/q\n\n    :param ne_id: Appliance id in the format of integer.NE e.g. ``3.NE``\n    :type ne_id: str\n    :param flow_id: Flow ID\n    :type flow_id: int\n    :param flow_seq_num: Flow sequence number\n    :type flow_seq_num: int\n    :return: Returns list of dictionaries for so far accumulated\n        bandwidth stats about the flow\n    :rtype: list[dict]\n    \"\"\"\n    return self._get(\n        \"/flow/flowBandwidthStats/{}/q?id={}&seq={}\".format(\n            ne_id, flow_id, flow_seq_num\n        )\n    )",
        "sha1": "99672d1ad0b4adebded4905cfc81e95b8adba098",
        "id": 32734
    },
    {
        "content": "def recognize_honkai_server(uid: int) -> str:\n    \"\"\"Recognizes which server a Honkai UID is from.\"\"\"\n    if 10000000 < uid < 100000000:\n        return \"overseas01\"\n    elif 100000000 < uid < 200000000:\n        return \"usa01\"\n    elif 200000000 < uid < 300000000:\n        return \"eur01\"\n\n    # From what I can tell, CN UIDs are all over the place,\n    # seemingly even overlapping with overseas UIDs...\n    # Probably gonna need some input from actual CN players here, but I know none.\n    # It could be that e.g. global range is 2e8 ~ 2.5e8\n    raise ValueError(f\"UID {uid} isn't associated with any server\")",
        "sha1": "5e99488e719199ff51459fd6ad10d1c21123030f",
        "id": 219697
    },
    {
        "content": "def get_page_id(title, query_results):\n    \"\"\"\n    Extracts the title's pageid from the query results.\n    Assumes queries of the form query:pages:id,\n    and properly handle the normalized method.\n    Returns -1 if it cannot find the page id\n    \"\"\"\n    if 'normalized' in query_results['query'].keys():\n        for normalized in query_results['query']['normalized']:\n            if title == normalized['from']:\n                title = normalized['to']\n    for page in query_results['query']['pages']:\n        if title == query_results['query']['pages'][page]['title']:\n            return str(query_results['query']['pages'][page]['pageid'])\n    return str(-1)",
        "sha1": "8ddce8c95b4a312b7478dc53d5b3a7fb53bba39e",
        "id": 37830
    },
    {
        "content": "from typing import Callable\nfrom typing import Dict\n\n\ndef loop(setup: Callable, cond: Callable, func: Callable) -> Dict:\n    \"\"\"A for loop like in C, C++, Rust, etc. Every function gets a dictionary for talking between them.\n\n    Args:\n        setup (Callable): The function that gets called before the loop\n        cond (Callable): The condition function returns True or False\n        func (Callable): The function to call for each iteration\n\n    Returns:\n        Dict: The dictionary\n    \"\"\"\n    gls = {}\n    setup(gls)\n    while cond(gls):\n        func(gls)\n    return gls",
        "sha1": "335a3b60be6f26f0e25fd2464c3f31a3302f5e0d",
        "id": 132205
    },
    {
        "content": "def import_name(modulename, name=None):\n    \"\"\" Import identifier ``name`` from module ``modulename``.\n\n        If ``name`` is omitted, ``modulename`` must contain the name after the\n        module path, delimited by a colon.\n\n        Parameters:\n            modulename (str): Fully qualified module name, e.g. ``x.y.z``.\n            name (str): Name to import from ``modulename``.\n\n        Returns:\n            object: Requested object.\n    \"\"\"\n    if name is None:\n        modulename, name = modulename.rsplit(':', 1)\n    module = __import__(modulename, globals(), {}, [name])\n    return getattr(module, name)",
        "sha1": "a320063e878db935f8a2409c7487617e2c9f1802",
        "id": 13117
    },
    {
        "content": "def replace_fix(text, source, target, *args):\n    \"\"\"\n    Replace ``source`` by ``target`` in ``text`` and return ``text``.\n    \"\"\"\n    return text.replace(source, target)",
        "sha1": "7b29fc143dbc104e0ad9eb10b675a5563cac5d8b",
        "id": 235405
    },
    {
        "content": "def guidance_UV(index):\n    \"\"\"Return Met Office guidance regarding UV exposure based on UV index\"\"\"\n    if 0 < index < 3:\n        guidance = \"Low exposure. No protection required. You can safely stay outside\"\n    elif 2 < index < 6:\n        guidance = \"Moderate exposure. Seek shade during midday hours, cover up and wear sunscreen\"\n    elif 5 < index < 8:\n        guidance = \"High exposure. Seek shade during midday hours, cover up and wear sunscreen\"\n    elif 7 < index < 11:\n        guidance = \"Very high. Avoid being outside during midday hours. Shirt, sunscreen and hat are essential\"\n    elif index > 10:\n        guidance = \"Extreme. Avoid being outside during midday hours. Shirt, sunscreen and hat essential.\"\n    else:\n        guidance = None\n    return guidance",
        "sha1": "07e1544a9f2683457d79ec9f0d9e84b7640d9d9c",
        "id": 623625
    },
    {
        "content": "def create_iterable_dataset(torch_transforms_module, pipeline_results):\n    \"\"\"\n    Create a PyTorch iterable dataset that loads samples from pipeline results.\n\n    :param torch_transforms_module: The imported torch.transforms module.\n    :param pipeline_results: Pipeline results iterator.\n    :return: Dataset that has valid PyTorch images saved as tensors and density maps.\n    \"\"\"\n    class PipelineDataset:\n        def __init__(self):\n            self.images_and_density_maps = pipeline_results\n            self.image_transform = torch_transforms_module.Compose([\n                torch_transforms_module.ToTensor()\n            ])\n\n        def __iter__(self):\n            for image, density_map in self.images_and_density_maps:\n                yield self.image_transform(image.copy().astype(\"float32\")), density_map.copy().astype(\"float32\")\n\n    return PipelineDataset()",
        "sha1": "1de2f8be910da07e1a63e177f59d9bf0467edbe1",
        "id": 12118
    },
    {
        "content": "from typing import List\n\n\ndef enumerate_responses(n: int = 5) -> List[str]:\n    \"\"\"\n    Generate a list of all possible guess responses.\n\n    Args:\n        n (int, optional): size of guess/response. Defaults to 5.\n\n    Returns:\n        List[str]: list of valid responses\n    \"\"\"\n    results = [\"\"]\n    possibilities = [\"r\", \"y\", \"g\"]\n    for _ in range(n):\n        results = [r + c for r in results for c in possibilities]\n    return results",
        "sha1": "ae8c8515371710f26b99c79cdd6fadf082280218",
        "id": 423783
    },
    {
        "content": "def _make_label_to_row_indices(labels):\n    \"\"\"\n    Returns a map from short labels (the first 5 elements of the label\n    vector) to the list of row indices of rows in the dense design matrix\n    with that label.\n\n    For Small NORB, all unique short labels have exactly one row index.\n\n    For big NORB, a short label can have 0-N row indices.\n    \"\"\"\n    result = {}\n\n    for row_index, label in enumerate(labels):\n\n        short_label = tuple(label[:5])\n        if result.get(short_label, None) is None:\n            result[short_label] = []\n\n        result[short_label].append(row_index)\n\n    return result",
        "sha1": "ae4ade8062d593b87f765c33f4ef44081482306f",
        "id": 258518
    },
    {
        "content": "def truncate(string, limit=1000):\n    \"\"\"\n    Truncate the string to the limit number of\n    characters\n    \"\"\"\n    if len(string) > limit:\n        return string[:limit-20]+'...'+string[-17:]\n    else:\n        return string",
        "sha1": "734bb79094ffa50f940d043ff01639d611b64732",
        "id": 157741
    },
    {
        "content": "def map_pathogen_id_to_name(pathogen_id):\n    \"\"\"\n    \"\"\"\n    mapping = {\n        \"p00\": \"Acinetobacter baumannii\",\n        \"p01\": \"Baceroides fragilis\",\n        \"p02\": \"Burkholderia cepacia\",\n        \"p03\": \"Candida albicans\",\n        \"p04\": \"Candida giabrata\",\n        \"p05\": \"Candida parapsilosis\",\n        \"p06\": \"Candida tropicalis\",\n        \"p07\": \"Citrobacter diversus\",\n        \"p08\": \"Citrobacter freundii\",\n        \"p09\": \"Citrobacter koseri\",\n        \"p10\": \"Clostridium difficile\",\n        \"p11\": \"Enterobacter aerogenes\",\n        \"p12\": \"Enterobacter cloacae\",\n        \"p13\": \"Enterococcus faecalis\",\n        \"p14\": \"Enterococcus faecium\",\n        \"p15\": \"Escherichia coli\",\n        \"p16\": \"Haemophilus influenzae\",\n        \"p17\": \"Klebsiella oxytoca\",\n        \"p18\": \"Klebsiella pneumoniae\",\n        \"p19\": \"Moraxella catarrhalis\",\n        \"p20\": \"Morganella morganii\",\n        \"p21\": \"Proteaus mirabilis\",\n        \"p22\": \"Pseudomonas aeruginosa\",\n        \"p23\": \"Serratia marcescens\",\n        \"p24\": \"Staphylococcus aureus (MSSA, MRSA)\",\n        \"p25\": \"Staphylococcus auricularis\",\n        \"p26\": \"Staphylococcus capitis ssp. capitis\",\n        \"p27\": \"Staphylococcus capitis ssp. unspecified\",\n        \"p28\": \"Staphylococcus coagulase negative\",\n        \"p29\": \"Staphylococcus cohnii\",\n        \"p30\": \"Staphylococcus epidermidis\",\n        \"p31\": \"Staphylococcus gallinarum\",\n        \"p32\": \"Staphylococcus haemolyticus\",\n        \"p33\": \"Staphylococcus hominis\",\n        \"p34\": \"Staphylococcus lentus\",\n        \"p35\": \"Staphylococcus lugdenensis\",\n        \"p36\": \"Staphylococcus saccharolyticus\",\n        \"p37\": \"Staphylococcus saprophyticus\",\n        \"p38\": \"Staphylococcus schleiferi\",\n        \"p39\": \"Staphylococcus sciuri\",\n        \"p40\": \"Staphylococcus simulans\",\n        \"p41\": \"Staphylococcus warneri\",\n        \"p42\": \"Staphylococcus xylosus\",\n        \"p43\": \"Stenotrophomonas maltophilia\",\n        \"p44\": \"Streptococcus group A (Streptococcus pyogenes)\",\n        \"p45\": \"Streptococcus group B (Streptococcus agalactiae)\",\n        \"p46\": \"Streptococcus group D (Sterptococcus bovis)\",\n        \"p47\": \"Streptococcus pneumoniae (pneumococcus)\",\n        \"p49\": \"Strepotcuccus viridans (includes angiosus, bovis, mitis, mutans, salivarius)\",\n        \"p48\": \"Torulopsis glabrata (Candida glabrata)\",\n        \"p50\": \"Other pathogen\",\n        }\n\n    pathogen = ''\n    if pathogen_id in mapping:\n        pathogen = mapping[pathogen_id]\n\n    return pathogen",
        "sha1": "2ef24ff2876320ce6a7f7da72376c412199eb7ba",
        "id": 678544
    },
    {
        "content": "def _gradient_extrapolation(extrapolation):\n    \"\"\"\nGiven the extrapolation of a field, returns the extrapolation mode of the corresponding gradient field.\n    :param extrapolation: string or struct of strings\n    :return: same type as extrapolation\n    \"\"\"\n    return {'periodic': 'periodic',\n            'boundary': 'constant',\n            'constant': 'constant'}[extrapolation]",
        "sha1": "95d4ea7cf5cd413b5dd69d25335b3270dedfb6ea",
        "id": 116207
    },
    {
        "content": "def getNthDigit(n):\n    \"\"\"return the nth digit created by concatenating the positive integers\"\"\"\n    \"\"\"12345678910111213...\"\"\"\n    i = 0\n    currNum = 1\n    while(True):\n        i += len(str(currNum))\n        if(i >= n):\n            diff = i - n\n            return int(str(currNum)[-1-diff])\n\n        currNum += 1",
        "sha1": "9f73c92aa1a4ea09cbc93569d5808144d1cb9e5a",
        "id": 223443
    },
    {
        "content": "import string\nimport random\n\n\ndef id_generator(size=6, chars=string.ascii_lowercase + string.digits):\n    \"\"\"GENERATE A RANDOM STRING TO BE USED AS AN ID\n\n    Args:\n        size (int, optional): size of the string. Defaults to 6.\n        chars (str, optional): charachters to be used to generate the string.\n                            Defaults to string.ascii_lowercase+string.digits.\n\n    Returns:\n        [str]: a random chain of charachters\n    \"\"\"\n    return \"\".join(random.choice(chars) for _ in range(size))",
        "sha1": "a236df96884ac9de0fdadf3ca1e290d46944721d",
        "id": 502191
    },
    {
        "content": "import math\n\n\ndef calculate_distance(coord1, coord2, box_length=None):\n    \"\"\"\n    Calculate the distance between two 3D coordinates.\n    \n    Parameters\n    ----------\n    coord1, coord2 : list\n        The atomic coordinates [x, y, z]\n        \n    box_length : float, optional\n        The box length. This function assumes box is a cube.\n        \n    Returns\n    -------\n    distance: float\n        The distance between the two atoms\n    \n    \"\"\"\n    #Do periodic boundary corrections if given a box_length\n    if box_length is not None:\n        #Distance = sqrt(sum of square differences of each dimension)\n        #initialize the sum of square differences to 0\n        sum_square_diff = 0\n        \n        #Iterate through dimensions\n        for i in range(len(coord1)):\n            \n            #Find the raw distance between the two coordinates in this dimension\n            dim_dist_uncorrected = math.fabs(coord1[i] - coord2[i])\n            \n            #Periodic boundary corrections\n            #If raw distance is less than half the box_length, no corrections are needed\n            if dim_dist_uncorrected <= box_length / 2:\n                dim_dist_corrected = dim_dist_uncorrected\n            \n            #If raw distance is greater than half the box length and less than one whole box length, correct accordingly\n            elif (dim_dist_uncorrected > box_length / 2 and dim_dist_uncorrected <= box_length):\n                dim_dist_corrected = box_length - dim_dist_uncorrected\n            \n            #If raw distance is greater than one whole box length, correct accordingly\n            else:\n                dim_dist_corrected = dim_dist_uncorrected - box_length * round(dim_dist_uncorrected / box_length)\n            \n            #Add the square difference to the total sum\n            sum_square_diff += (dim_dist_corrected)**2\n        \n        #Calculate distance after finding the sum of square differences\n        distance = math.sqrt(sum_square_diff)\n        \n    #Otherwise assume no periodic boundaries    \n    else:\n        sum_square_diff = 0\n    \n        for i in range(len(coord1)):\n            sum_square_diff += (coord1[i] - coord2[i])**2\n\n        distance = math.sqrt(sum_square_diff)\n\n    return distance",
        "sha1": "954bdf6dc66ca4edf58b1b96aa63343930e1a604",
        "id": 21539
    },
    {
        "content": "def make_data(data):\n    \"\"\"Create a data object.\"\"\"\n    return {\"data\": data, \"total\": len(data)}",
        "sha1": "8acd0104d81ef673a8b2f6497581aea89dedc0cb",
        "id": 187075
    },
    {
        "content": "def optimal_noverlap(win_name,win_len):\n    \"\"\"\n    This function is intended to support scipy.signal.stft calls with noverlap parameter.\n    :param win_name: (str) name of the window (has to follow scipy.signal.windows naming)\n    :param win_len: (int) lenght of the FFT window\n    :return : (int) optimal overlap in points for the given window type\n    \"\"\"\n\n    window_to_overlap_coef = {'hann': 0.5,\n                              'hamming': 0.75,\n                              'blackmanharris': 0.75,\n                              'blackman': 2/3,\n                              'flattop': 0.75,\n                              'boxcar': 0}\n    try:\n        noverlap = int(win_len*window_to_overlap_coef[win_name])\n    except KeyError as exc:\n        print(exc)\n        print('The window you have selected is not recognized or does not have optimal overlap. Setting window overlap to default 75%.')\n        noverlap = int(win_len*0.75)\n\n    return noverlap",
        "sha1": "d98db08a9e08b6639a38a324799d1141e65b1eb4",
        "id": 19554
    },
    {
        "content": "def parse_args(given, control):\n\t\"\"\"checks if some of the given args are valid for a condition\"\"\"\n\tpairs =  [(x, y) for x in given for y in control]\n\tfor g, c in pairs:\n\t\tif g == c:\n\t\t\treturn True\n\treturn False",
        "sha1": "e5fa311de742961c4a978ceb128c194238a2f6fa",
        "id": 349173
    },
    {
        "content": "def ValidateRDId(ID):\n  \"\"\" returns whether or not an RDId is valid\n\n  >>> ValidateRDId('RDCmpd-000-009-9')\n  1\n  >>> ValidateRDId('RDCmpd-009-000-009-8')\n  1\n  >>> ValidateRDId('RDCmpd-009-000-109-8')\n  0\n  >>> ValidateRDId('bogus')\n  0\n\n  \"\"\"\n  ID = ID.replace('_', '-')\n  splitId = ID.split('-')\n  if len(splitId) < 4:\n    return 0\n  accum = 0\n  for entry in splitId[1:-1]:\n    for char in entry:\n      try:\n        v = int(char)\n      except ValueError:\n        return 0\n      accum += v\n  crc = int(splitId[-1])\n  return accum % 10 == crc",
        "sha1": "c8149a135f211a237be73de448a931365e208667",
        "id": 202913
    },
    {
        "content": "def check_engine_for_migrate(engine):\n    \"\"\"\n    check if a db engine support database migration\n\n    Args:\n        engine (sqlalchemy.engine.base.Engine): a sqlalchemy engine\n\n    Return:\n        bool: whether the engine support migration\n    \"\"\"\n    return engine.dialect.supports_alter",
        "sha1": "d3d1d15831123f75cfc7d38173b9119d784be6a1",
        "id": 385358
    },
    {
        "content": "def parse_args(parser):\n    \"\"\"\n    Parse commandline arguments.\n    \"\"\"\n\n    parser.add_argument('--bench-class',  type=str, choices=['train', 'perf-infer', 'perf-train'], required=True, help='Choose test class')\n\n    return parser",
        "sha1": "93d041a9d4d4d4b540dc5831b268b4dc0649b1fd",
        "id": 588521
    },
    {
        "content": "def datetime_to_string(date_time):\n    \"\"\"\n    convert datetime object to a string\n    :param date_time: datetime in datetime format\n    :return: datetime in string format\n    \"\"\"\n    return date_time.strftime(\"%Y-%m-%d %H:%M:%S\")",
        "sha1": "bab1f9737b9d668996c837c4c3bc4c5e06082751",
        "id": 472485
    },
    {
        "content": "import re\n\n\ndef variables(text):\n    \"\"\"Retrieves dictionary of variables in text.\n\n    >>> text = '''\n    ... &namobs\n    ...    ln_sst = .TRUE. ! Logical switch for SST observations\n    ... /\n    ... '''\n    >>> variables(text)\n    {'ln_sst': '.TRUE.'}\n\n    :param text: Input text to process\n\n    :returns: A dictionary of variable, value pairs.\n\n    \"\"\"\n    data = {}\n    pairs = re.findall(r\"\\n\\s*(\\w+)\\s*=\\s*(.+?)\\s*(?=[!\\n])\", text)\n    for key, value in pairs:\n        data[key] = value\n    return data",
        "sha1": "d533cc68dd6f92fc8055652a3819b1b4a6434944",
        "id": 466564
    },
    {
        "content": "def process_row(bq_row):\n    \"\"\"\n    Convert bq_row into dictionary and replace identifiers to nominal values\n\n    Args:\n        bq_row: BigQuery row\n\n    Returns:\n         dictionary\n\n    \"\"\"\n\n    # modify opaque numeric race code into human-readable data\n    races = dict(\n        zip([1, 2, 3, 4, 5, 6, 7, 18, 28, 39, 48], [\n            'White', 'Black', 'American Indian', 'Chinese', 'Japanese',\n            'Hawaiian', 'Filipino', 'Asian bq_row', 'Korean', 'Samaon',\n            'Vietnamese'\n        ]))\n    instance = dict()\n\n    instance['is_male'] = str(bq_row['is_male'])\n    instance['mother_age'] = bq_row['mother_age']\n\n    if 'mother_race' in bq_row and bq_row['mother_race'] in races:\n        instance['mother_race'] = races[bq_row['mother_race']]\n    else:\n        instance['mother_race'] = 'Unknown'\n\n    instance['plurality'] = bq_row['plurality']\n    instance['gestation_weeks'] = bq_row['gestation_weeks']\n    instance['mother_married'] = str(bq_row['mother_married'])\n    instance['cigarette_use'] = str(bq_row['cigarette_use'])\n    instance['alcohol_use'] = str(bq_row['alcohol_use'])\n    instance['weight_pounds'] = str(bq_row['weight_pounds'])\n\n    return instance",
        "sha1": "8c3e4143b974ba5f43f2be8a9161a027792f6e58",
        "id": 477038
    },
    {
        "content": "import ntpath\n\n\ndef path_leaf(path):\n    \"\"\" guaranteed filename from path; works on Win / OSX / *nix \"\"\"\n    head, tail = ntpath.split(path)\n    return tail or ntpath.basename(head)",
        "sha1": "5c3bc4b4d172330da9cc553274e0eb58567dfbfe",
        "id": 684477
    },
    {
        "content": "def _extract_annotations_from_span(span):\n    \"\"\"Extract and convert time event annotations to Honeycomb annotations\"\"\"\n    if span.time_events is None:\n        return []\n\n    annotations = []\n    for time_event in span.time_events:\n        annotation = time_event.annotation\n        if not annotation:\n            continue\n\n        annotations.append({'timestamp': time_event.timestamp,\n                            'value': annotation.description})\n\n    return annotations",
        "sha1": "0bc378c392c2f1d8a466686feda9a6eaf9fd1935",
        "id": 497837
    },
    {
        "content": "import audioop\n\n\ndef convert_framerate(fragment, width, nchannels, framerate_in, framerate_out):\n    \"\"\"\n    Convert framerate (sampling rate) of the input fragment.\n\n    Parameters\n    ----------\n    fragment : bytes object\n        Specifies the original fragment.\n    width : int\n        Specifies the fragment's original sampwidth.\n    nchannels : int\n        Specifies the fragment's original nchannels.\n    framerate_in : int\n        Specifies the fragment's original framerate.\n    framerate_out : int\n        Specifies the fragment's desired framerate.\n\n    Returns\n    -------\n    bytes\n\n    \"\"\"\n    if framerate_in == framerate_out:\n        return fragment\n\n    new_fragment, _ = audioop.ratecv(fragment, width, nchannels, framerate_in, framerate_out, None)\n    return new_fragment",
        "sha1": "703290e1edc7b45742b3002faef81fc981b82642",
        "id": 577244
    },
    {
        "content": "def get_bd_addr(packet):\n  \"\"\"Helper function to get bd_addr filed from HCI CMD/EVT packet.\n\n  Args:\n    packet: packet object from pyshark.\n  Returns:\n    bd_addr value (MAC string). If the packet doesn't have bd_addr or not is not\n    HCI CMD/EVT, will return None.\n  \"\"\"\n  bd_addr = None\n  if 'bthci_evt' in packet:\n    bd_addr = packet['bthci_evt'].get('bd_addr')\n  elif 'bthci_cmd' in packet:\n    bd_addr = packet['bthci_cmd'].get('bd_addr')\n  return bd_addr",
        "sha1": "37b30d7b6770d526e9a60065358bcee1647e0726",
        "id": 646043
    },
    {
        "content": "import random\n\n\ndef mutate(individual, m, k):\n    \"\"\"\n    This function apply the mutation operator\n    :param k:Number of genes that were chosen to maybe mutate\n    :param individual:list\n    :param m:Mutation Rate\n    :return:Return the individual after the mutation\n    \"\"\"\n\n    lst = random.sample(range(0, len(individual)), k)\n    # print(lst)\n    for i in lst:\n        if random.random() < m:\n            if individual[i] == 0:\n                individual[i] = 1\n    # print(individual)\n    return individual",
        "sha1": "ad6da81d134ba83c4b464f42924f343ea871cb3c",
        "id": 578377
    },
    {
        "content": "def noop(**kwargs):\n    \"\"\"Takes everything, does nothing, classic no operation function\"\"\"\n    return kwargs",
        "sha1": "5308cf2736bcf1029c369a66cdf2db98423f90e2",
        "id": 512632
    },
    {
        "content": "def get_lazo_sketches(data_profile, filter_=None):\n    \"\"\"\n    Get Lazo sketches of the input dataset, if available.\n\n    :param data_profile: Profiled input dataset.\n    :param filter_: list of column indices to return.\n       If an empty list, return all the columns.\n    :return: dict, where key is the column index, and value is a tuple\n        (n_permutations, hash_values, cardinality)\n    \"\"\"\n\n    lazo_sketches = dict()\n\n    for column_index, column in enumerate(data_profile['columns']):\n        if 'lazo' in column:\n            if not filter_ or column_index in filter_:\n                lazo_sketches[(column_index,)] = (\n                    column['lazo']['n_permutations'],\n                    column['lazo']['hash_values'],\n                    column['lazo']['cardinality'],\n                )\n\n    return lazo_sketches",
        "sha1": "fa95561785faef3dcbd7615edf5a025b369fba4b",
        "id": 363867
    },
    {
        "content": "def assess_gene_knockout_viability(model, gene_name):\n    \"\"\"Assesses viability of model when `gene_name` is knocked out.\n    Returns True if viable, False if lethal.\"\"\"\n    with model:\n        model.genes.query(gene_name).pop().knock_out()\n        if model.optimize().objective_value == 0:\n            return False\n        return True",
        "sha1": "509adf6474a9105c675d09cc7d299ca25bec125a",
        "id": 578036
    },
    {
        "content": "from datetime import datetime\nfrom dateutil import tz\n\n\ndef utc_to_pst(timestamp_str, in_fmt, out_fmt):\n    \"\"\"Convert UTC timestamp to Local time (PST).\"\"\"\n    timestamp = datetime.strptime(timestamp_str, in_fmt)\n    utc_tz = tz.gettz('UTC')\n    pst_tz = tz.gettz('US/Pacific')\n    timestamp = timestamp.replace(tzinfo=utc_tz)\n    pst_timestamp = timestamp.astimezone(pst_tz)\n    return pst_timestamp.strftime(out_fmt)",
        "sha1": "3ce58d5dee03c6468c8acdd2ea9d4bfbe4435bbf",
        "id": 123809
    },
    {
        "content": "from typing import Dict\n\n\ndef parse_passport_fields(passport: str) -> Dict[str, str]:\n    \"\"\"convert the string representation of a single passport to a dict\"\"\"\n    fields = [f.split(\":\") for f in passport.replace(\"\\n\", \" \").split(\" \")]\n    return {k: v for k, v in fields}",
        "sha1": "b8efb1dc0885417fe1aea162edd1cd454558231e",
        "id": 149443
    },
    {
        "content": "from pathlib import Path\n\n\ndef get_tensors(path):\n    \"\"\"Generate paths for tensors.\"\"\"\n    tensors = {\n        \"tensor_psf\": Path(path, \"tensor.psf\"),\n        \"tensor_pdb\": Path(path, \"tensor.pdb\"),\n        \"tensor_para_psf\": Path(path, \"tensor_para.psf\"),\n        \"tensor_para_pdb\": Path(path, \"tensor_para.pdb\"),\n        \"tensor_dani_psf\": Path(path, \"tensor_dani.psf\"),\n        \"tensor_dani_pdb\": Path(path, \"tensor_dani.pdb\"),\n        }\n    return tensors",
        "sha1": "960d86753aa640a75840ab59326610b5822ebdd7",
        "id": 486366
    },
    {
        "content": "import math\n\n\ndef get_longitude_m_per_d(latitude_d: float) -> float:\n    \"\"\"Returns the distance per degree longitude at a latitude.\"\"\"\n    return math.cos(math.radians(latitude_d)) * 40075",
        "sha1": "1422e092b2e784c42c5eeceadf93211b170c6769",
        "id": 288684
    },
    {
        "content": "import re\n\n\ndef get_total_tag_counts(tag_bed_file):\n    \"\"\"\n    Get total tag counts given the current experimental run\n    file should be a bed file.\n    \"\"\"\n    counts =0;\n    infile = open(tag_bed_file,'r');\n    for line in infile:\n            \"\"\" check to make sure not a header line \"\"\"\n            if not re.match(\"track\", line):\n                counts += 1;\n    infile.close();\n    return counts;",
        "sha1": "05b062d496f676b36ac0634f245c8b310217fd0a",
        "id": 89452
    },
    {
        "content": "def _clean_lc(lc):\n    \"\"\" Perform Lightkurve operations on object.\n\n    Performes basic cleaning of a light curve, removing nans, outliers,\n    median filtering etc.\n\n    Parameters\n    ----------\n    lc : Lightkurve.LightCurve instance\n        Lightkurve object to be cleaned\n\n    Returns\n    -------\n    lc : Lightkurve.LightCurve instance\n        The cleaned Lightkurve object\n        \n    \"\"\"\n\n    lc = lc.remove_nans().flatten().remove_outliers()\n    return lc",
        "sha1": "821c4c917dbc9e9a33be311b113e5d984118bf3b",
        "id": 523488
    },
    {
        "content": "from datetime import datetime\n\n\ndef make_timestamp(date_string):\n    \"\"\"\n    A row-operation that converts an Efergy timestamp of the form\n    \"2015-12-31 12:34:56\" into a Python datetime object.\n    \"\"\"\n    try:\n        return datetime.strptime(date_string, '%Y-%m-%d %H:%M:%S')\n    except:\n        return None",
        "sha1": "a23846aa19e97df9ad55cb9b303d3b07ce3b1d01",
        "id": 693696
    },
    {
        "content": "def get_fw_by_task_index(wf, task_tag, index=1):\n    \"\"\"\n    Given a workflow object (with connection to the db) returns the wf corresponding to the task_type.\n\n    Args:\n        wf: a fireworks Workflow object.\n        task_tag: the task tag associated with the task as defined in abinit_workflows. Should not include the index.\n        index: the numerical or text index of the task. If negative the the last fw corresponding to task_tag will\n            be selected. If None, no index will be considered and the first match will be returned.\n\n    Returns:\n        a fireworks Firework object. None if no match is found.\n    \"\"\"\n    task_index = None\n    if index is not None and index >= 0:\n        task_index = \"{}_{}\".format(task_tag, index)\n\n    selected_fw = None\n    max_ind = -1\n    for fw in wf.fws:\n        fw_task_index = fw.spec.get('wf_task_index', '')\n        if task_index:\n            if fw_task_index == task_index:\n                return fw\n        else:\n            if task_tag in fw_task_index:\n                if index is None:\n                    return fw\n                # the last part of the task_index can be text (i.e. \"autoparal\") so the conversion to int may fail\n                # if no other indices has been found select that one\n                try:\n                    fw_ind = int(fw_task_index.split('_')[-1])\n                    if fw_ind > max_ind:\n                        selected_fw = fw\n                        max_ind = fw_ind\n                except Exception:\n                    if selected_fw is None:\n                        selected_fw = fw\n\n    return selected_fw",
        "sha1": "d6fcba6fe7d5cfa8ec1f98fe141c94922b14d60a",
        "id": 261483
    },
    {
        "content": "import torch\n\n\ndef loss_fn(outputs, labels):\n    \"\"\"\n    Compute the cross entropy loss given outputs and labels.\n    Args:\n        outputs: (Variable) dimension batch_size x 6 - output of the model\n        labels: (Variable) dimension batch_size, where each element is a value in [0, 1, 2, 3, 4, 5]\n    Returns:\n        loss (Variable): cross entropy loss for all images in the batch\n    Note: you may use a standard loss function from http://pytorch.org/docs/master/nn.html#loss-functions. This example\n          demonstrates how you can easily define a custom loss function.\n    \"\"\"\n    num_examples = outputs.size()[0]\n    return -torch.sum(outputs[range(num_examples), labels])/num_examples",
        "sha1": "4c828f784ae6cf58986016c416a91db65fdba21f",
        "id": 515205
    },
    {
        "content": "def role(request) -> str:\n    \"\"\"Return the job execution role to use in a test\"\"\"\n    return request.config.getoption(\"--role\")",
        "sha1": "f01afe4f3dc31b410cc6e69e6a8b5e7da7c973cd",
        "id": 301506
    },
    {
        "content": "def getAngleStatus(angle):\n    \"\"\" returns status based on the angle. Status codes:\n    0 - straight (-10 to +10)\n    1 - left or down (less than -10)\n    2 - right or up (greater than 10)\n    \"\"\"\n    if angle < -10:\n        return 1\n    elif angle > 10:\n        return 2\n    return 0",
        "sha1": "235c988bc285be4fbfbb3f3113030b79585c1976",
        "id": 26430
    },
    {
        "content": "def spacing(area, shape):\n    \"\"\"\n    Returns the spacing between grid nodes\n\n    Parameters:\n\n    * area\n        ``(x1, x2, y1, y2)``: Borders of the grid\n    * shape\n        Shape of the regular grid, ie ``(nx, ny)``.\n\n    Returns:\n\n    * ``[dx, dy]``\n        Spacing the y and x directions\n\n    Examples:\n\n    >>> print(spacing((0, 10, 0, 20), (11, 11)))\n    [1.0, 2.0]\n    >>> print(spacing((0, 10, 0, 20), (11, 21)))\n    [1.0, 1.0]\n    >>> print(spacing((0, 10, 0, 20), (5, 21)))\n    [2.5, 1.0]\n    >>> print(spacing((0, 10, 0, 20), (21, 21)))\n    [0.5, 1.0]\n\n    \"\"\"\n    x1, x2, y1, y2 = area\n    nx, ny = shape\n    dx = (x2 - x1)/(nx - 1)\n    dy = (y2 - y1)/(ny - 1)\n    return [dx, dy]",
        "sha1": "456a895baf875fb32dc9319602620848176b3ba1",
        "id": 62546
    },
    {
        "content": "def get_hashtags(tokens):\n    \"\"\"Extract hashtags from a set of tokens\"\"\"\n    hashtags = [x for x in tokens if x.startswith(\"#\")]\n    return hashtags",
        "sha1": "ae75465950411e447515a919ed73e303519ed9a6",
        "id": 531876
    },
    {
        "content": "def sanitize_sequence(seq):\n    \"\"\"\n    Check if sequence is valid, return list.\n\n    Parameters\n    ----------\n    seq : Union[Sequence[ints, ...], Sequence[floats, ...], DNDarray, torch.tensor]\n\n    Returns\n    -------\n    seq : List\n    \"\"\"\n    if isinstance(seq, list):\n        return seq\n    elif isinstance(seq, tuple):\n        return list(seq)\n    else:\n        raise TypeError(\"seq must be a list or a tuple, got {}\".format(type(seq)))",
        "sha1": "41bfe8bcbf0c5721e6edf574e1f0275a9c0005ad",
        "id": 528962
    },
    {
        "content": "def getVideoIdFromEntry(entry):\n    \"\"\"Get video ID from a YouTube entry.\"\"\"\n    return entry.id.text.split(\"/\")[-1]",
        "sha1": "a922cd4220e31ccb43f179f3598867c59d2b5eaa",
        "id": 575885
    },
    {
        "content": "def str_is_int(s):\n    \"\"\"Checks if a string can be converted to int.\"\"\"\n    if not isinstance(s, str):\n        raise TypeError('str_is_int function only accepts strings.')\n    try:\n        int(s)\n        return True\n    except ValueError as e:\n        if 'invalid literal' in str(e):\n            return False\n        else:\n            raise e",
        "sha1": "b0dbcc5065b07563cf4c38c7006ec92d2d121cc2",
        "id": 566088
    },
    {
        "content": "def hex_to_str(input: str) -> str:\n    \"\"\"\n    Convert a hex string into a human-readable string.\n    XRPL uses hex strings as inputs in fields like `domain`\n    in the `AccountSet` transaction.\n\n    Args:\n        input: hex-encoded string to convert\n\n    Returns:\n        Input encoded as a human-readable string.\n    \"\"\"\n    return bytes.fromhex(input).decode()",
        "sha1": "b701102d21ae9169f45fe58014234c830a696704",
        "id": 196094
    },
    {
        "content": "def d(d):\n    \"\"\"Decode the given bytes instance using UTF-8.\"\"\"\n    return d.decode('UTF-8')",
        "sha1": "07efd6024ddf39b2e2e592457d9d52d2256cfc22",
        "id": 627685
    },
    {
        "content": "def get_csi(LWdown, T, e, e_ad=0.22, k=0.47, return_all=False):\n    \"\"\"\n    Clear Sky Index after Marty and Philipona, 2000\n\n    Parameters\n    ----------\n    LWdown : ndarray\n        longwave down as measured/ simulated [W/m**2]\n    T : ndarray\n        temperature at lowest level [K]\n    e : ndarray\n        water vapor pressure [Pa]\n    e_ad : float\n        altitude dependent clear-sky emittance of a completely dry atmosphere\n    k : float\n        location dependent coefficient\n    return_all : bool\n        if False returns CSI, else returns (CSI, app_em, clear_sky_app_em)\n\n    Returns\n    -------\n    CSI : ndarray\n        clear sky index =\n\n    Notes\n    -----\n    According to Marty and Philipona\n    CSI <= 1 : clear sky, no clouds\n    CSI > 1 : cloudy sky, overcast\n    \"\"\"\n\n    # Stephan-Boltzmann constant\n    sigma = 5.67 * 10**-8\n\n    # apparent emittance of actual sky\n    app_em = LWdown / (sigma * T**4)\n\n    # clear sky apparent emittance\n    cs_app_em = e_ad + k * (e * T) ** (1./7)\n\n    CSI = app_em / cs_app_em\n\n    if return_all:\n        return (CSI, app_em, cs_app_em)\n    else:\n        return CSI",
        "sha1": "f5ea61e1a61ecb8beedf6fe5bf72b7a2b9bfcb41",
        "id": 206845
    },
    {
        "content": "def return_as_dict(data, description):\n    \"\"\"\n    :param data: return value of  cursor.execute.fetchone (single row only!)\n    example:\n        ('355e8...0a', datetime.datetime(2020, 10, 24, 22, 54), \\\n         'message', 100, 0, '[]', 0, '[]', 0, '[]', 0, 41825)\n\n    :param description:  value of cursor.description\n\n    :return: return dictionary of tuples {desc[0]: data[0], desc[1]: data[1]}\n    \"\"\"\n    columns = [column[0] for column in description]\n    dictionary = {}\n    if data:\n        for i in range(len(data)):\n            dictionary[columns[i]] = data[i]\n    else:\n        for column in columns:\n            dictionary[column] = None\n\n    return dictionary",
        "sha1": "bd2cc724bbeb16254e39a4b005da1778903a889e",
        "id": 122219
    },
    {
        "content": "def get_citation(citations, text):\n    \"\"\"\n        Return the 1st citation whose text matches the given text\n    \"\"\"\n    matched = [c for c in citations if c['text'] == text]\n    if matched:\n        return matched[0]\n    return None",
        "sha1": "e7d44c7b65694cbb17ec89027e5b6c2ff6fdd698",
        "id": 278997
    },
    {
        "content": "def _FlattenList(l):\n  \"\"\"Flattens lists of lists into plain lists, recursively.\n\n  For example, [[4, 5, 6], [7, 8], []] will become [4, 5, 6, 7, 8].\n\n  Non-list elements will get wrapped in a list, so 'foo' becomes ['foo'].\n  None becomes [].\n\n  Args:\n    l: a list, or not.\n\n  Returns:\n    A flattened list.\n  \"\"\"\n  ret = []\n  if l is None:\n    pass\n  elif not isinstance(l, list):\n    ret = [l]\n  else:\n    for item in l:\n      ret.extend(_FlattenList(item))\n\n  return ret",
        "sha1": "ca4df94f51568e93e217bdf50987f47fd0a257f8",
        "id": 650332
    },
    {
        "content": "from typing import Tuple\n\n\ndef split_tk_geometry_str(geom_str: str) -> Tuple[int, int, int, int]:\n    \"\"\"\n    Convert a tk geometry string into a tuple of integers.\n\n    :param geom_str: Must take the form of \"<width_in_pixels>x<height_in_pixels>+<pos_x>+<pos_y>\"\n\n                     where\n\n                        <width_in_pixels> is a decimal string\n                        <height_in_pixels> is a decimal string\n                        <pos_x> is a decimal string\n                        <pos_y> is a decimal string\n\n    :return: Four item tuple consisting of the width in pixels, the height in pixels, the x position\n             in pixels and the y position in pixels. All items are integers.\n    \"\"\"\n    geom_str = geom_str.lower().strip()\n    shape, posx, posy = geom_str.split('+')\n    width, height = shape.split('x')\n\n    return int(width), int(height), int(posx), int(posy)",
        "sha1": "33d8e855d475f58e3c50432577b3f238af0ce055",
        "id": 294191
    },
    {
        "content": "def percentage(percent, whole):\n    \"\"\"\n    Returns percentage value.\n    \"\"\"\n    return (percent * whole) / 100.0",
        "sha1": "a91d5f0b95349247cc49f41b3e9370b2bd724ae1",
        "id": 513880
    },
    {
        "content": "from typing import Any\n\n\ndef is_null(value: Any) -> bool:\n    \"\"\"\n    Test if a variable is None\n    \"\"\"\n    return not value",
        "sha1": "f45ec0abfd97082b66d5c642b1a58adf39596d8e",
        "id": 252957
    },
    {
        "content": "def count_table_rows(cursor, table: str):\n    \"\"\"Return the number of rows in a MySQL table.\n\n    Parameters\n    ----------\n    cursor : cursor.MySQLCursor\n        A MySQLCursor object. See return value ``cursor`` of :func:`.connect`.\n    table : str\n        Name of a table in the NEPC database at ``cursor``.\n\n    Returns\n    -------\n    : int\n        Number of rows in ``table``.\n\n    \"\"\"\n    cursor.execute(\"select count(*) from \" + table + \";\")\n    table_rows = cursor.fetchall()\n    return table_rows[0][0]",
        "sha1": "23633e0bd44a8253dc484a34db32b08a7bc1a873",
        "id": 524471
    },
    {
        "content": "def magND(v):\n    \"\"\"Returns magnitude of an nD vector\"\"\"\n    return sum(vv**2 for vv in v) ** 0.5",
        "sha1": "e5582072bd3c2eb79267e76c9a9a92d1efc2bd6b",
        "id": 298388
    },
    {
        "content": "def make_mergeable_tensors(t1, t2):\n    \"\"\"Expand a new dimension in t1 and t2 and expand them so that both\n    tensors will have the same number of timesteps.\n    Args:\n        t1 (torch.Tensor): tensor with shape (bs, ..., m, d1)\n        t2 (torch.Tensor): tensor with shape (bs, ..., n, d2)\n    Returns:\n        torch.Tensor: (bs, ..., m, n, d1)\n        torch.Tensor: (bs, ..., m, n, d2)\n    \"\"\"\n    assert t1.dim() == t2.dim()\n    assert t1.dim() >= 3\n    assert t1.shape[:-2] == t2.shape[:-2]\n    # new_shape = [-1, ..., m, n, -1]\n    new_shape = [-1 for _ in range(t1.dim() + 1)]\n    new_shape[-3] = t1.shape[-2]  # m\n    new_shape[-2] = t2.shape[-2]  # n\n    # (bs, ..., m, d1) -> (bs, ..., m, 1, d1) -> (bs, ..., m, n, d1)\n    new_t1 = t1.unsqueeze(-2).expand(new_shape)\n    # (bs, ..., n, d2) -> (bs, ..., 1, n, d2) -> (bs, ..., m, n, d2)\n    new_t2 = t2.unsqueeze(-3).expand(new_shape)\n    return new_t1, new_t2",
        "sha1": "6d99db7fca3506150aaa3e00e55683ed97c99c56",
        "id": 213876
    },
    {
        "content": "import torch\n\n\ndef make_positions(tensor, pad_index: int):\n    \"\"\"Replace non-padding symbols with their position numbers.\n    Position numbers begin at pad_index+1. Padding symbols are ignored.\n    \"\"\"\n    masked = tensor.ne(pad_index).long()\n    return torch.cumsum(masked, dim=1) * masked + pad_index",
        "sha1": "074e6dbc8074fef982ac3385374fb7e7ba2dcb71",
        "id": 79692
    },
    {
        "content": "import json\n\n\ndef clean_sql_json(x):\n    \"\"\"Cleans up JSON produced by SQL Server by reducing this pattern:\n        [{\"Key\": [{\"Key\": \"Value\"}]}]\n    to this:\n        [{'Key': ['Value']}]\n\n    Also removes duplicates (and ordering) from the reduced list.\n    Returns an object, not JSON.\n    \"\"\"\n\n    datas = json.loads(x)\n\n    for data in datas:\n        for key, value in data.items():\n            if (isinstance(value, list)\n                    and isinstance(value[0], dict)\n                    and len(value[0]) == 1\n                    ):\n                data[key] = list({\n                    list(item.values())[0]\n                    for item in value\n                })\n\n    return datas",
        "sha1": "0f52b23b3a30918be653111821a89bea5d1db76b",
        "id": 282505
    },
    {
        "content": "def getFileData(fileName):\n\t\"\"\"\n\t\tOpen a file, and read the contents. The with..open operation will auto-close the file as well.\n\t\"\"\"\n\twith open(fileName) as handle:\n\t\tdata = handle.read()\n\n\treturn data",
        "sha1": "b9f8f58246ee59e4377acd60d5a4d3fe2244914c",
        "id": 651789
    },
    {
        "content": "def is_valid_version_code(version):\n    \"\"\" Checks that the given version code is valid.\n    \"\"\"\n    return version is not None and len(version) == 5 and \\\n        int(version[:4]) in range(1990, 2050) and \\\n        version[4] in ['h', 'g', 'f', 'e', 'd', 'c', 'b', 'a']",
        "sha1": "520931bffdbd037424103f5b0d253bf6f983e245",
        "id": 296352
    },
    {
        "content": "import math\n\n\ndef angle3pt(a: tuple, b: tuple, c: tuple) -> float:\n    \"\"\"\n    Calculate the angle by turning from coordinate a to c around b.\n\n    :param a:             coordinate a (x, y)\n    :param b:             coordinate b (x, y)\n    :param c:             coordinate c (x, y)\n    :returns:\n        * **ang** -       angle between a and c\n\n    \"\"\"\n\n    ang = math.atan2(c[1] - b[1], c[0] - b[0]) - math.atan2(a[1] - b[1], a[0] - b[0])\n\n    if ang > math.pi:\n        ang -= 2 * math.pi\n    elif ang <= -math.pi:\n        ang += 2 * math.pi\n\n    return ang",
        "sha1": "e7ccbf67cedfcfebe8015115deb6cc884ee45a14",
        "id": 613932
    },
    {
        "content": "def clean_tags(self):\n    \"\"\"\n    Force all tags to lowercase.\n    \"\"\"\n    tags = self.get('tags', None)\n    if tags:\n        tags = [t.lower() for t in tags]\n\n    return tags",
        "sha1": "2a6ddc1e236d8d490a0946f6ae7904e7e3907380",
        "id": 376188
    },
    {
        "content": "def trunc(x, y, w, h):\n    \"\"\"Truncates x and y coordinates to live in the (0, 0) to (w, h)\n    \n    Args: \n        x: the x-coordinate of a point\n        y: the y-coordinate of a point\n        w: the width of the truncation box\n        h: the height of the truncation box. \n    \"\"\"\n    return min(max(x, 0), w - 1), min(max(y, 0), h - 1)",
        "sha1": "3edecdfbd9baf24f8b4f3f71b9e35a222c6be1ea",
        "id": 3889
    },
    {
        "content": "def _search_string(fullstring, prefix, suffix):\n    \"\"\"Returns the substring between two given substrings of a larger string.\n\n    Args:\n      fullstring: The larger string to search.\n      prefix: The substring that should occur directly before the returned string.\n      suffix: The substring that should occur directly after the returned string.\n    Returns:\n      A string occurring in fullstring exactly prefixed by prefix, and exactly\n      terminated by suffix. For example, (\"hello goodbye\", \"lo \", \" bye\") will\n      return \"good\". If there is no such string, returns the empty string.\n    \"\"\"\n\n    prefix_index = fullstring.find(prefix)\n    if (prefix_index < 0):\n        return \"\"\n    result_start_index = prefix_index + len(prefix)\n    suffix_index = fullstring.find(suffix, result_start_index)\n    if (suffix_index < 0):\n        return \"\"\n    return fullstring[result_start_index:suffix_index]",
        "sha1": "2f0927dba51b5b07a2fdf229f6081c4a5d7b6f3e",
        "id": 324653
    },
    {
        "content": "def load_table_meta(loader, filename, index):\n    \"\"\"\n    Load the meta-data data associated with a table from the specified index\n    within a file.\n    \"\"\"\n    return loader(filename, index)",
        "sha1": "323812b47a29d058d2153cdd61de842d0ec18b68",
        "id": 571992
    },
    {
        "content": "def p10(docs, n=10):\n    \"\"\"Precision at N\"\"\"\n    return len([doc for doc in docs[:n] if doc['relevant'] == \"true\"]) / n",
        "sha1": "93c1c4901cb480adfd26152e0a7a601860a2e9d6",
        "id": 150093
    },
    {
        "content": "import uuid\n\n\ndef processIndex(table, index, idxNameFunc=uuid.uuid4, indexExists=True, join=True):\n    \"\"\"\n    Given objects produced by the PyDBML library (or appropriately mocked), generate valid SQLite DDL for creating indexes.\n\n    Parameters:\n    table (Table): a Table object generated by the PyDBML library. This object should represent the SQLite table relevant to the index you want to create.\n    index (Index): an Index object generated by the PyDBML library. This object should represent the SQLite index you want to create.\n    idxNameFunc (function): defaults to `uuid.uuid4`. Can mock that function by passing a different function that returns a more predictable result. The result of calling this argument in either case is used as the name of an index if one is not provided for any `CREATE INDEX` statements.\n    indexExists (bool): Default is True. If True, the generated `CREATE INDEX` SQLite statement will have `IF NOT EXISTS` language included.\n    join (bool): Default is True. If True, function will `join` the result list of string segments with an empty string and return the resulting string to you. otherwise, the one-dimensional list of string segments will be returned to you directly.\n\n    Returns: \n    str or list of str: SQLite DDL for creating an index.\n    \"\"\"\n    parts = []\n    parts.append(f'CREATE{\" UNIQUE\" if index.unique else \"\"} INDEX ')\n    if indexExists:\n        parts.append('IF NOT EXISTS ')\n    if index.name != \"\" and index.name != None:\n        parts.append(index.name)\n    else:\n        parts.append('_' + ''.join(str(idxNameFunc()).split('-')))\n    parts.append(f' ON {table.name} (')\n    for i, col in enumerate(index.subjects):\n        parts.append(col.name)\n        if i < len(index.subjects) - 1:\n            parts.append(', ')\n    parts.append(');\\n')\n    if join:\n        parts = \"\".join(parts)\n    return parts",
        "sha1": "6002e007b878596f75dd404a04bd69e59ff13321",
        "id": 323763
    },
    {
        "content": "from typing import List\nimport torch\n\n\ndef iterable_to_device(data: List[torch.Tensor], device: str = \"cuda\") -> List[torch.Tensor]:\n    \"\"\"\n    Function maps data to a given device.\n    :param data: (List[torch.Tensor]) List of torch tensors\n    :param device: (str) Device to be used\n    :return: (List[torch.Tensor]) Input data mapped to the given device\n    \"\"\"\n    # Iterate over all tensors\n    for index in range(len(data)):\n        # Map tensors to device\n        data[index] = data[index].to(device)\n    return data",
        "sha1": "4708223410a3c80e38e78e733232b885726c5b26",
        "id": 532706
    },
    {
        "content": "def get_photon_energy(wavelengths):\n    \"\"\"\n    computes the energy of the photon of a given wavelength\n    :param wavelengths: [m]\n    :return: J = W*s\n    \"\"\"\n    plank_constant = 6.62606957 * 10**-34  # J*s\n    speed_of_light = 299792458  # m*s^-1\n    nu = speed_of_light / wavelengths  # s^-1\n    E = plank_constant * nu  # J = W*s\n    return E",
        "sha1": "4c6985c90465cbcd79f204219762f13bc4a71203",
        "id": 43669
    },
    {
        "content": "def check_resistive(netlist):\n    \"\"\"Check if all components in netlist are resistors\"\"\"\n\n    for component in netlist.components.values():\n        if component.type != \"R\":\n            return False\n    return True",
        "sha1": "9ce20539394e8c7a922e21a3788729aeca65fe3f",
        "id": 222832
    },
    {
        "content": "def tidy_dataframe(df, column, sep='|'):\n    \"\"\"\n    Given a dataframe with a delimiter-separated string, create a new dataframe\n    with separate rows for each value in each corresponding string.\n\n    E.g.::\n\n               gene  score\n        0     g1|g2      1\n        1        g3      5\n        2  g4|g5|g6      9\n\n    becomes::\n\n        tidy_dataframe(df, 'gene', sep='|')\n        #   gene  score\n        # 0   g1      1\n        # 0   g2      1\n        # 1   g3      5\n        # 2   g4      9\n        # 2   g5      9\n        # 2   g6      9\n    \"\"\"\n    s = df[column].str.split(sep, expand=True).stack()\n    i = s.index.get_level_values(0)\n    df2 = df.loc[i].copy()\n    df2[column] = s.values\n    return df2",
        "sha1": "ab72e3cde468a7225b38fe09a884c220d9a17d35",
        "id": 585509
    },
    {
        "content": "from typing import List\nfrom typing import Tuple\nfrom typing import Set\n\n\ndef unique_planets(orbits: List[Tuple[str, str]]) -> Set[str]:\n    \"\"\"Get the unique list of planets from all of the orbits.\"\"\"\n    planets: Set[str] = set()\n    planets.update([x[0] for x in orbits])\n    planets.update([x[1] for x in orbits])\n    return planets",
        "sha1": "5b41b4e1debd6ff5c06a3c9811df31c3b86535d1",
        "id": 563107
    },
    {
        "content": "def round_up(rounded, divider):\n    \"\"\"Round up an integer to a multiple of divider.\"\"\"\n    return int(rounded + divider - 1) // divider * divider",
        "sha1": "ededc0735b6f9e3167e3a826b49f6491df405013",
        "id": 124097
    },
    {
        "content": "import random\nimport string\n\n\ndef generate_String(length):\n\t\"\"\"Generate random string of length.\"\"\"\n\treturn ''.join([random.choice(string.ascii_letters + string.digits) for n in range(length)])",
        "sha1": "2d678bc1b5084157f050f7a69c08405856763ea2",
        "id": 558529
    },
    {
        "content": "def has_lower_letters(password):\n    \"\"\"Return True if password has at least one lower letter.\"\"\"\n    return any(char.islower() for char in password)",
        "sha1": "d55a37e994e289886efdce6e815a430777572b97",
        "id": 49410
    },
    {
        "content": "def Prefactor(flux,index,emin,emax,escale):\n    \"\"\"Compute the prefactor at the energy escale knowing\n    the flux and index between emin and emax\"\"\"\n\n    Denomin = pow(emax,-abs(index)+1.) -pow(emin,-abs(index)+1.)\n    return flux*(-abs(index)+1)*pow(escale,-abs(index)) / Denomin",
        "sha1": "5a445133acbd28a0135cd85fd58f43cb96377f8f",
        "id": 566406
    },
    {
        "content": "import re\n\n\ndef FloatStringToFloat(float_string, problems=None):\n    \"\"\"Convert a float as a string to a float or raise an exception\"\"\"\n    # Will raise TypeError unless a string\n    match = re.match(r\"^[+-]?\\d+(\\.\\d+)?$\", float_string)\n    # Will raise TypeError if the string can't be parsed\n    parsed_value = float(float_string)\n\n    if \"x\" in float_string:\n        # This is needed because Python 2.4 does not complain about float(\"0x20\").\n        # But it does complain about float(\"0b10\"), so this should be enough.\n        raise ValueError()\n\n    if not match and problems is not None:\n        # Does not match the regex, but it's a float according to Python\n        problems.InvalidFloatValue(float_string)\n    return parsed_value",
        "sha1": "2ab70906ce3a1ddc9fa7779f26867396411c0684",
        "id": 93185
    },
    {
        "content": "def find(projname, crstype, strict=False):\n    \"\"\"\n    Search for a projection name located in this module.\n\n    Arguments:\n\n    - **projname**: The projection name to search for.\n    - **crstype**: Which CRS naming convention to search (different\n        CRS formats have different names for the same projection).\n    - **strict** (optional): If False, ignores minor name mismatches\n        such as underscore or character casing, otherwise must be exact\n        match (defaults to False). \n    \"\"\"\n    if not strict:\n        projname = projname.lower().replace(\" \",\"_\")\n    for itemname,item in globals().items():\n        if itemname.startswith(\"_\"):\n            continue\n        try:\n            if hasattr(item.name, crstype):\n                itemname = getattr(item.name, crstype)\n                if not strict:\n                    itemname = itemname.lower().replace(\" \",\"_\")\n                if projname == itemname:\n                    return item\n        except:\n            pass\n    else:\n        return None",
        "sha1": "87e98e9695ec41dd2a222ff988a783e98cf31bb3",
        "id": 343045
    },
    {
        "content": "import torch\n\n\ndef rbbox2rroi(rbbox_list):\n    \"\"\"Convert a list of bboxes to roi format.\n\n    Args:\n        bbox_list (list[Tensor]): a list of bboxes corresponding to a batch\n            of images.\n\n    Returns:\n        Tensor: shape (n, 6), [batch_ind, x, y, w, h, theta]\n    \"\"\"\n    rrois_list = []\n    for img_id, bboxes in enumerate(rbbox_list):\n        if bboxes.size(0) > 0:\n            img_inds = bboxes.new_full((bboxes.size(0), 1), img_id)\n            rrois = torch.cat([img_inds, bboxes[:, :5]], dim=-1)\n        else:\n            rrois = bboxes.new_zeros((0, 6))\n        rrois_list.append(rrois)\n    rrois = torch.cat(rrois_list, 0)\n    return rrois",
        "sha1": "0eea5fc4c1c320de03eeb7f7747138da8dc48f97",
        "id": 328305
    },
    {
        "content": "import re\n\n\ndef group_strs_with_regex_patterns(\n        strings,\n        regex_patterns):\n    \"\"\"Given a list/tuple of strings (``strings``), it partitions it into various groups (sub-lists) as\n    specified in a set of patterns given in ``regex_patterns``. Note that the order\n    of patterns matter as patterns will be consumed sequentially and if two patterns\n    overlap, the one appearing first will get the string assigned to its group.\n    Also note that the result will be a partition (``str_groups``) without overlap\n    and any remaining element not satisfying any pattern will be returned in ``remainder``.\n\n    Parameters\n    ----------\n    strings : `list` [`str`] or `tuple` [`str`]\n        A list/tuple of strings which is to be partitioned into various groups\n    regex_patterns : `list` [`str`]\n        A list of strings each being a regex.\n\n    Returns\n    -------\n    result : `dict`\n        A dictionary with following items:\n\n        - \"str_groups\": `list` [`list` [`str`]]\n            A list of list of strings each corresponding to the patterns given in\n            ``regex_patterns``. Note that some of these groups might be empty\n            lists if that pattern is not satisfied by any regex pattern, or a\n            regex pattern appearing before has already consumed all such strings.\n        -\"remainder\": `list` [`str`]\n            The remaining elements in ``strings`` which do not satisfy any of the\n            patterns given in ``regex_patterns``. This list can be empty.\n\n    \"\"\"\n\n    strings_list = list(strings)\n    str_groups = []\n\n    for regex_pattern in regex_patterns:\n        group = [x for x in strings_list if bool(re.match(regex_pattern, x))]\n        str_groups.append(group)\n        strings_list = [x for x in strings_list if x not in group]\n\n    return {\"str_groups\": str_groups, \"remainder\": strings_list}",
        "sha1": "eec0f4bcb8929963b1509ea3ac7aae1dcd4a2243",
        "id": 221196
    },
    {
        "content": "def get_cleaned_attrs_from_object(\n    object_in, ignore_callable=True, ignore_types=None, ignore_prefixes=\"__\"\n):\n    \"\"\"\n    Return all the attributes of an object that pass criteria\n    :param object_in: Any python object\n    the attributes with the \"sub_object\" they belong to. Default: False\n    :param ignore_callable: Don't return attributes than can be called.\n    Default: True\n    :param ignore_types: Don't return attributes of this type\n    (or tuple of types). Default: None.\n    :param ignore_prefixes: Don't return attributes starting with prefixes\n     (as strings) in this list. Default: \"__\"\n    :return: list of attributes meeting criteria\n    \"\"\"\n\n    attributes = [attr for attr in dir(object_in)]\n\n    if ignore_callable:\n        attributes = [\n            attr\n            for attr in attributes\n            if not callable(getattr(object_in, attr))\n        ]\n\n    if ignore_types is not None:\n        attributes = [\n            attr\n            for attr in attributes\n            if not isinstance(getattr(object_in, attr), ignore_types)\n        ]\n\n    if ignore_prefixes is not None:\n        for prefix in ignore_prefixes:\n            attributes = [\n                attr for attr in attributes if not attr.startswith(prefix)\n            ]\n\n    return attributes",
        "sha1": "d83e917e96fcd0940939aae45f29efdc84a91ba2",
        "id": 539673
    },
    {
        "content": "def is_choice(space):\n    \"\"\"checks if an hp.space is hp.choice or not\"\"\"\n    if 'switch' in space.name:\n        return True\n    return False",
        "sha1": "b4472da5434be8aa04547063a25ee51cf1a5f1e7",
        "id": 441966
    },
    {
        "content": "def calcpf(MW, kA, kV):\n    \"\"\" Calculates three phase pf from MW, kV and kA\"\"\"\n    return MW / (kV * kA * 3**0.5)",
        "sha1": "e43840f19b0c242fbf3db9ea31ad95842239972b",
        "id": 344986
    },
    {
        "content": "def getduration(sample_start, sample_stop, sample_rate):\n    \"\"\" Get the duration of a tone sample\"\"\"\n    number_samples = sample_stop - sample_start\n    duration = number_samples/sample_rate\n    return duration",
        "sha1": "5bfbe32b5a0ed795232d332f57aa3763d3fa5da6",
        "id": 141376
    },
    {
        "content": "def charset_to_encoding(name):\n    \"\"\"Convert MySQL's charset name to Python's codec name\"\"\"\n    if name == 'utf8mb4':\n        return 'utf8'\n    return name",
        "sha1": "b97c9e2d8e91cec0be4d49d78154f3dddb77fde4",
        "id": 569605
    },
    {
        "content": "def get_start_and_end_revision(revision_range):\n  \"\"\"Return start and end revision for a regression range.\"\"\"\n  try:\n    revision_range_list = revision_range.split(':')\n    start_revision = int(revision_range_list[0])\n    end_revision = int(revision_range_list[1])\n  except:\n    return [0, 0]\n\n  return [start_revision, end_revision]",
        "sha1": "d008faf8cd9adb4c319059c274595532d363e138",
        "id": 334150
    },
    {
        "content": "import re\nfrom typing import Counter\n\n\ndef _get_table_width(table_spec):\n    \"\"\"Calculate the width of a table based on its spec.\n\n    :param table_spec:\n\n    :type table_spec: str\n\n    :return:\n    :rtype: int\n    \"\"\"\n\n    column_letters = ['l', 'c', 'r', 'p', 'm', 'b']\n\n    # Remove things like {\\bfseries}\n    cleaner_spec = re.sub(r'{[^}]*}', '', table_spec)\n    spec_counter = Counter(cleaner_spec)\n\n    return sum(spec_counter[l] for l in column_letters)",
        "sha1": "97764d26434fbbcd1564538fe60d789059631a7a",
        "id": 517986
    },
    {
        "content": "import math\n\n\ndef cart2sph(x, y, z):\n    \"\"\"\n    Transform Cartesian coordinates to spherical\n    :param x: X coordinate\n    :param y: Y coordinate\n    :param z: Z coordinate\n    :return: radius, elevation, azimuth\n    \"\"\"\n    x2_y2 = x**2 + y**2\n    r = math.sqrt(x2_y2 + z**2)                    # r\n    elev = math.atan2(z, math.sqrt(x2_y2))            # Elevation\n    az = math.atan2(y, x)                          # Azimuth\n    return r, elev, az",
        "sha1": "652e6ed1aff5c458958162d75ecb4bc959738e3e",
        "id": 533788
    },
    {
        "content": "from typing import Union\n\n\ndef as_str(v: Union[bytes, str]) -> str:\n    \"\"\"Force to a str.\"\"\"\n\n    if isinstance(v, bytes):\n        return v.decode(\"utf-8\")\n    elif isinstance(v, str):\n        return v\n    else:\n        raise TypeError(\"Unable to str-coerce \" + str(type(v)))",
        "sha1": "137381bca7b7994d4f0a68b4bc434d2c08a77e6f",
        "id": 267761
    },
    {
        "content": "def format_uuid(uuid: str):\n    \"\"\"\n    Returns UUID formatted according to https://tools.ietf.org/html/rfc4122#section-3 (8-4-4-4-12)\n\n    Parameters\n    ----------\n    module_name : str\n        unformatted UUID\n    \"\"\"\n    return f'{uuid[0:8]:s}-{uuid[8:12]:s}-{uuid[12:16]:s}-{uuid[16:20]:s}-{uuid[20:32]:s}'",
        "sha1": "c3383cab2bbcafb9d67c4535c6369f3646faafea",
        "id": 37781
    },
    {
        "content": "def node_def_add_node(sss):\n  \"\"\" add node{} to a string of an op.\n\n  Args:\n    sss: original string of an op.\n\n  Returns:\n    new string with node{} added.\n  \"\"\"\n  sss = sss.replace('\\n', '\\n\\t')\n  sss = \"node {\\n\\t\" + sss\n  sss = sss[0:len(sss)-1] + '}\\n'\n  return sss",
        "sha1": "656073db73db94f3942b4ac8d25b4f3fbf3e7356",
        "id": 96360
    },
    {
        "content": "def _split_csv(string):\n    \"\"\"Split string into a list, excluding empty strings\"\"\"\n    if string is None:\n        return []\n    return [n.strip() for n in string.split(',') if n]",
        "sha1": "de6b21c340ec4c24462f3120f3486e974feafb9d",
        "id": 27151
    },
    {
        "content": "def file_parts(filename):\n    \"\"\"\n    prefix is part before the first dot\n    extension is all the parts after the first dot\n    \"\"\"\n    prefix = filename.split(\".\")[0]\n    extension = \".\".join(filename.split(\".\")[1:]).lstrip(\".\")\n    return prefix, extension",
        "sha1": "2e232550e7f4d86ba00fdc99f08a613bac1d8caa",
        "id": 344435
    },
    {
        "content": "def sensor2int(float_sensor):\n    \"\"\"\n    Convert sensor data from float to int.\n    \"\"\"\n    int_time = int(float_sensor * 1000000)\n    return int_time",
        "sha1": "4c7f07eb800e9debcb5d0ef02b73fad5354b99b8",
        "id": 585920
    },
    {
        "content": "def _tbl_str_width(num, widths):\n    \"\"\"Returns the width-argument for an html table cell using the width from\n    the list of widths.\n    Returns width=\"[some number]\" or the empty-string if no width is\n    specified for this num\"\"\"\n    if num >= len(widths):\n        return \"\"\n    return ' width=\"%s\"' % widths[num]",
        "sha1": "3d35c82033ae504eeac143f35653303f1425da60",
        "id": 673710
    },
    {
        "content": "def average_prfacc(*evals, ndigits=4):\n    \"\"\"Average for multiple evaluations.\n\n    Args:\n        evals [tuple]: several evals without limitation\n        ndigits [int]: decimal number of float\n\n    Returns:\n        avg [dict]: dict of the average values of all the evaluation metrics\n    \"\"\"\n    avg = {}.fromkeys(evals[0].keys())\n    for key in avg:\n        values = [e[key] for e in evals]\n        if isinstance(values[0], dict):\n            avg[key] = average_prfacc(*values)\n        else:\n            avg[key] = round(sum(values) / len(values), ndigits)\n\n    return avg",
        "sha1": "6d86d67e878abec714db3abd249b46ef1f2613af",
        "id": 493253
    },
    {
        "content": "def xpath_matches(tree, xpath, namespaces):\n    \"\"\" Test if a node exists \"\"\"\n    if tree.xpath(xpath, namespaces=namespaces):\n        return True\n    return False",
        "sha1": "3c0cb8520b73481b1a67f94248aaa89b540a80b5",
        "id": 172634
    },
    {
        "content": "def purge_duplicates(list_in):\n    \"\"\"Remove duplicates from list while preserving order.\n\n    Parameters\n    ----------\n    list_in: Iterable\n\n    Returns\n    -------\n    list\n        List of first occurrences in order\n    \"\"\"\n    # Algorithm taken from Stack Overflow,\n    # https://stackoverflow.com/questions/480214. Content by Georgy\n    # Skorobogatov (https://stackoverflow.com/users/7851470/georgy) and\n    # Markus Jarderot\n    # (https://stackoverflow.com/users/22364/markus-jarderot), licensed\n    # under CC-BY-SA 4.0.\n    # https://creativecommons.org/licenses/by-sa/4.0/.\n\n    seen = set()\n    seen_add = seen.add\n    return [x for x in list_in if not (x in seen or seen_add(x))]",
        "sha1": "349211a9ad9b949fb7061e2b4ad21cb1ec3354f7",
        "id": 39832
    },
    {
        "content": "def is_utf8_encoded(bstring):\n    \"\"\"\n    Return ``True`` if the given byte string can be decoded\n    into a Unicode string using the UTF-8 decoder.\n\n    :param bytes bstring: the string to test\n    :rtype: bool\n    \"\"\"\n    try:\n        bstring.decode(\"utf-8\")\n        return True\n    except UnicodeDecodeError:\n        pass\n    return False",
        "sha1": "845f79d725b95adc8ba75feabda2c9e68214aa98",
        "id": 637062
    },
    {
        "content": "def neighbors(cell):\n    \"\"\"Returns the neighbors of a given cell.\"\"\"\n    x, y = cell\n    return [(x + 1, y), (x - 1, y), (x, y + 1), (x, y - 1), (x + 1, y + 1),\n            (x + 1, y - 1), (x - 1, y + 1), (x - 1, y - 1)]",
        "sha1": "c9797b0ed969ed8431f89666cd1cf5fa108a0733",
        "id": 409284
    },
    {
        "content": "def indices(s, n=None):\n    \"\"\"\n    Returns a list of the indices specified by slice `s`.\n\n    Parameters\n    ----------\n    s : slice\n      The slice to operate upon.\n\n    n : int, optional\n      The number of elements in the array being indexed,\n      used for computing *negative* start/stop points.\n\n    Returns\n    -------\n    list of ints\n    \"\"\"\n    if s.start is None and s.stop is None:\n        return []\n\n    if s.start is None:\n        start = 0\n    elif s.start < 0:\n        assert(n is not None), \"Must supply `n` to obtain indices of a slice with negative start point!\"\n        start = n + s.start\n    else: start = s.start\n\n    if s.stop is None:\n        assert(n is not None), \"Must supply `n` to obtain indices of a slice with unspecified stop point!\"\n        stop = n\n    elif s.stop < 0:\n        assert(n is not None), \"Must supply `n` to obtain indices of a slice with negative stop point!\"\n        stop = n + s.stop\n    else: stop = s.stop\n\n    if s.step is None:\n        return list(range(start, stop))\n    return list(range(start, stop, s.step))",
        "sha1": "1e7a180e3bafdfc0c7e4898a4f7662240d018d55",
        "id": 295925
    },
    {
        "content": "from typing import List\n\n\ndef fix_mtcnn_bb(max_y: int, max_x: int, bounding_box: List[int]) -> List[int]:\n    \"\"\" mtcnn results can be out of image so fix results\n    \"\"\"\n    x1, y1, dx, dy = bounding_box[:4]\n    x2 = x1 + dx\n    y2 = y1 + dy\n    x1 = max(min(x1, max_x), 0)\n    x2 = max(min(x2, max_x), 0)\n    y1 = max(min(y1, max_y), 0)\n    y2 = max(min(y2, max_y), 0)\n    return [x1, y1, x2, y2]",
        "sha1": "c93ea631f69e4059fed8c8685660302822f550e4",
        "id": 149472
    },
    {
        "content": "def pf_contact(phi):\n    \"\"\" Phase field contact function. \"\"\"\n    return (2. + 3.*phi - phi**3)/4.",
        "sha1": "7a14c75e73e185ccf0a827942c4c77b2a4d1d6ce",
        "id": 356609
    },
    {
        "content": "import sympy\n\n\ndef _sympy_tridiag(a, b):\n    \"\"\"Creates the tridiagonal sympy matrix tridiag(b, a, b).\n    \"\"\"\n    n = len(a)\n    assert n == len(b)\n    A = [[0 for _ in range(n)] for _ in range(n)]\n    for i in range(n):\n        A[i][i] = a[i]\n    for i in range(n - 1):\n        A[i][i + 1] = b[i + 1]\n        A[i + 1][i] = b[i + 1]\n    return sympy.Matrix(A)",
        "sha1": "b998658827ea05a6371d79d754076f59f91ee62e",
        "id": 209603
    },
    {
        "content": "def high_income_amount(responses, derived):\n    \"\"\" Return the guidelines table amount for a high income earner \"\"\"\n\n    try:\n        under = float(responses.get('child_support_amount_under_high_income', 0))\n    except ValueError:\n        under = 0\n\n    try:\n        over = float(responses.get('amount_income_over_high_income_limit', 0))\n    except ValueError:\n        over = 0\n\n    return under + over",
        "sha1": "588ce06fd276be6695b0cab852ab6020a404338f",
        "id": 642000
    },
    {
        "content": "import math\n\n\ndef _tileBoundary(x, y, z):\n    \"\"\"Return (left, bottom, right, top) of bbox of given tile\"\"\"\n    n = 2 ** z\n    mercToLat = lambda x: math.degrees(math.atan(math.sinh(x)))\n    top = mercToLat(math.pi * (1 - 2 * (y * (1/n))))\n    bottom = mercToLat(math.pi * (1 - 2 * ((y+1) * (1/n))))\n    left = x * (360/n) -180\n    right = left + (360/n)\n    return left, bottom, right, top",
        "sha1": "6a9a7b00cdadd7caa68c23ced26314c13d581581",
        "id": 420353
    },
    {
        "content": "def name_reverse(name):\n    \"\"\"Switch family name and given name\"\"\"\n    fam, giv = name.split(',', 1)\n    giv = giv.strip()\n    return giv + ', ' + fam",
        "sha1": "06815ade8f9b1c155f25fe69a64d118de62db65f",
        "id": 80452
    },
    {
        "content": "from typing import List\n\n\ndef propagate_jemalloc_env_var(*, jemalloc_path: str, jemalloc_conf: str,\n                               jemalloc_comps: List[str], process_type: str):\n    \"\"\"Read the jemalloc memory profiling related\n        env var and return the dictionary that translates\n        them to proper jemalloc related env vars.\n\n        For example, if users specify `CLOUDTIK_JEMALLOC_LIB_PATH`,\n        it is translated into `LD_PRELOAD` which is needed to\n        run Jemalloc as a shared library.\n\n        Params:\n            jemalloc_path (str): The path to the jemalloc shared library.\n            jemalloc_conf (str): `,` separated string of jemalloc config.\n            jemalloc_comps List(str): The list of components\n                that we will profile.\n            process_type (str): The process type that needs jemalloc\n                env var for memory profiling. If it doesn't match one of\n                jemalloc_comps, the function will return an empty dict.\n\n        Returns:\n            dictionary of {env_var: value}\n                that are needed to jemalloc profiling. The caller can\n                call `dict.update(return_value_of_this_func)` to\n                update the dict of env vars. If the process_type doesn't\n                match jemalloc_comps, it will return an empty dict.\n    \"\"\"\n    assert isinstance(jemalloc_comps, list)\n    assert process_type is not None\n    process_type = process_type.lower()\n    if (not jemalloc_path or process_type not in jemalloc_comps):\n        return {}\n\n    env_vars = {\n        \"LD_PRELOAD\": jemalloc_path,\n    }\n    if jemalloc_conf:\n        env_vars.update({\"MALLOC_CONF\": jemalloc_conf})\n    return env_vars",
        "sha1": "92ecfb9f5d1e68922134bbcedd533e47e254c6bb",
        "id": 124824
    },
    {
        "content": "from typing import Union\nfrom typing import Dict\nfrom typing import Optional\n\n\ndef get_config_float(\n    current: Union[int, float], config: Dict[str, str], name: str\n) -> Optional[float]:\n    \"\"\"\n    Convenience function to get config values as float.\n\n    :param current: current config value to use when one is not provided\n    :param config: config to get values from\n    :param name: name of config value to get\n    :return: current config value when not provided, new value otherwise\n    \"\"\"\n    value = config.get(name)\n    if value is not None:\n        if value == \"\":\n            value = None\n        else:\n            value = float(value)\n    else:\n        value = current\n    return value",
        "sha1": "d2bb436c4b2b4aef35a8f46927bc9145ecfed04c",
        "id": 703900
    },
    {
        "content": "def cut_tag_preserve(tags, tag):\n    \"\"\"\n    Cuts a tag from a list of tags without altering the original.\n    \"\"\"\n    tag_list = tags[:]\n    tag_list.remove(tag)\n    return \",\".join(tag_list)",
        "sha1": "691412fc466498413c32edd30eaca2d677d7e35a",
        "id": 35405
    },
    {
        "content": "def season(x):\n    \"\"\"Returns season based on month\"\"\"\n    x = x.month\n    if (x > 3) and (x <= 6):\n        return 1\n    elif (x > 6) and (x <= 9):\n        return 2\n    elif (x > 9) and (x <= 11):\n        return 3\n    else:\n        return 4",
        "sha1": "f7304e65933c681731050fb67c85724c6ac492e9",
        "id": 44476
    },
    {
        "content": "def _embed(tree_list, max_len):\n    \"\"\"Embeds a tree list with margins at the end of the strings\"\"\"\n    # Alignment to a single length (embedding with spaces)\n    new_tree_list = tree_list\n    for idx, row in enumerate(tree_list):\n        new_tree_list[idx] = row + \" \" * (max_len - len(row))\n    return new_tree_list",
        "sha1": "4c000f3ff44891edaf6e7415f2857771240d0e18",
        "id": 512174
    },
    {
        "content": "import math\n\n\ndef dist(a, b):\n    \"\"\"Compute the distance between two x,y points.\"\"\"\n    x0, y0 = a  # Destructuring assignment\n    x1, y1 = b\n\n    return math.sqrt((x1 - x0)**2 + (y1 - y0)**2)",
        "sha1": "efd39bce505b8da419a4ae14e1fff5c28bd86794",
        "id": 555064
    },
    {
        "content": "import pickle\n\n\ndef load_nodes(path):\n    \"\"\"\n    load nodes from storage file\n    \"\"\"\n    nodes = {}\n    with open(path, 'rb') as file:\n        nodes = pickle.load(file)\n\n    for node in nodes.values():\n        # reset old properties\n        node.online = False\n        node.index = None\n        node.clientcount = 0\n\n    return nodes",
        "sha1": "9f444f6f1a010d2822045ee359fa8bc109576bc9",
        "id": 31275
    },
    {
        "content": "def _reGroupDict(d, newgr):\n    \"\"\"Regroup keys in the d dictionary in subdictionaries, based on\n    the scheme in the newgr dictionary.\n    E.g.: in the newgr, an entry 'LD label': ('laserdisc', 'label')\n    tells the _reGroupDict() function to take the entry with\n    label 'LD label' (as received from the sql database)\n    and put it in the subsection (another dictionary) named\n    'laserdisc', using the key 'label'.\"\"\"\n    r = {}\n    newgrks = newgr.keys()\n    for k, v in d.items():\n        if k in newgrks:\n            r.setdefault(newgr[k][0], {})[newgr[k][1]] = v\n            # A not-so-clearer version:\n            ##r.setdefault(newgr[k][0], {})\n            ##r[newgr[k][0]][newgr[k][1]] = v\n        else: r[k] = v\n    return r",
        "sha1": "eed4344c415dbb12cdceb8edb54ef2a1fbc369d2",
        "id": 643855
    },
    {
        "content": "def getid(obj):\n    \"\"\"\n    Abstracts the common pattern of allowing both an object or an object's ID\n    (UUID) as a parameter when dealing with relationships.\n    \"\"\"\n    try:\n        return obj.id\n    except AttributeError:\n        return obj",
        "sha1": "aac958804646cae92c30229c4974da678a332bd5",
        "id": 428459
    },
    {
        "content": "def _adaptSigma(sigma, p_s, c=0.817):\n    \"\"\"\n        Adapt parameter sigma based on the 1/5th success rule\n\n        :param sigma:  Sigma value to be adapted\n        :param p_s:    Recent success rate, determines whether sigma is increased or decreased\n        :param c:      Factor c that is used to increase or decrease sigma\n        :returns:      New value sigma\n    \"\"\"\n\n    if p_s < 1/5:\n        sigma *= c\n    elif p_s > 1/5:\n        sigma /= c\n\n    return sigma",
        "sha1": "7c011f249057d6ce7b65946bf6933ff8dcbf7c89",
        "id": 282318
    },
    {
        "content": "def removeVowels(word):\n    \"\"\"\n    Recursive Function to remove alll vowels in a words/sentence.\n\n    Parameters:\n        word (string); the word in which the vowels are to be removed.\n\n    Returns:\n        A string with no vowels after the all recursions are complete.\n\n    Raises:\n        TypeError: If user enters an invalid number such as floats.\n        Exception: If any unexpected error occurs. Eg, RuntimeError when there are\n                   too many recursive calls.\n    \"\"\"\n\n    try:\n        \n        if not(isinstance(word,str)): #Checking if input is a valid string.\n            raise TypeError\n        \n        if len(word) == 0: #Base Case\n            return word\n    \n        elif word[0] in \"AEIOUaeiou\": \n            return removeVowels(word[1:]) #Skip that letter and proceed with the rest of letters in word\n        \n        else: # keep the first letter and proceed until length of word becomes 0.\n            return word[0] + removeVowels(word[1:]) \n\n    except TypeError: #If the provided input is not a string.\n        print(\"Error: Please provide a valid word/sentence of type string and try again.\")\n    \n    except: #If any other unexpected error occurs\n        print(\"Error in removing vowels. Please try again.\")",
        "sha1": "3e4d679160b911937df0fb3e9f94d913178330bc",
        "id": 19763
    },
    {
        "content": "def valid_filename(filename):\n    \"\"\"Check if filename is for a CSV file with no folder names.\"\"\"\n    return not \"/\" in filename and filename.endswith(\".csv\")",
        "sha1": "c42832a8e9e624c78a0cba668b4700cfbba61910",
        "id": 580345
    },
    {
        "content": "def choose(n,k):\n    \"\"\"Binomial coefficient\"\"\"\n    if n < k:\n        raise Exception(\"n cannot be less than k\")\n    if k < 0:\n        raise Exception(\"k must be nonnegative\")\n        \n    # Calculate the numerator and denominator seperately in order to avoid loss\n    # of precision for large numbers.\n    N = 1\n    D = 1\n    for i in range(1,k+1):\n        N *= (n+1-i)\n        D *= i\n    return N//D",
        "sha1": "c86fa4d9b315d60243d6b4c58973d85e0c886c52",
        "id": 89657
    },
    {
        "content": "def getIpIntStr(ipInt):\n    \"\"\"\n    Converts an IP address in host order integer to a string representation.\n\n    :param ipInt: an IP address integer\n\n    :rtype: str\n    :return: A string representation of the IP address\n    \"\"\"\n    return \".\".join(map(lambda n: str(ipInt >> n & 0xFF), [0, 8, 16, 24]))",
        "sha1": "c833d0946524cde93aadb2a6b721a17e9c00ab2c",
        "id": 692795
    },
    {
        "content": "def generate_backlinks(thread_dict):\n    \"\"\"Generates backlinks to posts based on the quotes in post_dicts of thread_dict\n\n    :param thread_dict: Dict containing keys of postnrs and fileurls with post_dict as values\"\"\"\n    for key, post_dict in thread_dict.items():\n        if \"/\" in key:\n            # key is fileurl\n            continue\n        elif key == \"OP\":\n            continue\n        else:\n            for quotenr in post_dict[\"quotes\"]:\n                # append postnr of quotING post to quotED posts backlink list\n                thread_dict[quotenr][\"backlinks\"].append(post_dict[\"post_nr\"])\n    return thread_dict",
        "sha1": "737ee4408387f7d1fe24abb193a9e24e1ba935a7",
        "id": 289743
    },
    {
        "content": "def json_str_list_to_int_list(json_list,json_number_base=16):\n    \"\"\"\n    returns a json list to a list of ints with base 'json_number_base' [default 16]\n\n    \"\"\"\n    return [int(k,json_number_base) for k in json_list]",
        "sha1": "3d32d262508bfa6da751b21a4c162252a4a0bd4b",
        "id": 285203
    },
    {
        "content": "def _GetHost(cpu, target_os):\n  \"\"\"Returns the host triple for the given OS and CPU.\"\"\"\n  if cpu == 'x64':\n    cpu = 'x86_64'\n  elif cpu == 'arm64':\n    cpu = 'aarch64'\n\n  if target_os == 'linux':\n    return cpu + '-unknown-linux'\n  elif target_os == 'mac':\n    return cpu + '-apple-darwin'\n  elif target_os == 'ios':\n    return cpu + '-ios-darwin'\n  else:\n    raise RuntimeError('Unsupported host')",
        "sha1": "1431239a7352a3cee23bcc0aec8d1961dcb7296f",
        "id": 110191
    },
    {
        "content": "def analyze_token(token):\n    \"\"\"Return the value of token if it can be analyzed as a number, or token.\"\"\"\n    try:\n        return int(token)\n    except (TypeError, ValueError):\n        try:\n            return float(token)\n        except (TypeError, ValueError):\n            # \u5982\u679c\u4e0d\u662f\u6570\u5b57\uff0c \u5219\u53ef\u80fd\u662f\u8868\u8fbe\u5f0f\uff0c \u5148\u76f4\u63a5\u8fd4\u56de\n            return token",
        "sha1": "10d5b3e47a9f8c4654e7db75f28884cd5fe1ff49",
        "id": 218211
    },
    {
        "content": "def is_criticality_balanced(temperature, neutrons_emitted):\n    \"\"\"Verify criticality is balanced.\n\n    :param temperature: temperature value in kelvin (integer or float)\n    :param neutrons_emitted: number of neutrons emitted per second (integer or float)\n    :return:  boolean True if conditions met, False if not\n\n    A reactor is said to be critical if it satisfies the following conditions:\n    - The temperature is less than 800 K.\n    - The number of neutrons emitted per second is greater than 500.\n    - The product of temperature and neutrons emitted per second is less than 500000.\n    \"\"\"\n\n    if temperature < 800 and neutrons_emitted > 500 and (temperature * neutrons_emitted) < 500000:\n        return True\n    return False",
        "sha1": "688327290c21e9abeececb0f0127f5623518468c",
        "id": 397849
    },
    {
        "content": "def read_from_8bit_eeprom(bus, address, count, bs=16):\n    \"\"\"\n    Reads from an 8-bit EEPROM. Only supports starting from 0x00, for now.\n    (to add other start addresses, you'll want to improve the address counting mechanism)\n    Default read block size is 16 bytes per read.\n    \"\"\"\n    data = [] # We'll add our reads to here\n    # If read count is not divisible by block size,\n    # we'll have one partial read at the last read\n    full_reads, remainder = divmod(count, bs)\n    if remainder: full_reads += 1 # adding that last read if needed\n    for i in range(full_reads):\n        start = i*bs # next block address\n        # If we're on last cycle and remainder != 0, not doing a full read\n        count = remainder if (remainder and i == full_reads-1) else bs\n        read = bus.read_i2c_block_data(address, start, count)\n        data += list(read)\n    return data",
        "sha1": "782dbc4f17144881fcd66e5f5fb51126070dc634",
        "id": 645654
    },
    {
        "content": "def landsat_c1_sr_cloud_mask(input_img, cloud_confidence=3, snow_flag=False):\n    \"\"\"Extract cloud mask from the Landsat Collection 1 SR pixel_qa band\n\n    Parameters\n    ----------\n    img : ee.Image\n        Image from a Landsat Collection 1 SR image collection with a pixel_qa\n        band (e.g. LANDSAT/LE07/C01/T1_SR).\n    cloud_confidence : int\n        Minimum cloud confidence value (the default is 3).\n    snow_flag : bool\n        If true, mask snow pixels (the default is False).\n\n    Returns\n    -------\n    ee.Image\n\n    Notes\n    -----\n    Output image is structured to be applied directly with updateMask()\n        i.e. 0 is cloud, 1 is cloud free\n\n    Assuming Cloud must be set to check Cloud Confidence\n\n    Bits\n        0: Fill\n        1: Clear\n        2: Water\n        3: Cloud Shadow\n        4: Snow\n        5: Cloud\n        6-7: Cloud Confidence\n\n    Confidence values\n        00: \"None\"\n        01: \"Low\"\n        10: \"Medium\"\n        11: \"High\"\n\n    References\n    ----------\n    https://landsat.usgs.gov/landsat-surface-reflectance-quality-assessment\n\n    \"\"\"\n    qa_img = input_img.select(['pixel_qa'])\n    cloud_mask = qa_img.rightShift(5).bitwiseAnd(1).neq(0)\\\n        .And(qa_img.rightShift(6).bitwiseAnd(3).gte(cloud_confidence))\\\n        .Or(qa_img.rightShift(3).bitwiseAnd(1).neq(0))\n    if snow_flag:\n        cloud_mask = cloud_mask.Or(qa_img.rightShift(4).bitwiseAnd(1).neq(0))\n\n    # Set cloudy pixels to 0 and clear to 1\n    return cloud_mask.Not()",
        "sha1": "da8ada1ac11733b0b36c20daac8e63c7babf1aa0",
        "id": 458915
    },
    {
        "content": "def _qname_matches(tag, namespace, qname):\n    \"\"\"Logic determines if a QName matches the desired local tag and namespace.\n\n    This is used in XmlElement.get_elements and XmlElement.get_attributes to\n    find matches in the element's members (among all expected-and-unexpected\n    elements-and-attributes).\n\n    Args:\n      expected_tag: string\n      expected_namespace: string\n      qname: string in the form '{xml_namespace}localtag' or 'tag' if there is\n             no namespace.\n\n    Returns:\n      boolean True if the member's tag and namespace fit the expected tag and\n      namespace.\n    \"\"\"\n    # If there is no expected namespace or tag, then everything will match.\n    if qname is None:\n        member_tag = None\n        member_namespace = None\n    else:\n        if qname.startswith('{'):\n            member_namespace = qname[1:qname.index('}')]\n            member_tag = qname[qname.index('}') + 1:]\n        else:\n            member_namespace = None\n            member_tag = qname\n    return ((tag is None and namespace is None)\n            # If there is a tag, but no namespace, see if the local tag\n            # matches.\n            or (namespace is None and member_tag == tag)\n            # There was no tag, but there was a namespace so see if the namespaces\n            # match.\n            or (tag is None and member_namespace == namespace)\n            # There was no tag, and the desired elements have no namespace, so check\n            # to see that the member's namespace is None.\n            or (tag is None and namespace == ''\n                and member_namespace is None)\n            # The tag and the namespace both match.\n            or (tag == member_tag\n                and namespace == member_namespace)\n            # The tag matches, and the expected namespace is the empty namespace,\n            # check to make sure the member's namespace is None.\n            or (tag == member_tag and namespace == ''\n                and member_namespace is None))",
        "sha1": "66aa9272fd6e4a6e281d39f03dd63acabad0bbe7",
        "id": 702451
    },
    {
        "content": "def float_encode(v):\n    \"\"\"Encode a floating point number to integer format with x1000 scale\"\"\"\n    return round(v*1000)",
        "sha1": "be6f6d42d5961f8ca41b7a1a3ef1a4537f4c364e",
        "id": 461857
    },
    {
        "content": "def getFile(fileName):\n\t\"\"\" Returns the file based on the fileName param. \n\t\tExit from script if something goes wrong. \"\"\"\n\ttry:\n\t\tfile = open(fileName, \"r\")\n\texcept IOError:\n\t\tprint(\"Unable to open the file: \" + fileName)\n\t\texit()\n\telse:\n\t\treturn file",
        "sha1": "e5942ce0d2be0291acb303d52f863f23697819d4",
        "id": 305127
    },
    {
        "content": "def _get_day_of_month(x):\n  \"\"\"From a datetime object, extract day of month.\"\"\"\n  return int(x.strftime('%d'))",
        "sha1": "530a2b76ea2a24c051833b8983f6e54182a7e6e8",
        "id": 567210
    },
    {
        "content": "def get_file_contents(file_path) -> str:\n    \"\"\"\n    Opens a file and returns leading and trailing\n    whitespace stripped documents.\n    :param file_path: filepath string. Should simply be the name of\n        sql file.\n    :return: string\n    \"\"\"\n    with open(file_path) as file:\n        query = file.read()\n    return query.strip()",
        "sha1": "0a73d9a46b59b5ba74bac26c5d62e2d101542653",
        "id": 193196
    },
    {
        "content": "def choose_from_dict(choices):\n    \"\"\"\n    The user must choose a value by inputting the associated key\n    :param dict choices: Dict of value/label the user can pick from\n    :return: The selected key\n    \"\"\"\n    # Displays the initial choices\n    input_dict = {x: y for (x, y) in choices.items()}\n    input_list = sorted(input_dict.keys())\n    input_set = set(input_list)\n    messages = [f\"{key}: {input_dict[key]}\" for key in input_list]\n    for message in messages:\n        print(message)\n    # Answers\n    answer = None\n    while answer is None:\n        answer = input()\n        if answer not in input_set:\n            answer = None\n            print(\"Invalid input. Your answer must be in the list above.\")\n    return answer",
        "sha1": "b6952e78e8ee7f62aa45658669b9a0a7b2b1de11",
        "id": 166780
    },
    {
        "content": "def times_numeric(text):\n    \"\"\"\n    A function to convert timepoints encoded within the filename\n    into a float corresponding to the value in nanoseconds.\n    \n    Parameters:\n    text (str): e.g. (100us)\n    \n    Returns:\n    float: e.g. (100,000)\n    \"\"\"\n    number = float(text[:-2])\n    if text.endswith(\"ns\"):\n        return number\n    elif text.endswith(\"us\"):\n        return 1e3*number\n    elif text.endswith(\"ms\"):\n        return 1e6*number\n    else:\n        print(\"scale could not be calculated\")",
        "sha1": "c911ad104a72770318accce169657635db25c388",
        "id": 412069
    },
    {
        "content": "import torch\n\n\ndef relaxed_allclose(x, y, atol=0.01, pct_wrong_allowable=0.1):\n    \"\"\"Comparing two Torch tensors using `allclose` repeatedly fails due to\n    numerical issues. This test relaxes the constraint that every single element\n    in the two tensors are similar. Instead, a percentage of elements must be\n    similar.\n    \"\"\"\n    res     = torch.isclose(x, y, atol=atol)\n    n_wrong = res.numel() - res.sum()\n    n_wrong_allowable = pct_wrong_allowable * res.numel()\n    return n_wrong <= n_wrong_allowable",
        "sha1": "50c7ddf2c152bef1ef28a67479e6ea952b7d8538",
        "id": 98116
    },
    {
        "content": "def get_bool(value):\n    \"\"\"Convert string value to bool.\"\"\"\n    if value is None:\n        return\n    if isinstance(value, bool):\n        return value\n    if value.lower() == \"true\":\n        return True\n    if value.lower() == \"false\":\n        return False",
        "sha1": "e7deba8c9ba3f882d51b6f67e6e2a8b213880f60",
        "id": 371828
    },
    {
        "content": "def _copy_with_exclude_idx(records, tgtidx):\n    \"\"\"\n    generate a new list of records without the target idx: tgtidx\n    \n    Arguments:\n            records {[list of list]} -- [original records]\n            tgtidx {[int]} -- [target idx will be excluded from records]\n    \n    Returns:\n            [list of list] -- [copy of records excluding the tgtidx record]\n    \"\"\"\n\n    return [record for idx, record in enumerate(records) if idx != tgtidx]",
        "sha1": "f35d6e85fc2430be98ddb4153371750b5b43d557",
        "id": 501680
    },
    {
        "content": "def num_of_sent(text):\n    \"\"\"Counts the number of sentences\"\"\"\n    return text.count(\". \") + 1",
        "sha1": "24a6fce423e1e54d3bf56d5ff13258300845ceb8",
        "id": 172184
    },
    {
        "content": "def get(d, key):\n    \"\"\" A helper function to get a key from dict or to return None if not found. \"\"\"\n\n    if key in d:\n        return d[key]\n    else:\n        return None",
        "sha1": "10064d0c79850a4d770a5b3b3dadbed5a5ad1376",
        "id": 185967
    },
    {
        "content": "def to_weekly(series, method='ffill', how='end'):\n    \"\"\"\n    Convenience method that wraps asfreq_actual\n    with 'M' param (method='ffill', how='end').\n    \"\"\"\n    return series.asfreq_actual('W', method=method, how=how)",
        "sha1": "c054a46765e9d1834b98356aa9c46874314362db",
        "id": 616264
    },
    {
        "content": "import hashlib\n\n\ndef md5sum(string):\n    \"\"\" Calculat md5sum value for string \"\"\"\n    return hashlib.md5(string.encode()).hexdigest()",
        "sha1": "8b3a1416cdf234e9f8e93b0b2105570b93b494a3",
        "id": 328341
    },
    {
        "content": "def filter_non_data(line):\n    \"\"\"\n    Returns true if the line contains data\n    :param line: line of a file (str)\n    :return: Bool\n    \"\"\"\n    if '*' in line or '-' in line or '>' in line:\n        res = True\n    else:\n        res = False\n    if 'calcSgsStuff' in line:\n        res = False\n    return res",
        "sha1": "8388f5c7d09a0c3a03b28a66c54bd1f4a3f86261",
        "id": 545525
    },
    {
        "content": "def _improve_entity_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_entity_text):\n    \"\"\"Returns token-level tokenized entity spans that better match the annotated entity.\"\"\"\n    tok_entity_text = \" \".join(tokenizer.basic_tokenizer.tokenize(orig_entity_text))\n\n    for new_start in range(input_start, input_end + 1):\n        for new_end in range(input_end, new_start - 1, -1):\n            text_span = \" \".join(doc_tokens[new_start:(new_end + 1)])\n            if text_span == tok_entity_text:\n                return (new_start, new_end)\n\n    return (input_start, input_end)",
        "sha1": "ba1b7d8a4a372c26fe8a4b37ed37df51ec4f396c",
        "id": 331301
    },
    {
        "content": "from openpyxl.styles import Alignment\n\n\ndef prepare_excel(workbook, filters=True):\n    \"\"\"\n    Formats the excel a bit in order to be displayed nicely\n\n        workbook: openpyxl workbook\n        filters: If True enable excel filtering headers\n        returns: formated workbook\n    \"\"\"\n    # openpyxl is an extra requirement\n\n    for worksheet in workbook:\n        # estimate column width\n        for col in worksheet.columns:\n            max_length = 0\n            column = col[0].column_letter\n            for cell in col:\n                if len(str(cell.value)) > max_length:\n                    max_length = len(str(cell.value))\n            adjusted_width = (max_length + 3) * 1.2\n            worksheet.column_dimensions[column].width = min([adjusted_width, 50])\n\n            # enable excel filters\n            if filters is True:\n                worksheet.auto_filter.ref = f\"A1:{column}1\"\n\n        # enable word wrap\n        for row in worksheet.iter_rows():\n            for cell in row:\n                cell.alignment = Alignment(wrap_text=True)\n                if isinstance(cell.value, str):\n                    cell.value = cell.value.strip()\n                    if cell.value.isdigit():\n                        cell.value = int(cell.value)\n\n    return workbook",
        "sha1": "e9f4b20747a5d8d3fca804117c1e4e3950dc1d45",
        "id": 19279
    },
    {
        "content": "from typing import Iterable\n\n\ndef toggle_page_menu(menu: Iterable) -> dict:\n    \"\"\"\n    Toggle page menu by updating CSS style for menu container.\n\n    Parameters\n    ----------\n    menu: list of menu items\n\n    Return value\n    ------------\n    style: CSS style for container\n    \"\"\"\n    if not menu:\n        return {'display': 'none'}\n\n    return {'display': 'block'}",
        "sha1": "996bbbc5f205071a2898fd694a1b367a9469a585",
        "id": 615617
    },
    {
        "content": "import logging\nimport json\n\n\ndef match_profile(profile_data, profile_dict, profile_matches):\n    \"\"\"\n    Match current profiles to any previously created profiles\n    \"\"\"\n    # If the profile_data dictionary was not populated in the read_profiles methods, there is nothing to match\n    if profile_data:\n        logging.info('Matching new profiles against profile file')\n        # Extract the sequence type and allele dictionary from the profile file\n        for st, allele_comprehension in profile_data.items():\n            # Freeze the allele comprehension as above\n            frozen_allele_comprehension = json.dumps(allele_comprehension, sort_keys=True)\n            try:\n                # Extract the samples that match this profile\n                matches = profile_dict[hash(frozen_allele_comprehension)]\n                # Update the dictionary with the matching samples\n                profile_matches[st] = matches\n            # The profile will not necessarily match any of the profiles found in the analysis\n            except KeyError:\n                pass\n    return profile_matches",
        "sha1": "a95bc32a7d893bdc6f165cd2fc04091f5a990419",
        "id": 443084
    },
    {
        "content": "def append(l, elem):\n    \"\"\"Append list with element and return the list modified\"\"\"\n    if elem is not None:\n        l.append(elem)\n    return l",
        "sha1": "c4e455bfe493f5949572022561271191c8d1dda0",
        "id": 124913
    },
    {
        "content": "def wrong_adjunction(left, right, cup):\n    \"\"\" Wrong adjunction error. \"\"\"\n    return \"There is no {0}({2}, {3}) in a rigid category. \"\\\n           \"Maybe you meant {1}({2}, {3})?\".format(\n               \"Cup\" if cup else \"Cap\", \"Cap\" if cup else \"Cup\", left, right)",
        "sha1": "263684e737a3212a1d44fcd88ba719fc9f1c07a1",
        "id": 27965
    },
    {
        "content": "def get_rpath_flag(path: str) -> str:\n    \"\"\"\n    Get the linker flag needed to add the given RPATH to the generated executable or library.\n    \"\"\"\n    return \"-Wl,-rpath,{}\".format(path)",
        "sha1": "ce71eda6775cd6ff0118a051ab695f9c2a6e2209",
        "id": 497054
    },
    {
        "content": "def split_container(path):\n    \"\"\"Split path container & path\n\n    >>> split_container('/bigdata/path/to/file')\n    ['bigdata', 'path/to/file']\n    \"\"\"\n    path = str(path)  # Might be pathlib.Path\n    if not path:\n        raise ValueError('empty path')\n    if path == '/':\n        return '', ''\n\n    if path[0] == '/':\n        path = path[1:]\n\n    if '/' not in path:\n        return path, ''  # container\n\n    return path.split('/', maxsplit=1)",
        "sha1": "53b0d1164ecc245146e811a97017f66bb1f032a7",
        "id": 9941
    },
    {
        "content": "def gen_locale(locale):  # type: (str) -> str\n    \"\"\"Returns the generated code for a given locale in the list.\"\"\"\n    # We assume that all locale codes have only letters, numbers and hyphens.\n    assert locale.replace('-', '').isalnum(), locale\n    # clang-format enforces a four-space indent for initializer lists.\n    return '    PLATFORM_LOCALE({locale})'.format(locale=locale)",
        "sha1": "747d8018d99b7e530b0fd4ac7476bc27df980270",
        "id": 50637
    },
    {
        "content": "def rotate_seq1(seq1, n):\n    \"\"\" Rotate a sequence left by n \"\"\"\n    # E.g: rotate([1,2,3,4,5], 2) => [4,5,1,2,3]\n\n    k = len(seq1) - n\n    return seq1[k:] + seq1[:k]",
        "sha1": "4be2aacedb47aefba1e7afa753fdef183c8e78c9",
        "id": 245105
    },
    {
        "content": "def get_face_roi_size(feature: list):\n    \"\"\"\n    Get face patch size\n    :param feature: list of features\n    :return: face patch size\n    \"\"\"\n    return list(feature[11])",
        "sha1": "dc6f370587ee2fdb347ac71910253d33d79fcc6e",
        "id": 114587
    },
    {
        "content": "def split_host_port(host_port):\n    \"\"\"\n    Splits a string of host and port separated by a semicolon.\n\n    :param host_port: host or host:port. Allowed values are like\n        10.20.31.1:3306 or just 10.20.31.1\n    :return: a tuple with host and port. If only address is specified it'll\n        return (address, None). If host_port is None it will\n        return (None, None)\n    :rtype: tuple\n    \"\"\"\n    try:\n        host = host_port.split(':')[0]\n        if not host:\n            host = None\n    except AttributeError:\n        host = None\n    try:\n        port = int(host_port.split(':')[1])\n    except (IndexError, AttributeError, ValueError):\n        port = None\n    return host, port",
        "sha1": "e41bb0c744ca0b55dd7e487bfa2b001e64ae1cce",
        "id": 502625
    },
    {
        "content": "import copy\n\n\ndef copy_neural_network(target_network):\n    \"\"\"Copy an Echo State Network.\n    :rtype: pyESN.ESN\n    :param target_network: network to copy\n    :return: pyESN Echo State Network model\n    \"\"\"\n    model = copy.deepcopy(target_network)\n    return model",
        "sha1": "3a08832c0e492678fafb5ee22faaf74ea39b275c",
        "id": 413705
    },
    {
        "content": "from typing import Optional\nfrom pathlib import Path\nimport tempfile\n\n\ndef mktmp(\n    prefix: Optional[str] = None,\n    suffix: Optional[str] = None,\n    separator: str = \"-\",\n    dir: Optional[Path] = None,\n) -> Path:\n    \"\"\"\n    Makes a temp file.  A wrapper for `tempfile.mkstemp`.\n    \"\"\"\n    if prefix is not None:\n        prefix = prefix + separator\n    if suffix is not None:\n        suffix = separator + suffix\n    _, path = tempfile.mkstemp(prefix=prefix, suffix=suffix, dir=dir)\n    return Path(path)",
        "sha1": "8bc1cb9448b497073defc7890fa85f3c0532e00c",
        "id": 611894
    },
    {
        "content": "from typing import List\n\n\ndef sanitize_rows(raw_rows: str) -> List[str]:\n    \"\"\"\n    remove blank lines and comments\n\n        :param raw_rows: the rows\n        :return: a list of 'cleaned' rows\n    \"\"\"\n    cleaned_rows = []\n\n    for row in raw_rows:\n        if row != \"\" and row[0:2] != \"//\":\n            cleaned_rows.append(row.partition(\"//\")[0])\n\n    return cleaned_rows",
        "sha1": "8b93ee9905f17e0ca28623156fe5f5ad6828e118",
        "id": 161434
    },
    {
        "content": "import math\n\n\ndef F(x):\n  \"\"\"This function returns the x-th Fibonacci number\"\"\"\n  sqrt_5 = math.sqrt(5)\n  golden_ratio = (1.0 + sqrt_5) / 2\n  return round(golden_ratio**(x + 1) / sqrt_5)",
        "sha1": "e082c18244fbe4c4e12f7d864783175e0e915c89",
        "id": 174321
    },
    {
        "content": "def is_number(s):\n  \"\"\"Returns true if the supplied item can be converted into a float; false\n  otherwise.\n\n  \"\"\"\n  try:\n    float(s)\n    return True\n  except Exception:\n    return False",
        "sha1": "e86598b7ef86801f5f0dc2592df0c79a08fc8d0a",
        "id": 228411
    },
    {
        "content": "def compute_mse(y, tx, beta):\n    \"\"\"compute the loss by mse.\"\"\"\n    e = y - tx.dot(beta)\n    mse = e.dot(e) / (2 * len(e))\n    return mse",
        "sha1": "9566eb1aaaf61565115ef8674e9a76cf83be27c5",
        "id": 280188
    },
    {
        "content": "import torch\n\n\ndef variance(data:torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Compute the variance for given input data.\n\n    Parameters\n    ----------\n    data: torch.Tensor\n        input data\n\n    Returns\n    -------\n    variance (torch.Tensor)\n\n    \"\"\"\n    return data.var(-1)",
        "sha1": "8c9f6c00ce914b43feb22f3c078aa844e0ddf99c",
        "id": 611289
    },
    {
        "content": "def normalize (pattern):\n    \"\"\"Normalizes the LED segment `pattern` by sorting it alphabetically.\"\"\"\n    return ''.join( sorted(pattern) )",
        "sha1": "d2fbfcf569401a6d33ad3032e676de42af7dea86",
        "id": 362284
    },
    {
        "content": "def find_keys(key_list, keys, require_match=False):\n    \"\"\"Find the indices of keys into a list of keys.\n\n    Parameters\n    ----------\n    key_list : iterable\n    keys : iterable\n    require_match : bool\n        Require that `key_list` contain every element of `keys`,\n        and if not, raise ValueError.\n\n    Returns\n    -------\n    indices : list of int or None\n        List of the same length as `keys` containing\n        the indices of `keys` in `key_list`.  If `require_match`\n        is False, then this can also contain None for keys\n        that are not contained in `key_list`.\n    \"\"\"\n    # Significantly faster than repeated calls to find_key\n    try:\n        dct = {tuple(kk): ii for ii, kk in enumerate(key_list)}\n        index = [dct.get(tuple(key)) for key in keys]\n    except TypeError:\n        dct = {kk: ii for ii, kk in enumerate(key_list)}\n        index = [dct.get(key) for key in keys]\n\n    if require_match and any([ind is None for ind in index]):\n        raise ValueError(\"Could not find all of the keys.\")\n    else:\n        return index",
        "sha1": "01220ff2118c3f3730726a576f0191434dfb86ce",
        "id": 345552
    },
    {
        "content": "def get(isamAppliance, check_mode=False, force=False):\n    \"\"\"\n    Retrieve available updates\n    \"\"\"\n    return isamAppliance.invoke_get(\"Retrieving available updates\",\n                                    \"/updates/available.json\")",
        "sha1": "3317a609759140f587a0156e012a0b3e4962bf1e",
        "id": 437682
    },
    {
        "content": "def make_list_slug(name):\n    \"\"\"Return the slug for use in url for given list name.\"\"\"\n    slug = name.lower()\n    # These characters are just stripped in the url\n    for char in '!@#$%^*()[]{}/=?+\\\\|-_':\n        slug = slug.replace(char, '')\n    # These characters get replaced\n    slug = slug.replace('&', 'and')\n    slug = slug.replace(' ', '-')\n    return slug",
        "sha1": "6f2bbd556ab0e668b8c9eef8f08329251105d7cc",
        "id": 213886
    },
    {
        "content": "import torch\n\n\ndef _masked_loss_func(output, masks, samples):\n    \"\"\" Computes the MSE loss.\n\n    :param output: tensor(Float); imputed samples\n    :param masks: tensor(Float); corresponding masks\n    :param samples: tensor(Float); original samples (with missing values)\n    :return: tensor(Float); loss obtained\n    \"\"\"\n    if masks is None:\n        masks = torch.zeros(samples.shape)\n    mse_loss = torch.sum((~masks.bool() * samples - ~masks.bool() * output) ** 2)\n    if torch.sum(1 - masks) > 0:\n        mse_loss /= torch.sum(1 - masks)\n    return mse_loss",
        "sha1": "2168991a8f79f2ed82b948bb508ef310de7236d9",
        "id": 114431
    },
    {
        "content": "def _range_to_int_list(rng: str):\n    \"\"\"Converts string in format num1:num2 to a list of integers\"\"\"\n    if \":\" in rng:\n        splt = rng.split(\":\")\n        start = splt[0]\n        end = splt[1]\n\n        lst = list(range(int(start), int(end) + 1))\n\n        return lst\n    return rng",
        "sha1": "87e97e354eeb6993292220e437a8865cadaf7bb4",
        "id": 505911
    },
    {
        "content": "def _get_pd_fields(d, prefix, id):\n    \"\"\"Retrieve form fields representing a product.\"\"\"\n    fields = ['name', 'price', 'quantity_per_package', 'unit', 'quantity_limit', 'quantum', 'unit_weight']\n    raw = {f: d.get(\"%s%d-%s\" % (prefix, id, f), None) for f in fields}\n    if not any(f for f in raw.values()):\n        return {'deleted': True}  # All fields empty means deleted\n    qpp = raw['quantity_per_package']\n    quota = raw['quantity_limit']\n    quantum = raw['quantum']\n    weight = raw['unit_weight']\n    return {'name': raw['name'],\n            'price': float(raw['price']),\n            'quantity_per_package': int(qpp) if qpp else None,\n            'unit': raw['unit'] or u'pi\u00e8ce',\n            'quantity_limit': int(quota) if quota else None,\n            'quantum': float(quantum) if quantum else None,\n            'unit_weight': float(weight) if weight else None,\n            'deleted': \"%s%d-deleted\" % (prefix, id) in d}",
        "sha1": "c7160759c43fbf9f9a7c94af4b3cf811eb3909e6",
        "id": 84492
    },
    {
        "content": "def bool_to_true_false(val):\n    \"\"\"Convert True/False to TRUE / FALSE\"\"\"\n    return 'TRUE' if val else 'FALSE'",
        "sha1": "a8a6784a47f542003c3cd5624cf721a1a4b613eb",
        "id": 421042
    },
    {
        "content": "def rate_black_pixel_ratio (\n        ratio: int,\n        thres_1: int = 20,\n        thres_2: int = 50\n    ) -> int:\n    \"\"\" Evaluates the given black-pixel-ratio and returns the detected marking.\n\n    Args:\n        ratio (int):\n            the ratio of black pixels compared to total pixels\n        thres_1 (int):\n            threshold for checked boxes\n        thres_2 (int):\n            threshold for corrected boxes\n\n    Returns:\n        -1 if a box is filled\n        1 if a box is checked\n        0 if a box is empty\n    \"\"\"\n    if ratio >= thres_2:\n        return -1 # corrected box\n    if ratio >= thres_1:\n        return 1 # checked\n    else: # empty box\n        return 0",
        "sha1": "3dc03bec5ba9ecaf9845a06421b77b84fcd964bd",
        "id": 201314
    },
    {
        "content": "def calRR(pre_close, prepre_close):\n    \"\"\"\n    :param pre_close: \u6628\u5929\u7684\u6536\u76d8\u4ef7\n    :param prepre_close: (returnPeriod+1)\u5929\u524d\u7684\u6536\u76d8\u4ef7\n    :return: \u6536\u76ca\u7387\n    \"\"\"\n    return 1.0 * (pre_close - prepre_close) / prepre_close",
        "sha1": "988336bb7ab21cd5f1f311292b1b01420117519b",
        "id": 397428
    },
    {
        "content": "from pathlib import Path\n\n\ndef bids_output_name(phsiyo_path, output_root):\n    \"\"\"\n    Create file name based on BIDS\n    \"\"\"\n    phsiyo_file = Path(phsiyo_path)\n    filename_root = phsiyo_file.name.split(\"_physio\")[0]\n    subject = filename_root.split(\"_\")[0].split(\"sub-\")[-1]\n    if \"ses\" in filename_root:\n        session = filename_root.split(\"_\")[1].split(\"ses-\")[-1]\n        output_dir = Path(output_root) / \"pyretroicor\" / f\"sub-{subject}\" / f\"ses-{session}\" \n        out_name = f\"{filename_root}_desc-retroicor_regressors.tsv\"\n    else:\n        output_dir = Path(output_root) / \"pyretroicor\" / f\"sub-{subject}\"\n        out_name = f\"{filename_root}_desc-retroicor_regressors.tsv\"\n\n    # create output dir if not exist\n    output_dir.mkdir(parents=True, exist_ok=True)\n    filepath = output_dir / out_name\n    return filepath",
        "sha1": "15fb8953ef07f33b06158d4f636d2cfa25fee2a9",
        "id": 255403
    },
    {
        "content": "def snapshot(data_core, dsn, tag):\n    \"\"\"\n    Get the snapshot value for a tag\n    :param data_core: A data core client instance\n    :param dsn: The fully qualified name of a data source\n    :param tag: The name of the tag to query.\n    \"\"\"\n    #do a snapshot query\n    return data_core.get_snapshot_data({dsn: [tag]})",
        "sha1": "8fb24af9f327b7e37932532a22ac729821fa72ba",
        "id": 356941
    },
    {
        "content": "def to_sncosmo(light_curve):\n    \"\"\"Convert an lcdata light curve to sncosmo format.\n\n    This adds the zp and zpsys keys that are required by sncosmo, and converts the band\n    name to a string instead of the bytes type used internally.\n\n    Parameters\n    ----------\n    light_curve : `~astropy.table.Table`\n        Light curve in lcdata format\n\n    Returns\n    -------\n    `~astropy.table.Table`\n        Light curve in sncosmo format\n    \"\"\"\n    light_curve = light_curve.copy()\n\n    # Convert the band names from bytes to strings\n    light_curve['band'] = light_curve['band'].astype(str)\n\n    # Add in zeropoint information.\n    light_curve['zp'] = 25.\n    light_curve['zpsys'] = 'ab'\n\n    return light_curve",
        "sha1": "d7b27f05df81517f6f2207fe20a7473b186d403f",
        "id": 401500
    },
    {
        "content": "def intersect(lst1, lst2):\n    \"\"\"intersection of two lists\n    \"\"\"\n    lst3 = [value for value in lst1 if value in lst2]\n    return lst3",
        "sha1": "4f579dd6eea95871ffa544852b76feb2df25d036",
        "id": 674836
    },
    {
        "content": "def create_hand_record(context):\n    \"\"\"Create an empty hand and store it to the current hands.\n\n    Args:\n        context (dict): See function `create_score_records` above.\n\n    Returns:\n        A new dict object.\n    \"\"\"\n\n    game = context['games'][-1]\n    hands = game['hands']\n    hand = dict(\n        action_table=[],\n        balance={},\n        game=game, # parent\n        seat_table=[None] * 4,\n        start_hand_table=[None] * 4,\n        dora_table=[],\n        chows=[],\n        pungs=[],\n        kongs=[],)\n    hands.append(hand)\n\n    assert context['games'][-1]['hands'][-1] == hand\n    return hand",
        "sha1": "dcaad230f984391993b64b1eb264c5c6eec9aee0",
        "id": 423629
    },
    {
        "content": "def byte_to_signed_int(x):\n    \"\"\"\n    Returns a signed integer from the byte\n    :param x:  Byte\n    \"\"\"\n    if (x & 0x80) >> 7 == 1:\n        return -((x-1) ^ 0xff)\n    else:\n        return x",
        "sha1": "c3fda8f63e5702e80e422adbc2a9126d2d0a25e7",
        "id": 575805
    },
    {
        "content": "def ctype(v, t):\n    \"\"\"Convert \"v\" to type \"t\"\n\n    >>> ctype('10', int)\n    10\n    >>> ctype('10', list)\n    ['1', '0']\n    >>> ctype(10, list)\n    10\n    >>> ctype('10a', float)\n    '10a'\n\n    :param tp.Any v:\n    :param tp.TypeVar t:\n    :return tp.Any:\n    \"\"\"\n    try:\n        return t(v) or v\n    except (ValueError, TypeError):\n        return v",
        "sha1": "d2a6596c0df16cf82bc8ae7f1d888dc9cc192d25",
        "id": 213516
    },
    {
        "content": "def Trim(lst, limit):\n  \"\"\"Trims a given list so that it is not longer than given limit.\n\n  Args:\n    lst: A list to trim.\n    limit: A maximum number of elements in the list after trimming.\n\n  Returns:\n    A suffix of the input list that was trimmed.\n  \"\"\"\n  limit = max(0, limit)\n\n  clipping = lst[limit:]\n  del lst[limit:]\n  return clipping",
        "sha1": "8b450a14eb94bd4f8bb71bb63c159f36468ca6b2",
        "id": 123107
    },
    {
        "content": "def create_output_header(data_type, species_list, biomet_var_list=[]):\n    \"\"\"\n    A helper function to create the header for output data frame.\n\n    Parameters\n    ----------\n    data_type : str\n        The type of output dataframe\n        * 'flux' - flux data\n        * 'diag' - curve fitting diagnostics\n    species_list : list of str\n        List of gas species\n    biomet_var_list : list of str\n        List of biometeorological variable names\n\n    Returns\n    -------\n    header : list of str\n        Table header for the output dataframe. If `data_type` is illegal,\n        return a blank list.\n    \"\"\"\n    if data_type == 'flux':\n        header = ['doy_utc', 'doy_local', 'ch_no', 'ch_label', 'A_ch', 'V_ch']\n        for conc_suffix in ['_atmb', '_chb', '_cha', '_atma']:\n            header += [s + conc_suffix for s in species_list]\n            header += ['sd_' + s + conc_suffix for s in species_list]\n\n        header += [s + '_chc_iqr' for s in species_list]\n        for flux_method in ['_lin', '_rlin', '_nonlin']:\n            header += ['f' + s + flux_method for s in species_list]\n            header += ['se_f' + s + flux_method for s in species_list]\n\n        # add quality flags for fluxes\n        header += ['qc_' + s for s in species_list]\n\n        # add number of valid observations of concentrations\n        header += ['n_obs_' + s for s in species_list]\n\n        # biomet variables and other auxiliary variables\n        header += ['flow_lpm', 't_turnover', 't_lag_nom', 't_lag_optmz',\n                   'status_tlag', 'pres', 'T_log', 'T_inst'] + biomet_var_list\n    elif data_type == 'diag':\n        header = ['doy_utc', 'doy_local', 'ch_no']\n        for s in species_list:\n            header += ['k_lin_' + s, 'b_lin_' + s, 'r_lin_' + s,\n                       'p_lin_' + s, 'rmse_lin_' + s, 'delta_lin_' + s]\n\n        for s in species_list:\n            header += ['k_rlin_' + s, 'b_rlin_' + s,\n                       'k_lolim_rlin_' + s, 'k_uplim_rlin_' + s,\n                       'rmse_rlin_' + s, 'delta_rlin_' + s]\n\n        for s in species_list:\n            header += ['p0_nonlin_' + s, 'p1_nonlin_' + s,\n                       'se_p0_nonlin_' + s, 'se_p1_nonlin_' + s,\n                       'rmse_nonlin_' + s, 'delta_nonlin_' + s]\n    else:\n        return []\n\n    return header",
        "sha1": "7ae280b59433a39ac2244ee9ee5306e1676818cd",
        "id": 643866
    },
    {
        "content": "import re\n\n\ndef matching(value, pattern, casesensitive=True):\n    \"\"\"\n    Filter that performs a regex match\n\n    :param value: Input source\n    :type value: str\n    :param pattern: Regex Pattern to be matched\n    :return: True if matches. False otherwise\n    :rtype: bool\n    \"\"\"\n    flags = re.I if not casesensitive else 0\n    return re.match(str(pattern), str(value), flags) is not None",
        "sha1": "2a54309730cd320e39d69db30a13e8cfead0524b",
        "id": 684007
    },
    {
        "content": "def extract_xyrange(box):\n    \"\"\"\n    Gets range for a side of a box. Returns range(x), range(y)\n\n    Args:\n        box (tuple): Coordinates of the box in (x1, x2, y1, y2)\n\n    Returns:\n        Two values (int): x_range (int), y_range (int)\n\n    \"\"\"\n    x_range = range(int(round(box[0])), int(round(box[2])))\n    y_range = range(int(round(box[1])), int(round(box[3])))\n    return x_range, y_range",
        "sha1": "b970010ebfd97528ed4a7d68eea45622e2585781",
        "id": 154094
    },
    {
        "content": "def _lower_keys(key, subkey):\n    \"\"\"Make sure keys are always lower key.\"\"\"\n    return key.lower(), subkey.lower()",
        "sha1": "da3477c3a6514b472818fb5eaca235b911bd929e",
        "id": 272292
    },
    {
        "content": "def str_space_to_int_list(string):\n    \"\"\" Converts a spaced string in a list of integer.\n\n    The string consists of the succession of the elements of the list separated by spaces.\n\n    :param string: the string to convert.\n    :return: the corresponding list of integer.\n    \"\"\"\n    string_list = string.split(\" \")\n    int_list = []\n    if string_list != '':\n        for element in string_list:\n            if element != '':\n                int_list.append(int(element))\n    return int_list",
        "sha1": "c52dc4ff6eea9c55b87607d0fe868e486bfc175f",
        "id": 347367
    },
    {
        "content": "def human_readable_size(size):\n    \"\"\"Return a string for better assessing large number of bytes.\"\"\"\n    if size < 2**10:\n        return \"%s\" % size\n    elif size < 2**20:\n        return \"%.2f KB\" % (size / float(2**10))\n    elif size < 2**30:\n        return \"%.2f MB\" % (size / float(2**20))\n    elif size < 2**40:\n        return \"%.2f GB\" % (size / float(2**30))\n    else:\n        return \"%.2f TB\" % (size / float(2**40))",
        "sha1": "a32d991ec79d9d68872ba3be16d03faa7f613f7a",
        "id": 321126
    },
    {
        "content": "import builtins\n\n\ndef no_matplotlib(monkeypatch):\n    \"\"\" Mock an import error for matplotlib\"\"\"\n    import_orig = builtins.__import__\n\n    def mocked_import(name, globals, locals, fromlist, level):\n        \"\"\" \"\"\"\n        if name == 'matplotlib.pyplot':\n            raise ImportError(\"This is a mocked import error\")\n        return import_orig(name, globals, locals, fromlist, level)\n\n    monkeypatch.setattr(builtins, '__import__', mocked_import)",
        "sha1": "681ba8c0e70387e46ad7ed42ffb11ce8aa7f23bc",
        "id": 703154
    },
    {
        "content": "def reduce_next_sentence_label_dimension(batch):\n  \"\"\"Change next_sentence_labels's shape from (-1, 1) to (-1,).\n\n  Args:\n    batch: A dictionary mapping keys to arrays.\n\n  Returns:\n    Updated batch.\n  \"\"\"\n\n  batch['next_sentence_labels'] = batch['next_sentence_labels'][:, 0]\n  return batch",
        "sha1": "32731bd22eb28693e336ca981d897feaacd8c816",
        "id": 650993
    },
    {
        "content": "def _update_progress(\n    start_time,\n    current_time,\n    progress_increment,\n    current_progress,\n    total,\n    unit,\n    callback_function,\n):\n    \"\"\"Helper function for updating progress of a function and making a call to the progress callback\n    function, if provided. Adds the progress increment to the current progress amount and returns the\n    updated progress amount.\n\n    If provided, the callback function should accept the following parameters:\n        - update (int): change in progress since last call\n        - progress (int): the progress so far in the calculations\n        - total (int): the total number of calculations to do\n        - unit (str): unit of measurement for progress/total\n        - time_elapsed (float): total time in seconds elapsed since start of call\n    \"\"\"\n    if callback_function is not None:\n        new_progress = current_progress + progress_increment\n        elapsed_time = current_time - start_time\n        callback_function(progress_increment, new_progress, total, unit, elapsed_time)\n\n        return new_progress",
        "sha1": "20199876a2126d7ffe904d806d84f8d31fb34df4",
        "id": 402381
    },
    {
        "content": "def bytes2human(n: int) -> str:\n    \"\"\"Convert `n` bytes into a human readable string.\n\n    >>> bytes2human(10000)\n    '9.8K'\n    >>> bytes2human(100001221)\n    '95.4M'\n    \"\"\"\n    # http://code.activestate.com/recipes/578019\n    symbols = (\"K\", \"M\", \"G\", \"T\", \"P\", \"E\", \"Z\", \"Y\")\n    prefix = {}\n    for i, s in enumerate(symbols):\n        prefix[s] = 1 << (i + 1) * 10\n    for s in reversed(symbols):\n        if n >= prefix[s]:\n            value = float(n) / prefix[s]\n            return \"%.1f%s\" % (value, s)\n    return \"%sB\" % n",
        "sha1": "6929ad5c3fb865a32e733ba6c0d9fdfed0493efc",
        "id": 653693
    },
    {
        "content": "import hashlib\n\n\ndef HashFile(filename):\n    \"\"\"Returns SHA-256 hash of a given file.\"\"\"\n    if isinstance(filename, list):\n        filename = filename[0]\n    try:\n        return hashlib.sha256(\n            open(filename, \"rb\").read()).hexdigest()\n    except IOError:\n        return \"UNKNOWN FILE HASH\"",
        "sha1": "6529334d79246ad113bed7bfcb3b84cc59679f90",
        "id": 37708
    },
    {
        "content": "import math\nimport itertools\n\n\ndef make_subplot_coords(n_subplot, ncol):\n    \"\"\"Make subplot coords.\n\n    Parameters\n    ----------\n    n_subplot : int\n        number of subplots to make\n    ncol : int\n        Number of columns with subplots\n    Returns\n    x : tuple \n       (subplot_coords_x-y,nrow,ncol)\n    \"\"\"\n    # setting up subplots\n    ncol = int(ncol)\n    n_subplot = int(n_subplot)\n    nrow = int(math.ceil(n_subplot / float(ncol)))\n    \n    # subplot coords \n    coords = itertools.product(range(nrow), range(ncol))\n    \n    return (coords, nrow)",
        "sha1": "38c64c23b4ba9760b2939c4756d5429dfd153bfc",
        "id": 484548
    },
    {
        "content": "from typing import Sequence\nfrom typing import Any\nfrom typing import Optional\n\n\ndef rindex(items: Sequence[Any], needle: Any) -> Optional[int]:\n    \"\"\"\n    Args:\n        items (sequence): The items to search\n        needle (object): The object to search for\n\n    Returns:\n        The rightmost index of `needle`, or `None`.\n    \"\"\"\n    for idx, item in enumerate(reversed(items)):\n        if item == needle:\n            return len(items) - idx - 1\n    return None",
        "sha1": "51361177e933a7e18c7cb3f9dac9958552a6b256",
        "id": 106761
    },
    {
        "content": "from typing import Iterable\nfrom typing import AnyStr\n\n\ndef join_separated(iterable: Iterable[AnyStr], sep: AnyStr) -> AnyStr:\n    \"\"\"\n    Join the elements of ``iterable`` together, separating consecutive elements\n    with ``sep``\n\n    :param iterable: an iterable of binary or text strings\n    :param sep: a binary or text string\n    :rtype: a binary or text string\n    \"\"\"\n    return sep.join(iterable)",
        "sha1": "e2cd30bf76b0f17623d00dbd302a7976ee8230db",
        "id": 577643
    },
    {
        "content": "import requests\n\n\ndef quota(email, host=\"http://www.compbio.dundee.ac.uk/jpred4/cgi-bin/rest\", suffix=\"quota\", silent=False):\n    \"\"\"Check how many jobs you have already submitted on a given day\n    (out of 1000 maximum allowed jobs per user per day).\n\n    :param str email: E-mail address.\n    :param str host: JPred host address.\n    :param str suffix: Host address suffix.\n    :param silent: Should the work be done silently?\n    :type silent: :py:obj:`True` or :py:obj:`False`\n    :return: Response.\n    :rtype: requests.Response\n    \"\"\"\n    quota_url = \"{}/{}/{}\".format(host, suffix, email)\n    response = requests.get(quota_url)\n    if not silent:\n        print(response.text)\n    return response",
        "sha1": "7654fe4591360b50bf3d24144be6e170aa466642",
        "id": 194867
    },
    {
        "content": "def count_orphan_resource_providers(db):\n    \"\"\"Count all orphan resource providers in the nova_api database.\"\"\"\n    sql = '''\\\n    SELECT COUNT(*)\n    FROM   nova_api.resource_providers rp JOIN nova.compute_nodes cn\n    ON     cn.hypervisor_hostname = rp.name\n    WHERE  cn.deleted = 0\n       AND rp.uuid != cn.uuid\n    '''\n    return db.query(sql)",
        "sha1": "18875ee9f6afd788198f9044397cec8a6383f05d",
        "id": 636046
    },
    {
        "content": "def include_approved(request):\n    \"\"\"\n    Convert 'include_approved' GET parameter value to boolean.\n    Accept 'true' and 'True' as valid values.\n    \"\"\"\n    query_param_value = request.GET.get('include_approved')\n    return (query_param_value in ['true', 'True'])",
        "sha1": "cdae3d86dfa5ab674fb1a4750547339959d5c3e4",
        "id": 465996
    },
    {
        "content": "import collections\n\n\ndef get_sorted_transitive_dependencies(top, deps_func):\n    \"\"\"Gets the list of all transitive dependencies in sorted order.\n\n    There should be no cycles in the dependency graph (crashes if cycles exist).\n\n    Args:\n      top: A list of the top level nodes\n      deps_func: A function that takes a node and returns a list of its direct\n          dependencies.\n    Returns:\n      A list of all transitive dependencies of nodes in top, in order (a node\n      will appear in the list at a higher index than all of its dependencies).\n    \"\"\"\n    # Find all deps depth-first, maintaining original order in the case of ties.\n    deps_map = collections.OrderedDict()\n\n    def discover(nodes):\n        for node in nodes:\n            if node in deps_map:\n                continue\n            deps = deps_func(node)\n            discover(deps)\n            deps_map[node] = deps\n\n    discover(top)\n    return list(deps_map.keys())",
        "sha1": "7e758410c785e7f1b6df0dbd2a3571a402b95641",
        "id": 34547
    },
    {
        "content": "def create_request(session_nr, cell_nr, request_str):\n    \"\"\"\n        Returns a request `dict` to be used by lottus\n        :param session_nr: the session identifier\n        :param cell_nr: the cell identifier\n        :param request_str: the string with the request from the client\n    \"\"\"\n    return {'session_nr': session_nr, 'cell_nr': cell_nr, 'request_str': request_str}",
        "sha1": "e3a2656793ee00a7effe40cb409c2c53a7b3cffa",
        "id": 490888
    },
    {
        "content": "def _split_str(in_str, split_size = 50):\n    \"\"\"Split a string into two parts separated by an ellipsis if it is longer than split_size\"\"\"\n    if len(in_str) < split_size:\n        return in_str\n    return in_str[:25] + ' ... ' + in_str[-25:]",
        "sha1": "4eda50e21272a89038111ce8f942e8b6099a0f73",
        "id": 147700
    },
    {
        "content": "def binary_search(array, value):\n    \"\"\"Search for value in sorted array, using binary search\n\n    Continually divide (sub-) array in half until value is found, or entire\n    array has been searched. Iterative approach.\n\n    Parameters\n    array : list\n        List to search. Must be sorted.\n    value : any\n        Value to search for. Must be same type as elements in array.\n\n    Returns\n    -------\n    bool\n        True if array contains value, False otherwise\n    \"\"\"\n    # Set starting indexes, which will be used to index sub-array as it shrinks\n    low = 0\n    high = len(array) - 1\n\n    # Keep searching until low and high pointers overlap\n    while (high - low) > 0:\n\n        mid = int((high + low) / 2)\n\n        # Check to see if dividing index (mid) equals value we're searching for\n        if array[mid] == value:\n            return True\n\n        elif value < array[mid]:\n            # -1 since don't want to check value at mid again (redundant)\n            high = array[mid] - 1\n\n        elif value > array[mid]:\n            # +1 since don't want to check value at mid again (redundant)\n            low = array[mid] + 1\n\n    return False",
        "sha1": "08fc6be6571854a0003a7ccc354c397dfb791059",
        "id": 24586
    },
    {
        "content": "def can_link_to(person, contact, model):\n    \"\"\"\n    Determines whether or not the specified person can form a social network\n    link to the specified contact.\n\n    :param person: The person wanting to link.\n    :param contact: The contact to determine link eligibility.\n    :param model: The model to use.\n    \"\"\"\n    if model.require_mutual and \\\n                    len(contact.contacts) >= contact.max_contacts and \\\n            not person.unique_id in contact.contacts:\n        return False\n    return True",
        "sha1": "8250b3d2ff7fb5be2cb47e83caa99c827517ea79",
        "id": 689344
    },
    {
        "content": "def has_all_keys(dict_to_scan, keys):\n    \"\"\"\n    Returns True if the given dict has all of the given keys\n\n    Args:\n        dict_to_scan (Dict[str, Any]):\n        keys (Iterable[str]): Iterable of keys to check for\n\n    Returns:\n        bool: True if the given dict has all of the given keys\n    \"\"\"\n    return all(key in dict_to_scan for key in keys)",
        "sha1": "a13e7bdaf4c63e164eb8664390e68e5b6d783437",
        "id": 553874
    },
    {
        "content": "def sort_all(batch, lens):\n    \"\"\" Sort all fields by descending order of lens, and return the original indices.\n    unsorted_all[0]: length of each element in batch\n    unsorted_all[1]: running index to keep track\n    unsorted_all[2]: elements in batch\n    unsorted_all:                                   [[5, 4, 2, 3], [0, 1, 2, 3], ['a', 'b', 'c', 'd']]\n\n    zip(*unsorted_all):                             [(5, 0, 'a'), (4, 1, 'b'), (2, 2, 'c'), (3, 3, 'd')]\n    sorted(zip(*unsorted_all), reverse=True):       [(5, 0, 'a'), (4, 1, 'b'), (3, 3, 'd'), (2, 2, 'c')]\n    zip(*sorted(zip(*unsorted_all), reverse=True)): [(5, 4, 3, 2), (0, 1, 3, 2), ('a', 'b', 'd', 'c')]\n    \"\"\"\n    unsorted_all = [lens] + [range(len(lens))] + list(batch)\n    sorted_all = [list(t) for t in zip(*sorted(zip(*unsorted_all), reverse=True))]\n    return sorted_all[2:], sorted_all[1]",
        "sha1": "c71c5c205079d59ac84806b5a1677b5acbceab6c",
        "id": 209699
    },
    {
        "content": "def same_module(cls1: type, cls2: type) -> bool:\n    \"\"\"Return if two classes come from the same module via the ``__module__`` attribute.\"\"\"\n    return cls1.__module__.split(\".\")[0] == cls2.__module__.split(\".\")[0]",
        "sha1": "fee2451b84b38d9a212c09edd49b83880a59909a",
        "id": 123807
    },
    {
        "content": "from typing import List\nimport math\n\n\ndef from_matrix(R : List[List[float]]) -> float:\n    \"\"\"Returns the rotation angle of a rotation matrix.\"\"\"\n    return math.atan2(R[1][0],R[0][0])",
        "sha1": "6b07ebb4cdd3aac766e98c595ac84c8ccdad0319",
        "id": 353512
    },
    {
        "content": "def modify_ref_method_str(df, param):\n    \"\"\"Subroutine for Ref_API_Query tha replaces various characters in data\n    columns containing text, including the method name and the parameter units.\n\n    Instrument Method names retrieved from the method code lookup table are\n    specified in all upper case characters. These are converted to title cased\n    characters (e.g., This Is Title Cased Text). While this imrpoves legibility\n    some phrases (particularly acronyms, conjunctions, prepositions, ect.)\n    should not be title cased. This function replaces specific phrases with the\n    expected format.\n\n    In addition, abbreviated unit names (used by AirNow) are converted to\n    long format text to ensure consistency for reference data retreived from\n    AQS, AirNow, and AirNowTech, and also to improve legibility.\n\n    Args:\n        df (pandas DataFrame):\n            Dataframe resulting from API query.\n        param (str):\n            The evaluation parameter.\n\n    Returns:\n        df (pandas DataFrame):\n            Modified dataframe with method and unit strings corrected.\n\n    \"\"\"\n    # Lookup dictionary of phrases that shouldn't be title cased\n    replace = {'Method': {'Api': 'API',\n                          'Frm': 'FRM',\n                          'Fem': 'FEM',\n                          'Lpm': 'LPM',\n                          ' At ': ' at ',\n                          'Bam': 'BAM',\n                          'Pm': 'PM',\n                          'Vscc': 'VSCC',\n                          'Te': 'TE',\n                          ' Or ': ' or ',\n                          'W/': 'w/',\n                          ' And ': ' and '},\n               'Unit': {'PPB': 'Parts per billion',\n                        'PPM': 'Parts per million',\n                        'UG/M3': 'Micrograms/cubic meter'}\n               }\n\n    for attrib in replace:\n        for oldstr, newstr in zip(replace[attrib], replace[attrib].values()):\n            col = param + '_' + attrib\n            df[col] = df[col].astype(str).str.replace(oldstr, newstr)\n\n    return df",
        "sha1": "eef1516eb1a5ec3345c2e7f7652374efad081d96",
        "id": 68182
    },
    {
        "content": "def lissuperset(L1,L2):\n    \"\"\"In : L1 (language : set of strings)\n            L2 (language : set of strings)\n       Out: L1 is superset or equal to L2 (True/False)\n    \"\"\"\n    return L1 >= L2",
        "sha1": "23d7129bb579b241e32635629e56216646664ff8",
        "id": 389644
    },
    {
        "content": "import requests\n\n\ndef get_tv_episode_detail(key, id, season, episode, language=\"en-US\"):\n    \"\"\"\n    function get_tv_episode_detail\n    Get the TV episode details by id.\n    inputs:  key         - TMDB API key.\n             id          - id of the movie\n             season      - Season of the tv series (INT)\n             episode     - Episode number of the series (INT)\n             language    - send in variable 'loc' which is set at runtime - defaults to 'en-US'\n    returns: status_code - HTTP - Status code - 200 is success anything else is considered an error\n             jdata       - A JSON data structure containing information on the tv series.\n    \"\"\"\n    url = f\"https://api.themoviedb.org/3/tv/{id}/season/{season}/episode/{episode}?api_key={key}&language={language}\"\n    resp = requests.get(url)\n    print(f\"StatusCode: {resp.status_code}\")\n    if resp.status_code == 200:\n        jdata = resp.json()\n    else:\n        jdata = None\n    return resp.status_code, jdata",
        "sha1": "ce968c984a454be04d2869ccc61edaef6cc24f93",
        "id": 683743
    },
    {
        "content": "def getSumOf3Numbers(array , target):\n    \"\"\"\n    Write a function that takes in a non-empty array of distinct integers\n    and an integer representing a target sum. The function should find all\n    triplets in the array that sum up to the target sum and return a\n    two-dimensional array of all these triplets. The numbers in each\n    triplet should be ordered in ascending order, and the triplets\n    themselves should be ordered in ascending order with respect to the\n    numbers they hold. If no three numbers sum up to the target sum, the\n    function should return an empty array.\n    \"\"\"\n    #sort the array\n    array.sort()\n    NumSums = []\n\n    for i in range(len(array)-2):\n        right = len(array)-1\n        left = i+1\n\n        while left < right:\n            # print(right  , left)\n            currSum = array[i]+array[left]+array[right]\n\n            if currSum == target:\n                NumSums.append([array[i],array[left],array[right]])\n                left +=1\n                right -=1\n            elif currSum < target:\n                left +=1\n            elif currSum > target:\n                right -=1\n            else:\n                print(\"passs\")\n                pass\n\n    return NumSums",
        "sha1": "36ba0449d0311b1debc95ad9251bc973bb135c0d",
        "id": 357024
    },
    {
        "content": "def _get_rows(x):\n    \"\"\"\n    Return 2D signal rows.\n\n    \"\"\"\n    return [x[i, :] for i in range(x.shape[0])]",
        "sha1": "5de2275515b9b5df7139cb96c0a13f89e4bc9ccf",
        "id": 549655
    },
    {
        "content": "def read_version_file(file_path):\n  \"\"\"Reads a one-line file containing a version number.\"\"\"\n  with open(file_path, 'r') as version_file:\n    return version_file.read().strip()",
        "sha1": "575a222084eadcffdaec9114e22f0a476bcbb701",
        "id": 225070
    },
    {
        "content": "def _transform_points(points, attrs):\n    \"\"\"Transform points to another format for better usability.\n\n    Args:\n        points (dict): Points\n        attrs (list): list of point attributes\n\n    Returns:\n        list: List of Points in the format of [{'label': 'foo', ...}, {'label': 'bar'}]\n    \"\"\"\n    items = zip(*[points[k] for k in attrs])\n    return [\n        {\n            key.lower().replace(' ', '_'): item[idx] for idx, key in enumerate(attrs)\n        }\n        for item in items\n    ]",
        "sha1": "d0700a1668382c84ec2658482e803a8535dd8b98",
        "id": 463544
    },
    {
        "content": "def get_local_hostname() -> str:\n    \"\"\"\n    A function to get the information from /etc/hostname\n    :return: string. Hostname\n    \"\"\"\n    hostname_path = \"/etc/hostname\"\n    with open(hostname_path) as f:\n        try:\n            s = f.readlines()\n            return s[0].strip()\n        except IOError:\n            raise IOError(\"Error opening {}\".format(hostname_path))",
        "sha1": "205c748679f47e1dc331d1e1200ea7ecdc55cbc6",
        "id": 206298
    },
    {
        "content": "import itertools\n\n\ndef marker_cycle(markers=[\"D\", \"s\", \"o\", \"+\", \"*\"]):\n    \"\"\"\n    Cycle through a set of markers.\n\n    Parameters\n    ----------\n    markers : :class:`list`\n        List of markers to provide to matplotlib.\n    \"\"\"\n    return itertools.cycle(markers)",
        "sha1": "2b808fdea282eaa692684c867150173daefe8acc",
        "id": 367496
    },
    {
        "content": "def _are_anagrams(word1, word2):\n    \"\"\"Check if word1 and word2 are anagrams.\"\"\"\n    return sorted(list(word1)) == sorted(list(word2))",
        "sha1": "6e9f9c11406c86a45b4b69046278a6e5c3f1d39a",
        "id": 257318
    },
    {
        "content": "def conformer_eligible_for_topology_detection(conformer):\n  \"\"\"Returns whether this conformer is worthy of topology detection.\n\n  Simple duplicate marking or conformers with unreliable geometries are not\n  generally useful to do topology detection.\n\n  Args:\n    conformer: dataset_pb2.Conformer\n\n  Returns:\n    bool\n  \"\"\"\n  return (conformer.duplicated_by == 0 and\n          conformer.properties.errors.status >= 0 and\n          conformer.properties.errors.status < 512)",
        "sha1": "34ef2f72d4f4430edd7b76e0f2701b19d4e5a1e9",
        "id": 339982
    },
    {
        "content": "def output_modal(func, output: set):\n    \"\"\"Updates output with the result of func, a modal which returns a set.\n     Cheap way to monitor the modal result of a bpy.Operator.\"\"\"\n    def wrap(self, context, event):\n        result = func(self, context, event)\n        output.update(result)\n        return result\n    return wrap",
        "sha1": "a8cd9a2b2d05621ce37f6cff08a3845b83005724",
        "id": 618946
    },
    {
        "content": "import random\nimport string\n\n\ndef latin(minimum=3, maximum=8, separator=' '):\n    \"\"\"\n    Simple function to generate fake (two parts) Latin species names.\n    \n    :param minimum: int, minimum number of letters in each part of the Latin name.\n    :param maximum: int, maximum number of letters in each part of the Latin name.\n    :param separator: str, a delimiter which separated two parts of the Latin name.\n    :return: str, a space separated two parts fake Latin name.\n    \"\"\"\n    \n    genus = ''.join(random.sample(string.ascii_lowercase, random.randint(3, 8)))\n    epithet = ''.join(random.sample(string.ascii_lowercase, random.randint(3, 8)))\n    return separator.join([genus.title(), epithet])",
        "sha1": "ed5f0c7625af90e783015cd6304a2e2dd77d1762",
        "id": 642010
    },
    {
        "content": "from random import randint\n\n\ndef random_pick(board, available_nodes):\n    \"\"\"\n    Offers a node at random.\n    \"\"\"\n\n    index = randint(0, len(available_nodes) - 1)\n    return index",
        "sha1": "746567fe4b66a9b111a24d6ce6823357c8dc290d",
        "id": 514711
    },
    {
        "content": "import requests\n\n\ndef http_request(method, url, headers, data=None):\n    \"\"\"\n    Request util\n    :param method: GET or POST or PUT\n    :param url: url\n    :param headers: headers\n    :param data: optional data (needed for POST)\n    :return: response text\n    \"\"\"\n    response = requests.request(method, url, headers=headers, data=data)\n    if response.status_code not in [200, 201, 204]:\n        http_error_msg = u'%s HTTP request failed: %s for url: %s' % (response.status_code, response.text, url)\n        #print (\"utils.http_request \", http_error_msg)\n        raise requests.exceptions.HTTPError(response.text)\n    return response.text",
        "sha1": "6d0453be79b3ae0f7ed60b5a8759b9295365dd6c",
        "id": 707814
    },
    {
        "content": "def _separate_dirs_files(models):\n    \"\"\"\n    Split an iterable of models into a list of file paths and a list of\n    directory paths.\n    \"\"\"\n    dirs = []\n    files = []\n    for model in models:\n        if model['type'] == 'directory':\n            dirs.append(model['path'])\n        else:\n            files.append(model['path'])\n    return dirs, files",
        "sha1": "4c16305e4ede2b6a49486f30c980d123b7a4dfed",
        "id": 65940
    },
    {
        "content": "import requests\nimport io\n\n\ndef fetch(url_or_path):\n    \"\"\"Fetches a file from an HTTP or HTTPS url, or opens the local file.\"\"\"\n    if str(url_or_path).startswith(\"http://\") or str(url_or_path).startswith(\"https://\"):\n        r = requests.get(url_or_path)\n        r.raise_for_status()\n        fd = io.BytesIO()\n        fd.write(r.content)\n        fd.seek(0)\n        return fd\n    return open(url_or_path, \"rb\")",
        "sha1": "368f8ee43e61e7714c12fa5194283af731d02c68",
        "id": 92193
    },
    {
        "content": "def sentence_case(string):\n    \"\"\"\n    Converts a string to sentence case.\n    From http://stackoverflow.com/questions/39969202/convert-uppercase-string-to-sentence-case-in-python\n    Args:\n        string: The input string\n\n    Returns:\n        The string, now in sentence case\n    \"\"\"\n    return '. '.join(i.capitalize() for i in string.split('. '))",
        "sha1": "da43d5b5325655f1331c7ec780a807541383c6b4",
        "id": 310627
    },
    {
        "content": "def _post_processing(metric_map: dict[str, float]) -> dict[str, float]:\n    \"\"\"\n    unit conversion etc...\n\n    time:\n    taskTime, executorDeserializeTime, executorRunTime, jvmGcTime are milliseconds\n    executorDeserializeCpuTime, executorCpuTime are nanoseconds\n    \"\"\"\n    metric_map[\"executorDeserializeCpuTime\"] = metric_map[\n        \"executorDeserializeCpuTime\"] * 1e-6\n    metric_map[\"executorCpuTime\"] = metric_map[\"executorCpuTime\"] * 1e-6\n    return metric_map",
        "sha1": "23ff301d55e0dc2d2208aca5761059fb8ade3e4e",
        "id": 699831
    },
    {
        "content": "def get_service_name(config):\n    \"\"\"Get the name of the systemctl service used for the agent\"\"\"\n    service_name=config['deploy']['service_name']\n    return f\"{service_name}.service\"",
        "sha1": "7615852f8c5b52a622a3100f6dae797f789ad373",
        "id": 527526
    },
    {
        "content": "def fanout(name, fanout):\n    \"\"\"\n    Generate fanout for a particular name.\n\n    For example:\n\n    .. code-block:: pycon\n\n      >>> fanout('spam', [1, 2])\n      ['s', 'pa']\n      >>> fanout('spameggs', [2, 2, 2])\n      ['sp', 'am', 'eg']\n\n    Parameters:\n        name: The name to fan out. Can be any sliceable sequence; usually a\n            string.\n        fanout: A sequence indicating how many characters per segment should be\n            included.\n\n    Returns:\n        A list of the segments sliced from *name*.\n    \"\"\"\n    ret = []\n    pos = 0\n    for length in fanout:\n        ret.append(name[pos:pos + length])\n        pos += length\n    return ret",
        "sha1": "0f2e9443e69f9784dfc8540adb7e75a2bb42eca8",
        "id": 285073
    },
    {
        "content": "def GetFunctionToExtensionsMap(extensions):\n  \"\"\"Construct map from a function names to extensions which define the\n  function.\n\n  Args:\n    extensions: Map of extension name => functions.\n  Returns:\n    Map of function name => extension names.\n  \"\"\"\n  function_to_extensions = {}\n  for extension, functions in extensions.items():\n    for function in functions:\n      if not function in function_to_extensions:\n        function_to_extensions[function] = set([])\n      function_to_extensions[function].add(extension)\n  return function_to_extensions",
        "sha1": "3415acca26bbf129c468cf36f3a6028ac1478814",
        "id": 274427
    },
    {
        "content": "from typing import Pattern\n\n\ndef get_first_match(pattern: Pattern, string: str) -> str:\n    \"\"\"Return the first matching substring.\n\n    Args:\n        pattern: The pattern to find.\n        string: The string to search.\n\n    Returns:\n        The first occurence of the pattern in the string.\n\n    Raises:\n        ValueError if the string does not match the pattern.\n    \"\"\"\n    match = pattern.match(string)\n    if match is None:\n        raise ValueError(\"String '{}' does not match the pattern '{}'\"\n                         .format(string, pattern.pattern))\n    return match.group(1)",
        "sha1": "56eb1d0ec2a7bf9a5dd8af76d9d423f7f9ab325d",
        "id": 246029
    },
    {
        "content": "def format_tags(tags):\n    \"\"\"Formats tags as comma-separated list of urls to tags index.\"\"\"\n    return \", \".join([f'<a href=\"tags.html#{tag}\">{tag}</a>' for tag in tags])",
        "sha1": "5371cc166650eb51bc94a23fcb4b2e2709c7d9d8",
        "id": 605435
    },
    {
        "content": "def katdal_uvw(uvw, refwave):\n    \"\"\"\n    Converts AIPS UVW coordinates in wavelengths *at the reference frequency*\n    to katdal UVW coordinates in metres. Set :function:`aips_uvw` for\n    further discussion.\n\n    Parameters\n    ----------\n    uvw : np.ndarray\n        AIPS UVW coordinates in wavelengths at the reference frequency\n    refwave : float\n        Reference wavelength in metres\n\n    Returns\n    -------\n    np.ndarray\n        katdal UVW coordinates, in metres\n    \"\"\"\n    return refwave * uvw",
        "sha1": "01cb127c9652e5026e3e18608d71139048ed51a9",
        "id": 209014
    },
    {
        "content": "def binary_search(arr: list[int], val: int) -> int:\n    \"\"\"Binary search modified to operate on both ascending and descending sorted arrays\n\n    Complexity:\n        Time: O(logn) & \u03a9(1)\n        Space: O(1)\n\n    Args:\n        arr: array of numbers sorted in ascending or descending order\n        val: element in arr for which to search\n\n    Returns: index of val being searched if val is in items, else -1\n\n    Examples:\n        >>> binary_search([4, 6, 10],10)\n        2\n        >>> binary_search([1, 2, 3, 4, 5, 6, 7],5)\n        4\n        >>> binary_search([10, 6, 4],10)\n        0\n        >>> binary_search([10, 6, 4],4)\n        2\n        >>> binary_search([],0)\n        -1\n        >>> binary_search([1],0)\n        -1\n\n    \"\"\"\n    ## EDGE CASES ##\n    if not arr:\n        return -1\n\n    \"\"\"ALGORITHM\"\"\"\n    check_left = lambda: val < arr[mid] if is_ascending else val > arr[mid]\n    check_right = lambda: val > arr[mid] if is_ascending else val < arr[mid]\n    ## INITIALIZE VARS\n    start, end = 0, len(arr) - 1\n    is_ascending = arr[start] <= arr[end]\n\n    while start <= end:\n        mid = (start + end) // 2\n        if check_left():\n            end = mid - 1\n        elif check_right():\n            start = mid + 1\n        else:\n            return mid\n    else:\n        return -1",
        "sha1": "56e1a513138bc11e488166c600f41e663e35dcc6",
        "id": 282738
    },
    {
        "content": "def extract_types(elements, annotation):\n    \"\"\"\n    Extracts the types of the identified diagram elements.\n\n    Parameters:\n        elements: A list of diagram elements.\n        annotation: A dictionary of AI2D annotation.\n\n    Returns:\n         A dictionary with element types as keys and identifiers as values.\n    \"\"\"\n    # Check for correct input type\n    assert isinstance(elements, list)\n    assert isinstance(annotation, dict)\n\n    # Define the target categories for various diagram elements\n    targets = ['arrowHeads', 'arrows', 'blobs', 'text', 'containers',\n               'imageConsts']\n\n    # Create a dictionary for holding element types\n    element_types = {}\n\n    # Loop over the diagram elements\n    for e in elements:\n\n        try:\n            # Search for matches in the target categories\n            for t in targets:\n\n                # Get the identifiers for each element category\n                ids = [i for i in annotation[t].keys()]\n\n                # If the element is found among the identifiers, add the\n                # type to the dictionary\n                if e in ids:\n                    element_types[e] = t\n\n        # Skip if the category is not found\n        except KeyError:\n            continue\n\n    # Return the element type dictionary\n    return element_types",
        "sha1": "7c5fd65e4d0779e25e05cd8f2b736f3ac2e2e8b9",
        "id": 376887
    },
    {
        "content": "def aslist(generator):\n    \"\"\"A decorator to convert a generator into a list.\"\"\"\n\n    def wrapper(*args, **kwargs):\n        return list(generator(*args, **kwargs))\n\n    return wrapper",
        "sha1": "dd69c0e2ce3309599e025b948b931ec8400885be",
        "id": 243863
    },
    {
        "content": "def get_function_name(f):\n    \"\"\"\n    Get name for either a function or a functools.partial.\n    \"\"\"\n    try:\n        return f.__name__\n    except AttributeError:\n        pass\n\n    # this works for functools.partial objects\n    try:\n        return f.func.__name__\n    except AttributeError:\n        pass\n\n    return type(f).__name__",
        "sha1": "9a3ed506bedcda96f15f3e65a0a66f13244deb5c",
        "id": 518474
    },
    {
        "content": "def jinja_filter_bbox_overlaps(bounds, geometry_col_name, srid=3857):\n    \"\"\"\n    Check whether the boundary of the geometry intersects with the bounding\n    box.\n\n    Note that the usual meaning of \"overlaps\" in GIS terminology is that the\n    boundaries of the box and polygon intersect, but not the interiors. This\n    means that if the box or polygon is completely within the other, then\n    st_overlaps will be false.\n\n    However, that's not what we want. This is used for boundary testing, and\n    while we don't want to pull out a whole country boundary if the bounding\n    box is fully within it, we _do_ want to if the country boundary is within\n    the bounding box.\n\n    Therefore, this test has an extra \"or st_contains\" test to also pull in any\n    boundaries which are completely within the bounding box.\n    \"\"\"\n\n    min_point = 'ST_MakePoint(%.12f, %.12f)' % (bounds[0], bounds[1])\n    max_point = 'ST_MakePoint(%.12f, %.12f)' % (bounds[2], bounds[3])\n    bbox_no_srid = 'ST_MakeBox2D(%s, %s)' % (min_point, max_point)\n    bbox = 'ST_SetSrid(%s, %d)' % (bbox_no_srid, srid)\n    bbox_filter = \\\n        '((%(col)s && %(bbox)s) AND (' \\\n        '  st_overlaps(%(col)s, %(bbox)s) OR' \\\n        '  st_contains(%(bbox)s, %(col)s)' \\\n        '))' \\\n        % dict(col=geometry_col_name, bbox=bbox)\n    return bbox_filter",
        "sha1": "2e509e0ba2bf75f3d00df837b884aabdd28699a0",
        "id": 86777
    },
    {
        "content": "import re\n\n\ndef camelcase_to_underscore(camelcase_str):\n    \"\"\"Convert the given string from ``camelCase`` to ``underscore_case``.\n\n    Underscores at the beginning and end of the string are preserved. All capital letters are cast to lower case.\n\n    Parameters\n    ----------\n    camelcase_str : str\n        String in camelCase to convert to underscore style.\n\n    Returns\n    -------\n    underscore_str : str\n        String in underscore style.\n\n    Examples\n    --------\n    >>> camelcase_to_underscore('myVariable')\n    'my_variable'\n    >>> camelcase_to_underscore('__my_Variable_')\n    '__my__variable_'\n\n    \"\"\"\n    underscore_str = re.sub(r'([A-Z])', '_\\g<1>', camelcase_str)\n    return underscore_str.lower()",
        "sha1": "b8ad6b9b017b334aadcdf9e020bc4559bffe6bfd",
        "id": 381722
    },
    {
        "content": "def yaml_transformer(value: str) -> str:\n    \"\"\"A YAML transformer for Ruamel that places a blank line between each top level section.\"\"\"\n    lines = value.splitlines()\n    output = []\n\n    for line_no, line in enumerate(lines):\n        if line_no and not line.startswith(' '):\n            output.append('')\n\n        output.append(line)\n\n    return '\\n'.join(output) + '\\n'",
        "sha1": "b8aa628f78607fe837083421e068b801f4689648",
        "id": 647276
    },
    {
        "content": "def count_elements(reference, to_count):\n    \"\"\" Count how many elements of a list are in the reference list.\n\n    :param reference: (list)\n    :param to_count: (list)\n    :returns: (int)\n\n    \"\"\"\n    n = 0\n    for x in to_count:\n        if x in reference:\n            n += 1\n    return n",
        "sha1": "ecc9333306fde0748776e7dd3cfac6ef5fb519db",
        "id": 379767
    },
    {
        "content": "def calculate_answer1_rating(data: list[str], context: str) -> int:\n    \"\"\"Calculate the gamma/epsilon rating by taking the most common bit in each position and converting to decimal.\"\"\"\n    most_common_bit_binary_number = ''\n\n    transposed_lists = list(map(list, zip(*data)))\n\n    for bit_group in transposed_lists:\n        ones = bit_group.count('1')\n        zeros = bit_group.count('0')\n\n        if ones >= zeros and context == 'gamma':\n            most_common_bit_binary_number += '1'\n        elif ones < zeros and context == 'epsilon':\n            most_common_bit_binary_number += '1'\n        else:\n            most_common_bit_binary_number += '0'\n\n    binary_to_decimal = int(most_common_bit_binary_number, 2)\n\n    return binary_to_decimal",
        "sha1": "66bbbaeafe164a390c8361adbe8157814b42038e",
        "id": 333817
    },
    {
        "content": "import io\n\n\ndef read_file(path):\n    \"\"\"Returns all the lines of a file at path as a List\"\"\"\n    file_lines = []\n    with io.open(path, mode=\"rt\", encoding=\"utf-8\") as the_file:\n        file_lines = the_file.readlines()\n    return file_lines",
        "sha1": "8e220e0b90ded168a1d1d8d37b7451b7796b1ed5",
        "id": 699612
    },
    {
        "content": "def Numbad(Index, Offset: int, Tr_Length: int):\n    \"\"\" Returns the number of Label instances that finish after the end of the Epoch (Prevents Index errors propagating)\n\n    Parameters:\n    ----------\n    Index: list\n        A list of Lists containing the Start Times of only One type of Label in each Clipping.\n        Also Note that the Starts Argument must be converted to the 1 KHz Sampling Frequency\n\n    Offset: int\n\n    Tr_Length: int\n\n    return:\n    -------\n    count: int\n        number of bad indexes\n    \"\"\"\n    Count = 0\n    for i in range(len(Index)):\n        Count = Count + len([x for x in range(len(Index[i])) if Index[i][x] < (Offset + Tr_Length)])\n    return Count",
        "sha1": "5a5c5a5b6af8fadb25d689ff19ad5e2827fc90c0",
        "id": 256894
    },
    {
        "content": "import collections\n\n\ndef get_vocab_from_dict(lines, tokenizer):\n    \"\"\"Gets a vocab from a dict of lines to their count.\n\n    Returns:\n        vocab (list): (word as a tuple of int, count), sorted descending by count\n    \"\"\"\n    vocab = collections.Counter()\n\n    for line, count in lines.items():\n        words = tokenizer.tokenize(line)\n        for word in words:\n            vocab[word] += count\n\n    vocab = list(vocab.items())\n    vocab.sort(key=lambda k: k[1], reverse=True)\n    return vocab",
        "sha1": "354ce5d3b3dde4d996138d227fff7d47192a7f86",
        "id": 348238
    },
    {
        "content": "from datetime import datetime\n\n\ndef prepend_timestamp(line):\n    \"\"\"Add ISO 8601 timestamp to line\"\"\"\n    timestamp = datetime.now().replace(microsecond=0).isoformat()\n    return \"{} {}\".format(timestamp, line)",
        "sha1": "926573337579084b67593204b5bb99171ab96b7c",
        "id": 665403
    },
    {
        "content": "def create_first_n_1_bits_mask(n, k):\n    \"\"\" Return a binary mask of first n bits of 1, k bits of 0s\"\"\"\n    if n < 0 or k < 0:\n        raise ValueError(\"n and k cannot be negative number\")\n\n    if n == 0:\n        return 0\n\n    mask = (2 << n) - 1\n    return mask << k",
        "sha1": "5a9b637a8973f004da2330c8ebb06ea63fd542c3",
        "id": 699809
    },
    {
        "content": "def square(number):\n  \"\"\"\n  this function returns a square of a number\n  \"\"\"\n  result = number ** 2\n  return result",
        "sha1": "10a3dc80d5ab0f709bd393c8956bd67be3ec8746",
        "id": 259861
    },
    {
        "content": "def checkConditions(cond, sol):\n    \"\"\"\n    Check whether the given conditions hold for the given values of variables.\n    \"\"\"\n    if isinstance(cond, list):\n        return all(checkConditions(cnd, sol) for cnd in cond)\n    elif isinstance(cond, tuple):\n        return any(checkConditions(cnd, sol) for cnd in cond)\n    else:\n        return cond.subs(sol)",
        "sha1": "8ed924812a6783b6e627a0329af50954b4487afe",
        "id": 244858
    },
    {
        "content": "def render_definition_list(contents):\n    \"\"\" Render a definition list with the given items \"\"\"\n    return '{}\\n'.format('\\n'.join(contents))",
        "sha1": "ebfda302e8ab153eb762e9513ebce6d2da4a1714",
        "id": 154298
    },
    {
        "content": "def field2nullable(field, **kwargs):\n    \"\"\"Return the dictionary of swagger field attributes for a nullable field.\n\n    :param Field field: A marshmallow field.\n    :rtype: dict\n    \"\"\"\n    attributes = {}\n    if field.allow_none:\n        omv = kwargs['openapi_major_version']\n        attributes['x-nullable' if omv < 3 else 'nullable'] = True\n    return attributes",
        "sha1": "dd5d4cd63aeede4ef9356baa9fe9a48bd5f87841",
        "id": 5490
    },
    {
        "content": "def convert_velocity(val, old_scale=\"km/h\", new_scale=\"m/s\"):\n    \"\"\"\n    Convert from a velocity scale to another one among km/h, and m/s.\n\n    Parameters\n    ----------\n    val: float or int\n        Value of the velocity to be converted expressed in the original scale.\n\n    old_scale: str\n        Original scale from which the angle value will be converted.\n        Supported scales are km/h or m/s.\n\n    new_scale: str\n        New scale from which the angle value will be converted.\n         Supported scales are km/h or m/s.\n\n    Raises\n    -------\n    NotImplementedError if either of the scales are not one of the requested\n    ones.\n\n    Returns\n    -------\n    res: float\n        Value of the converted velocity expressed in the new scale.\n    \"\"\"\n    # Convert from 'old_scale' to m/s\n    if old_scale == 'm/s':\n        temp = val\n    elif old_scale == 'km/h':\n        temp = val * 3.6\n    else:\n        raise AttributeError(\n            f'{old_scale} is unsupported. km/h and m/s are supported')\n    # and from m/s to 'new_scale'\n    if new_scale == 'm/s':\n        result = temp\n    elif new_scale == 'km/h':\n        result = temp / 3.6\n    else:\n        raise AttributeError(\n            f'{new_scale} is unsupported. km/h and m/s are supported')\n    return result",
        "sha1": "6a38ac70090f14a4a0d3989ce5a1a884067afd77",
        "id": 674757
    },
    {
        "content": "def ft_to_toplevel(fasttext_lbl):\n    \"\"\"Example: '__label__STEM.Technology' -> 'STEM'\"\"\"\n    return fasttext_lbl.replace('__label__','').split('.')[0]",
        "sha1": "95b46460553b11c22339ef8ede6c2a2e761bc894",
        "id": 96994
    },
    {
        "content": "def merge(d1, d2):\n    \"\"\"Merges d2 into d1 without overwriting keys.\"\"\"\n    left = d1.copy()\n    right = d2.copy()\n\n    for key in right.keys():\n        if key in left:\n            left_is_dict = isinstance(left[key], dict)\n            right_is_dict = isinstance(right[key], dict)\n\n            if left_is_dict and right_is_dict: \n                left[key] = merge(left[key], right[key])\n            elif right_is_dict:\n                left[key] = merge({'': left[key]}, right[key])\n            elif left_is_dict:\n                left[key] = merge(left[key], {'': right[key]})\n            else:\n                left[key] = right[key]\n        else:\n            left[key] = right[key]\n\n    return left",
        "sha1": "568254522547b47687cdfaab54bfc21e0b535f93",
        "id": 665456
    },
    {
        "content": "from typing import Any\n\n\ndef find_idxs_of_element_in_list(lst: list, element: Any) -> list:\n    \"\"\"This function returns the indexes of the input list that have value == element\n    Args:\n        lst (list): input list where we search for indexes\n        element (Any): element of which we want to find the indexes\n    Returns:\n        idxs (list): list of indexes corresponding to element\n    \"\"\"\n    idxs = [i for i, x in enumerate(lst) if x == element]\n    \n    return idxs",
        "sha1": "3e9c17daf8e7f5c5da7908d1f69f75e18f982b6b",
        "id": 184258
    },
    {
        "content": "from pathlib import Path\nfrom typing import Sequence\nfrom typing import Mapping\nfrom typing import Any\nimport json\n\n\ndef read_in_schemas(schema_path: Path) -> Sequence[Mapping[str, Any]]:\n    \"\"\"Read in schema information from *schema_path*.\n\n    Arguments:\n        schema_path: Path to the schemas as SDF.\n\n    Returns:\n        List of schemas.\n    \"\"\"\n    with open(schema_path) as handle:\n        response: Sequence[Mapping[str, Any]] = json.load(handle)[\"schemas\"]\n        return response",
        "sha1": "9c7f6a7e31ab2e738f43d8dbe6b906688ea68221",
        "id": 231341
    },
    {
        "content": "import torch\n\n\ndef conv1x1(in_planes, out_planes, stride=1):\n    \"\"\"1x1 convolution\"\"\"\n    return torch.nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)",
        "sha1": "2649d1c65e329da472ff79e893b356b0c8f4248f",
        "id": 619946
    },
    {
        "content": "def calculateIntersection(a0, a1, b0, b1):\n    \"\"\"\n    Calculate intersection between interval.\n    Reference: https://stackoverflow.com/a/48537479\n    \"\"\"\n    if a0 >= b0 and a1 <= b1: # Contained\n        intersection = a1 - a0\n    elif a0 < b0 and a1 > b1: # Contains\n        intersection = b1 - b0\n    elif a0 < b0 and a1 > b0: # Intersects right\n        intersection = a1 - b0\n    elif a1 > b1 and a0 < b1: # Intersects left\n        intersection = b1 - a0\n    else: # No intersection (either side)\n        intersection = 0\n\n    return intersection",
        "sha1": "f4f85123be3613ff041d8746ac71107fdd32c893",
        "id": 436555
    },
    {
        "content": "def find_all(text, pat):\n    \"\"\"Returns list of all overlapping occurrences of a pattern in a text.\n    \n    Each item in the (sorted) list is the index of one of the matches.\n    \"\"\"\n    result = []\n    last = 0\n    try:\n        while 1:\n            curr = text.index(pat, last)\n            result.append(curr)\n            last = curr + 1\n    except ValueError:  #raised when no more matches\n        return result",
        "sha1": "cfdb8f0f53a0d016a433c2a9d180595883ad0ca8",
        "id": 383759
    },
    {
        "content": "def move1(state,b1,dest):\n    \"\"\"\n    Generate subtasks to get b1 and put it at dest.\n    \"\"\"\n    return [('get', b1), ('put', b1,dest)]",
        "sha1": "c84a2d8246017fa94a73dd2e3408f7f05cf3573d",
        "id": 696739
    },
    {
        "content": "def get_lookup(field_name, versionable):\n    \"\"\"Returns a filtering lookup.\n\n    If passed a versionable (which is a VersionableItem for the content object),\n    it makes it so that lookup uses <grouper>__<content> form.\n\n    :param field_name:\n    :param versionable: VersionableItem or None\n    \"\"\"\n    if versionable:\n        content_name = versionable.grouper_field.remote_field.get_accessor_name()\n        return \"{}__{}\".format(field_name, content_name)\n    return field_name",
        "sha1": "719f7c14acd4705459f60d41b18f03774289b2b7",
        "id": 559895
    },
    {
        "content": "def getReward(state, marker):\n    \"\"\"Given the state (a board) determine the reward.\"\"\"\n\n    if ( state.gameWon() ):\n        return marker * 10\n\n    else:\n        return 0",
        "sha1": "b4532c64ae1229c15c4c4c391b07d4fb1fd0eba4",
        "id": 367005
    },
    {
        "content": "def has_magnet(self):\n    \"\"\"Return if any of the Holes have magnets\n\n    Parameters\n    ----------\n    self : LamHole\n        A LamHole object\n\n    Returns\n    -------\n    has_magnet : bool\n        True if any of the Holes have magnets\n    \"\"\"\n\n    has_mag = [hole.has_magnet() for hole in self.hole]\n    return any(has_mag)",
        "sha1": "16b52b586b4057d33bb5e636fb41f86403d21852",
        "id": 96182
    },
    {
        "content": "def city_country(city, country):\n    \"\"\"Return a combined string to show city and country\"\"\"\n    return f\" {city.title()}, {country.title()}\"",
        "sha1": "5f7a8a20ac5e40b789fb891afcda787b3e6ec2ee",
        "id": 74505
    },
    {
        "content": "def reduceWith(reducer, seed, iterable):\n    \"\"\"\n    reduceWith takes reducer as first argument, computes a reduction over iterable.\n    Think foldl from Haskell.\n    reducer is (b -> a -> b)\n    Seed is b\n    iterable is [a]\n    reduceWith is (b -> a -> b) -> b -> [a] -> b\n    \"\"\"\n    accumulation = seed\n    for value in iterable:\n        accumulation = reducer(accumulation, value)\n    return accumulation",
        "sha1": "edc217ce3a53521e872ad04b461d6ecad9f549d2",
        "id": 663726
    },
    {
        "content": "def filter_non_src_files(files):\n    \"\"\"\n    Filters out all files which don't contain prefix Inst\n    i.e. all files that aren't pymod dicts for TestComponent\n    \"\"\"\n    src_files = []\n    for file in files:\n        if file[:4] == \"Inst\":\n            src_files.append(file)\n\n    return src_files",
        "sha1": "d94310dc144c107bc99e17e964db9c26663cf3de",
        "id": 104366
    },
    {
        "content": "def expand_curie_to_uri(curie, context_info):\n    \"\"\"Expand curie to uri based on the context given\n\n    :arg str curie: curie to be expanded (e.g. bts:BiologicalEntity)\n    :arg dict context_info: jsonld context specifying prefix-uri relation (e.g. {\"bts\": \"http://schema.biothings.io/\"})\n    \"\"\"\n    # as suggested in SchemaOrg standard file, these prefixes don't expand\n    PREFIXES_NOT_EXPAND = [\"rdf\", \"rdfs\", \"xsd\"]\n    if not curie:\n        return curie\n    if type(curie) == int:\n        curie = str(curie)\n    # determine if a value is curie\n    if len(curie.split(':')) == 2:\n        prefix, value = curie.split(\":\")\n        if prefix in context_info and prefix not in PREFIXES_NOT_EXPAND:\n            return context_info[prefix] + value\n    # if the input is not curie, return the input unmodified\n        else:\n            return curie\n    else:\n        return curie",
        "sha1": "9bf1e64cad58ac22d88171f32e8310658d93fcdf",
        "id": 397495
    },
    {
        "content": "def GetCanonicalLineItem(line_item):\n    \"\"\"Simplify product and sku names.\"\"\"\n    return line_item.replace('com.google.cloud/services/', '')",
        "sha1": "9efa5961ab32bbf391652a6c79d8577197842907",
        "id": 88206
    },
    {
        "content": "def get_all_tables(soup):\n    \"\"\"Extracts and returns all tables in a soup object\"\"\"\n    return soup.find_all(\"table\")",
        "sha1": "1795d76c38313a409e7173d57a1756e74fe33dd0",
        "id": 298795
    },
    {
        "content": "def add_to_dict_if_not_none(target_dict, target_key, value):\n    \"\"\"Adds the given value to the give dict as the given key\n       only if the given value is not None.\n    \"\"\"\n\n    if value is None:\n        return False\n\n    target_dict[target_key] = value\n    return True",
        "sha1": "c3b205e775b55b86e6ea994c10df37f56e302da9",
        "id": 442573
    },
    {
        "content": "def UnbiasPmf(pmf, label=''):\n    \"\"\"Returns the Pmf with oversampling proportional to 1/value.\n\n    Args:\n      pmf: Pmf object.\n      label: string label for the new Pmf.\n\n     Returns:\n       Pmf object\n    \"\"\"\n    new_pmf = pmf.Copy(label=label)\n\n    for x, p in pmf.Items():\n        new_pmf.Mult(x, 1.0/x)\n        \n    new_pmf.Normalize()\n    return new_pmf",
        "sha1": "1146d952bbac0ef3031e3259d98e0f343103598e",
        "id": 701023
    },
    {
        "content": "import re\n\n\ndef preprocess(text, chars='', remove_all_special=True, expand=True, split_numbers=True):\n    \"\"\"\n    function that removes whitespaces, converts to lowercase, etc.\n    :param split_numbers: split 45787 to 4 5 7 8 7\n    :param remove_all_special: remove all characters but  alpha-numerical, spaces, hyphens, quotes\n    :param expand: expand 'll, 'm and similar expressions to reduce the number of different tokens\n    :param text: text input\n    :param chars: chars to be removed\n    :return: cleaned up text\n    \"\"\"\n\n    # fix bad newlines (replace with spaces), unify quotes\n    text = text.replace('\\\\n', ' ').replace('\u2018', '\\'').replace('\u2019', '\\'').replace('\u201d', '\"').replace('\u201c', '\"')\n\n    # optionally remove all given characters\n    for c in chars:\n        if c in text:\n            text = text.replace(c, '')\n\n    # convert to lowercase\n    text = text.lower()\n\n    # remove all characters except alphanum, spaces and - ' \"\n    if remove_all_special:\n        text = re.sub('[^ \\-\\sA-Za-z0-9\"\\']+', ' ', text)\n\n    # split numbers into digits to avoid infinite vocabulary size if random numbers are present:\n    if split_numbers:\n        text = re.sub('[0-9]', ' \\g<0> ', text)\n\n    # expand unambiguous 'm, 't, 're, ... expressions\n    if expand:\n        text = text. \\\n            replace('\\'m ', ' am '). \\\n            replace('\\'re ', ' are '). \\\n            replace('won\\'t', 'will not'). \\\n            replace('n\\'t', ' not'). \\\n            replace('\\'ll ', ' will '). \\\n            replace('\\'ve ', ' have '). \\\n            replace('\\'s', ' \\'s')\n\n    return text",
        "sha1": "b6c076dde6f9b7d0986eb3a4c3b29c54d2f5a96f",
        "id": 416786
    },
    {
        "content": "def get_pairs_from_correlations(correlating_features):\n    \"\"\" Get a list of pairs of correlating features.\n\n    :param correlating_features: list of indices from the correlation matrix\n    :return: list of tuples\n    \"\"\"\n    pairs = []\n    for i in range(len(correlating_features[0])):\n        pairs.append((correlating_features[0][i], correlating_features[1][i]))\n    del pairs[::2]\n\n    return pairs",
        "sha1": "912485d371f09c6c3805b39524908730a7966fe3",
        "id": 159951
    },
    {
        "content": "def getByName( dblist, retname, keystr ):\n  \"\"\" Gets value of specified database list key\n\n  Parameters:\n    * dblist (dict): survey database list, check odpy.getDBList for docs\n    * retname (str): key to return from dblist\n    * keystr (str): value to return from retname\n\n  Returns:\n    * str: database object value\n\n  Example:\n  \n  >>> import odpy.dbman as dbman\n  >>> dbman.getByName(dblist, 'F03-4', 'IDs')\n      '100050.4'\n\n  \"\"\"\n\n  curentryidx = dblist['Names'].index( retname )\n  return dblist[keystr][curentryidx]",
        "sha1": "b8aa638d10dd28d4c2c30a16334399ad48aa12c3",
        "id": 502430
    },
    {
        "content": "import math\n\n\ndef get_width(length, full_width):\n    \"\"\"\n    Returns the number of bits used by Minecraft to represent indices into a\n    list of the given length.\n    \"\"\"\n\n    width = int(math.ceil(math.log(length, 2)))\n    if width < 4:\n        return 4\n    elif width > 8:\n        return full_width\n    else:\n        return width",
        "sha1": "f46891ee00db894ebcaf97a58960516e239a8259",
        "id": 110577
    },
    {
        "content": "def _uniquify_fetches(fetch_mappers):\n  \"\"\"Uniquifies fetches from a list of fetch_mappers.\n\n  This is a utility function used by _ListFetchMapper and _DictFetchMapper.  It\n  gathers all the unique fetches from a list of mappers and builds a list\n  containing all of them but without duplicates (unique_fetches).\n\n  It also returns a 2-D list of integers (values_indices) indicating at which\n  index in unique_fetches the fetches of the mappers are located.\n\n  This list is as follows:\n    values_indices[mapper_index][mapper_fetch_index] = unique_fetches_index\n\n  Args:\n    fetch_mappers: list of fetch mappers.\n\n  Returns:\n    A list of fetches.\n    A 2-D list of integers.\n  \"\"\"\n  unique_fetches = []\n  value_indices = []\n  seen_fetches = {}\n  for m in fetch_mappers:\n    m_value_indices = []\n    for f in m.unique_fetches():\n      j = seen_fetches.get(f)\n      if j is None:\n        j = len(seen_fetches)\n        seen_fetches[f] = j\n        unique_fetches.append(f)\n      m_value_indices.append(j)\n    value_indices.append(m_value_indices)\n  return unique_fetches, value_indices",
        "sha1": "e5cf26c2856d04e51cf9c5c87e0d0047a887290d",
        "id": 90475
    },
    {
        "content": "import typing\n\n\ndef unnest(dct: typing.Dict[str, typing.Any], key: str):\n    \"\"\"\n    Function to revert the way Marshmallow sets dictionaries.\n    When Marshmallow sets dictionaries, it assumes keys with a '.'\n    are for paths.\n\n    ::\n\n       >>> d = {'ter': {' mi': {' nal': 'val'}}}\n       >>> k, v = unnest(d, 'ter')\n       >>> k\n       'ter. mi. nal'\n       >>> v\n       'val'\n\n    \"\"\"\n    if isinstance(dct[key], dict):\n        new_dct = {}\n        int_key = [*dct[key]][0]\n        new_key = f'{key}.{int_key}'\n        new_dct[new_key] = dct[key][int_key]\n        return unnest(new_dct, new_key)\n    else:\n        return key, dct[key]",
        "sha1": "d3da8f185dafbf76d54b4262a862f339caccba96",
        "id": 496067
    },
    {
        "content": "def get_qualifier(module):\n    \"\"\"\n    Returns the function qualifier as a version or alias or None.\n\n    :param module:\n    :return:\n    \"\"\"\n\n    qualifier = None\n    if module.params['version'] > 0:\n        qualifier = str(module.params['version'])\n    elif module.params['alias']:\n        qualifier = str(module.params['alias'])\n\n    return qualifier",
        "sha1": "2e954db79e46596e2994e40465b44c6b3a12322e",
        "id": 185816
    },
    {
        "content": "def is_missing_atom_map(molecule):\n    \"\"\"\n    Checks if any atom in molecule is missing a map index. If even only one atom is missing a map index will return True\n    Parameters\n    ----------\n    molecule: oechem.OEMOl\n\n    Returns\n    -------\n    bool\n\n    \"\"\"\n    MISSING_ATOM_MAP = False\n    for atom in molecule.GetAtoms():\n            if atom.GetMapIdx() == 0:\n                MISSING_ATOM_MAP = True\n                return MISSING_ATOM_MAP\n    return MISSING_ATOM_MAP",
        "sha1": "617b4fb51c777daf89d4ce0c8b6ba71752306dad",
        "id": 635414
    },
    {
        "content": "def or_(*args):\n    \"\"\"Build OR filters.\"\"\"\n\n    return {\n                'operator': 'or',\n                'criteria': list(args)\n           }",
        "sha1": "dbda5c5eb6d2e5b50d3c95c48e83b2c09219e809",
        "id": 177508
    },
    {
        "content": "def gettext(element):\n    \"\"\"Returns the atcual content of a leaf element as a text string.\"\"\"\n    return '.'.join([node.data for node in element.childNodes\n                    if node.nodeType == node.TEXT_NODE])",
        "sha1": "9124ff6f478c0d297d74c985124a06f895dc2874",
        "id": 183856
    },
    {
        "content": "def vaf_to_ar(vaf):\n    \"\"\"\n    Convert VAF to AR.\n\n    VAF (variant allele frequency) = V-AF\n    AR (allele ratio) = V-AF / WT-AF\n    V-AF + WT-AF = 100 (%)\n\n    Note:\n        if VAF == 100:\n            AR = -1\n            (instead of 100 / 0)\n\n    Args:\n        vaf (dc.Decimal): VAF to convert.\n\n    Returns:\n        AR (dc.Decimal)\n    \"\"\"\n    if vaf == 100:\n        return -1\n    return vaf/(100 - vaf)",
        "sha1": "bf1847fd9d832d278f059e036d02cb198fc076f3",
        "id": 362622
    },
    {
        "content": "def _is_int(num_str):\n    \"\"\"\n    Returns whether or not a given string is an integer\n\n    :param num_str: The string to test\n\n    :return: Whether or not the string is an integer\n    \"\"\"\n    try:\n        int(num_str)\n        return True\n    except ValueError:\n        return False",
        "sha1": "656c86f9cc666ae88fd49dd42831b7de0bbb85c9",
        "id": 581132
    },
    {
        "content": "def get_uncovered_character(number_of_mines : int) -> str:\n    \"\"\"Gets the correct character for a number tile.\n\n    Args:\n        number_of_mines (int): The number of mines surrounding the tile.\n\n    Returns:\n        str: The character representation of the tile.\n    \"\"\"\n    if number_of_mines == 0:\n        return \".\"\n    return str(number_of_mines)",
        "sha1": "937cde550b1960c4490af6ed18fa6c4645aa0b36",
        "id": 156028
    },
    {
        "content": "def format_test_case(test_case):\n  \"\"\"Format test case from `-[TestClass TestMethod]` to `TestClass_TestMethod`.\n\n  Args:\n    test_case: (str) Test case id in format `-[TestClass TestMethod]` or\n               `[TestClass/TestMethod]`\n\n  Returns:\n    Test case id in format TestClass/TestMethod.\n  \"\"\"\n  return test_case.replace('[', '').replace(']', '').replace(\n      '-', '').replace(' ', '/')",
        "sha1": "7cdc157239a707450fcf74c4ba01c3eb79470f42",
        "id": 299273
    },
    {
        "content": "def is_c2d_topic(topic, device_id):\n    \"\"\"\n    Topics for c2d message are of the following format:\n    devices/<deviceId>/messages/devicebound\n    :param topic: The topic string\n    \"\"\"\n    if \"devices/{}/messages/devicebound\".format(device_id) in topic:\n        return True\n    return False",
        "sha1": "b55d686d7d50cdb2717e506651655dc7174e386e",
        "id": 241470
    },
    {
        "content": "def strip_leading_dashdash(name):\n    \"\"\"Remove leading ``'--'`` if present.\"\"\"\n    if name.startswith('--'):\n        return name[2:]\n    else:\n        return name",
        "sha1": "97f86a839d088f7bd96d2836f746819bf2003697",
        "id": 607635
    },
    {
        "content": "def truncate_column(data_column, truncation_point):\n    \"\"\"\n    Abstraction of numpy slicing operation for 1D array truncation.\n    :param data_column: 1D np.array\n    :param truncation_point: int of truncation index to test\n    :return: np.array\n    \"\"\"\n    assert (len(data_column.shape) == 1)  # Assert data_column is a 1D numpy array\n    return data_column[-1 * truncation_point:]",
        "sha1": "bc444517953228d003fc18851f5c22b2b70f6e55",
        "id": 46984
    },
    {
        "content": "def define_separator(filename):\n    \"\"\" Select separator for data values based on filename extension.\n    Returns separator.\n    \"\"\"\n    if filename.endswith(\".tsv\"):\n        separator = \"\\t\"\n    else:\n        separator = \",\"\n    return separator",
        "sha1": "ac74cfdafe48d437b81ddd8a1a130f14586f250f",
        "id": 592340
    },
    {
        "content": "def tupleToString(vector):\n    \"\"\"\n    Transform a tuple of numbers into a string of format\n    '[n](x_1, x_2, ..., x_n)'\n    \"\"\"\n    string = '[%d](' % len(vector)\n    for x in vector[:-1]:\n        string += '%f,' % x\n    string += '%f)' % vector[-1]\n    return string",
        "sha1": "f229728c26a49d97ac09d2ef03d922f2b8e1c3b7",
        "id": 462844
    },
    {
        "content": "def _clang_compilation_mode_flags(objc_fragment):\n  \"\"\"Returns additional clang flags for the current compilation mode.\"\"\"\n\n  # In general, every compilation mode flag from native objc_ rules should be\n  # passed, but -g seems to break Clang module compilation. Since this flag does\n  # not make much sense for module compilation and only touches headers,\n  # it's ok to omit.\n  native_clang_flags = objc_fragment.copts_for_current_compilation_mode\n\n  return [x for x in native_clang_flags if x != \"-g\"]",
        "sha1": "1136625189c8e4c188abb6f25daeba9a3f5ae480",
        "id": 355299
    },
    {
        "content": "import hashlib\nimport json\nimport base64\n\n\ndef hash_dict(obj):\n    \"\"\"\n    Hashes the json representation of obj using sha-256 to have almost certain uniqueness.\n    @param obj: dict to be hashed\n    @return: sha256 b64 encoded hash of the dict\n    \"\"\"\n    m = hashlib.sha256()\n    s = json.dumps(obj, sort_keys=True)\n    m.update(bytes(s, 'utf-8'))\n    return base64.b64encode(m.digest()).decode('utf-8')",
        "sha1": "45628ec94d01e0aac6696b8d65627968754f7f4b",
        "id": 47332
    },
    {
        "content": "def get_degree_distribution(df):\n    \"\"\"given a dataframe with a 'deg' column, find the counts of each degree value\n\n    :param df: a dataframe with a 'deg' column representing degree of nodes\n    :type df: pandas DataFrame\n    :return: a dictionary showing counts of each degree\n    :rtype: dict\n    \"\"\"\n    return df.groupby('deg').count().to_dict()['orf_name']",
        "sha1": "816c16d219b14c6211082221037fd02b0c75e1f6",
        "id": 446022
    },
    {
        "content": "def get_skip_comments(events, skip_users=None):\n    \"\"\"\n    Determine comment ids that should be ignored, either because of\n        deletion or because the user should be skipped.\n\n    Args:\n        events: a list of (event_type str, event_body dict, timestamp).\n    Returns:\n        comment_ids: a set of comment ids that were deleted or made by\n            users that should be skipped.\n    \"\"\"\n    skip_users = skip_users or []\n    skip_comments = set()\n    for event, body, _timestamp in events:\n        action = body.get('action')\n        if event in ('issue_comment', 'pull_request_review_comment'):\n            comment_id = body['comment']['id']\n            if action == 'deleted' or body['sender']['login'] in skip_users:\n                skip_comments.add(comment_id)\n    return skip_comments",
        "sha1": "30663624714104bc7b9aa0fd4da45f537b06420f",
        "id": 18086
    },
    {
        "content": "def get_file_extension(filepath):\n    \"\"\"Return full file extension from filepath\"\"\"\n    filename = filepath.split('/')[-1]\n\n    return filename[filename.index('.'):]",
        "sha1": "33bf9cfb797612e84d130750d97c1ced0233b55c",
        "id": 92083
    },
    {
        "content": "def code(string: str) -> str:\n    \"\"\"\n    Add code md style.\n\n    :param string: The string to process.\n    :type string:str\n    :return: Formatted String.\n    :rtype: str\n    \"\"\"\n    return f\"`{string}`\"",
        "sha1": "4926d6981a589dca3926205b42d3b43aa99da962",
        "id": 597869
    },
    {
        "content": "def get_rowIndex(uid):\n    \"\"\"\n    return row index as integer\n    \"\"\" \n    return uid.rowIndex",
        "sha1": "77c10637b7bcb9736beaccf38148bcdc9f0b61df",
        "id": 53984
    },
    {
        "content": "def has_el(el, el_name):\n    \"\"\"Return True if an element with a given name exists in the branch rooted at el\"\"\"\n    return True if el.xpath(f'.//{el_name}') else False",
        "sha1": "8deba680775ba0d0b49c99c2acecbd2833da793d",
        "id": 692026
    },
    {
        "content": "import re\n\n\ndef camel_case(words):\n    \"\"\"Convert a noun phrase to a CamelCase identifier.\"\"\"\n    result = ''\n    for word in re.split(r'(?:\\W|_)+', words):\n        if word:\n            if word[:1].islower():\n                word = word.capitalize()\n            result += word\n    return result",
        "sha1": "4707e5c48f803c78ebd08a2b3099bc5b25cab0df",
        "id": 655138
    },
    {
        "content": "import traceback\n\n\ndef format_traceback_string(exception):\n    \"\"\"Format exception traceback as a single string.\n\n    Args:\n        exception: Exception object.\n\n    Returns:\n        Full exception traceback as a string.\n    \"\"\"\n    return '\\n'.join(\n        traceback.TracebackException.from_exception(exception).format()\n    )",
        "sha1": "debdf53966b26b6562671bf48d283a3bf10d85d5",
        "id": 5025
    },
    {
        "content": "def find_unique_value_error(exc_detail):\n    \"\"\"Find unique value error in exception details.\"\"\"\n    for field, errors in exc_detail.items():  # noqa: B007\n        for error in errors:\n            if error.code == 'unique':\n                return error\n\n    return None",
        "sha1": "aa25202b311e03e19c842174bc641ad1236db920",
        "id": 42940
    },
    {
        "content": "def plot_identifier(name, plot_id):\n    \"\"\"Return file,plot_id identifier.\"\"\"\n    prefix, number = name.split(\"_\")\n    return \"{}-{}\".format(number, plot_id)",
        "sha1": "26d86dba9a222ad648a42119aff37ccad939e05c",
        "id": 534075
    },
    {
        "content": "def retry_attempts() -> int:\n    \"\"\"How many times retry to download an account before giving up\"\"\"\n    return 5",
        "sha1": "b6e396e2aa5c13c29b6d1c58be3d489ed57be875",
        "id": 448804
    },
    {
        "content": "def hex_to_string(val):\n    \"\"\"Converts hex string to utf-8 string.\n    \n    Accepts padded or unpadded values.\n    \"\"\"\n    if val is None:\n        return \"\"\n    s = val.strip('0x').rstrip('0')\n    if len(s) % 2 == 1:\n        s += \"0\"\n    return bytes.fromhex(s).decode('utf-8')",
        "sha1": "dfe4445a37d07e146df706607028d58d64df74a0",
        "id": 95555
    },
    {
        "content": "def datetime_to_matsim_time(dt):\n    \"\"\"\n    Convert datetime to matsim format time (08:27:33)\n    \"\"\"\n    return dt.strftime(\"%H:%M:%S\")",
        "sha1": "528e2e99dd87714fafda13b5502ff6f033148904",
        "id": 142831
    },
    {
        "content": "import torch\n\n\ndef noise_fuc(x,noise_level = 1):\n    \"\"\"\n    add guassian noise to the images during agumentation procedures\n\n    Inputs\n    --------------------\n    x: torch.tensor, batch_size x 3 x height x width\n    noise_level: float, standard deviation of the gaussian distribution\n    \"\"\"\n    generator = torch.distributions.normal.Normal(0,noise_level)\n    return x + generator.sample(x.shape)",
        "sha1": "770652614add823936c4123f5de8c9aeea210792",
        "id": 371742
    },
    {
        "content": "def bisect_func_right(x, lo, hi, func):\n    \"\"\"Bisect `func(i)`, returning an index such that consecutive values are\n    greater than `x`. If `x` is present, the returned index is past its last\n    occurrence. EOF is assumed if `func` returns None.\"\"\"\n    while lo < hi:\n        mid = (lo + hi) // 2\n        k = func(mid)\n        if k is not None and x < k:\n            hi = mid\n        else:\n            lo = mid + 1\n\n    return lo",
        "sha1": "9e8385efc215ce3b4690162765715146f0082d95",
        "id": 349712
    },
    {
        "content": "def float_pop(d, k):\n    \"\"\"Pop a value from dict by key, then convert to float if not None.\n\n    Args:\n        d (dict): dict to remove key k from.\n        k: Key to pop from dict d.\n    Returns:\n        v: Value popped from dict d, convert to float if not None.\n    \"\"\"\n    v = d.pop(k)\n    if v is not None:\n        return float(v)\n    return v",
        "sha1": "bcb8bed1f5e43f905769dfb8373685cec4816f47",
        "id": 112681
    },
    {
        "content": "def update(i, v, xs):\n    \"\"\"Returns a new copy of the array with the element at the provided index\n    replaced with the given value\"\"\"\n    return [v if i == ind else x for ind, x in enumerate(xs)]",
        "sha1": "096a3e6713c0a66bfb71ca9e49ff4f43738f477e",
        "id": 128322
    },
    {
        "content": "def rreplace(s, old, new, occurrence, only_if_not_followed_by=None):\n    \"\"\"replaces occurences of character, counted from last to first, with new \n    character. \n    Arguments:\n    s - string\n    old - character/string to be replaced\n    new - character/string to replace old with\n    occurence - nth occurence up to which instances should be replaced, counting \n    from end to beginning. e.g. if set to 2, the two last instances of \"old\" \n    will be replaced\n    only_if_not_followed_by - if this is set to a string, [old] is only replaced\n    by [new] if [old] is not followed by [only_if_not_followed_by] in [s]. Useful\n    for gene names like abc-1.d, in that case we do not want to replace the dash.\n    Returns:\n    string with replaced character\n    \"\"\"\n    if old not in s:\n        return s\n    elif only_if_not_followed_by != None:\n        # test if latest instance of old is followed by [only_if_not_followed_by]\n        last_inst_old = s.rfind(old)\n        last_inst_oinfb = s.rfind(only_if_not_followed_by)\n        if last_inst_oinfb > last_inst_old:\n            return s\n    li = s.rsplit(old, occurrence)\n    return new.join(li)",
        "sha1": "8db045d1399697d5bc3bfa7fb3d345019c04cfd8",
        "id": 166087
    },
    {
        "content": "import random\n\n\ndef roulette(probabilities):\n    \"\"\"Performs roulette wheel selection given the probabilities.\n    \n    Given a list of probabilities, selects one of the items randomly.  The\n    probabilities will be scaled if necessary, so the values do not need to\n    sum to 1.0.  Returns the index of the selected item.\n    \n    Examples\n    --------\n        # Randomly selected between two items, preferring the first\n        roulette([0.75, 0.25])\n    \n    Parameters\n    ----------\n    probabilities : list of float\n        List of probabilities of selecting each item.\n    \"\"\"\n    rand = random.uniform(0.0, sum(probabilities))\n    value = 0.0\n    \n    for i in range(len(probabilities)):\n        value += probabilities[i]\n            \n        if value > rand:\n            return i\n        \n    return 0",
        "sha1": "38f80bd3ead8080c4d8a157a9f63602abce2bef9",
        "id": 572025
    },
    {
        "content": "def linear_derivative(z):\n    \"\"\"Linear activation function derivative\"\"\"\n    return 1",
        "sha1": "9e8dd4cdbb54d0ddfa90a375a4b539706db9d0e9",
        "id": 655602
    },
    {
        "content": "def timestamp_to_ms(timestamp):\n    \"\"\"\n    Converts a Unix timestamp into one with millisecond resolution.\n\n    Parameters\n    ----------\n    timestamp : float or int\n        Unix timestamp.\n\n    Returns\n    -------\n    int\n        `timestamp` with millisecond resolution (13 integer digits).\n\n    \"\"\"\n    num_integer_digits = len(str(timestamp).split('.')[0])\n    return int(timestamp*10**(13 - num_integer_digits))",
        "sha1": "2e019aa0f6ad228662abf73d29978c2d7b9a35b4",
        "id": 135874
    },
    {
        "content": "import torch\n\n\ndef sequence_mask(lengths, max_len=None):\n    \"\"\"Creates a boolean mask from sequence lengths.\n    Args:\n        lengths (torch.Tensor): lengths with shape (bs,)\n        max_len (int, optional): max sequence length.\n            if None it will be setted to lengths.max()\n    Returns:\n        torch.Tensor: (bs, max_len)\n    \"\"\"\n    if max_len is None:\n        max_len = lengths.max()\n    aranges = torch.arange(max_len).repeat(lengths.shape[0], 1)\n    aranges = aranges.to(lengths.device)\n    return aranges < lengths.unsqueeze(1)",
        "sha1": "d388702b98d2e9059266abc0713b5a8302a543d6",
        "id": 95094
    },
    {
        "content": "import re\n\n\ndef rm_quotation_marks(sent):\n    \"\"\" Remove single quotes used as quotation marks (e.g. some 'phrase in quotes')\n    Remove double quotes used as quotation marks (e.g. some \"phrase in quotes\" or\n    ``phrase in quotes'')\n    \"\"\"\n    sent = re.sub(r\"\\s'([\\w\\s]+[\\w])'\\s\", r' \\1 ', sent)\n    return re.sub(r'[\"\u201c\u201d\u2018\u2019]', r' ', sent)",
        "sha1": "0977773e9b9b72af9566ca3c31d3c30f9800e165",
        "id": 621600
    },
    {
        "content": "def calc_time_axis(df):\n    \"\"\"Returns the number of minutes between midnight of the earliest date\n    and midnight of the day after the latest date in the dataset\"\"\"\n    time_min = df.Timestamp.min()\n    time_max = df.Timestamp.max()\n    \n    diff_in_days =(time_max.date() - time_min.date())\n    diff_in_minutes = (diff_in_days.days + 1)*24*60\n    return diff_in_minutes",
        "sha1": "d7d5ef397381815405e38e018a01a8618db9c348",
        "id": 642081
    },
    {
        "content": "def _build_conflict_constraint_string(ctr):\n    \"\"\" Build the string used to represent a constraint in conflict refiner\n    Args:\n        ctr:  Constraint to print\n    Returns:\n        Constraint string\n    \"\"\"\n    return str(ctr)",
        "sha1": "a18bdf707bc9a9dcad2d243f97869c3cabff8f4a",
        "id": 565342
    },
    {
        "content": "import itertools\n\n\ndef generate_strings(length = 6):\n    \"\"\" Generates all possible strings of length N using the alphabet AUGC\n\n    args:\n        length -- int\n    returns:\n        list<str>\n    \"\"\"\n    combinations = [p for p in itertools.combinations_with_replacement(\"AUGC\", length)]\n    strings = set()\n    for c in combinations:\n        for p in itertools.permutations(c, length):\n            strings.add(\"\".join(p))\n    return strings",
        "sha1": "1d8f0a23ad40c21672a7cbc505d53fe49449ef6c",
        "id": 415545
    },
    {
        "content": "def dew_point(t, h):\n    \"\"\"Calculate dewpoint\n\n    Args:\n        t (float): temperature in \u00b0C\n        h (float): relative humidity in %\n\n    Returns:\n        calculated dewpoint in \u00b0C\n\n    Ref:\n        https://www.ajdesigner.com/phphumidity/dewpoint_equation_dewpoint_temperature.php\n    \"\"\"\n    return round((h / 100) ** (1 / 8) * (112 + 0.9 * t) + 0.1 * t - 112, 2)",
        "sha1": "4c4ae45206a3020fd8d4bb8d3a43b5065f8bd91e",
        "id": 195388
    },
    {
        "content": "def _get_subclass_entry(cls, clss, exc_msg=\"\", exc=NotImplementedError):\n    \"\"\"In a list of tuples (cls, ...) return the entry for the first\n    occurrence of the class of which `cls` is a subclass of.\n    Otherwise raise `exc` with the given message\"\"\"\n\n    for clstuple in clss:\n        if issubclass(cls, clstuple[0]):\n            return clstuple\n    raise exc(exc_msg % locals())",
        "sha1": "b8fb2b821f4315fd0af6a1bfdbf2a5cef11b37e5",
        "id": 284708
    },
    {
        "content": "def extract_job_url(job):\n    \"\"\"\n    parse the job data and extract the str for the URL of the job posted\n    params:\n        job str: html str representation from bs4\n    returns:\n        url str: relative URL path of the job ad\n    \"\"\"\n    return job.a[\"href\"]",
        "sha1": "7517badcc2814e641c04a8f880353d897d434b7f",
        "id": 1049
    },
    {
        "content": "from datetime import datetime\n\n\ndef calculate_duration(start_time: datetime, end_time: datetime) -> float:\n    \"\"\"Calculate the duration of a process instance given start and end timestamp.\n\n    :param start_time: start\n    :param end_time: end\n    :return: duration in seconds\n    \"\"\"\n    return (end_time - start_time).total_seconds()",
        "sha1": "7a19973f5ab5668934e622759aa403514b5df89e",
        "id": 355472
    },
    {
        "content": "import re\n\n\ndef remove_comments(html):\n    \"\"\"remove_comments(html) -> html, without comments\"\"\"\n    return re.sub(r\"<!--.*?-->\", \" \", html)",
        "sha1": "f79e9ee1d4b92fa6046d9de1beb9959912e69d48",
        "id": 471481
    },
    {
        "content": "def get_page_content(page_ann, base_text):\n    \"\"\"Return page content from base text\n\n    Args:\n        page_ann (dict): span of page, page reference, page index are included\n        base_text (str): base text\n\n    Returns:\n        str: page content\n    \"\"\"\n    page_span = page_ann[\"span\"]\n    page_start_idx = page_span[\"start\"]\n    page_end_idx = page_span[\"end\"]\n    page_content = base_text[page_start_idx : page_end_idx + 1]\n    return page_content",
        "sha1": "87f9905796786d5ea4c3826db455aca110714b7b",
        "id": 384652
    },
    {
        "content": "import random\n\n\ndef beta(alpha=.5, beta=.5):\n    \"\"\"\n    Returns value between 0 and 1 from the beta distribution (https://en.wikipedia.org/wiki/Beta_distribution).\n    When a=b=1 the distribution is uniform. When alpha=beta, the distribution is \n    symmetric around .5. When alpha<1 and beta<1 then the density of larger and\n    smaller numbers increases. When alpha>1 and beta>1, density is similar to the\n    gaussian distribution.\n    \"\"\"\n    ra = 1.0 / alpha\n    rb = 1.0 / beta\n    while True:\n        r1 = random.random()\n        r2 = random.random()\n        y1 = r1 ** ra\n        y2 = r2 ** rb\n        y3 = y1 + y2\n        if y3 <= 1.0:\n            return (y1 / y3)",
        "sha1": "fd4fa5bac1da4c5d7c1de74f87d52628314ef5c6",
        "id": 149754
    },
    {
        "content": "def get_complete_fund_code(code):\n    \"\"\"\n    Append 'sz' to the front if code starts with 0, 2 or 3. Append 'sh' otherwise\n    \"\"\"\n    if code[0] in \"023\":\n        return \"sz.{}\".format(code)\n    else:\n        return \"sh.{}\".format(code)",
        "sha1": "6bfa3f2d5907b2572ee4ff268510595a007d0a2a",
        "id": 233337
    },
    {
        "content": "def any_not_none(*args):\n    \"\"\"\n    Returns a boolean indicating if any argument is not None.\n    \"\"\"\n    return any(arg is not None for arg in args)",
        "sha1": "ac73f9cdadb167713d3867f36b1103bc4501b5c3",
        "id": 571608
    },
    {
        "content": "def hill_equation(val, diss_cf, hill_cf):\n    \"\"\"\n    Hill equation\n    :param val: input value\n    :param diss_cf: dissociation coefficient\n    :param hill_cf: Hill coefficient\n    :return: Hill equation for input *val*\n    \"\"\"\n    if val == 0:\n        return 0\n    else:\n        return 1 / (1 + (diss_cf / val) ** hill_cf)",
        "sha1": "96ee40c173a160ee4f557bd7ee125869927a3cd4",
        "id": 634098
    },
    {
        "content": "def seq_decode(_string, symbols):\n    \"\"\"\n    Decode a number from a using encoded by a sequence of symbols.\n\n    :param str _string: string to decode\n    :param symbols: sequence key\n    :type symbols: str or list[char]\n    :returns: decoded value\n    :rtype: int\n    \"\"\"\n    value = 0\n    base = len(symbols)\n    for i, c in enumerate(reversed(_string)):\n        value += symbols.index(c) * i**base\n    return value",
        "sha1": "58dd62af6a7f1fa1ece16824c2b133f403260fb3",
        "id": 521276
    },
    {
        "content": "def diff(x):\n    \"\"\"Differences between subsequent values of a numpy.array\"\"\"\n    return x[1:] - x[:-1]",
        "sha1": "a71d1434637d0da238ae0e462ff1e323f8d19710",
        "id": 616672
    },
    {
        "content": "def args_from_props(props, names):\n    \"\"\" returns a tuple with the properties in props for the given names\n    \"\"\"\n    return tuple(getattr(props, name) for name in names)",
        "sha1": "e83c8cae43fcf0207a7705b3d56b63432bf6cbba",
        "id": 196155
    },
    {
        "content": "def lemma_8_3_3_generalized(scalars, vectors):\n    \"\"\"\n    Demonstrates that if v1... vn are mutually orthogonal that for any coefficients a1.... an\n    ||a1 * v1 + ... + an * vn||^2 == a1^2 * ||v1||^2 + ... + an^2 * ||vn||^2\n    :param scalars: n length list of scalars\n    :param vectors: n length list of m vectors\n    :return: 2-tuple with each side of description equation\n    >>> scalars = [2, 3]\n    >>> vectors = [list2vec([1, 0]), list2vec([0, 1])]\n    >>> result = lemma_8_3_3_generalized(scalars, vectors)\n    >>> result[0] == result[1]\n    True\n    >>> scalars2 = [2, 3]\n    >>> vectors2 = [list2vec([1, 1]), list2vec([0, 1])] # not orthogonal\n    >>> result = lemma_8_3_3_generalized(scalars2, vectors2)\n    >>> result[0] != result[1]\n    True\n    >>> scalars3 = [2, 3, 4]\n    >>> vectors3 = [list2vec([1, 0, 0]), list2vec([0, 1, 0]), list2vec([0, 0, 1])]\n    >>> result = lemma_8_3_3_generalized(scalars3, vectors3)\n    >>> result[0] == result[1]\n    True\n    >>> scalars4 = [2, 3, 4]\n    >>> vectors4 = [list2vec([1, 0, 1]), list2vec([0, 1, 0]), list2vec([0, 0, 1])] # not orthogonal\n    >>> result = lemma_8_3_3_generalized(scalars4, vectors4)\n    >>> result[0] != result[1]\n    True\n    \"\"\"\n    t1 = sum([(s * v) for s, v in zip(scalars, vectors)])\n    t1 *= t1 # because the Vec clas does not have the 'pow' operator implemented\n    t2 = sum([s**2 * ((v*v)**(1/2))**2 for s, v in zip(scalars, vectors)])\n    return t1, t2",
        "sha1": "23302badfdd4db44b29c175d97e3744f19bb2af1",
        "id": 278243
    },
    {
        "content": "def rvp_to_dbz(x):\n    \"\"\"Calculates dBZ-values from DWD RVP6 values as given in DX-product\n    files.\n\n    Parameters\n    ----------\n    x : int\n        a number or an array\n\n    Examples\n    --------\n    >>> from wradlib.trafo import rvp_to_dbz\n    >>> print(rvp_to_dbz(65.))\n    0.0\n    \"\"\"\n    return x * 0.5 - 32.5",
        "sha1": "02438330ba5b5fcc52a0096d6fb2b7ed37162564",
        "id": 247391
    },
    {
        "content": "def parse_syntax(line):\n    \"\"\"\n    >>> parse_syntax('syntax: glob')\n    'glob'\n    >>> parse_syntax('syntax: regexp')\n    'regexp'\n    >>> parse_syntax('syntax: none')\n    Traceback (most recent call last):\n    ...\n    Exception: Unknown syntax \"none\"\n    \"\"\"\n    line = line.replace(':', ' ')\n    _, syntax = line.split()\n    if syntax in ['glob', 'regexp']:\n        return syntax\n    else:\n        raise Exception('Unknown syntax \"%s\"' % syntax)",
        "sha1": "8f619b216ede8cc9bdd8198f6433add997c87768",
        "id": 28528
    },
    {
        "content": "def check_arg_type(arg_name: str, arg_value: str):\n    \"\"\"\n        Checks that RST Threat Feed API parameters are valid.\n\n        Args:\n          arg_name (str): paramater name\n          arg_value (str): paramater value to verify\n        Returns:\n          (str): a null string means OK while any text is an error\n        \"\"\"\n    output = ''\n    try:\n        isinstance(int(arg_value), int)\n        value = int(arg_value)\n        if 'threshold' in arg_name:\n            if value < 0 or value > 100:\n                output = str(arg_name) + ': the value must be between 0 and 100; '\n        if 'indicator_expiration' in arg_name:\n            if value < 0:\n                output = str(arg_name) + ': the value must be positive (>0); '\n    except Exception:\n        return str(arg_name) + ': bad format, must be a number; '\n    return output",
        "sha1": "16c12473ea5a8f8ea9cca84454736296c53bd5fe",
        "id": 238332
    },
    {
        "content": "def get_fields(model):\n    \"\"\"\n    Returns django fields of current ``model``.\n\n    Does not include inherited fields.\n    \"\"\"\n    return model._meta.get_fields(include_parents=False)",
        "sha1": "1b006d891a86e18ca0b55953094437f36a88d688",
        "id": 199888
    },
    {
        "content": "def insert_string(string: str, index: int, insert: str):\n    \"\"\"Inserts a string at a given index within another strings.\"\"\"\n    return string[:index] + insert + string[index:]",
        "sha1": "18c33f55374940c8235971fe39197ba8a87db636",
        "id": 56474
    },
    {
        "content": "def suma(*args):\n    \"\"\"Sum indefinite number of integers and/or floats.\n    \n    Parameters\n    ----------\n    args : ints and/or floats\n        An indeterminate number of ints and/or floats.\n    \n    Returns\n    -------\n    value : int, float\n        Returns the result of the operation    \n    \"\"\"\n    value = 0\n    for a in args:\n        value += a\n    return value",
        "sha1": "cfab581c0b618ec4311c967f607a37f9d2352005",
        "id": 244608
    },
    {
        "content": "def calculateContactsVar(deltaR, epsMax):\n    \"\"\"\n        Calculate the variation of epsilon according to the contact ratio\n\n        :param deltaR: Change in contact ratio\n        :type deltaR: float\n        :param epsMax: Maximum value of epsilon\n        :type epsMax: float\n\n        :returns: float -- Epsilon variation\n    \"\"\"\n    if deltaR < 0.1:\n        return 0\n    elif deltaR > 1.0:\n        return epsMax * 0.09\n    else:\n        return epsMax * 0.09 * deltaR",
        "sha1": "c923bd19203c46d39eeb82e58f78b0ce42cc5795",
        "id": 338034
    },
    {
        "content": "def set_bit(v, index, x):\n  \"\"\"Set the index:th bit of v to 1 if x is truthy, else to 0, and return the new value.\"\"\"\n  mask = 1 << index   # Compute mask, an integer with just bit 'index' set.\n  v &= ~mask          # Clear the bit indicated by the mask (if x is False)\n  if x:\n    v |= mask         # If x was True, set the bit indicated by the mask.\n  return v            # Return the result, we're done.",
        "sha1": "0da3129b2915771b002bf7af30d653335f78a0b8",
        "id": 491213
    },
    {
        "content": "def comp_height_opening(self, Ndisc=200):\n    \"\"\"Compute the height of the opening area (Hslot - Hactive)\n\n    Parameters\n    ----------\n    self : Slot\n        A Slot object\n    Ndisc : int\n        Number of point to discretize the lines\n\n    Returns\n    -------\n    Hwind: float\n        Height of the opening area [m]\n\n    \"\"\"\n\n    return self.comp_height(Ndisc=Ndisc) - self.comp_height_active(Ndisc=Ndisc)",
        "sha1": "2690083e370897ad0f77cfdddf5b904a2adbe5b2",
        "id": 91444
    },
    {
        "content": "import torch\n\n\ndef x1y1wh_to_x1y1x2y2(boxes):\n    \"\"\"\n    Converts boxes from x1y1wh (upper left, width and height) format \n    to x1y1x2y2 (upper left, bottom right) format\n    \"\"\"\n    x1 = boxes[:,0]\n    y1 = boxes[:,1]\n    w = boxes[:,2]\n    h = boxes[:,3]\n    x2 = x1 + w\n    y2 = y1 + h\n    boxes = torch.stack([x1,y1,x2,y2], dim=1)\n    return boxes",
        "sha1": "c66c9c71e01ddbbe0aca1cfbfcae38ac84beeff2",
        "id": 212326
    },
    {
        "content": "import configparser\n\n\ndef read_setup_cfg(fname=\"setup.cfg\"):\n    \"\"\"\n    Read the setup.cfg file into a dictionary.\n    \"\"\"\n    config = configparser.ConfigParser()\n    # Use read_file to get a FileNotFoundError if setup.cfg is missing. Using\n    # read returns an empty config instead of raising an exception.\n    with open(fname, \"rt\") as config_source:\n        config.read_file(config_source)\n    config_dict = {\n        section: dict((key, value.strip()) for key, value in config.items(section))\n        for section in config.sections()\n    }\n    return config_dict",
        "sha1": "312c300dcf09060d86a8ce8f2d09a30b594f230f",
        "id": 647304
    },
    {
        "content": "def angle_dis (a1, a2, factor=1.0) :\n    \"\"\" Get distance between angles, around 360-degree bound\n    args:\n        a1: angle 1, scalar or ndarray\n        a2: angle 2, scalar or ndarray, if both a1 and a2 are ndarray, they must have same shape\n        factor: a shrink factor, usually is 1.0/cos(dec) or 1.0/cos(lat)\n    returns:\n        distance between a1 and a2\n    \"\"\"\n    d = ((a1 - a2 + 180.0) % 360.0 - 180.0) * factor\n    return d",
        "sha1": "0b7f258289a7f94aa876e2b94007cdf717bc4781",
        "id": 204770
    },
    {
        "content": "def get_view_name(sis_term_id, week, label):\n    \"\"\"\n    Returns DB view name for the given sis_term_id, week, and view label.\n\n    :param sis_term_id:\n    :type str: sis-term-id to create view for\n    :param week:\n    :type int: week number to create view for\n    :param label: label for view.\n        Choose from (assignments, participations, or rad)\n    :type str:\n    \"\"\"\n    if label not in [\"assignments\", \"participations\", \"rad\"]:\n        raise ValueError(f\"Unknown DB view label {label}. \"\n                         f\"Choose from (assignments, participations, or rad).\")\n    sis_term_id = sis_term_id.replace(\"-\", \"_\")\n    view_name = f\"{sis_term_id}_week_{week}_{label}\"\n    return view_name",
        "sha1": "7a76704ba06757995e30fde0047422d178ebf0a0",
        "id": 456388
    },
    {
        "content": "def calc_ap(prec, recall):\n    \"\"\"\n    Computes average precision from precision recall curves. Curves\n    are expected to be same size and 1D.\n    \"\"\"\n    assert prec.size() == recall.size(), \"Precision and recall curves must be same size\"\n    ap = 0.0\n    if len(prec) == 0:\n        return ap\n    prev_r = 0.0\n    prev_p = prec[0]\n    for p, r in zip(prec, recall):\n        ap += (r - prev_r) * (p + prev_p) / 2.0\n        prev_r = r\n        prev_p = p\n\n    return ap",
        "sha1": "2c067125a110219ccc7a332c14dc145d492b953e",
        "id": 558554
    },
    {
        "content": "def list_texts(texts):\n    \"\"\"\n    Compile (Oxford) comma-delimited list of texts.\n\n    :param texts: [list] non-empty sequence of texts\n    :return: compiled text\n    \"\"\"\n    all_texts_but_last = texts[:-1]\n    if all_texts_but_last:\n        last_delimiter = (',' if len(all_texts_but_last) > 1 else '') + ' and '\n\n        return last_delimiter.join([', '.join(all_texts_but_last), texts[-1]])\n    else:\n\n        return texts[-1]",
        "sha1": "19bb6f12ad3b2ee72837065739fe1514e137ad95",
        "id": 602083
    },
    {
        "content": "def _checkshape(shape, maxshape):\n    \"\"\"Return True if the shape is consistent with the maximum allowed shape.\n\n    Each element of shape must be less than or equal to the \n    corresponding element of maxshape, unless the latter is set to None, in \n    which case the value of the shape element is unlimited.\n    \n    Parameters\n    ----------\n    shape : tuple of int\n        Shape to be checked.\n    maxshape : tuple of int\n        Maximum allowed shape\n    \n    Returns\n    -------\n    bool\n        True if the shape is consistent.\n    \"\"\"\n    for i, j in [(_i, _j) for _i, _j in zip(maxshape, shape)]:\n        if i is not None and i < j:\n            return False\n    return True",
        "sha1": "31a1c438598b895e0c660e39ca92a1bfbf92474f",
        "id": 166539
    },
    {
        "content": "def read_text(path):\n    \"\"\"\n    Read a file and return cleaned rows of content as a list.\n    \"\"\"\n    with open(path) as f_in:\n        lines = f_in.read().splitlines()\n\n    return [line.replace(\"\\\\n\", \"\\n\") for line in lines if line]",
        "sha1": "9e20b028357ab46d5b31b884aba75d67580e5ed5",
        "id": 257074
    },
    {
        "content": "def _update_report_path(case_obj, report_path, report_type):\n    \"\"\"Updates the report path\n\n    Args:\n        case_obj     (Case):         Case object\n        report_path  (string):       Path to CNV report\n        report_type  (string):       Type of report\n    \"\"\"\n    case_obj[report_type] = report_path\n    return True",
        "sha1": "efca254d5602ea93c1df01cc81f3bd9cecfecee4",
        "id": 551176
    },
    {
        "content": "def args_to_argline(prm, filters=[], underscore_to_dash=True,\n                    bool_argparse=True):\n    \"\"\"Naive transformation of dictionary into argument line\n\n    Parameters\n    ----------\n    prm : dict\n        desired pair of key-val. Keys must corresponds to argument line options\n    filters : list, optional\n        string in keys to avoid\n    underscore_to_dash : bool, optional\n        Replace underscore for dash for keys\n    bool_argparse : bool, optional\n        use --no-{key} if False, --key if True.\n\n    Returns\n    -------\n    argline : str\n        argument line string\n\n    \"\"\"\n    argline = ''\n    for k, v in prm.items():\n        # Filter keys\n        remove = 0\n        for f in filters:\n            if f in k:\n                remove += 1\n                break\n        if remove:\n            continue\n\n        if underscore_to_dash:\n            k = k.replace('_', '-')\n\n        if isinstance(v, str):\n            v = '\"{}\"'.format(v)\n        elif isinstance(v, list):\n            v = ' '.join(map(str, v))\n        elif isinstance(v, bool) and bool_argparse:\n            if not v:\n                k = 'no-' + k\n            v = ''\n\n        # Add keys\n        argline += '--{} {} '.format(k, str(v))\n    return argline",
        "sha1": "bc23e571d98503deab3540ef60a70964889d0309",
        "id": 57504
    },
    {
        "content": "def higher(high, temp):\n\t\"\"\"\n\t:param high: int, the highest temperature data before\n\t:param temp: int, the new temperature data\n\t:return: int, the higher one between high and temp, which becomes the highest temperature\n\t\"\"\"\n\tif temp > high:\n\t\treturn temp\n\treturn high",
        "sha1": "754eca69ad0857cb8501f0f38c6b9bde6247427a",
        "id": 385028
    },
    {
        "content": "def float_list(s):\n    \"\"\"Convert a string of comma separated floats to a list of floats.\"\"\"\n    return sorted(map (float, s.split(',')))",
        "sha1": "e185cb35c932adb97b6da95d12deea7b2631044f",
        "id": 82342
    },
    {
        "content": "def transport_cost(a_i, b_j, F_i, G_j):\n    \"\"\"Returns the entropic transport cost associated to the dual variables F_i and G_j.\"\"\"\n    return a_i.dot(F_i) + b_j.dot(G_j)",
        "sha1": "bcfd4a2b2fa6145899b2ea15f09f8f2adbe6509f",
        "id": 180198
    },
    {
        "content": "from typing import List\n\n\ndef replace_words_with_prefix_hash(dictionary: List[str], sentence: str) -> str:\n    \"\"\"\n    Replace words in a sentence with those in the dictionary based on the prefix match.\n\n    Intuition:\n\n    For each word in the sentence, we'll look at successive prefixes and see if we saw them before\n\n    Algorithm:\n\n    Store all roots in a set. Then for each word, look at successive prefixes for that word. If we find a prefix that is\n    root, replace the word with that prefix. Otherwise, the prefix will just be the word itself, and we should add that\n    to the final sentence\n\n    Complexity Analysis:\n\n    - Time Complexity: O(\u2211w i2) where wi is the length of the i-th word. We might check every prefix,\n    the i-th of which is O(w_i^2) work.\n\n    - Space Complexity: O(N) where N is the length of our sentence; the space used by root_set.\n\n    @param dictionary: List of roots\n    @param sentence: words separated by space to perform replacement on\n    @return: New sentence with replaced words\n    \"\"\"\n    root_set = set(dictionary)\n\n    def replace(word: str) -> str:\n        for x in range(1, len(word)):\n            if word[:x] in root_set:\n                return word[:x]\n        return word\n\n    return \" \".join(map(replace, sentence.split()))",
        "sha1": "bb46b0dc61eab2be358d44f8fb46782f867e3c30",
        "id": 24990
    },
    {
        "content": "def any_true_p (seq, pred) :\n    \"\"\"Returns first element of `seq` for which `pred` returns True,\n       otherwise returns False.\n    \"\"\"\n    for e in seq :\n        if pred (e) :\n            return e\n    else :\n        return False",
        "sha1": "24c374cfbbef49be7a76ba471b61e4122f0619e3",
        "id": 675161
    },
    {
        "content": "def merge_two_dicts(dict_1, dict_2):\n    \"\"\"\n    Given two dicts, return one merged dict.\n    \"\"\"\n    return {**dict_1, **dict_2}",
        "sha1": "3439fe0a37098a2e3d208c907d3ce25ace32e597",
        "id": 361954
    },
    {
        "content": "def scale(data):\n    \"\"\"\n    Scale the data between -1 and 1.\n\n    :param data: The data to be scaled.\n    :return: Scaled data.\n    \"\"\"\n    return (data - data.mean()) / (data.max() - data.min())",
        "sha1": "e85b7888acf2f7bbd5289e5df866246b1835c88c",
        "id": 196383
    },
    {
        "content": "import yaml\n\n\ndef load_file(filename):\n    \"\"\"\n    Loads a YAML file and returns the document contents.\n\n    :param filename: YAML file name.\n    :type filename: unicode | str\n    :return: Contents of the YAML file.\n    \"\"\"\n    with open(filename, 'r') as f:\n        return yaml.safe_load(f)",
        "sha1": "b4ad1462906b15be6e5248521be73ed90f045507",
        "id": 556963
    },
    {
        "content": "def format_error_msg(msg: list) -> str:\n\n    \"\"\"\n    Sample:\n    Traceback (most recent call last):\n        File \"/home/addy/Documents/PEst/pesto/../pesto/classes.py\", line 17, in run\n            self.func()\n        File \"/home/addy/Documents/PEst/demo/function_test.py\", line 12, in test_awesome_function_cases\n            expect(my_awesome_function(1, 1)).to_be(3.5)\n        File \"/home/addy/Documents/PEst/pesto/../pesto/expect.py\", line 14, in to_be\n            assert self.val == a, str(self.val) + \" does not equal \" + str(a)\n    AssertionError: 35.5 does not equal 3.5\n    \"\"\"\n\n    phrases_to_skip = [\"Traceback\", \"pesto\", \"assert \" \"self.\"]\n    lines_to_keep = []\n    for line in msg:\n        to_keep = True\n        # line = line.strip()\n        for phrase in phrases_to_skip:\n            if phrase in line:\n                to_keep = False\n                break\n        if to_keep:\n            lines_to_keep.append(line)\n\n    return \"\\n\".join(lines_to_keep[1:])",
        "sha1": "b096452007c68eb02bb191a050029e704de521d8",
        "id": 405946
    },
    {
        "content": "def emission_probability(symbol, word, emission_probabilities, symbol_counts):\n    \"\"\"\n    Takes a symbol, a word, a nested dictionary of emission probabilities,\n    and a dictionary of symbol counts\n    and returns the emission probability for that symbol and word\n    If the word has not been encountered in the training data\n    we assign it a fixed probability based on the symbol count\n    \"\"\"\n\n    unseen_word = True\n\n    for sym in emission_probabilities:\n        if word in emission_probabilities[sym]:\n            unseen_word = False\n\n    if unseen_word:\n        return 1 / (1 + symbol_counts[symbol])\n    else:\n        if word in emission_probabilities[symbol]:\n            return emission_probabilities[symbol][word]\n        else:\n            return 0",
        "sha1": "df41ef4bbb01e993416c8e5e581e44b2a5292fa3",
        "id": 629375
    },
    {
        "content": "import yaml\n\n\ndef read_config(config_yml):\n    \"\"\"Read and parse run-time yaml configuration file\n\n    Args:\n        config_yml (string): File name for yaml configuration file\n    \n    Returns a dictionary\n    \"\"\"\n    with open(config_yml, 'r') as f:\n        config = yaml.safe_load(f)\n\n    # Geo\n    geobbox = dict(\n        south=config['geo']['bbox']['south'],\n        west=config['geo']['bbox']['west'],\n        north=config['geo']['bbox']['north'],\n        east=config['geo']['bbox']['east'],\n    )\n\n    to_crs = f\"epsg:{config['geo']['to_epsg_crs']}\"\n    to_resolution = config['geo']['to_epsg_crs']\n\n    # Create a sligthly larger box for the data query, relative to geobbox\n    buffered_geobbox = geobbox.copy()\n    buffered_geobbox['south'] = geobbox['south'] - config['geo']['bbox']['buffer_north']\n    buffered_geobbox['north'] = geobbox['north'] + config['geo']['bbox']['buffer_north']\n    buffered_geobbox['west'] = geobbox['west'] - config['geo']['bbox']['buffer_east']\n    buffered_geobbox['east'] = geobbox['east'] + config['geo']['bbox']['buffer_east']\n\n    config_out = dict(\n        to_crs=to_crs,\n        to_resolution=to_resolution,\n        geobbox=geobbox,\n        buffered_geobbox=buffered_geobbox,\n        start_date=config['start_date'],\n        end_date=config['end_date'],\n        nc_export_fname=config['nc_export_fname'],\n        nc_export_fdir=config['nc_export_fdir']\n    )\n\n    return config_out",
        "sha1": "5bd4d181cd917af8f03e3fa37864b1b4d178236b",
        "id": 509031
    },
    {
        "content": "import torch\n\n\ndef validate_bbox3d(boxes: torch.Tensor) -> bool:\n    \"\"\"Validate if a 3D bounding box usable or not. This function checks if the boxes are cube or not.\n\n    Args:\n        boxes: a tensor containing the coordinates of the bounding boxes to be extracted. The tensor must have the shape\n            of Bx8x3, where each box is defined in the following ``clockwise`` order: front-top-left, front-top-right,\n            front-bottom-right, front-bottom-left, back-top-left, back-top-right, back-bottom-right, back-bottom-left.\n            The coordinates must be in the x, y, z order.\n    \"\"\"\n    if not (len(boxes.shape) == 3 and boxes.shape[1:] == torch.Size([8, 3])):\n        raise AssertionError(f\"Box shape must be (B, 8, 3). Got {boxes.shape}.\")\n\n    left = torch.index_select(boxes, 1, torch.tensor([1, 2, 5, 6], device=boxes.device, dtype=torch.long))[:, :, 0]\n    right = torch.index_select(boxes, 1, torch.tensor([0, 3, 4, 7], device=boxes.device, dtype=torch.long))[:, :, 0]\n    widths = left - right + 1\n    if not torch.allclose(widths.permute(1, 0), widths[:, 0]):\n        raise AssertionError(f\"Boxes must have be cube, while get different widths {widths}.\")\n\n    bot = torch.index_select(boxes, 1, torch.tensor([2, 3, 6, 7], device=boxes.device, dtype=torch.long))[:, :, 1]\n    upper = torch.index_select(boxes, 1, torch.tensor([0, 1, 4, 5], device=boxes.device, dtype=torch.long))[:, :, 1]\n    heights = bot - upper + 1\n    if not torch.allclose(heights.permute(1, 0), heights[:, 0]):\n        raise AssertionError(f\"Boxes must have be cube, while get different heights {heights}.\")\n\n    depths = boxes[:, 4:, 2] - boxes[:, :4, 2] + 1\n    if not torch.allclose(depths.permute(1, 0), depths[:, 0]):\n        raise AssertionError(f\"Boxes must have be cube, while get different depths {depths}.\")\n\n    return True",
        "sha1": "9b6d5a73423466252bb13607a98d14a30899fc73",
        "id": 112210
    },
    {
        "content": "def remove_soft_hyphens(line):\n    \"\"\"Removes any soft hyphens or middle dots\"\"\"\n    line = line.replace(u'\\u00AC', '')  # not sign (Word's soft hyphen)\n    line = line.replace(u'\\u00AD', '')  # soft hyphen\n    line = line.replace(u'\\u00B7', '')  # middle dot\n    return line",
        "sha1": "de4c95238017400b6218a456a0dcb8a5ef06b1a6",
        "id": 311083
    },
    {
        "content": "def compute_kxy(kxx, dTx, dTy, width, length, Resistance=5000):\n    \"\"\"\n    Computes the thermal Hall conductivity aka kxy in W / Km\n\n    Parameters:\n    ----------------------------------------------------------------------------\n    kxx:      1d array\n    The values of the longitudinal thermal conductivity.\n    dTx:    1d array\n    The values of the longitudinal temperature gradient in the sample.\n    dTy:    1d array\n    The values of the transverse temperature gradient in the sample.\n    width:  float\n    The sample's width in meters\n    length: 1d array\n    The sample's length in meters\n    \"\"\"\n\n    length_ratio = length/width\n    delta_ratio = dTy/dTx\n    kxy = kxx*delta_ratio*length_ratio\n\n    return kxy",
        "sha1": "75125182a9e84cc82f94949b3d5ede1476b53ccc",
        "id": 68436
    },
    {
        "content": "def is_blank_line(line, allow_spaces=0):\n    \"\"\"is_blank_line(line, allow_spaces=0) -> boolean\n\n    Return whether a line is blank.  allow_spaces specifies whether to\n    allow whitespaces in a blank line.  A true value signifies that a\n    line containing whitespaces as well as end-of-line characters\n    should be considered blank.\n\n    \"\"\"\n    if not line:\n        return 1\n    if allow_spaces:\n        return line.rstrip() == ''\n    return line[0] == '\\n' or line[0] == '\\r'",
        "sha1": "7134381c54d9db56e8b297b337cb49fbdcc3c5a8",
        "id": 593465
    },
    {
        "content": "import math\n\n\ndef derive_similarity_tag(dag, log_base=1.618):\n    \"\"\"Derive the tag for similarity check from one computational DAG.\n    The DAGs with the same tag are considered as similar tasks.\n\n    The tag format is <op1-tag>_<op2-tag> ... <log(flop)>.\n\n    If the tag is \"\", then the task is not considered to be similar to any other tasks.\n\n    Parameters\n    ----------\n    dag: ComputeDAG\n        The input computational DAG\n    log_base: float = 1.618\n        The base of log to normalize FLOPS\n\n    Returns\n    -------\n    tag: str\n        The tag of this computational DAG.\n    \"\"\"\n    ret = \"\"\n    for op in dag.ops:\n        tag = op.attrs.get(\"auto_scheduler_task_scheduler_tag\", None)\n        if tag:\n            ret += op.attrs[\"auto_scheduler_task_scheduler_tag\"] + \"_\"\n    if ret:\n        ret += \"%d\" % int(math.log(dag.flop_ct + 1, log_base))\n    return ret",
        "sha1": "6a499422bfa5b9b05bb3b2a5f68e3fba31d224d8",
        "id": 599029
    },
    {
        "content": "def array_to_string(array):\n    \"\"\"\n    Converts a numeric array into the string format in mujoco.\n    Examples:\n        [0, 1, 2] => \"0 1 2\"\n    Args:\n        array (n-array): Array to convert to a string\n    Returns:\n        str: String equivalent of @array\n    \"\"\"\n    return \" \".join([\"{}\".format(x) for x in array])",
        "sha1": "8ae2a9e60fb250c5608621fd7fd67cdbc4936e5a",
        "id": 375911
    },
    {
        "content": "import dateutil.parser as dp\n\n\ndef convert_date_time_epoch(datetime:str) -> int:\n    \"\"\"Convert Datetime into epoch\"\"\"\n    parsed_t = dp.parse(datetime)\n    t_in_seconds = parsed_t.timestamp()\n    return int(t_in_seconds)",
        "sha1": "0d8ffdaa8ef182016709c036d5ab0e12228e4b97",
        "id": 612020
    },
    {
        "content": "def single(a, b, distance_function):\n    \"\"\"\n    Given two collections ``a`` and ``b``, this will return the distance of the\n    points which are closest together.  ``distance_function`` is used to\n    determine the distance between two elements.\n\n    Example::\n\n        >>> single([1, 2], [3, 4], lambda x, y: abs(x-y))\n        1  # (distance between 2 and 3)\n    \"\"\"\n    left_a, right_a = min(a), max(a)\n    left_b, right_b = min(b), max(b)\n    result = min(distance_function(left_a, right_b),\n                 distance_function(left_b, right_a))\n    return result",
        "sha1": "a8252954d129d67755ade0cd119a6582dd063ee1",
        "id": 341074
    },
    {
        "content": "def get_players(table, hascards=False, haschips=False):\n    \"\"\" Returns a list of seats at the table. If the button is set, it is\n        ordered from first after button, to Button Last. Can specify if seats\n        have cards and/or chips.\n    \"\"\"\n    if table.TOKENS['D'] == -1:\n        btn = 0\n    else:\n        btn = table.TOKENS['D']\n\n    length = len(table)\n    first = (btn + 1) % length\n    seats = list(range(first, length)) + list(range(first))\n\n    seatlist = [table.seats[s] for s in seats if table.seats[s].occupied()]\n\n    if hascards is True:\n        seatlist = list(filter((lambda x: x.has_hand() == True), seatlist))\n\n    if haschips is True:\n        seatlist = list(filter((lambda x: x.has_chips() == True), seatlist))\n\n    return seatlist",
        "sha1": "6577fe09abcbd27c322b5169fd94059c59ece79b",
        "id": 529180
    },
    {
        "content": "def _event(title, subtitle, icon):\n    \"\"\"Build a event used as returned item\n\n    :param str title: the title of this item\n    :param str subtitle: the subtitle of this item\n    :icon: the icon for this item\n    :returns: a list of dict, actually just one entry\n\n    \"\"\"\n    return [dict(title=title, subtitle=subtitle, valid=False, icon=icon)]",
        "sha1": "16fba63910d24dbf639dacc991f919ddf3d03b1c",
        "id": 445897
    },
    {
        "content": "import re\n\n\ndef delete_whitespace(text: str) -> str:\n    \"\"\"\n    Removes whitespaces from text.\n    \"\"\"\n    return re.sub(r'\\s+', '', text).strip()",
        "sha1": "a4aacdc74ba16fb5072c6fa1a4e2a924fedcdd1b",
        "id": 630925
    },
    {
        "content": "def isStringInList(astring, blist):\n    \"\"\"\n    Returns if `astring` is in `blist` regardless of cases.\n    \"\"\"\n    return astring.upper() in [y.upper() for y in blist]",
        "sha1": "c08a326d74a51f0d999373cd1bff6a04689bc2a0",
        "id": 169633
    },
    {
        "content": "def compute_bb(df, column_source, column_target_bb, time_period, stdev_factor=2):\n    \"\"\"\n    Compute Bollinger Bands (BB) With Simple Moving Average (SMA).\n\n    :param df: dataframe (sorted in ascending time order)\n    :param column_source: name of source column in dataframe  with values to compute SMA (e.g. close price)\n    :param column_target_bb: prefix of target column in dataframe for BB results\n    :param time_period: number of days over which to average\n    :param stdev_factor: standard deviation scaling factor for upper and lower bands\n    :return: modified dataframe\n    \"\"\"\n    # compute BB and add results back to dataframe\n    key_sma = column_target_bb + \"-sma-{:d}-{:d}\".format(time_period, stdev_factor)\n    key_upper_band = column_target_bb + \"-upper-{:d}-{:d}\".format(time_period, stdev_factor)\n    key_lower_band = column_target_bb + \"-lower-{:d}-{:d}\".format(time_period, stdev_factor)\n    df[key_sma] = df[column_source].rolling(window=time_period, min_periods=1).mean()\n    sma_stdev = df[column_source].rolling(window=time_period, min_periods=1).std(ddof=0)\n    df[key_upper_band] = df[key_sma] + (sma_stdev * stdev_factor)\n    df[key_lower_band] = df[key_sma] - (sma_stdev * stdev_factor)\n\n    return df",
        "sha1": "c0d861431ea5acf55ebcd20f6f9d81aee45dbec2",
        "id": 316220
    },
    {
        "content": "from pathlib import Path\n\n\ndef get_scan_input_location(download_location):\n    \"\"\"\n    Returns the input location for the ScanCode process, based on the\n    given `download_location` directory content.\n\n    If the `download_location` contains only a single downloaded file,\n    the given `download_location` is returned.\n\n    If the`download_location` contains the downloaded file and its related\n    \"-extract\" directory, that directory is returned as the scan input location.\n\n    All other cases returns False.\n    \"\"\"\n    path = Path(download_location)\n    if not path.is_dir():\n        return False\n\n    directory_content_len = len(list(path.iterdir()))\n    extract_directory = list(path.glob(\"*-extract\"))\n\n    if directory_content_len == 1:\n        return download_location\n\n    elif directory_content_len == 2 and extract_directory:\n        return extract_directory[0]\n\n    return False",
        "sha1": "793feb1fb5aa5bedd6cc2caf6347219979f5f6e1",
        "id": 387485
    },
    {
        "content": "def relist_streamlines(points, offsets):\n    \"\"\" Given a representation of a set of streamlines as a large array and\n    an offsets array return the streamlines as a list of shorter arrays.\n\n    Parameters\n    -----------\n    points : array\n    offsets : array\n\n    Returns\n    -------\n    streamlines: sequence\n    \"\"\"\n\n    streamlines = []\n\n    streamlines.append(points[0: offsets[0]])\n\n    for i in range(len(offsets) - 1):\n        streamlines.append(points[offsets[i]: offsets[i + 1]])\n\n    return streamlines",
        "sha1": "bff394b1a488ec29f0d26bc46a04d6bac7c24908",
        "id": 593495
    },
    {
        "content": "def should_check_integrity(f):\n    \"\"\"Returns True if f should be checked for integrity.\"\"\"\n    return f not in ('README.md', 'TRAINING_LOG', 'checksum.md5', 'data') and not f.startswith('.')",
        "sha1": "1807916ed8744d01288a5047c996efcaa0b6d76f",
        "id": 613760
    },
    {
        "content": "def right_tokens(token):\n    \"\"\"\n    Get all tokens to the right of a given token\n    \"\"\"\n    return [tok\n            for right_tok in token.rights\n            for tok in right_tok.subtree]",
        "sha1": "50b422564b73ca244e8241d4b53bcf98f22f0e4e",
        "id": 74668
    },
    {
        "content": "def turn(vx=1.0, vy=0.0, wz=0.5, t=16):\n    \"\"\"Generate a trajectory where the robot takes a right turn.\"\"\"\n    time_points = range(t)\n    speed_points = (\n        # One side\n        (0, 0, 0, 0),\n        (vx, 0, 0, 0),\n        (vx, 0, 0, 0),\n        (0, 0, 0, wz),\n        (0, 0, 0, wz),\n        (0, 0, 0, wz),\n        (vx, 0, 0, 0),\n        (vx, 0, 0, 0),\n        # Other side\n        (vx, 0, 0, 0),\n        (vx, 0, 0, 0),\n        (vx, 0, 0, 0),\n        (vx, 0, 0, 0),\n        (vx, 0, 0, 0),\n        (vx, 0, 0, 0),\n        (vx, 0, 0, 0),\n        (vx, 0, 0, 0))\n    return time_points, speed_points",
        "sha1": "ab3cb54f738bab158ddee08f804d1b251e30350c",
        "id": 486820
    },
    {
        "content": "from typing import Dict\n\n\ndef bigint_func(x: str, _: Dict) -> bool:\n    \"\"\"Tests if the string is a valid bigint\"\"\"\n    if x == \"\":\n        return True\n    x = x.rstrip(\"0\").rstrip(\".\")\n    # rstrip would take 1.1.0.0 -> 1.1, so we do it in two steps. Technically,\n    # this would take 1234..0 -> 1234, but that's a problem for future me\n    # The solution is to use removesuffix(\".\"), but it was only added in 3.9 :(\n    try:\n        y = int(x)\n        assert -9223372036854775808 <= y <= 9223372036854775807\n        return True\n    except:  # noqa\n        return False",
        "sha1": "bb8fe806ea00d17624cb9b1965601f0fdfd12fa6",
        "id": 513197
    },
    {
        "content": "def ppi2ppm(val):\n    \"\"\"convert pixels per inch to pixels per mm\"\"\"\n    return val/25.4",
        "sha1": "df526f71fc99426e3122c2598180def565c520c9",
        "id": 394038
    },
    {
        "content": "def try_import(mod_name, attr_name=None, attr_default=None):\n    \"\"\"Try to import a module and get an attribute from it.\n\n    Parameters\n    ----------\n    mod_name : str\n        Module name.\n    attr_name : str, optional\n        Name of an attribute in the module.\n    attr_default : optional\n        Default value if the attribute does not exist in the module.\n\n    Returns\n    -------\n    A pair indicating whether the module could be imported and the module or\n    object or attribute value.\n    \"\"\"\n    try:\n        module = __import__(mod_name, globals(), locals(), [\"__package__\"])\n        if attr_name is None:\n            return True, module\n        return True, getattr(module, str(attr_name), attr_default)\n    except ImportError:\n        return False, None",
        "sha1": "9523ac58174c1494eb8803ae075bb77061de746e",
        "id": 578376
    },
    {
        "content": "def is_full_section(section):\n    \"\"\"Is this section affected by \"config.py full\" and friends?\"\"\"\n    return section.endswith('support') or section.endswith('modules')",
        "sha1": "bf11103b876432ac1d6eefc6884bf81647c8ca41",
        "id": 658590
    },
    {
        "content": "def disectTheResults(output):\n    \"\"\"Disect given output line by line.\n\n    :param output: Text to be disected\n    :type output: string\n    :return: Return of the Python script\n    :rtype: A list of strings where each element represents a line of the\n      total return\n    \"\"\"\n    outputs = [item if output\n               else 'Nothing to put out!'\n               for item in output.rstrip('\\n').split('\\n')\n               ]\n\n    return outputs",
        "sha1": "75fbbc8d5f06f70c8d695800c1f289df6c2444aa",
        "id": 86380
    },
    {
        "content": "def literal(data: str) -> str:\n    \"\"\"Replaces ``\\\\n``, ``\\\\r`` and ``\\\\t`` in a string, with the real literal newline, carraige return, and tab characters.\"\"\"\n    return str(data).replace(\"\\\\n\", \"\\n\").replace(\"\\\\r\", \"\\r\").replace(\"\\\\t\", \"\\t\")",
        "sha1": "6ebbaf2dfb262719a5d2d5050395924f7b605e96",
        "id": 475655
    },
    {
        "content": "import re\n\n\ndef _mk_version_tuple(version):\n    \"\"\"Create a version tuple as required by a version info file.\n\n    Note: non-numeric parts of the version are stripped entirely and\n          empty numbers are replaced by zero's (1..1 -> 1.0.1)\n\n    Args:\n        version: a version string, e.g. '1', '1.2', '1.2.3', '1.2.dev3'\n\n    Returns:\n        tuple with a cleaned up 3 digit version and a string with a\n        4 digit version 'tuple', e.g.:\n            '1'        -> ('1.0.0', '(1, 0, 0, 0)')\n            '1.2'      -> ('1.2.0', '(1, 2, 0, 0)')\n            '1.2.3'    -> ('1.2.3', '(1, 2, 3, 0)')\n            '1.2.dev3' -> ('1.2.3', '(1, 2, 3, 0)')\n            '1.2.3.4'  -> ('1.2.3', '(1, 2, 3, 4)')\n            '1..3.4'   -> ('1.0.3', '(1, 0, 3, 4)')\n    \"\"\"\n    version = [re.sub(\"[^0-9]\", \"\", n) for n in version.split(\".\")[:4]]\n    version = [int(n) if n else 0 for n in version]\n    if len(version) < 4:\n        version.extend([0] * (4 - len(version)))\n    version_text = \".\".join([str(n) for n in version[:3]])\n    version_tuple = \", \".join([str(n) for n in version])\n    return version_text, f\"({version_tuple})\"",
        "sha1": "8c8fc95376ced35e2fbcddb67de26b3e03cb7a47",
        "id": 596410
    },
    {
        "content": "def extended_partition(outcomes, indices, part, ctr):\n    \"\"\"\n    Expand a partition over a subset of outcome indices to the entire outcome.\n\n    Parameters\n    ----------\n    outcomes : list\n        List of outcomes.\n    indices : [int]\n        A list of indices corresponding to which indices of each outcome an\n        element of `part` corresponds to.\n    part : frozenset[frozenset]\n        A partition on some subset of indices of the outcomes.\n    ctr : func\n        The constructor to build sub-outcomes from.\n\n    Returns\n    -------\n    partition : frozenset[frozenset]\n        The expanded partition.\n    \"\"\"\n    return frozenset([frozenset([o for o in outcomes if ctr(o[i] for i in indices) in p]) for p in part])",
        "sha1": "6cd922cb7e10fd9f852fe9849d7866e25bf4d2b5",
        "id": 512954
    },
    {
        "content": "def find_substr_itimes(_str, _substr, i):\n    \"\"\"\n    find the location of the substr appered i times in str\n    :param _str:\n    :param _substr:\n    :param i:\n    :return:\n    \"\"\"\n    count = 0\n    while i > 0:\n        index = _str.find(_substr)\n        if index == -1:\n            return -1\n        else:\n            _str = _str[index+1:]\n            i -= 1\n            count = count + index + 1\n    return count - 1",
        "sha1": "12d28e080b429cb9b8b4eb3c185d66793d31a327",
        "id": 265589
    },
    {
        "content": "def kappa(A: float, B: float, C: float):\n    \"\"\"\n    Calculate Ray's asymmetry parameter for a given set of A, B, and C rotational constants.\n    This parameter determines how asymmetric a molecule is by setting a range between two limits: the prolate (+1)\n    and the oblate (-1) limits.\n\n    Parameters\n    ----------\n    A, B, C: float\n        Rotational constant in MHz for each respective axis\n\n    Returns\n    -------\n    kappa: float\n        Ray's asymmetry parameter\n    \"\"\"\n    return (2 * B - A - C) / (A - C)",
        "sha1": "f2628858582645a43ffbe706d1f838196c6d4f20",
        "id": 45342
    },
    {
        "content": "def smart_split(str, sep, lim, default=\"\"):\n    \"\"\"\n    Split into at least lim elements using default value if needed.\n    \"\"\"\n    s = str.split(sep, lim - 1)\n    if len(s) < lim:\n        s.extend([default] * (lim - len(s)))\n    return s",
        "sha1": "80f440c44e17b32534b3038453063b03a3c5a0d0",
        "id": 377429
    },
    {
        "content": "import copy\n\n\ndef train_qtab(agent, environment, episodes):\n\n    \"\"\"\n    Trains a tabular Q-learning agent.\n\n    Parameters\n    ----------\n    :param agent : AgentQtab instance\n           Tabular Q-learning agent to be trained.\n    :param environment : Env instance\n           Environment instance specifying and simulating the dynamics of\n           the environment the agent is in.\n    :param episodes : int\n           Number of episodes to train the agent.\n\n    Returns\n    -------\n    :returns agent : AgentQtab instance\n             Trained agent.\n    :returns states : list of lists\n             States experienced in each episode.\n    :returns actions : list of lists\n             Actions taken in each episode.\n    :returns rewards : list of lists\n             Rewards received in each episode.\n    :returns new_states : list of lists\n             New states experienced in each episode.\n    :returns pred : list of ndarrays\n             Q-value predictions made during the training.\n    \"\"\"\n\n    print(\"Training tabular Q-learning agent.\")\n\n    agent = copy.deepcopy(agent)\n    env = copy.deepcopy(environment)\n\n    # create lists to save variables:\n    states = [[] for _ in range(episodes)]\n    actions = [[] for _ in range(episodes)]\n    rewards = [[] for _ in range(episodes)]\n    new_states = [[] for _ in range(episodes)]\n    pred = []\n\n    for episode in range(episodes):\n\n        # reset environment to start state:\n        env.reset()\n\n        # increase iteration count in agent:\n        agent.iter += 1\n\n        # train agent:\n        while not env.done:\n\n            s = env.get_state()\n            a = agent.choose_action()\n            r, ss, done, _ = env.take_action(a)\n\n            states[episode].append(s)\n            actions[episode].append(a)\n            rewards[episode].append(r)\n            new_states[episode].append(ss)\n\n            q_tab = agent.update(a, r)\n            pred.append(q_tab)\n\n    return agent, states, actions, rewards, new_states, pred",
        "sha1": "4129af1da19fbe66cb384dc17b22a94616a3a00b",
        "id": 393970
    },
    {
        "content": "import re\n\n\ndef get_prefix(curie):\n    \"\"\"Get prefix from CURIE.\"\"\"\n    match = re.fullmatch(r'([a-zA-Z.]+):\\w+', curie)\n    if match is None:\n        raise ValueError(f'{curie} is not a valid CURIE')\n    return match[1]",
        "sha1": "84e295bcb764a83cc2fb0432caef220589ab94c9",
        "id": 680806
    },
    {
        "content": "import json\n\n\ndef json_to_dict(col):\n    \"\"\"\n    Given a json object as bytes, convert it to a Python dictionary.\n\n    :param col:\n    :type col: bytes\n    :rtype: dict\n    \"\"\"\n    if isinstance(col, dict):\n        return col\n    elif isinstance(col, bytes):\n        col = col.decode(\"utf-8\")\n    return json.loads(col)",
        "sha1": "920a28a6070998d12c176dd27be2ede263c265d0",
        "id": 42298
    },
    {
        "content": "def checkParamsNum(funcName,distribName,runDistribName,distribParams,correctNum):\n    \"\"\"See if the correct number of parameters was supplied.  More than one number may apply\n\n    Args:\n        | funcName (string):  distribution name\n        | distribName (string):  distribution name\n        | distribParams ([float]): list of distribution parameters (see below)\n        | correctNum ([int]): list with the possible numbers of parameters\n\n    Returns:\n        | True if the requirements are matched\n\n    Raises:\n        | No exception is raised.\n    \"\"\"\n\n    proceed = True\n    if not len(distribParams) in correctNum:\n        print('{} Variates Generation:\\n {} {} {} {} {}'.format(distribName, \n            'Wrong number of parameters (run ', funcName, \"('\", runDistribName, \"') for help) \"))\n        proceed = False\n\n    # print(distribParams, correctNum, proceed)\n\n    return proceed",
        "sha1": "b78f2b30dc7182a6828e6f8aa2103da9daf024c3",
        "id": 317844
    },
    {
        "content": "def get_game_id(data, texter_number):\n    \"\"\"\n    >>> get_game_id({'id1':{ 'numbers': ['#1', '#2']},'id2': { 'numbers': [\"#3\", \"#4\"]}}, '#1')\n    'id1'\n    >>> get_game_id({'id1':{ 'numbers': ['#1', '#2']},'id2': { 'numbers': [\"#3\", \"#4\"]}}, '#4')\n    'id2'\n    >>> get_game_id({'id1':{ 'numbers': ['#1', '#2']},'id2': { 'numbers': [\"#3\", \"#4\"]}}, '#5')\n\n    \"\"\"\n    for game_id in data:\n        for phone_number in data[game_id]['numbers']:\n            if phone_number == texter_number:\n                return game_id\n    return None",
        "sha1": "4fee9739a55108696e25d0931690a1b6ccc13e7b",
        "id": 446308
    },
    {
        "content": "def save_csvs(csm, tables_rows):\n    \"\"\" write relational tables to csv files.\n    tables_rows -- dict {table_name: [rows]} of tables of rows to save\"\"\"\n    written = {}\n    for table_name in tables_rows:\n        written[table_name] = csm.write_csv(table_name,\n                                            tables_rows[table_name])\n    return written",
        "sha1": "8a059a0331d7668bcd593521a5b1e6a406b2d251",
        "id": 620813
    },
    {
        "content": "def CollateDeps(deps_content):\n  \"\"\"\n  Take the output of deps_utils.GetDepsContent and return a hash of:\n\n  { submod_name : [ [ submod_os, ... ], submod_url, submod_sha1 ], ... }\n  \"\"\"\n  spliturl = lambda x: list(x.partition('@')[0::2]) if x else [None, None]\n  submods = {}\n  # Non-OS-specific DEPS always override OS-specific deps. This is an interim\n  # hack until there is a better way to handle OS-specific DEPS.\n  for (deps_os, val) in deps_content[1].iteritems():\n    for (dep, url) in val.iteritems():\n      submod_data = submods.setdefault(dep, [[]] + spliturl(url))\n      submod_data[0].append(deps_os)\n  for (dep, url) in deps_content[0].iteritems():\n    submods[dep] = [['all']] + spliturl(url)\n  return submods",
        "sha1": "a3c43e14e45b5478bf2859c2d8ea7b7bac857d99",
        "id": 684113
    },
    {
        "content": "def required(sfx=''):\n    \"\"\" Load the requirements from the requirements.txt file\"\"\"\n    with open(f\"requirements{sfx}.txt\") as f:\n        return [ln.strip() for ln in f.readlines() if not ln.startswith('-') and not ln.startswith('#') and ln.strip() != '']",
        "sha1": "2a6ad70acc1012f0cf7dea21e7752819e0230323",
        "id": 431796
    },
    {
        "content": "def convert_gempak_color(c, style='psc'):\n    \"\"\"Convert GEMPAK color numbers into corresponding Matplotlib colors.\n\n    Takes a sequence of GEMPAK color numbers and turns them into\n    equivalent Matplotlib colors. Various GEMPAK quirks are respected,\n    such as treating negative values as equivalent to 0.\n\n    Parameters\n    ----------\n    c : int or sequence of ints\n        GEMPAK color number(s)\n    style : str, optional\n        The GEMPAK 'device' to use to interpret color numbers. May be 'psc'\n        (the default; best for a white background) or 'xw' (best for a black background).\n\n    Returns\n    -------\n        List of strings of Matplotlib colors, or a single string if only one color requested.\n\n    \"\"\"\n    def normalize(x):\n        \"\"\"Transform input x to an int in range 0 to 31 consistent with GEMPAK color quirks.\"\"\"\n        x = int(x)\n        if x < 0 or x == 101:\n            x = 0\n        else:\n            x %= 32\n        return x\n\n    # Define GEMPAK colors (Matplotlib doesn't appear to like numbered variants)\n    cols = ['white',       # 0/32\n            'black',       # 1\n            'red',         # 2\n            'green',       # 3\n            'blue',        # 4\n            'yellow',      # 5\n            'cyan',        # 6\n            'magenta',     # 7\n            '#CD6839',     # 8 (sienna3)\n            '#FF8247',     # 9 (sienna1)\n            '#FFA54F',     # 10 (tan1)\n            '#FFAEB9',     # 11 (LightPink1)\n            '#FF6A6A',     # 12 (IndianRed1)\n            '#EE2C2C',     # 13 (firebrick2)\n            '#8B0000',     # 14 (red4)\n            '#CD0000',     # 15 (red3)\n            '#EE4000',     # 16 (OrangeRed2)\n            '#FF7F00',     # 17 (DarkOrange1)\n            '#CD8500',     # 18 (orange3)\n            'gold',        # 19\n            '#EEEE00',     # 20 (yellow2)\n            'chartreuse',  # 21\n            '#00CD00',     # 22 (green3)\n            '#008B00',     # 23 (green4)\n            '#104E8B',     # 24 (DodgerBlue4)\n            'DodgerBlue',  # 25\n            '#00B2EE',     # 26 (DeepSkyBlue2)\n            '#00EEEE',     # 27 (cyan2)\n            '#8968CD',     # 28 (MediumPurple3)\n            '#912CEE',     # 29 (purple2)\n            '#8B008B',     # 30 (magenta4)\n            'bisque']      # 31\n\n    if style != 'psc':\n        if style == 'xw':\n            cols[0] = 'black'\n            cols[1] = 'bisque'\n            cols[31] = 'white'\n        else:\n            raise ValueError('Unknown style parameter')\n\n    try:\n        c_list = list(c)\n        res = [cols[normalize(x)] for x in c_list]\n    except TypeError:\n        res = cols[normalize(c)]\n    return res",
        "sha1": "a8308a96941ca410bfb6407a0cba96b873cd124e",
        "id": 137119
    },
    {
        "content": "def nltobr(string):\n    \"\"\"Replace and return string with newlines replaced by <br/>.\"\"\"\n    \n    return string.replace(\"\\r\\n\", \"<br/>\").replace(\"\\r\", \"<br/>\").replace(\"\\n\", \"<br/>\")",
        "sha1": "b7144893154f49a3ddbdc97e07ca97e76b9aacc8",
        "id": 543027
    },
    {
        "content": "def hex2Bin ( hexVal ):\n  \"\"\"\n\n  This function will convert the given hex string to a binary string.\n\n  Parameters\n  ----------\n  hexVal:\n    An hex string without the leading '0x'\n  \n  Returns\n  -------\n  binVal:\n    A binary string without the leading '0b'\n\n  \"\"\"\n  binVal = ''.join( bin( int( val, 16 ) )[ 2: ].zfill( 4 ) for val in hexVal )\n  return binVal",
        "sha1": "9076bfa7ccb98283685342c402392be92c37be6f",
        "id": 253834
    },
    {
        "content": "def extract_category_information(category_tuples, element):\n    \"\"\"\n    Returns a list of category strings or counts extracted from the tuples in the input list.\n    :param category_tuples: list of tuples in the form of (name, count)\n    :param element: 0 for labels and 1 for counts\n    :return: list of strings representing the category names\n    \"\"\"\n\n    if element > 1 or element < 0:\n        print(\"Invalid element value provided to extract category information.\")\n        return None\n\n    return [category[element] for category in category_tuples]",
        "sha1": "21b29c72b4a51d4e17df3fccf4ff1c39ff4e81da",
        "id": 369500
    },
    {
        "content": "def rec_copy(d: dict) -> dict:\n    \"\"\"Local replacement for copy.deepcopy(), as transcrypt cannot import the copy module.\n    We recursively make a copy of a dict.\n    This code can only handle values that are dicts or scaler types\n    \"\"\"\n    newdct = dict()\n    for k, v in d.items():\n        if isinstance(v, dict):\n            newdct[k] = rec_copy(v)\n        else:\n            newdct[k] = v\n    return newdct",
        "sha1": "1f7b9e56c5ae97b1ce042cc00401830dd312e48a",
        "id": 126610
    },
    {
        "content": "def f(x):\n    \"\"\"Compute quadratic function.\"\"\"\n    return x ** 2",
        "sha1": "568f3a53024e7598d3e4fae755fd6541e858ed8c",
        "id": 573474
    },
    {
        "content": "def as_macro_define(m, v):\n    \"\"\"Return as a C preprocessor command line macro definition\"\"\"\n    \n    if v:\n        return '-D%s=%s' % (m, v)\n    return '-D%s' % (m,)",
        "sha1": "3301c7083ec5d16aa87f03d86ca38536e3a45c9a",
        "id": 626482
    },
    {
        "content": "def CheckDoNotSubmitInFiles(input_api, output_api):\n  \"\"\"Checks that the user didn't add 'DO NOT ' + 'SUBMIT' to any files.\"\"\"\n  keyword = 'DO NOT ' + 'SUBMIT'\n  # We want to check every text files, not just source files.\n  for f, line_num, line in input_api.RightHandSideLines(lambda x: x):\n    if keyword in line:\n      text = 'Found ' + keyword + ' in %s, line %s' % (f.LocalPath(), line_num)\n      return [output_api.PresubmitError(text)]\n  return []",
        "sha1": "2c347ef3497335ae0b33128e9a9d60c36cc65017",
        "id": 110838
    },
    {
        "content": "def round2lon(num):\n    \"\"\"Round number to nearest timezone longitude.\"\"\"\n    return 15 * round(num / 15.)",
        "sha1": "4f9416a23de858e1c490df9ed7e0027e681c93a0",
        "id": 439245
    },
    {
        "content": "import json\n\n\ndef is_json(payload):\n    \"\"\"Check if a payload is valid JSON.\"\"\"\n    try:\n        json.loads(payload)\n    except (TypeError, ValueError):\n        return False\n    else:\n        return True",
        "sha1": "a02499ffd0a890fa4697f1002c5deb0fc894cac0",
        "id": 705525
    },
    {
        "content": "import json\n\n\ndef print_r(ind):\n    \"\"\"pretty-print a dict as JSON\"\"\"\n    return json.dumps(ind, indent=4, separators=(',', ': '))",
        "sha1": "cff613e0aad75dce450ad404f07a64fc11718a93",
        "id": 318853
    },
    {
        "content": "def mean_filter(array):\n    \"\"\"\n    a mean filter function\n    :param array: the input array\n    :return: the mean of the array\n    \"\"\"\n    return sum(array) / len(array)",
        "sha1": "79c0ccf3efe6fe7f89d565b00ff0c44f72943608",
        "id": 431868
    },
    {
        "content": "def links_differ(link1, link2):\n    \"\"\"Return whether two links are different\"\"\"\n    differ = True\n    if link1 == link2:\n        differ = False\n    # Handle when url had training slash\n    if link1[0:-1] == link2:\n        differ = False\n    if link2[0:-1] == link1:\n        differ = False\n    return differ",
        "sha1": "3d916882ba9608623f39d0ebbfc8f8fb5f3a67ca",
        "id": 278473
    },
    {
        "content": "def is_point_inside_object(obj, obj_BVHtree, point):\n    \"\"\" Checks whether the given point is inside the given object.\n\n    This only works if the given object is watertight and has correct normals\n\n    :param obj: The object\n    :param obj_BVHtree: A bvh tree of the object\n    :param point: The point to check\n    :return: True, if the point is inside the object\n    \"\"\"\n    # Look for closest point on object\n    nearest, normal, _, _ = obj_BVHtree.find_nearest(point)\n    # Compute direction\n    p2 = nearest - point\n    # Compute dot product between direction and normal vector\n    a = p2.normalized().dot((obj.rotation_euler.to_matrix() @ normal).normalized())\n    return a >= 0.0",
        "sha1": "d838aece6338858fc4d7dd945418bd6db3a48c42",
        "id": 23305
    },
    {
        "content": "def extension(distance, radius):\n    \"\"\"\n    Calculate the extension of the DNA by simply subtracting the radius from\n    the distance.\n    \"\"\"\n    return distance - radius",
        "sha1": "8c0c356e4576b8faae83304a28a1951d5d2bfcf1",
        "id": 149134
    },
    {
        "content": "def find_coord_vars(ncds):\n    \"\"\"\n    Finds all coordinate variables in a dataset.\n\n    A variable with the same name as a dimension is called a coordinate variable.\n    \"\"\"\n    coord_vars = []\n\n    for d in ncds.dimensions:\n        if d in ncds.variables and ncds.variables[d].dimensions == (d,):\n            coord_vars.append(ncds.variables[d])\n\n    return coord_vars",
        "sha1": "fd4d33c2785a3cee894ba7420d0d94a0de852c09",
        "id": 573542
    },
    {
        "content": "def mp_replicate(orccls, x, rngcls, seed):\n    \"\"\"\n    Wrap `Oracle.g` in a top-level function for use with multiprocessing.\n\n    Parameters\n    ----------\n    orccls : Oracle class\n    x : tuple of int\n    rngcls : random.Random class\n    seed : tuple of int\n\n    Returns\n    -------\n    isfeas : bool\n    objvals : tuple of float\n\n    See also\n    --------\n    Oracle.replicate\n    \"\"\"\n\n    rng = rngcls(seed)\n    orc = orccls(rng)\n    orc.set_crnflag(False)\n    isfeas, objvals = orc.g(x, rng)\n    return isfeas, objvals",
        "sha1": "4d046b037d136b3daad58fd71590696f9ebf4a98",
        "id": 237595
    },
    {
        "content": "from io import StringIO\nimport unittest\n\n\ndef run_test_suites_into_buffer(suites):\n    \"\"\"Run a suite of unittests but into a buffer so we can read the result.\"\"\"\n    terminal_output = StringIO()\n    unittest.TextTestRunner(stream=terminal_output).run(suites)\n    return terminal_output.getvalue()",
        "sha1": "8f8f9cd7a0fb5c24e8c841ea645c0ec6e61ac488",
        "id": 327879
    },
    {
        "content": "def clean_text(df, text):\n    \"\"\"\n    Cleans text by replacing unwanted characters with blanks\n    Replaces @ signs with word at\n    Makes all text lowercase\n    \"\"\"\n    # pdb.set_trace()\n    df[text] = df[text].str.replace(r'[^A-Za-z0-9()!?@\\s\\'\\`\\*\\\"\\_\\n\\r\\t]', '', regex=True)\n    df[text] = df[text].str.replace(r'@', 'at', regex=True)\n    df[text] = df[text].replace(r'\\s+|\\\\n', ' ', regex=True)\n    df[text] = df[text].str.lower()\n\n    return df",
        "sha1": "4c8f4b8dae76ef47314ef57b62f51f72ceaacf34",
        "id": 54193
    },
    {
        "content": "import base64\n\n\ndef encode_message(string: str) -> str:\n    \"\"\"\n    Encode string in base64.\n    Args:\n        string (str): String to decode.\n\n    Returns:\n        str: Encoded string.\n\n    \"\"\"\n    message_bytes = string.encode('utf-8')\n\n    return base64.b64encode(message_bytes).decode(\"utf-8\")",
        "sha1": "ff6869b0a6c4781a1c5e53116f771c919d4e1784",
        "id": 293698
    },
    {
        "content": "def deep_get(obj, keys):\n    \"\"\"\n    Recurses through a nested object get a leaf value.\n\n    There are cases where the use of inheritance or polymorphism-- the use of allOf or\n    oneOf keywords-- will cause the obj to be a list. In this case the keys will\n    contain one or more strings containing integers.\n\n    :type obj: list or dict\n    :type keys: list of strings\n    \"\"\"\n    if not keys:\n        return obj\n\n    if isinstance(obj, list):\n        return deep_get(obj[int(keys[0])], keys[1:])\n    else:\n        return deep_get(obj[keys[0]], keys[1:])",
        "sha1": "fd5cc0216eb3e0b40d9a7347ae8506dbddaca5aa",
        "id": 115034
    },
    {
        "content": "def spans_year(start_date, end_date):\n    \"\"\"\n    Determine if dates passed are in the same year.\n\n    Parameters\n    ----------\n    start_date: pandas Timestamp\n    end_date: pandas Timestamp\n\n    Returns\n    -------\n    bool\n    \"\"\"\n    if start_date.year != end_date.year:\n        return True\n    else:\n        return False",
        "sha1": "1eaf1c80dc5fe48b2515cbcffb8a4999416def05",
        "id": 475654
    },
    {
        "content": "def istest(func):\n    \"\"\"Decorator to mark a function or method as a test\n    \"\"\"\n    func.__test__ = True\n    return func",
        "sha1": "0021f4a49a7d394b57874d7efcb615e0553f2556",
        "id": 437383
    },
    {
        "content": "def _mm_to_pts(mm):\n    \"\"\"\n\n    Convert millimeters to points.\n\n    \"\"\"\n    return mm / (25.4 / 72)",
        "sha1": "2daad2ff40ac6097df2bd04510d7d0e28c62eb27",
        "id": 357176
    },
    {
        "content": "import torch\n\n\ndef is_float_or_torch_tensor(x):\n    \"\"\"\n    Return whether input x is a float or a torch.Tensor.\n    \"\"\"\n    return isinstance(x, torch.Tensor) or isinstance(x, float)",
        "sha1": "4e27ba1fcc251c846c138f58ddc0811702609196",
        "id": 107810
    },
    {
        "content": "def get_col(array, column_index):\n    \"\"\"Returns the column of a 2D array.\"\"\"\n\n    return [row[column_index] for row in array]",
        "sha1": "a9788c2b4fb2adf5bed4520b9dabf5ecfc661706",
        "id": 552315
    },
    {
        "content": "def process_connected_devices(results):\n    \"\"\"Process Connected Devices\n\n    Args:\n        results (Element): XML results from firewall\n\n    Returns:\n        devices_dict (dict): A dictionary containing hostnames, IP addresses, and models\n    \"\"\"\n    xml_list = results.findall('./result/devices/entry')\n    devices_dict = {}\n    for device in xml_list:\n        hostname = device.find('./hostname').text\n        serial = device.find('./serial').text\n        ip_address = device.find('./ip-address').text\n        model = device.find('./model').text\n        devices_dict[serial] = {'hostname': hostname, 'ip_address': ip_address, 'model': model}\n    return devices_dict",
        "sha1": "fe037b24c1924958038b1ec649bcd921b7372e97",
        "id": 58523
    },
    {
        "content": "def _HandleSpaces(line):\n  \"\"\"Handles spaces in a line.\n\n  In particular, deals with trailing spaces (which are stripped unless\n  escaped) and escaped spaces throughout the line (which are unescaped).\n\n  Args:\n    line: str, the line\n\n  Returns:\n    str, the line with spaces handled\n  \"\"\"\n  def _Rstrip(line):\n    \"\"\"Strips unescaped trailing spaces.\"\"\"\n    # First, make the string into \"tokens\": a backslash followed by any\n    # character is one token; any other character is a token on its own.\n    tokens = []\n    i = 0\n    while i < len(line):\n      curr = line[i]\n      if curr == '\\\\':\n        if i + 1 >= len(line):\n          tokens.append(curr)\n          break  # Pass through trailing backslash\n        tokens.append(curr + line[i+1])\n        i += 2\n      else:\n        tokens.append(curr)\n        i += 1\n\n    # Then, strip the trailing space tokens.\n    res = []\n    only_seen_spaces = True\n    for curr in reversed(tokens):\n      if only_seen_spaces and curr == ' ':\n        continue\n      only_seen_spaces = False\n      res.append(curr)\n\n    return ''.join(reversed(res))\n\n  def _UnescapeSpaces(line):\n    \"\"\"Unescapes all spaces in a line.\"\"\"\n    return line.replace('\\\\ ', ' ')\n\n  return _UnescapeSpaces(_Rstrip(line))",
        "sha1": "4d16d5bb47399391a3f2e6e56975628b349e9a28",
        "id": 253811
    },
    {
        "content": "def attr_visitor_name(attr_name: str) -> str:\n    \"\"\"\n    Returns the visitor_method name for `attr_name`, e.g.::\n\n        >>> attr_visitor_name('class')\n        'attr_class'\n    \"\"\"\n    # assert re.match(r'\\w+$', node_name)\n    return 'attr_' + attr_name",
        "sha1": "5fb31f9ea9d563ba05b4a80046c0b48ace35e9b5",
        "id": 700471
    },
    {
        "content": "def compact(text, **kw):\n    \"\"\"\n    Compact whitespace in a string and format any keyword arguments into the\n    resulting string. Preserves paragraphs.\n\n    :param text: The text to compact (a string).\n    :param kw: Any keyword arguments to apply using :py:func:`str.format()`.\n    :returns: The compacted, formatted string.\n    \"\"\"\n    return '\\n\\n'.join(' '.join(p.split()) for p in text.split('\\n\\n')).format(**kw)",
        "sha1": "66c095b7cf180dc2db7e83a306e6d9bb8af0cecc",
        "id": 688552
    },
    {
        "content": "def get_coordinate(coordinate, change, limit):\n    \"\"\"Get next coordinate, within 0 and limit.\"\"\"\n    next_coordinate = coordinate + change\n    if next_coordinate < 0:\n        return 0\n    elif next_coordinate >= limit:\n        return limit - 1\n    return next_coordinate",
        "sha1": "1eee6106b31fd36ca2bbbd0111d747413a695f13",
        "id": 581593
    },
    {
        "content": "def bgr2rgb(img):\n    \"\"\"Converts an RGB image to BGR and vice versa\n    \"\"\"\n    return img[..., ::-1]",
        "sha1": "ece41c92d036ccf28b27d18019f0e0fc1b6d315b",
        "id": 26243
    },
    {
        "content": "import click\n\n\ndef _cb_key_val(ctx, param, value):\n    \"\"\"\n    click callback to validate `--opt KEY1=VAL1 --opt KEY2=VAL2` and collect\n    in a dictionary like the one below, which is what the CLI function receives.\n    If no value or `None` is received then an empty dictionary is returned.\n\n        {\n            'KEY1': 'VAL1',\n            'KEY2': 'VAL2'\n        }\n\n    Note: `==VAL` breaks this as `str.split('=', 1)` is used.\n\n    \"\"\"\n    if not value:\n        return {}\n    else:\n        out = {}\n        for pair in value:\n            if \"=\" not in pair:\n                raise click.BadParameter(\n                    \"Invalid syntax for KEY=VAL arg: {}\".format(pair)\n                )\n            else:\n                k, v = pair.split(\"=\", 1)\n                k = k.lower()\n                v = v.lower()\n                out[k] = None if v.lower() in [\"none\", \"null\", \"nil\", \"nada\"] else v\n        return out",
        "sha1": "6f0172de843624338b9b463925c367d23c2e226c",
        "id": 540295
    },
    {
        "content": "def norm_histogram(hist):\n    \"\"\"\n    takes a histogram of counts and creates a histogram of probabilities\n\n    :param hist: list\n    :return: list\n    \"\"\"\n    total_sum = sum(hist)\n    hist_prob = []\n\n    for i in hist:\n        prob = i / total_sum\n        hist_prob.append(prob)\n\n    return(hist_prob)\n    pass",
        "sha1": "89a57af0a0c393cf7429e866c4663ae41ca0a567",
        "id": 380213
    },
    {
        "content": "def contar_caracteres(s):\n    \"\"\"Fun\u00e7\u00e3o que conta os caracteres de uma string\n    ex.:\n    >>> contar_caracteres('ola')\n    {'a': 1, 'l': 1, 'o': 1}\n    >>> contar_caracteres('maca')\n    {'a': 2, 'c': 1, 'm': 1}\n\n    :param s: string a ser contada\n\n    \"\"\"\n\n    caracteres_ordenados = sorted(s)\n    caracter_anterior  = caracteres_ordenados[0]\n    contagem = 1\n    resultado={}\n    for caracter in caracteres_ordenados[1:]:\n        if caracter == caracter_anterior:\n            contagem+= 1\n        else:\n            resultado[caracter_anterior] = contagem\n            caracter_anterior = caracter\n            contagem = 1\n    resultado[caracter_anterior] = contagem\n    return resultado",
        "sha1": "3e9660f7d0c3f0a578131b5c1cc1d0de9bbd6065",
        "id": 175120
    },
    {
        "content": "def _arg(bytes, signed, dvi, _):\n    \"\"\"Read *bytes* bytes, returning the bytes interpreted as a\n    signed integer if *signed* is true, unsigned otherwise.\"\"\"\n    return dvi._arg(bytes, signed)",
        "sha1": "ad5ba774d27a671b9d84428a2e5b13600d5fe516",
        "id": 67979
    },
    {
        "content": "def _make_tqdm_description(average_loss, average_em, average_f1):\n    \"\"\"\n    Build the string to use as the tqdm progress bar description.\n    \"\"\"\n    metrics = {\n        \"Train Loss\": average_loss,\n        \"Train EM\": average_em,\n        \"Train F1\": average_f1\n    }\n    return \", \".join([\"%s: %.3f\" % (name, value) for name, value\n                      in metrics.items()]) + \" ||\"",
        "sha1": "7eeb8a28d8febc956da9757bdcbb601ce6c2d7c2",
        "id": 406999
    },
    {
        "content": "def get_header_with_refresh_token(refresh_token):\n    \"\"\"\n    Returns the header with the user refresh token.\n\n    :param refresh_token: refresh token\n    :type refresh_token: str\n    :return: request header\n    :rtype: dict\n    \"\"\"\n    return {\"Authorization\": f\"Bearer {refresh_token}\"}",
        "sha1": "55c824046253c287c614b5041f8415d932f7eca9",
        "id": 181695
    },
    {
        "content": "def replace_parens(tokens):\n    \"\"\"Replace all instances of -LRB- and -RRB- with actual parentheses.\"\"\"\n    parens_map = {'-LRB-': '(', '-RRB-': ')'}\n    return [parens_map.get(s, s) for s in tokens]",
        "sha1": "b1a4b35ff61855d3a3e3623a73155591913a445c",
        "id": 248484
    },
    {
        "content": "def dt_to_tw(d):\n    \"\"\" datetime object ---> Taskwarrior JSON date. \"\"\"\n    return d.strftime(\"%Y%m%dT%H%M%SZ\")",
        "sha1": "39d3ebbed2292c797c00f96f842ceda0d6d9cc88",
        "id": 190991
    },
    {
        "content": "def safe_gene_name(symbol):\n    \"\"\"Normalize gene symbols for use as file names.\"\"\"\n    return (\n        symbol.replace(\"(\", \"\")\n        .replace(\")\", \"\")\n        .replace(\":\", \"\")\n        .replace(\"&\", \"\")\n        .replace(\"|\", \"\")\n        .replace(\".\", \"\")\n        .replace(\" \", \"\")\n    )",
        "sha1": "015dd5b5c15609c2fe17dda1335e65fbe56a0686",
        "id": 213287
    },
    {
        "content": "def get_country_name_from_iso_code(data_dict, country_code):\n    \"\"\"\n    Returns the Passport-statndard coutnry name from a given country code (fast)\n\n    :param data_dict: Dictionary containing mappy of ISO-2 country codes to country names\n    :param country_code: the country code (two characters capitalized)\n    :return: The country name if found, otherwise the original country code (string)\n    \"\"\"\n    try:\n        country_nm = data_dict[str(country_code)]\n        return str(country_nm)\n    except:\n        return str(country_code)",
        "sha1": "fdaef38431156b5dbe528a2437588a1d549c81ba",
        "id": 66172
    },
    {
        "content": "from functools import reduce\n\n\ndef dimension_list_size(item_list):\n    \"\"\"Compute the total size of an dimension list.\n\n    Parameters\n    ----------\n    item_list : ItemList\n        The Dimension List to compute the size of.\n\n    Returns\n    -------\n    Integer\n        Size of the passed Dimension List\n\n    \"\"\"\n    if item_list.count == 0:\n        return 1\n    elif item_list.count == 1:\n        return item_list[0].size\n    else:\n        return reduce(lambda x, y: x * y, [i.size for i in item_list])",
        "sha1": "638ce765896368f509c8e41ec9d7bae5fbef7ca1",
        "id": 209636
    },
    {
        "content": "def coord2flow(coord1, coord2, b, h, w):\n    \"\"\"\n    Convert flat homogeneous coordinates 1 and 2 to optical flow. \n    Args:\n        coord1: bx3x(h*w)\n        coord2: bx3x(h*w)\n    Output:\n        flow: bx2xhxw, torch.float32\n    \"\"\"\n    coord1 = coord1[:, :2, :] # bx2x(h*w)\n    coord2 = coord2[:, :2, :] # bx2x(h*w)\n    flow = coord2 - coord1\n    flow = flow.reshape(b, 2, h, w)\n    return flow",
        "sha1": "9f8a61d0db518e645307049f39c13792576ecc09",
        "id": 238617
    },
    {
        "content": "def NAMED(n, e):\n    \"\"\"\n    Puts the expression in a named-group\n\n    :param:\n\n     - `n`: the name of the group\n     - `e`: regular expression\n\n    :return: named-group\n    \"\"\"\n    return \"(?P<{n}>{e})\".format(n=n, e=e)",
        "sha1": "6a3b5958e1a2f29007ea37e7382a0333a3b41033",
        "id": 365393
    },
    {
        "content": "import copy\n\n\ndef merge_dict(bot, top):\n    \"\"\"Helper function for merging dict fields over another dict.\n    Merge top dict onto bot dict, so that the returned new dict has the updated top vals.\"\"\"\n    new = copy.deepcopy(bot)\n    for k, v in top.items():\n        new[k] = v\n    return new",
        "sha1": "e52acd0bf1864cf8adb47b8ce85179fb74c25269",
        "id": 667796
    },
    {
        "content": "def sort_places_by_rating(places):\n    \"\"\"\n    Return list of places, sorted by ranking (highest to lowest)\n    :param places: list of places\n    :return: list of places, sorted by rating\n    \"\"\"\n    for place in places:\n        if not place[\"gm_rating\"]:\n            place[\"gm_rating\"] = 1\n    places = sorted(places, key=lambda x: x[\"gm_rating\"], reverse=True)\n    return places",
        "sha1": "f84d8890be1c736423fa554456d15a9055eb647c",
        "id": 530653
    },
    {
        "content": "def key(property_name, key_name):\n    \"\"\"A decorator function for mapping key names to properties.\"\"\"\n\n    def store_key_map(class_reference):\n        \"\"\"Store the key map.\"\"\"\n        if not hasattr(class_reference, \"__deserialize_key_map__\"):\n            setattr(class_reference, \"__deserialize_key_map__\", {})\n\n        class_reference.__deserialize_key_map__[property_name] = key_name\n\n        return class_reference\n\n    return store_key_map",
        "sha1": "761c42b2246e189ac2bdd2c7e13ff8f9d636c29b",
        "id": 184669
    },
    {
        "content": "def index_containing_substring(list_str, substring):\n    \"\"\"For a given list of strings finds the index of the element that contains the\n    substring.\n\n    Parameters\n    ----------\n\n    list_str: list of strings\n    substring: substring\n\n    Returns\n    -------\n\n    index: containing the substring or -1\n\n    \"\"\"\n    for i, s in enumerate(list_str):\n        if substring in s:\n            return i\n    return -1",
        "sha1": "2816899bc56f6b2c305192b23685d3e803b420df",
        "id": 705945
    },
    {
        "content": "def generate_annotation_index(annotations):\n    \"\"\"Turn annotations into annotation index dictionary with\n    image id as key and list of coco-like annotation objects as value.\n\n    :param annotations: coco-like annotations list;\n    :return: annotation index dictionary with image id as key and\n             list of coco-like annotation objects as value;\n    \"\"\"\n    annotation_index = {}\n    for annotation in annotations:\n        image_id = annotation['image_id']\n        if image_id not in annotation_index:\n            annotation_index[image_id] = []\n        annotation_index[image_id].append(annotation)\n    return annotation_index",
        "sha1": "1266938bbe7f015e10b8a338e599f38c61780ab8",
        "id": 249101
    },
    {
        "content": "def get_data(afos):\n    \"\"\"Return the text data for a given afos identifier\"\"\"\n    return (\n        open(f\"../examples/{afos}.txt\", \"rb\")\n        .read()\n        .decode(\"ascii\")\n        .replace(\"\\r\", \"\")\n        .replace(\"\\001\\n\", \"\")\n    )",
        "sha1": "709d68ac7e872fc85af99fc456db63d38c574428",
        "id": 153679
    },
    {
        "content": "import jinja2\n\n\ndef sif(variable, val):\n    \"\"\"\"string if\": `val` if `variable` is defined and truthy, else ''\"\"\"\n    if not jinja2.is_undefined(variable) and variable:\n        return val\n    else:    \n        return \"\"",
        "sha1": "dc64cb59595dcbcf1e050da422b11eb8b28fa799",
        "id": 52550
    },
    {
        "content": "import yaml\n\n\ndef read_yaml(yaml_path):\n    \"\"\"\n    Reads yaml file as dict.\n    \"\"\"\n    with open(yaml_path) as f:\n        yaml_data = yaml.load(f, Loader=yaml.FullLoader)\n\n    return yaml_data",
        "sha1": "dc7c73b19fa6ceef279979c034c5c63b9be59cc8",
        "id": 515679
    },
    {
        "content": "def stage_sep_range(v_ss, f_ss=0.02):\n    \"\"\"Estimate the downrange distance at stage separation.\n\n    Arguments:\n        v_ss (positive scalar): Velocity at stage separation [units: meter second**-1].\n        f_ss (positive scalar): Stage separation downrange distance model coefficient\n            [units: meter**-1 second**2].\n    Returns:\n        scalar: downrange distance at stage separation [units: meter].\n    \"\"\"\n    return f_ss * v_ss**2",
        "sha1": "fcb57388929796438ce648ada986f22bc24c2e4a",
        "id": 380919
    },
    {
        "content": "import re\n\n\ndef sentence_segmenter(paragr):\n    \"\"\"\n    Function to break a string 'paragraph' into a list of sentences based on\n    the following rules:\n\n    1. Look for terminal [.,?,!] followed by a space and [A-Z]\n    2. If ., check against abbreviation list ABBREV_LIST: Get the string\n    between the . and the previous blank, lower-case it, and see if it is in\n    the list. Also check for single-letter initials. If true, continue search\n    for terminal punctuation\n    3. Extend selection to balance (...) and \"...\". Reapply termination rules\n    4. Add to sentlist if the length of the string is between MIN_SENTLENGTH\n    and MAX_SENTLENGTH\n    5. Returns sentlist\n\n    Parameters\n    ----------\n\n    paragr: String.\n            Content that will be split into constituent sentences.\n\n    Returns\n    -------\n\n    sentlist: List.\n                List of sentences.\n\n    \"\"\"\n    # this is relatively high because we are only looking for sentences that\n    # will have subject and object\n    MIN_SENTLENGTH = 100\n    MAX_SENTLENGTH = 512\n\n    # sentence termination pattern used in sentence_segmenter(paragr)\n    terpat = re.compile('[\\.\\?!]\\s+[A-Z\\\"]')\n\n    # source: LbjNerTagger1.11.release/Data/KnownLists/known_title.lst from\n    # University of Illinois with editing\n    ABBREV_LIST = ['mrs.', 'ms.', 'mr.', 'dr.', 'gov.', 'sr.', 'rev.', 'r.n.',\n                   'pres.', 'treas.', 'sect.', 'maj.', 'ph.d.', 'ed. psy.',\n                   'proc.', 'fr.', 'asst.', 'p.f.c.', 'prof.', 'admr.',\n                   'engr.', 'mgr.', 'supt.', 'admin.', 'assoc.', 'voc.',\n                   'hon.', 'm.d.', 'dpty.',  'sec.', 'capt.', 'c.e.o.',\n                   'c.f.o.', 'c.i.o.', 'c.o.o.', 'c.p.a.', 'c.n.a.', 'acct.',\n                   'llc.', 'inc.', 'dir.', 'esq.', 'lt.', 'd.d.', 'ed.',\n                   'revd.', 'psy.d.', 'v.p.',  'senr.', 'gen.', 'prov.',\n                   'cmdr.', 'sgt.', 'sen.', 'col.', 'lieut.', 'cpl.', 'pfc.',\n                   'k.p.h.', 'cent.', 'deg.', 'doz.', 'Fahr.', 'Cel.', 'F.',\n                   'C.', 'K.', 'ft.', 'fur.',  'gal.', 'gr.', 'in.', 'kg.',\n                   'km.', 'kw.', 'l.', 'lat.', 'lb.', 'lb per sq in.', 'long.',\n                   'mg.', 'mm.,, m.p.g.', 'm.p.h.', 'cc.', 'qr.', 'qt.', 'sq.',\n                   't.', 'vol.',  'w.', 'wt.']\n\n    sentlist = []\n    # controls skipping over non-terminal conditions\n    searchstart = 0\n    terloc = terpat.search(paragr)\n    while terloc:\n        isok = True\n        if paragr[terloc.start()] == '.':\n            if (paragr[terloc.start() - 1].isupper() and\n                    paragr[terloc.start() - 2] == ' '):\n                        isok = False      # single initials\n            else:\n                # check abbreviations\n                loc = paragr.rfind(' ', 0, terloc.start() - 1)\n                if loc > 0:\n                    if paragr[loc + 1:terloc.start() + 1].lower() in ABBREV_LIST:\n                        isok = False\n        if paragr[:terloc.start()].count('(') != paragr[:terloc.start()].count(')'):\n            isok = False\n        if paragr[:terloc.start()].count('\"') % 2 != 0:\n            isok = False\n        if isok:\n            if (len(paragr[:terloc.start()]) > MIN_SENTLENGTH and\n                    len(paragr[:terloc.start()]) < MAX_SENTLENGTH):\n                sentlist.append(paragr[:terloc.start() + 2])\n            paragr = paragr[terloc.end() - 1:]\n            searchstart = 0\n        else:\n            searchstart = terloc.start() + 2\n\n        terloc = terpat.search(paragr, searchstart)\n\n    # add final sentence\n    if (len(paragr) > MIN_SENTLENGTH and len(paragr) < MAX_SENTLENGTH):\n        sentlist.append(paragr)\n\n    return sentlist",
        "sha1": "127467d8128ee9e47d325ca20905c508fbd17d18",
        "id": 143479
    },
    {
        "content": "def narrow_seat_position(\n    char: str, lower_char: str, upper_char: str, lower_limit: int, upper_limit: int\n) -> tuple[int, int]:\n    \"\"\"Half the range of the seat position ([lower_limit, upper_limit]) given the value of char.\"\"\"\n\n    half_range = (upper_limit - lower_limit + 1) / 2\n\n    if char == lower_char:\n\n        return lower_limit, int(upper_limit - half_range)\n\n    elif char == upper_char:\n\n        return int(lower_limit + half_range), upper_limit\n\n    else:\n\n        raise ValueError(\n            f\"unexpected char ({char}) with lower_char ({lower_char}) and upper_char ({upper_char})\"\n        )",
        "sha1": "be332a78ac7e6dc2c6d86168723f8098a99e9ec1",
        "id": 578874
    },
    {
        "content": "def count_all_pixels_per_block(clr, supports):\n    \"\"\"\n    Calculate total number of pixels per rectangular block of a contact map\n    defined as a paired-combination of genomic \"support\" regions.\n\n    Parameters\n    ----------\n    clr : cooler.Cooler\n        Input cooler\n    supports : list\n        list of genomic support regions\n\n    Returns\n    -------\n    blocks : dict\n        dictionary with total number of pixels per pair of support regions\n    \"\"\"\n    n = len(supports)\n    x = [clr.extent(region)[1] - clr.extent(region)[0]\n         for region in supports]\n    blocks = {}\n    for i in range(n):\n        for j in range(i + 1, n):\n            blocks[supports[i], supports[j]] = x[i] * x[j]\n    return blocks",
        "sha1": "d97aecf4429e39682f545e7661bb8caffb7d6f47",
        "id": 253987
    },
    {
        "content": "def get_key_2(item):\n    \"\"\"Utility funtion to sort list with second item\"\"\"\n    return item[2]",
        "sha1": "08a277946eca0922f2c65b49b33f65b6318a82a6",
        "id": 241704
    },
    {
        "content": "import base64\n\n\ndef _string_to_base64(string):\n    \"\"\"Encodes string to utf-8 and then base64\"\"\"\n    utf8_encoded = string.encode('utf-8')\n    return base64.urlsafe_b64encode(utf8_encoded)",
        "sha1": "5303ab83fc07154ae8b13589c80dd4debc71ca23",
        "id": 255678
    },
    {
        "content": "def even_or_odd(n):\n    \"\"\"Return a string odd or even for odd or even values of n.\"\"\"\n    if n % 2 == 0:\n        return 'Even'\n    else:\n        return 'Odd'",
        "sha1": "956e071eeb4be5b9ec2851fc1b566ff1e3e2ef98",
        "id": 11256
    },
    {
        "content": "from datetime import datetime\n\n\ndef _date_from_filename (filename):\n    \"\"\"\n    Extract date and time from the filename. Return a datetime object\n    \n    Parameters\n    ----------\n    filename : string\n    The filename must follow this format :\n    XXXX_yyyymmdd_hhmmss.wav\n    with yyyy : year / mm : month / dd: day / hh : hour (24hours) /\n    mm : minutes / ss : seconds\n            \n    Returns\n    -------\n    date : object datetime\n        This object contains the date of creation of the file extracted from\n        the filename postfix. \n    \"\"\"\n    # date by default\n    date = datetime(1900,1,1,0,0,0,0)\n    # test if it is possible to extract the recording date from the filename\n    if filename[-19:-15].isdigit(): \n        yy=int(filename[-19:-15])\n    else:\n        return date\n    if filename[-15:-13].isdigit(): \n        mm=int(filename[-15:-13])\n    else:\n        return date\n    if filename[-13:-11].isdigit(): \n        dd=int(filename[-13:-11])\n    else:\n        return date\n    if filename[-10:-8].isdigit(): \n        HH=int(filename[-10:-8])\n    else:\n        return date\n    if filename[-8:-6].isdigit(): \n        MM=int(filename[-8:-6])\n    else:\n        return date\n    if filename[-6:-4].isdigit(): \n        SS=int(filename[-6:-4])\n    else:\n        return date\n\n    # extract date and time from the filename\n    date = datetime(year=yy, month=mm, day=dd, hour=HH, minute=MM, second=SS, \n                    microsecond=0)\n    \n    return date",
        "sha1": "d027a10c00b0558e5fcf8b70c1fbedd28588d0d9",
        "id": 528069
    },
    {
        "content": "def to_list(sql_results):\n    \"\"\"\n    Transform the results from mysql query (tuple) to list\n    \"\"\"\n    ret = list()\n    for tup in sql_results:\n        if len(tup) == 1:\n            ret.append(tup[0])\n        else:\n            ret.append(list(tup))\n    return ret",
        "sha1": "59d3dd7bac7cae08ccb80fd24924b9a24751280b",
        "id": 572402
    },
    {
        "content": "def get_el_path(el):\n    \"\"\"\n    Returns itself and its ancestors.\n\n    >>> import lxml.html\n    >>> doc = lxml.html.fromstring('<html><body><h1>Hi <em>there</em></h1><p>Bye.</p></body></html>')\n    >>> path = get_el_path(doc.xpath('//em')[0])\n    >>> [p.tag for p in path]\n    ['em', 'h1', 'body', 'html']\n    \"\"\"\n    parents = []\n    cur = el\n    while cur is not None:\n        parents.append(cur)\n        cur = cur.getparent()\n    return parents",
        "sha1": "68ee1ea4009710be962570230c2d8366aca1475a",
        "id": 196078
    },
    {
        "content": "def HHMM_to_dec_deg(HHMM):\n  \"\"\"Converts a string HHMM to decimal hours.\"\"\"\n  return 15*(int(HHMM[0:2])+float(HHMM[2:4])/60.)",
        "sha1": "fb183f3f2935e9d104532285e560a6a4fe14233b",
        "id": 624844
    },
    {
        "content": "def method_name(main_line, smali_file):\n    \"\"\"\n    Returns the first line of the method - method definition - of a method that\n    executes the given line\n\n    :param int main_line: the executed line to get the method definition from\n    :param str smali_file: the file to extract the method definition from\n    :return: a pretty_grep list with the line with the method definition or\n    an empty list if not found\n    \"\"\"\n\n    # prepare vraiables and read content\n    main_line = int(main_line)\n    with open(smali_file, \"r\") as fp:\n        smali = fp.read()\n\n    for i, line in enumerate(reversed(smali.split(\"\\n\")[:main_line])):\n        if \".method \" in line:\n            return [{\n                \"line\": main_line - i,\n                \"details\": line.strip()\n            }]\n\n    return []",
        "sha1": "629cbaf5e3e41bd0e805a92cf27f297cfca7098a",
        "id": 307110
    },
    {
        "content": "def parse_size(image_size):\n    \"\"\"\n    Parses a string like 640x480 and returns width, height.\n    \"\"\"\n    width, height = image_size.split('x')\n    return int(width), int(height)",
        "sha1": "d2cd3a27f999c236c5a8fc18f424e97e5cae0233",
        "id": 128262
    },
    {
        "content": "def armstrong(x: int) -> bool:\n    \"\"\"\n    This function determines whether the number given as input is `Armstrong` or not.\n    If a number passes `Armstrong` test this function returns `True` else `False`\n\n    Args: This function takes exactly one argument.\n\n    `x:int` : x should be an integer for calculating the armstrong number.\n    \"\"\"\n\n    m = x\n    sum = 0\n    temp = x\n    count = 0\n    while x!=0:\n        x//=10\n        count = count+1\n    for i in range(0, count):\n        num =temp%10\n        sum = sum + pow(num,count)\n        temp//=10\n    if sum != m:\n        return False\n    else:\n        return True",
        "sha1": "14194acb15d9b03d9383824f51596532eb2d186a",
        "id": 443022
    },
    {
        "content": "def _remove_new_line_from_usage_patterns(docstr):\n    \"\"\"Removes new line from usage patterns in docopt CLI description.\n    :param str docstr: docopt CLI description.\n    :return: Reformatted docstring ready for docopt consumption.\n    :rtype: :py:class:`str`\n    \"\"\"\n    lines = []\n\n    usage = False\n    for line in docstr.split(\"\\n\"):\n        if line.startswith(\"Usage:\"):\n            usage = True\n        elif line.startswith(\"Options:\"):\n            lines.append(\"\")\n            usage = False\n\n        if usage:\n            if not line:\n                continue\n            else:\n                lines.append(line)\n        else:\n            lines.append(line)\n\n    return \"\\n\".join(lines)",
        "sha1": "99efb5adcbebaf137e91420fee71820d6366fcad",
        "id": 308679
    },
    {
        "content": "from importlib import import_module\n\n\ndef update_settings_from_module(settings, module):\n    \"\"\"Updates a given settings dict from a module denoted by a dotted path.\n    :param dict settings:\n    :param str module:\n    \"\"\"\n\n    settings_module = import_module(module)\n\n    settings_dict = {\n        key: value\n        for key, value in settings_module.__dict__.items()\n        if key.upper() == key and not key.startswith('_')\n    }\n    settings.update(settings_dict)\n\n    return settings",
        "sha1": "0d71ba823cfcd0db3905850708ec73353d5ecf8c",
        "id": 509896
    },
    {
        "content": "def get_tweet_content(tweet, location=False):\n    \"\"\"Return dictionary with data from tweet (a Status object).\"\"\"\n    fields = {}\n    fields['screen_name'] = tweet.user.screen_name\n\n    # get the tweet's text\n    try:  \n        fields['text'] = tweet.extended_tweet.full_text\n    except: \n        fields['text'] = tweet.text\n\n    if location:\n        fields['location'] = tweet.user.location\n\n    return fields",
        "sha1": "3d03bbf54b8294ea2ac63bf21b199bf250af96aa",
        "id": 339678
    },
    {
        "content": "def testCaseHasAttribute(testCase, attributeName):\n    \"\"\"Determine whether a unittest.TestCase has a given attribute.\"\"\"\n    if hasattr(getattr(testCase, testCase._testMethodName), attributeName):\n        return True\n    if hasattr(testCase.__class__, attributeName):\n        return True\n    return False",
        "sha1": "e17f7a0b4150bd259a18b911a7c5ca6db55ecd71",
        "id": 388271
    },
    {
        "content": "def _concat(slice: str) -> str:\n    \"\"\"helper to concatenate each template slice.\"\"\"\n    return \"{}\\n\".format(slice)",
        "sha1": "c6de3de2c184d98b65e8a5b4829d9f16ce398bcf",
        "id": 477948
    },
    {
        "content": "def compose_redis_key(vim_name, identifier, identifier_type=\"vdu\"):\n    \"\"\"Compose the key for redis given vim name and vdu uuid\n\n    Args:\n        vim_name (str): The VIM name\n        identifier (str): The VDU or VNF uuid (NFVI based)\n        identifier_type (str): the identifier type. Default type is vdu. Also vnf is supported.\n\n    Returns:\n        str: the key for redis\n    \"\"\"\n    if identifier_type == \"vnf\":\n        return \"{}:vnf#{}\".format(vim_name.lower(), identifier)\n    else:\n        return \"{}:{}\".format(vim_name.lower(), identifier)",
        "sha1": "e9a03cf9ff704fea8b9cdf75c59695568e366649",
        "id": 709026
    },
    {
        "content": "import requests\n\n\ndef get_github_list(base_url):\n    \"\"\"\n    Helper to traverse paginated results from the GitHub API.\n    Used to retrieve lists of tags and releases.\n    \"\"\"\n    results = []\n    per_page = 100\n    page_number = 1\n    while True:\n        response = requests.get(\"%s?per_page=%s&page=%s\" %\n            (base_url, per_page, page_number)\n        )\n        response_results = response.json()\n        results.extend(response_results)\n        page_number += 1\n        if len(response_results) < per_page:\n            break\n    return results",
        "sha1": "bc10abf6317802091b7f504f7760ff7628cbf2ac",
        "id": 654165
    },
    {
        "content": "import requests\n\n\ndef open_recipe_file(file, recipes_path=None, github_repo='bioconda/bioconda-recipes'):\n    \"\"\"\n    Open a file at a particular location and return contents as string\n    \"\"\"\n    if recipes_path:\n        return open(f'{recipes_path}/{file}').read()\n    else:  # if no clone of the repo is available locally, download from GitHub\n        r = requests.get(f'https://raw.githubusercontent.com/{github_repo}/master/{file}')\n        if r.status_code == 404:\n            raise OSError\n        else:\n            return r.content",
        "sha1": "ce5fc3c054bc937203966459e9981a5befdae40b",
        "id": 10329
    },
    {
        "content": "def batch_matrix(w, batch_size):\n    \"\"\"\n    batch a weight matrix\n    :param w: [M, N]\n    :param batch_size: B\n    :return: [B, M, N]\n    \"\"\"\n    return w.unsqueeze(0).expand(batch_size, -1, -1)",
        "sha1": "c13e3f83cb72b5c4b24166df5ad58697660fc8e4",
        "id": 632652
    },
    {
        "content": "def z_step(step):\n    \"\"\"Takes in a step number and returns the redshift of the simulation.\n    This assumes 500 steps from z=200 to z=0, with spacing in scale factor.\n    \"\"\"\n    a_in = 1./(1.+200)\n    a_fin = 1./(1.)\n    delta = (a_fin-a_in)/500.\n    a = a_in + delta*(step+1.)\n    z = 1./a -1\n    return z",
        "sha1": "1c72d2f2e52e933378fe540cfac018cb34905b3a",
        "id": 92854
    },
    {
        "content": "def interactive_strategy(game) -> int:\n    \"\"\"\n    Return a move for game through interactively asking the user for input.\n    \"\"\"\n    move = input(\"Enter a move: \")\n    return int(move)",
        "sha1": "c2327d241f85ce69997655d0d414cccecd0fc512",
        "id": 270985
    },
    {
        "content": "def getIndex(line: str, word: str):\n    \"\"\"\n    Get the starting and ending index of a word in a given string\n\n    Parameters\n    ----------\n    line : str\\n\n            A string containing the 'word' from which indexes are to be extracted\n            from\n    word : str\\n\n            A string that need to be found in the 'line', and need indexes\n            extracted\n\n    Returns\n    -------\n    tuple\\n\n        A tuple of the form (start, end), where start and end are the starting\n        and ending indexes of the 'word' in the given 'line'\n    \"\"\"\n\n    index = line.find(word)\n\n    if word in ['url']:\n        start = (index + len(word) + 2)\n        quote = line[start - 1]\n        if quote not in ['\\'', '\"']:\n            start = (index + len(word) + 1)\n            quote = line[start - 1]\n            if quote == '(':\n                end = line.find(')', start)\n            else:\n                end = line.find(quote, start)\n        else:\n            end = line.find(quote, start)\n    else:\n        start = (index + len(word) + 2)\n        quote = line[start - 1]\n        end = line.find(quote, start)\n\n    return (start, end)",
        "sha1": "2c7a20c0a769cfa4fc7f0fd4b4d7e9fec143f73e",
        "id": 479803
    },
    {
        "content": "def median(values):\n  \"\"\"Get the median of a list of values\n\n  Args:\n    values (iterable of float): A list of numbers\n\n  Returns:\n    float\n  \"\"\"\n  # Write the median() function\n  midpoint = int(len(values) / 2)\n  if len(values) % 2 == 0:\n    median = (values[midpoint - 1] + values[midpoint]) / 2\n  else:\n    median = values[midpoint]\n\n  return float(median)",
        "sha1": "8450a57455cdd1bc496b1a2bdb310de66d172764",
        "id": 182449
    },
    {
        "content": "def integrate(func, interval=None, rects=100000):\n    \"\"\"\n    Returns the result of the integral from the inclusive\n    interval (a, b) using a Riemann sum approximation\n\n    \"\"\"\n    if interval is None or not isinstance(interval, tuple):\n        interval = eval(input('Interval (a, b): '))\n    a, b = interval\n    if a > b:\n        print('note: the calculated area will be negative')\n    if b - a > rects:\n        rects = b - a\n    area = 0\n    x = a\n    dx = (b - a) / rects\n    for n in range(rects):\n        try:\n            area += func(x) * dx\n        except Exception as e:\n            print('Error:', e)\n        x += dx\n    return area",
        "sha1": "1068e78718c411c151952de27bca5c9b6bb3dcf5",
        "id": 21377
    },
    {
        "content": "def getLastCol(array):\n    \"\"\"Returns the last column of the entered array.\"\"\"\n    col = array[:,len(array)-1:]\n    return col",
        "sha1": "6229b461149ef5c23209f00332ea5fd43b7e80d7",
        "id": 442169
    },
    {
        "content": "def get_all_resources(components):\n  \"\"\"\n    Provides a set of all resources used among all components\n    @ In, components, list, HERON component objects\n    @ Out, resources, list, resources used in case\n  \"\"\"\n  res = set()\n  for comp in components:\n    res.update(comp.get_resources())\n  return res",
        "sha1": "f7c835d4da7cc0f402d6118c686a67e3e993e08c",
        "id": 191028
    },
    {
        "content": "import shlex\n\n\ndef type_shell_split(string):\n    \"\"\"\n    Parse and split shell arguments string into a list of shell arguments.\n\n    Recognize `,` as a separator as well as white spaces.\n    string: -BAR=\"foo bar\" -BAZ='foo,bar',-QUX 42\n    into\n    ['-BAR=foo bar', '-BAZ=foo,bar', \"-QUX\", \"42\"]\n    \"\"\"\n    lex = shlex.shlex(string, posix=True)\n    lex.whitespace_split = True\n    lex.whitespace += ','\n    return list(lex)",
        "sha1": "863ca083872906e5a9e39dec82cac2d9dc53fb64",
        "id": 343095
    },
    {
        "content": "from typing import Any\nimport textwrap\n\n\ndef indent(text: str, num_spaces: int, **tw_args: Any) -> str:\n    \"\"\"Wrapper around textwrap.indent\n\n    The `tw_args` are passed on to textwrap.indent.\n\n    Args:\n        text:        Text that will be indented.\n        num_spaces:  Number of spaces that will be used for indentation.\n\n    Returns:\n        Indented string.\n    \"\"\"\n    tw_args[\"prefix\"] = \" \" * num_spaces\n    return textwrap.indent(text, **tw_args)",
        "sha1": "95af5bc85726656ea0f645b6efbe028553cb94a7",
        "id": 246987
    },
    {
        "content": "def procdict(fname):\n    \"\"\"Returns a dictionary of key-values from the given file.\n    These are in `/proc` format::\n        key:[\\t]*value\n    \"\"\"\n    d = dict(l.strip().split(':', 1) for l in open(fname))\n    for k in d:\n        d[k] = d[k].strip()\n    return d",
        "sha1": "5f2fa87cd8a4be5a9c34fe0c89766b4db4867b49",
        "id": 119462
    },
    {
        "content": "def sensitivity_metric(event_id_1, event_id_2):\n    \"\"\"Determine similarity between two epochs, given their event ids.\"\"\"\n    if event_id_1 == 1 and event_id_2 == 1:\n        return 0  # Completely similar\n    if event_id_1 == 2 and event_id_2 == 2:\n        return 0.5  # Somewhat similar\n    elif event_id_1 == 1 and event_id_2 == 2:\n        return 0.5  # Somewhat similar\n    elif event_id_1 == 2 and event_id_1 == 1:\n        return 0.5  # Somewhat similar\n    else:\n        return 1",
        "sha1": "b04c5fa27ef655dd3f371c3ce6ef0410c55dd05b",
        "id": 3309
    },
    {
        "content": "def cxxs(q, ip):\n    \"\"\"\n    Single charge exchange cross section according to the Mueller Salzborn formula\n\n    Parameters\n    ----------\n    q : int\n        Charge state of the colliding ion\n    ip : float\n        <eV>\n        Ionisation potential of the collision partner (neutral gas)\n\n    Returns\n    -------\n    float\n        <m^2>\n        Charge exchange cross section\n    \"\"\"\n    return 1.43e-16 * q**1.17 * ip**-2.76",
        "sha1": "ea18f5afbc687d23df392a1f74b075a7e17bdceb",
        "id": 293705
    },
    {
        "content": "def split_dataset(data_df,\n                  set_col='set',\n                  train_set_name='training',\n                  val_set_name='validation',\n                  test_set_name='test'):\n    \"\"\"\n    Splits dataset into\n        (1) `trainval_df`: training + validation set\n        (2) `train_df`: test set\n    based on the value of the column `set_col`, which is then dropped. Also\n    adds a `val_set_name` column to trainval_df indicating which data is to be\n    reserved for validation (as opposed to gradient descent).\n\n    Parameters\n    ----------\n    data_df: (pd.DataFrame)\n        Dataset to split\n\n    set_col: (str)\n        Column of data_df indicating training, validation, or test set\n\n    train_set_name: (str)\n        Value in data_df[set_col] indicating allocation to training set\n\n    val_set_name: (str)\n        Value in data_df[set_col] indicating allocation to validation set\n\n    test_set_name: (str)\n        Value in data_df[set_col] indicating allocation to test set\n\n    Returns\n    -------\n    trainval_df: (pd.DataFrame)\n        Training + validation dataset. Contains a column named `val_set_name`\n        indicating whether a row is allocated to the training or validation\n        set.\n\n    test_df: (pd.DataFrame)\n        Test dataset.\n    \"\"\"\n\n    # Specify training + validation sets\n    trainval_ix = data_df[set_col].isin([train_set_name, val_set_name])\n    trainval_df = data_df[trainval_ix].copy().reset_index(drop=True)\n    trainval_df.insert(loc=0,\n                       column=val_set_name,\n                       value=trainval_df[set_col].eq(val_set_name))\n    trainval_df.drop(columns=set_col, inplace=True)\n\n    # Specify test set\n    test_ix = data_df[set_col].eq(test_set_name)\n    test_df = data_df[test_ix].copy().reset_index(drop=True)\n    test_df.drop(columns=set_col, inplace=True)\n\n    # return\n    return trainval_df, test_df",
        "sha1": "836673d32d4a38e328e287fd5ad4658d80465468",
        "id": 604183
    },
    {
        "content": "def split_cdl_line(line):\n    \"\"\"\n    Splits a line of a \"cdl\" file into fields taking into account \"{\" and \"}\"\n    brackets.\n\n    >>> split_cdl_line(\"define WL_bank 0\")\n    ['define', 'WL_bank', '0']\n    >>> split_cdl_line(\"define_WL_bank 0 -range {0 310}\")\n    ['define_WL_bank', '0', '-range', ['0', '310']]\n    >>> split_cdl_line(\"define abc {10, {20, 30}, {40, 50}}\")\n    ['define', 'abc', ['10,', ['20,', '30'], ',', ['40,', '50']]]\n    \"\"\"\n\n    def split(itr):\n        fields = []\n        temp = \"\"\n\n        # Parse the field\n        while True:\n\n            # Get char\n            try:\n                c = next(itr)\n            except StopIteration:\n                break\n\n            # Got an opening bracket\n            if c == \"{\":\n\n                # Split everything before the bracket\n                temp = temp.strip()\n                if temp:\n                    fields.extend(temp.split())\n                    temp = \"\"\n\n                # Recurse\n                fields.append(split(itr))\n\n            # Got a closing bracket. Terminate\n            elif c == \"}\":\n                break\n\n            # Collect chars\n            else:\n                temp += c\n\n        # Split tailing string\n        temp = temp.strip()\n        if temp:\n            fields.extend(temp.split())\n\n        return fields\n\n    # Split the line\n    line = line.strip()\n    return split(iter(line))",
        "sha1": "a7322f36f33cf2e539428c0752cc16f88ce7ae2c",
        "id": 222813
    },
    {
        "content": "def scopes_to_string(scopes):\n  \"\"\"Converts scope value to a string.\n\n  If scopes is a string then it is simply passed through. If scopes is an\n  iterable then a string is returned that is all the individual scopes\n  concatenated with spaces.\n\n  Args:\n    scopes: string or iterable of strings, the scopes.\n\n  Returns:\n    The scopes formatted as a single string.\n  \"\"\"\n  if isinstance(scopes, str):\n    return scopes\n  else:\n    return ' '.join(scopes)",
        "sha1": "e2056df05a6bca2550bf6b35cb77f9baaf882fb1",
        "id": 190257
    },
    {
        "content": "import math\n\n\ndef calc_avg_deg_rate(mean_concentration, half_life):\n    \"\"\" Calculate the average degradation rate of a species over a cell cycle\n\n        Args:\n            mean_concentration (:obj:`float`): species mean concentration\n            half_life (:obj:`float`): species half life\n\n        Returns:\n            :obj:`float`: the average degradation rate of the species\n    \"\"\"\n    ave_degradation_rate = math.log(2) / half_life * mean_concentration\n\n    return ave_degradation_rate",
        "sha1": "c4cb04755c19f0e3768c9b6a9357db6c25905cfb",
        "id": 402483
    },
    {
        "content": "def f(value):\n    \"\"\"Format a float value to have 4 digits after the decimal point\"\"\"\n    return \"{0:.4f}\".format(value)",
        "sha1": "1c4107a8f9c6a3b8e450a0c45647dc51816160ba",
        "id": 195667
    },
    {
        "content": "def mediana(valores):\n    \"\"\"\n    Encontra a mediana de 'valores', isto \u00e9, o valor que ocupa a posi\u00e7\u00e3o\n    central da lista ordenada. Quando a lista tem tamanho par,\n    definimos a mediana como o valor da primeira posi\u00e7\u00e3o na segunda\n    metada da lista ordenada.\n\n    Par\u00e2metros: lista de floats.\n    Retorna: a mediana de 'valores'.\n    \"\"\"\n\n    valores.sort()\n\n    if (len(valores) % 2 == 1):\n        mediana = valores[int(((len(valores)+1) / 2) - 1)] \n    else:\n        mediana = valores[int((len(valores) / 2))] \n\n    return mediana",
        "sha1": "01897bf8cecf7292c3d02148e4fb8f1f8cd44195",
        "id": 539713
    },
    {
        "content": "def calc_mean_std(feat, eps=1e-5):\n    \"\"\"Calculate mean and std for adaptive_instance_normalization.\n\n    Args:\n        feat (Tensor): 4D tensor.\n        eps (float): A small value added to the variance to avoid\n            divide-by-zero. Default: 1e-5.\n    \"\"\"\n    size = feat.size()\n    assert len(size) == 4, 'The input feature should be 4D tensor.'\n    n, c = size[:2]\n    feat_var = feat.view(n, c, -1).var(dim=2) + eps\n    feat_std = feat_var.sqrt().view(n, c, 1, 1)\n    feat_mean = feat.view(n, c, -1).mean(dim=2).view(n, c, 1, 1)\n    return feat_mean, feat_std",
        "sha1": "a72daff9b7f4a718127eaf0da18e6206797ced27",
        "id": 379108
    },
    {
        "content": "import time\n\n\ndef datetime_format(epoch):\n    \"\"\"\n    Convert a unix epoch in a formatted date/time string\n    \"\"\"\n    datetime_fmt = '%Y-%m-%dT%H:%M:%SZ'\n    return time.strftime(datetime_fmt, time.gmtime(epoch))",
        "sha1": "e45f7874bebdbe99a1e17e5eb41c5c92e15a96b3",
        "id": 27859
    },
    {
        "content": "def rk4(rhs, initial, t_initial, t_final, dt):\n    \"\"\"RK4 integrator.\n\n    Inputs:\n        - rhs: a callable that takes arguments (t, y)\n        - initial: initial value\n        - t_initial: initial time\n        - t_final: final time\n        - dt: step size\n\n    Returns:\n        The solution computed at the final time.\n    \"\"\"\n    t = t_initial\n    sol = initial\n\n    while t < t_final:\n        dt = min(dt, t_final - t)\n        s0 = rhs(t, sol)\n        s1 = rhs(t + dt/2, sol + dt/2 * s0)\n        s2 = rhs(t + dt/2, sol + dt/2 * s1)\n        s3 = rhs(t + dt, sol + dt * s2)\n        sol = sol + dt / 6 * (s0 + 2 * s1 + 2 * s2 + s3)\n        t += dt\n\n    return sol",
        "sha1": "ae010db5a3a81f201340f4c21207538764bc9cf6",
        "id": 538126
    },
    {
        "content": "def _get_response_status(response) -> int:\n    \"\"\"Get the HTTP status code from any type of response object.\"\"\"\n    if hasattr(response, \"status\"):\n        # aiohttp-style\n        return response.status\n    elif hasattr(response, \"status_code\"):\n        # starlette-style\n        return response.status_code\n    raise TypeError(f\"Don't know how to find the path for {type(response)}\")",
        "sha1": "1a9286db6277601240545e36c4a51536555a83d0",
        "id": 30669
    },
    {
        "content": "def create_events_dict(config):\n    \"\"\"Create nested events dictionary to count terminal states for each different scenario from config.py\n\n    :param config: CrowdSim config\n    :type config: Config\n    :return: dictionary containing all events as keys\n    :rtype: dict of dicts\n    \"\"\"\n    # breakdown cases according to scenarios\n    num_events = {\n        \"success\": {},\n        \"collision\": {},\n        \"timeout\": {},\n    }\n    # create a set of unique keys\n    scenarios = set(config.sim.train_val_sim).union(set(config.sim.test_sim))\n    for key in num_events.keys():\n        num_events[key][\"total\"] = 0\n        for scenario in scenarios:\n            num_events[key][scenario] = 0\n    return num_events",
        "sha1": "b50bbfa9aab1de911bfe6e07cf89da1561f3a5ae",
        "id": 196891
    },
    {
        "content": "import glob\n\n\ndef _get_json_file(dirpath):\n    \"\"\"Returns the first .json filename it encounters on the passed directory\n    path.\n\n    :param dirpath: ``str`` name of the directory we're going to search for json\n                    files.\n    :return: the first json filename we encounter.\n    \"\"\"\n    file_list = glob.glob(dirpath + '/*.json')\n    return file_list[0]",
        "sha1": "0db12b294790618b21c548139a543d6690581a54",
        "id": 518215
    },
    {
        "content": "from datetime import datetime\n\n\ndef get_date_string_from_ts(ts: datetime) -> str:\n    \"\"\"Returns the string of a given date\"\"\"\n    return datetime.strftime(ts, \"%Y-%m-%d\")",
        "sha1": "abbcf5ec546af2c3177eac01b0fb6a9517885f90",
        "id": 589163
    },
    {
        "content": "from pathlib import Path\n\n\ndef get_file_format(filepath):\n    \"\"\"Returns file extension without dot\"\"\"\n    return Path(filepath).suffix[1:]",
        "sha1": "65b12c9b0405ba1a59ee76cfce23d3a5e56abeef",
        "id": 329513
    },
    {
        "content": "import inspect\n\n\ndef get_default_args(func):\n    \"\"\"\n    Return dict for parameter name and default value.\n\n    Parameters\n    ----------\n    func : Callable\n        A function to get parameter name and default value.\n\n    Returns\n    -------\n    Dict\n        Parameter name and default value.\n\n    Examples\n    --------\n    >>> def test_func(a: int, b: str = \"c\") -> int:\n    ...     return a+1\n    >>> get_default_args(test_func)\n    {'b': 'c'}\n\n    >>> def test_func2(a: int = 1, b=\"c\") -> int:\n    ...     return a+1\n    >>> get_default_args(test_func2)\n    {'a': 1, 'b': 'c'}\n    \"\"\"\n    signature = inspect.signature(func)\n    return {\n        k: v.default\n        for k, v in signature.parameters.items()\n        if v.default is not inspect.Parameter.empty\n    }",
        "sha1": "dcc75dceae1385868866d668aa021584547190df",
        "id": 707409
    },
    {
        "content": "def getChunkPartition(chunk_id):\n    \"\"\" return partition (if any) for the given chunk id.\n    Parition is encoded in digits after the initial 'c' character.\n    E.g. for:  c56-12345678-1234-1234-1234-1234567890ab_6_4, the\n    partition would be 56.\n    For c-12345678-1234-1234-1234-1234567890ab_6_4, the\n    partition would be None.\n    \"\"\"\n    if not chunk_id or chunk_id[0] != 'c':\n        raise ValueError(\"unexpected chunk id\")\n    n = chunk_id.find('-') # go to first underscore\n    if n == 1:\n        return None  # no partition\n    partition = int(chunk_id[1:n])\n    return partition",
        "sha1": "461ebdd552ad6881a6f4566b0d40399656720fcd",
        "id": 631765
    },
    {
        "content": "def train_val_split(data, train_size=0.9):\n    \"\"\"Splits the data into training and validation set. \n    \n    Parameters\n    ----------\n    data : pandas DataFrame\n        The data to be split\n    train_size : float (default is 0.9)\n        The size of the training set. Size of validation set is 1 - train_size\n        \n    Returns\n    -------\n    pandas DataFrame\n        The training set\n    pandas DataFrame\n        The validation set\n    \"\"\"\n    \n    train = data.iloc[: int(len(data) * train_size), :]\n    val = data.iloc[int(len(data) * train_size) :, :]\n    \n    return train, val",
        "sha1": "f7c9c78a312764d45c2847fd4f1855894078721e",
        "id": 471225
    },
    {
        "content": "def calc_games(win_pcts):\n    \"\"\"\n    Input:\n        win_pcts (list) - list of win percentages based on score for each week\n\n    Returns:\n        games (integer) - number of games played\n    \"\"\"\n    return len(win_pcts)",
        "sha1": "0d61d9ff88f0ffb26bde6595ad777004de0732c1",
        "id": 515462
    },
    {
        "content": "import re\n\n\ndef get_filename_suffix(path):\n    \"\"\"\n    1. file's name; 2. suffix;\n    :param path: video's path, e.g. 'xxx/yyy/zzz.avi'\n    :return:\n        file_name : str = 'zzz'\n        suffix : str = 'avi'\n    \"\"\"\n    suffix = re.split('\\.', path)[-1]\n    video_filename = re.split('\\/', re.split('\\.', path)[0])[-1]\n    return video_filename, suffix",
        "sha1": "660e8420ea000f495dda55b83191a2dae6ccd431",
        "id": 150117
    },
    {
        "content": "def add_delay_columns(data):\n    \"\"\"Compute edge durations and delays.\"\"\"\n    data[\"event_delay\"] = (\n        data[\"event_time_real\"] - data[\"event_time_planned\"]\n    ).dt.total_seconds().astype(\"float32\") / 60\n    data[\"edge_duration_planned\"] = (\n        data[\"next_event_time_planned\"] - data[\"event_time_planned\"]\n    ).dt.total_seconds().astype(\"float32\") / 60\n    data[\"edge_duration_real\"] = (\n        data[\"next_event_time_real\"] - data[\"event_time_real\"]\n    ).dt.total_seconds().astype(\"float32\") / 60\n    data[\"edge_delay\"] = (\n        data[\"edge_duration_real\"] - data[\"edge_duration_planned\"]\n    ).astype(\"float32\")\n\n    return data",
        "sha1": "a9c3d623638a1506bc0b79cf2a356eecd8b20ee5",
        "id": 331562
    },
    {
        "content": "def holdout(p, dataframe):\n    \"\"\"\n    Separate dataframe randomly into training\n    and testing sets\n\n    Parameters:\n        p: percentage of datagrame used for training\n        dataframe: pandas dataframe that should be  divided\n\n    Return:\n        testing_set\n        training_set\n    \"\"\"\n\n\n    train=dataframe.sample(frac=p)\n    test=dataframe.drop(train.index)\n\n    return (train, test)",
        "sha1": "b22c909e28b5d514f8fa6fb60eecadb789c1856e",
        "id": 286391
    },
    {
        "content": "def pause_slicer(samp: int, width: int) -> slice:\n    \"\"\"Returns a slice object which satisfies the range of indexes for a pause\n    point.\n\n    The incoming numbers for samp are 1-based pixel numbers, so must\n    subtract 1 to get a list index.\n    The width values are the number of pixels to affect, including the\n    pause point pixel.  If positive they start with the pause point pixel\n    and count 'up.'  If negative, they start with the pause point pixel\n    and count 'down.'\n    \"\"\"\n    # We don't need to protect for indices less than zero or greater than the\n    # length of the list, because slice objects can take values that would not\n    # be valid for item access.\n    if width > 0:\n        s_start = samp - 1\n        s_stop = s_start + width\n    else:\n        s_start = samp + width\n        s_stop = samp\n    return slice(s_start, s_stop)",
        "sha1": "9c698ebc3f3125017913e92b68c7eece21a905db",
        "id": 301905
    },
    {
        "content": "def getChildCount(elem):\n    \"\"\"\n    Calculate the number of children the given element has.  This loops\n    over all the children of that element and counts them.\n    \"\"\"\n    count = 0\n    child = elem.firstChild\n    while child:\n        if child.nodeType == 1:\n            count += 1\n        child = child.nextSibling\n    return count",
        "sha1": "e7ac84b771d67590730edf0708ef692833115e31",
        "id": 171271
    },
    {
        "content": "def _bonferroni(p_values, alpha):\n    \"\"\"\n    Perform the `Bonferroni correction`_ on a set of p-values and test for significance.\n\n    :param p_values: p-values to adjust for family-wise error rate\n    :type p_values: sequence\n    :param alpha: Confidence level between 0 and 1\n    :type alpha: float\n    :return: Whether or not each result was significant, and corrected p-values.\n    :rtype: (numpy.array[bool], numpy.array[float])\n\n    .. _`Bonferroni correction`: http://en.wikipedia.org/wiki/Bonferroni_correction\n    \"\"\"\n    n = len(p_values)\n    corrected_pvals = p_values*n\n\n    return corrected_pvals < alpha, corrected_pvals",
        "sha1": "9f361ca388fceb5481b26e6e3f589825e6afe996",
        "id": 175189
    },
    {
        "content": "def _receive_all(socket, num_bytes):\n    \"\"\"Reads `num_bytes` bytes from the specified socket.\n\n    :param socket: open socket instance\n    :param num_bytes: number of bytes to read\n\n    :return: received data\n    \"\"\"\n\n    buffer = ''\n    buffer_size = 0\n    bytes_left = num_bytes\n    while buffer_size < num_bytes:\n        data = socket.recv(bytes_left)\n        delta = len(data)\n        buffer_size += delta\n        bytes_left -= delta\n        buffer += data\n    return buffer",
        "sha1": "7177bdd513be965d9a1f36d16c23f53b9f2a2f75",
        "id": 439118
    },
    {
        "content": "import itertools\n\n\ndef construct_pipelines(config):\n    \"\"\" Uses the (parsed) configuration dict to generate a preprocessing pipeline.\n\n    Args:\n        config <dict>: parsed contents of a configuration file.\n\n    Returns:\n        pipelines <list>: a list of nippy preprocessing pipelines.\n    \"\"\"\n\n\n    def _get_argument_combinations(arguments):\n        \"\"\" Utility to function to obtain all permutations of preprocessing arguments. \"\"\"\n        arg_names = sorted(arguments)\n        combinations = itertools.product(*(arguments[arg] for arg in arg_names))\n        combinations = [dict(zip(arg_names, arg_values)) for arg_values in combinations]\n        return combinations\n\n    options = {}\n    for key in config.keys():\n        # 1. Check if we got also_skip\n        if 'also_skip' in config[key] and config[key]['also_skip']:\n            config[key].pop('also_skip')\n            options[key] = _get_argument_combinations(config[key])\n            options[key].append(None)\n        else:\n            options[key] = _get_argument_combinations(config[key])\n\n    return _get_argument_combinations(options)",
        "sha1": "b640226294393f61018c56fdf2d4366b5468a199",
        "id": 135623
    },
    {
        "content": "def digest_lines(digest_input):\n    \"\"\"\n    Read the lines of the Digest file output\n    \"\"\"\n    outlines = []\n    with open(digest_input, 'r') as infile:\n        for line in infile:\n            outlines.append(line)\n    return outlines",
        "sha1": "fe2627af2a15d51f399364bcfd0c0ef68e4973df",
        "id": 34714
    },
    {
        "content": "def update_smag_metadata(col_name):\n    \"\"\"Update SuperMAG metadata\n\n    Parameters\n    -----------\n    col_name : (str)\n        Data column name\n\n    Returns\n    --------\n    col_dict : (dict)\n       Dictionary of strings detailing the units and long-form name of the data\n\n    \"\"\"\n\n    smag_units = {'IAGA': 'none', 'N': 'nT', 'E': 'nT', 'Z': 'nT',\n                  'MLT': 'hours', 'MLAT': 'degrees', 'SZA': 'degrees',\n                  'IGRF_DECL': 'degrees', 'SMU': 'none', 'SML': 'none',\n                  'datetime': 'YYYY-MM-DD HH:MM:SS',\n                  'GEOLON': 'degrees', 'GEOLAT': 'degrees',\n                  'AACGMLON': 'degrees', 'AACGMLAT': 'degrees',\n                  'STATION_NAME': 'none',\n                  'OPERATOR_NUM': 'none', 'OPERATORS': 'none'}\n    smag_name = {'IAGA': 'Station Code',\n                 'N': 'B along local magnetic North',\n                 'E': 'B along local magnetic East',\n                 'Z': 'B vertically downward',\n                 'MLT': 'Magnetic Local Time',\n                 'MLAT': 'Magnetic Latitude',\n                 'SZA': 'Solar Zenith Angle',\n                 'IGRF_DECL': 'IGRF magnetic declination',\n                 'SMU': ' '.join(['Maximum eastward auroral electrojets',\n                                  'strength.\\nUpper envelope of N-component',\n                                  'for stations between 40 and 80 degrees'\n                                  'magnetic north.']),\n                 'SML': ' '.join(['Maximum westward auroral electrojets',\n                                  'strength.\\nLower envelope of N-component',\n                                  'for stations between 40 and 80 degrees',\n                                  'magnetic north.']),\n                 'datetime': 'UT date and time',\n                 'GEOLON': 'geographic longitude',\n                 'GEOLAT': 'geographic latitude',\n                 'AACGMLON': ' '.join(['Altitude-Adjusted Corrected',\n                                       'Geomagnetic longitude']),\n                 'AACGMLAT': ' '.join(['Altitude-Adjusted Corrected',\n                                       'Geomagnetic latitude']),\n                 'STATION_NAME': 'Long form station name',\n                 'OPERATOR_NUM': 'Number of station operators',\n                 'OPERATORS': 'Station operator name(s)', }\n\n    ackn = \"When using this data please include the following reference:\\n\"\n    ackn += \"Gjerloev, J. W., The SuperMAG data processing technique, \"\n    ackn += \"Geophys. Res., 117, A09213, doi:10.1029/2012JA017683, 2012\\n\\n\"\n    ackn += \"For publications and presentations, please include the following\"\n    ackn += \"acknowledgement:\\nFor the ground magnetometer data we gratefully \"\n    ackn += \"acknowledge: Intermagnet; USGS, Jeffrey J. Love; CARISMA, PI Ian \"\n    ackn += \"Mann; CANMOS; The S-RAMP Database, PI K. Yumoto and Dr. K. \"\n    ackn += \"Shiokawa; The SPIDR database; AARI, PI Oleg Troshichev; The \"\n    ackn += \"MACCS program, PI M. Engebretson, Geomagnetism Unit of the \"\n    ackn += \"Geological Survey of Canada; GIMA; MEASURE, UCLA IGPP and Florida\"\n    ackn += \" Institute of Technology; SAMBA, PI Eftyhia Zesta; 210 Chain, PI \"\n    ackn += \"K. Yumoto; SAMNET, PI Farideh Honary; The institutes who maintain\"\n    ackn += \" the IMAGE magnetometer array, PI Eija Tanskanen; PENGUIN; \"\n    ackn += \"AUTUMN, PI Martin Connors; DTU Space, PI Dr. Rico Behlke; South \"\n    ackn += \"Pole and McMurdo Magnetometer, PI's Louis J. Lanzarotti and Alan \"\n    ackn += \"T. Weatherwax; ICESTAR; RAPIDMAG; PENGUIn; British Artarctic \"\n    ackn += \"Survey; McMac, PI Dr. Peter Chi; BGS, PI Dr. Susan Macmillan; \"\n    ackn += \"Pushkov Institute of Terrestrial Magnetism, Ionosphere and Radio \"\n    ackn += \"Wave Propagation (IZMIRAN); GFZ, PI Dr. Juergen Matzka; MFGI, PI \"\n    ackn += \"B. Heilig; IGFPAS, PI J. Reda; University of L\u2019Aquila, PI M. \"\n    ackn += \"Vellante; BCMT, V. Lesur and A. Chambodut; Data obtained in \"\n    ackn += \"cooperation with Geoscience Australia, PI Marina Costelloe; \"\n    ackn += \"SuperMAG, PI Jesper W. Gjerloev.\"\n\n    col_dict = {'units': smag_units[col_name],\n                'long_name': smag_name[col_name],\n                'acknowledgements': ackn}\n\n    return col_dict",
        "sha1": "8d39c0e1f9a719b4e80d72a612fc351be7cfe4d4",
        "id": 355927
    },
    {
        "content": "import re\n\n\ndef extract_regexp_groups(regexp,string):\n    \"\"\"\n    returns all matched groups from string usign given regular expression\n    NB: regexp must be provided with group names (?P<group>) \n    \"\"\"\n    valid = re.match(regexp, string)\n    if valid:\n        return valid.groupdict()",
        "sha1": "aa6cb38eb3eb2e4616ecd85a6a85c2ad0c7e3c36",
        "id": 278452
    },
    {
        "content": "def strip_suffix(string, suffix):\n    \"\"\"Remove a suffix from a string if it exists.\"\"\"\n    if string.endswith(suffix):\n        return string[:-(len(suffix))]\n    return string",
        "sha1": "0ca354328ce8579fcce4f16f4f0dfdeac4708391",
        "id": 41843
    },
    {
        "content": "def convert_list(value, separator=\",\"):\n    \"\"\"Convert a list into a separator-separated string of its items.\n\n    Examples:\n        convert_list([1,2,3])        -> '1,2,3'\n        convert_list([1,2,3], \" \")   -> '1 2 3'\n        convert_list([1,2,3], \", \")  -> '1, 2, 3'\n\n    Args:\n        value (list): list to convert into a string\n        separator (str, optional): list item separator. Defaults to \",\".\n\n    Returns:\n        str: a single string containing all the list items separated by the\n            separator.\n\n    \"\"\"\n    return separator.join([str(item) for item in value])",
        "sha1": "474d02a593c6437133ea28337a2c79f9f99e39b2",
        "id": 360335
    },
    {
        "content": "def listify(text, separator=None):\n    \"\"\"Turn 'text' into a list using 'separator'\"\"\"\n    if isinstance(text, list):\n        return text\n\n    if isinstance(text, (set, tuple)):\n        return list(text)\n\n    if separator:\n        text = text.replace(\"\\n\", separator)\n\n    return [s.strip() for s in text.split(separator) if s.strip()]",
        "sha1": "99cc01a206c79bc0f16668f2c94107f6dea3a07e",
        "id": 596542
    },
    {
        "content": "def make_goal(func):\n    \"\"\"Decorator that turns a function of the form f(Substitution, ...) into\n    a goal-creating function.\n\n    For example:\n        @make_goal\n        def same(s, u, v):\n            ...\n\n    is equivalent to\n        def same(u, v):\n            def goal(s):\n                ...\n            return goal\n    \"\"\"\n\n    def wrap(*args, **kwargs):\n        def goal(s):\n            return func(s, *args, **kwargs)\n\n        return goal\n\n    if func.__doc__ is not None:\n        wrap.__doc__ = \"produce a \" + func.__doc__\n    return wrap",
        "sha1": "d5320d46d45fa749b7f3e6cea173fd41acca29b0",
        "id": 695440
    },
    {
        "content": "def rename_restart_variables(ds, towards_gchp=True):\n    \"\"\"\n    Renames restart variables according to GEOS-Chem Classic and GCHP conventions.\n\n    Args:\n        ds: xarray.Dataset\n            The input dataset\n\n    Keyword Args (optional):\n        towards_gchp: bool\n            Whether renaming to (True) or from (False) GCHP format\n            Default value: True\n\n    Returns:\n        xarray.Dataset\n            Input dataset with variables renamed\n    \"\"\"\n\n    if towards_gchp:\n        old_str = 'SpeciesRst'\n        new_str = 'SPC'\n    else:\n        old_str = 'SPC'\n        new_str = 'SpeciesRst'\n    return ds.rename({name: name.replace(old_str, new_str, 1)\n                      for name in list(ds.data_vars)\n                      if name.startswith(old_str)})",
        "sha1": "68d4424f34ac7009956905e64558022651f0f49e",
        "id": 238144
    },
    {
        "content": "import itertools\n\n\ndef list_all_symbols_for_element(name):\n    \"\"\"\n    Creates list of all valid symbols for an element\n    :param name: Element name\n    :return: List of all valid symbols (unsorted)\n    \"\"\"\n\n    if len(name) < 2:\n        raise ValueError('Element name must be at least 2 chars long')\n\n    result = set()\n    for i, _ in enumerate(name, start=0):\n        for pair in itertools.product(name[:i], name[i:]):\n            result.add(''.join(pair).capitalize())\n    return result",
        "sha1": "5de829985ac14b8e4259cd28e039098235bcf7c3",
        "id": 522145
    },
    {
        "content": "import json\n\n\ndef create_json(dicti):\n    \"\"\" converts given dict into json and returns it as a string\"\"\"\n    json_string = json.dumps(dicti)\n    return json_string",
        "sha1": "727193942dee7fc569c4c9b002b41cbcd873e86b",
        "id": 288762
    },
    {
        "content": "import codecs\nimport re\n\n\ndef clean_readme(fname):\n    \"\"\"Cleanup README.rst for proper PyPI formatting.\"\"\"\n    with codecs.open(fname, 'r', 'utf-8') as f:\n        return ''.join(\n            re.sub(r':\\w+:`([^`]+?)( <[^<>]+>)?`', r'``\\1``', line)\n            for line in f\n            if not (line.startswith('.. currentmodule') or line.startswith('.. toctree'))\n        )",
        "sha1": "d832249a274dd1afb431d0205ddf4d38e5916ca1",
        "id": 472817
    },
    {
        "content": "def parse_blob_connection_str(conn_str):\n    \"\"\"\n    :param conn_str: A Blob Storage connection str\n    :type conn_str: str\n    Returns a dict of the components making up the string\n    \"\"\"\n    conn_str = conn_str.rstrip(\";\")\n    conn_settings = [s.split(\"=\", 1) for s in conn_str.split(\";\")]\n    if any(len(tup) != 2 for tup in conn_settings):\n        raise ValueError(\"Connection string is either blank or malformed.\")\n    return dict(conn_settings)",
        "sha1": "ce546fa53d5544ae55c2e49a73a6ca45bc8480f0",
        "id": 256568
    },
    {
        "content": "import logging\nimport requests\nimport hashlib\n\n\ndef fetch_tarball_sha256(url):\n    \"\"\" Get the sha256 of a tarball \"\"\"\n    logging.info(\"Fetching tarball from {}...\".format(url))\n    response = requests.get(url, stream=True)\n    sha256 = hashlib.sha256()\n    for chunk in response.iter_content(chunk_size=1024 * 1024):\n        sha256.update(chunk)\n    hex_hash = sha256.hexdigest()\n    logging.info(\"Downloaded {} with hash {}\".format(url, hex_hash))\n    return hex_hash",
        "sha1": "71d3a71c1b66a599c13ac29731049d6d39087a08",
        "id": 605608
    },
    {
        "content": "def drop_empty_props(item):\n    \"\"\"Remove properties with empty strings from nested dicts.\n    \"\"\"\n    if isinstance(item, list):\n        return [drop_empty_props(i) for i in item]\n    if isinstance(item, dict):\n        return {\n            k: drop_empty_props(v)\n            for k, v in item.items() if v != ''\n        }\n    return item",
        "sha1": "c427c01dc282b2cf42fa2fa701cdc707142c3088",
        "id": 100420
    },
    {
        "content": "def duplicate_node(graph, node, connectToNeighbours):\n    \"\"\"\n    Duplicates node in a graph, *not* including all ingoing and outgoing edges\n    \n    @type  graph: GraphWrapper\n    @param graph: the graph in which to duplicate the node\n    \n    @type  node: graph_representation.node.Node\n    @param node: node to duplicate\n    \n    @type  connectToNeighbours: boolean\n    @param connectToNeighbours: indicating wheter the duplicated node should be connected to the\n                                neigbours of the original node\n    \n    @rtype  Node\n    @return the duplicated node\n    \"\"\"\n    dupNode = node.copy()\n    dupNode.isDuplicated = True\n    graph.add_node(dupNode)\n    \n    if connectToNeighbours:\n        for curNeighbour in graph.neighbors(node):\n            graph.add_edge((dupNode, curNeighbour), graph.edge_label((node, curNeighbour)))\n    \n    return dupNode",
        "sha1": "5c9ec2d6d855190b2029e4d65dfb41d41b481ee2",
        "id": 154915
    },
    {
        "content": "from typing import OrderedDict\n\n\ndef init_model_attr(model):\n    \"\"\"\n    Initialize pscale attribute of a model object\n    \"\"\"\n    if not hasattr(model, 'pscale'):\n        model.pscale = OrderedDict()\n        for k in model.param_names:\n            model.pscale[k] = 1.\n    else:\n        for k in model.param_names:\n            if k not in model.pscale:\n                model.pscale[k] = 1.\n    \n    return model",
        "sha1": "68c6de4e6c85afaf1774c285d751c7b78784140f",
        "id": 567947
    },
    {
        "content": "def extract_channel_platform(url):\n    \"\"\"Returns last two elements in URL: (channel/platform-arch)\n    \"\"\"\n    parts = [x for x in url.split('/')]\n    result = '/'.join(parts[-2:])\n    return result",
        "sha1": "73c71ed879f07e8eedf2730db174e1cb81177276",
        "id": 37844
    },
    {
        "content": "import re\n\n\ndef fmtlog(txt):\n    \"\"\"\n    Reformat the text of the one-line log as LaTeX.\n\n    Arguments:\n        txt: string to reformat.\n\n    Returns:\n        A LaTeX formatted version of the input.\n    \"\"\"\n    # Replace TeX special characters in the whole text.\n    specials = (\"_\", \"#\", \"%\", r\"\\$\", \"{\", \"}\")\n    for s in specials:\n        txt = re.sub(r\"(?<!\\\\)\" + s, \"\\\\\" + s, txt)\n    # Remove periods at the end of lines.\n    txt = re.sub(r\"\\.$\", \"\", txt, flags=re.MULTILINE)\n    lines = txt.split(\"\\n\")\n    # Remove reference to HEAD\n    lines[0] = re.sub(r\"\\(.*\\) \", \"\", lines[0])\n    # Use typewriter font for the commit id.\n    lines = [r\"\\texttt{\" + re.sub(\" \", r\"} \", ln, count=1) for ln in lines if ln]\n    return \"\\\\\\\\\\n\".join(lines)",
        "sha1": "fbd49446b027c58303edabd60f96978ce19b2c57",
        "id": 34887
    },
    {
        "content": "def bearing_to_cartesian(heading):\n    \"\"\"\n    Bearing (heading from North) to cartesian orientation CCW from east\n\n    :param heading: CW from North, in degrees\n    :type heading: float\n    :returns: Cartesian direction, CCW from East, in degrees\n    :rtype: float\n    \"\"\"\n    return 90 - heading;",
        "sha1": "d583be85d4c7e529e31206c12752310aadce3a3c",
        "id": 42369
    },
    {
        "content": "def diff_identifiers(a, b):\n    \"\"\"Return list of tuples where identifiers in datasets differ.\n\n    Tuple structure:\n    (identifier, present in a, present in b)\n\n    :param a: first :class:`dtoolcore.DataSet`\n    :param b: second :class:`dtoolcore.DataSet`\n    :returns: list of tuples where identifiers in datasets differ\n    \"\"\"\n\n    a_ids = set(a.identifiers)\n    b_ids = set(b.identifiers)\n\n    difference = []\n\n    for i in a_ids.difference(b_ids):\n        difference.append((i, True, False))\n    for i in b_ids.difference(a_ids):\n        difference.append((i, False, True))\n\n    return difference",
        "sha1": "5e08802654ed6da8bfb4f2850a8733c5548f9d75",
        "id": 63317
    },
    {
        "content": "import asyncio\n\n\ndef create_future(loop):\n    \"\"\"Creates a Future object.\n\n    Uses loop.create_future if it is available. Otherwise, create the object directly.\n\n    Use of loop.create_future is preferred because it allows third parties to provide their own\n    Future object, but it is only available in 3.5.2+\n\n    :returns: A new Future object.\n    \"\"\"\n    try:\n        future = loop.create_future()\n    except AttributeError:\n        future = asyncio.Future(loop=loop)\n    return future",
        "sha1": "435174291c790e1e89c05a2b337b9064345df3ef",
        "id": 341761
    },
    {
        "content": "import click\n\n\ndef validate_nonempty(ctx, param, value):\n    \"\"\"Validate parameter is not an empty string.\"\"\"\n    if not value.strip():\n        raise click.BadParameter('value cannot be empty')\n\n    return value",
        "sha1": "a8dd9a81c7fc7b0d0064fe34e849d35c0ce52a04",
        "id": 701660
    },
    {
        "content": "def convert_timedelta(duration):\n    \"\"\"\n    Return time duration as formatted string.\n\n    Args:\n        duration (timedelta): :class:`timedelta` object representing\n            the difference between two :class:`datetime` objects\n\n    Returns:\n        str: string representation of the duration\n    \"\"\"\n    _, seconds = duration.days, duration.seconds\n    hours = seconds // 3600\n    minutes = (seconds % 3600) // 60\n    seconds = (seconds % 60)\n    return '{}h:{}m:{}s'.format(hours, minutes, seconds)",
        "sha1": "07f9834671be54e4f50407e064a275b1c7b7a146",
        "id": 405664
    },
    {
        "content": "from typing import List\n\n\ndef moving_average(values:List[float])->float:\n    \"\"\" A very simple function to average a time-series of values favouring later values \"\"\"\n    avg = values[0]\n    for val in values:\n        avg = avg*0.4 + val*0.6\n    return avg",
        "sha1": "47d36db13de559ea6f9133f7120a651f82c91706",
        "id": 318118
    },
    {
        "content": "def getvector(wordvecs,term):\n    \"\"\"\n    Retrieve the vector for a term\n    Parameters are a pair of lists, wordvecs[0] - the terms, wordvecs[1] - the vectors\n    \"\"\"\n    if term in wordvecs[0]:\n        index = wordvecs[0].index(term)\n        return wordvecs[1][index]\n    else:\n        return None",
        "sha1": "ff01ace1de05cab521651bcad5483ec027275fc4",
        "id": 184040
    },
    {
        "content": "def type_to_str(t, separator=\":\"):\n    \"\"\"convert a type, e.g. a class to its unique module name.\n\n    Constructed by default as:\n        PATH_TO_MODULE:CLASS_NAME\n    \"\"\"\n    return f\"{t.__module__}{separator}{t.__name__}\"",
        "sha1": "e742965ba18e0eae4d5d4ce6aecfc9400893077d",
        "id": 558937
    },
    {
        "content": "def _is_unpoly(self) -> bool:\n    \"\"\"Request is triggered by Unpoly\"\"\"\n    return (\n        'HTTP_X_UP_VERSION' in self.META\n        or 'HTTP_X_UP_MODE' in self.META\n        or 'HTTP_X_UP_TARGET' in self.META\n        or 'HTTP_X_UP_VALIDATE' in self.META\n    )",
        "sha1": "b203678a4fe0cadef120c8cb259d9287418e54d8",
        "id": 632789
    },
    {
        "content": "def get_output_detections_image_file_path(input_file_path, suffix=\"--detections\"):\n    \"\"\"Get the appropriate output image path for a given image input.\n\n    Effectively appends \"--detections\" to the original image file and \n    places it within the same directory.\n\n    Parameters\n    -----------\n    input_file_path: str\n        Path to input image.\n\n    suffix: str\n        Suffix appended to the file.\n        Default: \"--detections\"\n\n    Returns\n    -------\n    str\n        Full path for detections output image.\n    \"\"\"\n    input_file_path = input_file_path.replace('--original.', '.')\n    input_file_paths = input_file_path.split('.')\n    input_file_paths[-2] = input_file_paths[-2]+suffix\n    return '.'.join(input_file_paths)",
        "sha1": "b8d060dff6800750c418c70c61bd4d8e0b7bb416",
        "id": 709366
    },
    {
        "content": "def is_prime(num : int) -> bool:\n    \"\"\"is_prime will return if a number is prime or not\n\n    Args:\n        num (int): the number that wil be checked if it's prime\n\n    Returns:\n        bool: True if the number is prime, false if not\n    \"\"\"\n    if num == 0 or num == 1:\n        return False\n    else:\n        for i in range(2,num):\n            res = num%i\n            if  res == 0:\n                return False\n            else:\n                pass\n        return True",
        "sha1": "ab783953e5a27bb9b97637b0d7ee854c87d60766",
        "id": 258138
    },
    {
        "content": "def cloud_radiative_effect(base, pert):\n    \"\"\"Calculate the cloud radiative effect intended for longwave fluxes.\n\n    Parameters\n    ----------\n    base, pert: dict of array_like\n        CMIP diagnostics required to calculate longwave cloud radiative effect. The\n        dicts should contain two keys:\n\n        rlut    : top-of-atmosphere outgoing longwave flux\n        rlutcs  : top-of-atmosphere longwave flux assuming clear sky\n\n    Returns\n    -------\n    erfari_lw, erfaci_lw : array_like\n        Longwave ERFari and ERFaci estimates.\n    \"\"\"\n    # check all required diagnostics are present\n    check_vars = [\"rlut\", \"rlutcs\"]\n    for var_dict in [base, pert]:\n        for check_var in check_vars:\n            if check_var not in var_dict.keys():\n                raise ValueError(f\"{check_var} not present in {var_dict}\")\n        var_dict[\"rlut\"] = var_dict[\"rlut\"]\n        var_dict[\"rlutcs\"] = var_dict[\"rlutcs\"]\n\n    erf_lw = -pert[\"rlut\"] - (-base[\"rlut\"])\n    erfaci_lw = erf_lw - (-pert[\"rlutcs\"] - (-base[\"rlutcs\"]))\n    erfari_lw = erf_lw - erfaci_lw\n    return erfari_lw, erfaci_lw",
        "sha1": "8d3928eea46e526b65ecdbe9a2c907ba8e1a1fdb",
        "id": 156626
    },
    {
        "content": "def labels_to_onehot(labels, classes):\n\n    \"\"\" Convert a list of labels (integers) into one-hot format.\n\n    Parameters\n    ----------\n    labels : list\n        A list of integer labels, counting from 0.\n    classes : int\n        Number of class integers.\n\n    Returns\n    -------\n    list\n        List of one-hot lists.\n\n    \"\"\"\n\n    for c, i in enumerate(labels):\n        onehot    = [0] * classes\n        onehot[i] = 1\n        labels[c] = onehot\n\n    return labels",
        "sha1": "886f7ccc954ffb86cc97db131b5108484ec40fa3",
        "id": 593208
    },
    {
        "content": "import math\n\n\ndef distant_level(reference_level, distance, reference_distance=1.0):\n    \"\"\"\n    Calculates the sound pressure level\n    in dependence of a distance\n    where a perfect ball-shaped source and spread is assumed.\n\n    reference_level: Sound pressure level in reference distance in dB\n    distance: Distance to calculate sound pressure level for, in meters\n    reference_distance: reference distance in meters (defaults to 1)\n    \"\"\"\n    rel_dist = float(reference_distance) / float(distance)\n    level = float(reference_level) + 20.0 * (math.log(rel_dist) / math.log(10))\n    return level",
        "sha1": "78665b9efac59caaece41cd5147e2d6a7912dece",
        "id": 380164
    },
    {
        "content": "def isChildDir(parent:str, cand:str) -> bool:\n    \"\"\" Returns true if parent is an ancestor of cand. \"\"\"\n    if cand.startswith(parent) and len(cand) > len(parent):\n        return True\n    return False",
        "sha1": "7468d4981ef8d539efd94ee5c6629d5cd95e5b69",
        "id": 72502
    },
    {
        "content": "def _cast_types(args):\n\t\"\"\"\n\tThis method performs casting to all types of inputs passed via cmd.\n\t:param args: argparse.ArgumentParser object.\n\t:return: argparse.ArgumentParser object.\n\t\"\"\"\n\targs.x_val = None if args.x_val == 'None' else int(args.x_val)\n\targs.test_size = float(args.test_size)\n\targs.n_neighbors = int(args.n_neighbors)\n\targs.leaf_size = int(args.leaf_size)\n\targs.p = int(args.p)\n\n\t# metric_params.\n\tif args.metric_params == \"None\" or args.metric_params == 'None':\n\t\targs.metric_params = None\n\n\targs.n_jobs = None if args.n_jobs == 'None' else int(args.n_jobs)\n\t#  --- ------------- --- #\n\treturn args",
        "sha1": "17d2af173c1a43ef42748704a5c1787ab1dcf242",
        "id": 499523
    },
    {
        "content": "import configparser\n\n\ndef get_configuration(config_path):\n    \"\"\"\n    Read .ini config file from given path\n    \"\"\"\n    if isinstance(config_path, configparser.ConfigParser):\n        return config_path\n    ref_path_config = configparser.ConfigParser()\n    ref_path_config.read(config_path)\n\n    total_config = {}\n    for name, section in ref_path_config.items():\n        for k, v in section.items():\n            total_config[k] = v\n    return total_config",
        "sha1": "2d60113897114c92cba9e100c1428554c454e2c4",
        "id": 503749
    },
    {
        "content": "def destagger(var, stagger_dim, meta=False):\n    \"\"\"Return the variable on the unstaggered grid.\n\n    This function destaggers the variable by taking the average of the\n    values located on either side of the grid box.\n\n    Args:\n\n        var (:class:`xarray.DataArray` or :class:`numpy.ndarray`): A variable\n            on a staggered grid.\n\n        stagger_dim (:obj:`int`): The dimension index to destagger.\n            Negative values can be used to choose dimensions referenced\n            from the right hand side (-1 is the rightmost dimension).\n\n        meta (:obj:`bool`, optional): Set to False to disable metadata and\n            return :class:`numpy.ndarray` instead of\n            :class:`xarray.DataArray`.  Default is False.\n\n    Returns:\n\n        :class:`xarray.DataArray` or :class:`numpy.ndarray`:\n        The destaggered variable.  If xarray is enabled and\n        the *meta* parameter is True, then the result will be a\n        :class:`xarray.DataArray` object.  Otherwise, the result will be a\n        :class:`numpy.ndarray` object with no metadata.\n\n    \"\"\"\n    var_shape = var.shape\n    num_dims = var.ndim\n    stagger_dim_size = var_shape[stagger_dim]\n\n    # Dynamically building the range slices to create the appropriate\n    # number of ':'s in the array accessor lists.\n    # For example, for a 3D array, the calculation would be\n    # result = .5 * (var[:,:,0:stagger_dim_size-2]\n    #                    + var[:,:,1:stagger_dim_size-1])\n    # for stagger_dim=2.  So, full slices would be used for dims 0 and 1, but\n    # dim 2 needs the special slice.\n    full_slice = slice(None)\n    slice1 = slice(0, stagger_dim_size - 1, 1)\n    slice2 = slice(1, stagger_dim_size, 1)\n\n    # default to full slices\n    dim_ranges_1 = [full_slice] * num_dims\n    dim_ranges_2 = [full_slice] * num_dims\n\n    # for the stagger dim, insert the appropriate slice range\n    dim_ranges_1[stagger_dim] = slice1\n    dim_ranges_2[stagger_dim] = slice2\n\n    result = .5*(var[tuple(dim_ranges_1)] + var[tuple(dim_ranges_2)])\n\n    return result",
        "sha1": "89bb08618fa8890001f72a43da06ee8b15b328be",
        "id": 706219
    },
    {
        "content": "import re\n\n\ndef _texify(s: str) -> str:\n    \"\"\" Convert python GA operator notation to LaTeX \"\"\"\n    repl_pairs = [\n        (r'\\|', r'\\cdot '),\n        (r'\\^(?!{)', r'\\W '),\n        (r'\\*', ' '),\n        (r'\\brgrad\\b', r'\\bar{\\boldsymbol{\\nabla}} '),\n        (r'\\bgrad\\b', r'\\boldsymbol{\\nabla} '),\n        (r'>>', r' \\times '),\n        (r'<<', r' \\bar{\\times} '),\n        (r'<', r'\\rfloor '),\n        (r'>', r'\\lfloor '),\n    ]\n\n    def repl_func(m):\n        # only one group will be present, use the corresponding match\n        return next(\n            r\n            for (p, r), g in zip(repl_pairs, m.groups())\n            if g is not None\n        )\n    pattern = '|'.join(\"({})\".format(p) for p, _ in repl_pairs)\n    return re.sub(pattern, repl_func, s)",
        "sha1": "44fcbff17e3f4674cae1400a9b1a112417d6aa55",
        "id": 346034
    },
    {
        "content": "def _html_tag(tag, contents, attr_string=''):\n    \"\"\"Wraps 'contents' in an HTML element with an open and closed 'tag', applying the 'attr_string' attributes. \"\"\"\n    return '<' + tag + attr_string + '>' + contents + '</' + tag + '>'",
        "sha1": "d6c08b117a6ec89e8011a4e87db22fc7d1fa0ada",
        "id": 670599
    },
    {
        "content": "def uses_only(w, s):\n    \"\"\"\n    Check if the word consists only of the characters specified in a string.\n    Return False if it does not\n    \"\"\"\n    for l in w:\n        if s.find(l) == -1:\n            return False\n    return True",
        "sha1": "4b9e2b69296f50360cda4a28b38c18b51df95e3d",
        "id": 584249
    },
    {
        "content": "def smart_split(search, string):\n  \"\"\"Split string while maintaining blocks encapsulated by brackets.\"\"\"\n  result = []\n  open_brackets = 0\n  last_split = 0\n  for i, char in enumerate(string):\n    if char == '(':\n      open_brackets += 1\n    elif char == ')':\n      open_brackets -= 1\n    elif open_brackets <= 0 and char == search:\n      result.append(string[last_split:i])\n      last_split = i + 1\n  result.append(string[last_split:])\n  return result",
        "sha1": "464d91a3301e931d30edaa54e92b0fac2585af6f",
        "id": 423335
    },
    {
        "content": "def freq_by_location(d):\n    \"\"\"\n    Takes in a dictionary of novel objects mapped to relative frequencies.\n    Returns a dictionary with frequencies binned by publication location into lists\n\n    list names key:\n\n    - *location_UK* - published in the United Kingdom\n    - *location_US* - published in the US\n    - *location_other* - published somewhere other than the US and England\n\n    :param d: dictionary\n    :return: dictionary\n\n    >>> from gender_analysis import document\n    >>> from pathlib import Path\n    >>> from gender_analysis import common\n    >>> from gender_analysis.analysis.gender_frequency import freq_by_location\n    >>> novel_metadata = {'author': 'Austen, Jane', 'title': 'Persuasion', 'date': '1818',\n    ...                   'country_publication': 'United Kingdom', 'filename':  'austen_persuasion.txt',\n    ...                   'filepath': Path(common.TEST_DATA_PATH, 'sample_novels', 'texts', 'austen_persuasion.txt')}\n    >>> austen = document.Document(novel_metadata)\n    >>> novel_metadata2 = {'author': 'Hawthorne, Nathaniel', 'title': 'Scarlet Letter', 'date': '1900',\n    ...                   'country_publication': 'United States', 'filename':'hawthorne_scarlet.txt',\n    ...                   'filepath': Path(common.TEST_DATA_PATH, 'sample_novels', 'texts', 'hawthorne_scarlet.txt')}\n    >>> scarlet = document.Document(novel_metadata2)\n    >>> d = {scarlet:0.5, austen:0.3}\n    >>> freq_by_location(d)\n    {'United States': [0.5], 'United Kingdom': [0.3]}\n    \"\"\"\n    data = {}\n\n    for k, v in d.items():\n        location = getattr(k, 'country_publication', None)\n        if location is None:\n            continue\n\n        if location not in data:\n            data[location] = []\n        data[location].append(v)\n\n    return data",
        "sha1": "e29cae94c39985e31bdd3b87863254a6d4fbbe60",
        "id": 571069
    },
    {
        "content": "def split(input_file=\"\"):\n    \"\"\"Read the SMILES into a list\"\"\"\n    input_list = []\n    with open(input_file, mode=\"r\") as newfile:\n        for entry in newfile:\n            input_list.append(str(entry).strip())\n\n    return input_list",
        "sha1": "357b52b4faeed98603acc65c817eafd5da7d2ad0",
        "id": 156209
    },
    {
        "content": "def _build_string(tokens):\n    \"\"\"Builds string from token list.\"\"\"\n\n    return ' '.join(tokens)",
        "sha1": "53119717844fb8c0ea0efb1dccc303a4b1b0129b",
        "id": 171344
    },
    {
        "content": "import re\n\n\ndef _tokenizer_pattern_from(tokens):\n    \"\"\" Generates regular expression tokenizer pattern from token list.\"\"\"\n\n    tokens.sort(key=len, reverse=True)\n    regex_str = \"(\" + \"|\".join([re.escape(_) for _ in tokens]) + \")\"\n    return regex_str",
        "sha1": "b99bdcb8467f25fdca8e0a3edbdf5245764e619e",
        "id": 379644
    },
    {
        "content": "def helper_sf_gather(sf_tensor, indeces):\n    \"\"\"\n    Helper function, given a SF tensor and action indeces, return only the\n    SFs associated with the actions\n    :param sf_tensor: SF tensor with action specific dimension of -2, here\n                      with size (batch_n, *, |A|, d)\n    :param indeces: action indeces, size (batch_n, *, 1)\n    :return: action-sf tensor of size (batch_n, *, d)\n    \"\"\"\n    sizes = list(sf_tensor.size())  # list: [batch, *, |A|, d]\n    sizes[-2] = 1  # list: [batch, *, 1, d]\n\n    idxs = indeces.clone().unsqueeze(-1)  # (batch_n, 1, 1)\n    while len(sizes) > len(idxs.size()):\n        idxs = idxs.unsqueeze(-2)  # (batch_n, *(1), 1, 1)\n    idxs = idxs.expand(sizes)  # (batch_n, *, 1, d)\n\n    # gathering. For dim == -2\n    # out[i]...[j][k] = input[i]...[idxs[i]...[j][k]][k]\n    sf_a_tensor = sf_tensor.gather(-2, idxs)  # (batch_n, *, 1, d)\n    return sf_a_tensor.squeeze(-2)",
        "sha1": "05a59b5cc59b48f4e27fa98e2f053d6bef773fce",
        "id": 590494
    },
    {
        "content": "def fix_pc_references(s):\n    \"\"\"Translate references to the current program counter from ca65 to ASM6.\n\nca65 uses * for PC; ASM6 uses $.\nOnly references at the start or end of an expression or of a\nparenthesized subexpression get translated.  But that should be\nenough for our use case, as the source code can use (*) to produce\n($) in the translation.\n\"\"\"\n    if s.startswith('*'):\n        s = '$' + s[1:]\n    if s.endswith('*'):\n        s = '$' + s[1:]\n    return s.replace('(*', '($').replace('*)', '$)')",
        "sha1": "bee0e8bbf130136d72b30fc444bb75dde3c2e0d2",
        "id": 36975
    },
    {
        "content": "def _get_short_satellite_code(platform_code):\n    \"\"\"\n    Get shortened form of satellite, as used in GA Dataset IDs. (eg. 'LS7')\n    :param platform_code:\n    :return:\n\n    >>> _get_short_satellite_code('LANDSAT_8')\n    'LS8'\n    >>> _get_short_satellite_code('LANDSAT_5')\n    'LS5'\n    >>> _get_short_satellite_code('LANDSAT_7')\n    'LS7'\n    >>> _get_short_satellite_code('AQUA')\n    'AQUA'\n    >>> _get_short_satellite_code('TERRA')\n    'TERRA'\n    >>> _get_short_satellite_code('Invalid')\n    Traceback (most recent call last):\n    ...\n    ValueError: Unknown platform code 'Invalid'\n    \"\"\"\n    if platform_code.startswith('LANDSAT_'):\n        return 'LS' + platform_code.split('_')[-1]\n\n    if platform_code in ('AQUA', 'TERRA', 'NPP'):\n        return platform_code\n\n    raise ValueError('Unknown platform code %r' % platform_code)",
        "sha1": "af17634b473db89881fbdcc07de7e3f7256bed5a",
        "id": 307994
    },
    {
        "content": "import requests\n\n\ndef token(code, redirect_uri, client_id, client_secret):\n    \"\"\"Generates access token used to call Spotify API \n\n    Args:\n        code (str): Authorization code used to generate token\n        redirect_uri (str): Allowed redirect URL set in Spotify's developer dashboard\n        client_id (str): Application's id needed for OAuth 2.0 \n        client_secret (str): Application's secret needed for OAuth 2.0 \n\n    Returns:\n        token (str): Access token for Spotify API \n\n    See:\n        https://developer.spotify.com/documentation/general/guides/authorization-guide/\n\n    \"\"\"\n    params = {\n        \"grant_type\":    \"authorization_code\",\n        \"client_id\":    client_id,\n        \"client_secret\": client_secret,\n        \"code\":          code,\n        \"redirect_uri\":  redirect_uri\n    }\n    return requests.post(url='https://accounts.spotify.com/api/token', data=params).json()['access_token']",
        "sha1": "a834a0361972fe0828d11d2ff8a29464a30b34c6",
        "id": 673521
    },
    {
        "content": "import networkx\n\n\ndef walls_to_graph(walls):\n    \"\"\"Return a networkx Graph object given the walls of a maze.\n\n    Parameters\n    ----------\n    walls : list[(x0,y0), (x1,y1), ...]\n         a list of wall coordinates\n\n    Returns\n    -------\n    graph : networkx.Graph\n         a networkx Graph representing the free squares in the maze and their\n         connections. Note that 'free' in this context means that the corresponding\n         square in the maze is not a wall (but can contain food or bots).\n\n    Notes\n    -----\n    Nodes in the graph are (x,y) coordinates representing squares in the maze\n    which are not walls.\n    Edges in the graph are ((x1,y1), (x2,y2)) tuples of coordinates of two\n    adjacent squares. Adjacent means that you can go from one square to one of\n    its adjacent squares by making ore single step (up, down, left, or right).\n    \"\"\"\n    graph = networkx.Graph()\n    extreme = max(walls)\n    width =  extreme[0] + 1\n    height = extreme[1] + 1\n    for x in range(width):\n        for y in range(height):\n            if (x, y) not in walls:\n                # this is a free position, get its neighbors\n                for delta_x, delta_y in ((1,0), (-1,0), (0,1), (0,-1)):\n                    neighbor = (x + delta_x, y + delta_y)\n                    # we don't need to check for getting neighbors out of the maze\n                    # because our mazes are all surrounded by walls, i.e. our\n                    # deltas will not put us out of the maze\n                    if neighbor not in walls:\n                        # this is a genuine neighbor, add an edge in the graph\n                        graph.add_edge((x, y), neighbor)\n    return graph",
        "sha1": "eccfd3d9ca582ee4eafe9cb385c252277b52c3b7",
        "id": 630090
    },
    {
        "content": "from typing import Callable\nimport functools\nfrom typing import Any\nimport warnings\n\n\ndef deprecated(func: Callable) -> Callable:\n    \"\"\"Decorator that can be used to mark functions as deprecated, emitting a warning\n    when the function is used.\n\n    Arguments:\n        func:\n            The function to be decorated.\n\n    Returns:\n        The decorated function, which emits a warning when used.\n    \"\"\"\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs) -> Any:\n        \"\"\"Wrapped function to be returned by the decorator.\n\n        Returns:\n            The original function evaluation.\n        \"\"\"\n        warnings.simplefilter(\"always\", DeprecationWarning)  # turn off filter\n        warnings.warn(\n            \"Deprecated function {} invoked\".format(func.__name__),\n            category=DeprecationWarning,\n            stacklevel=2,\n        )\n        warnings.simplefilter(\"default\", DeprecationWarning)  # reset filter\n        return func(*args, **kwargs)\n\n    return wrapper",
        "sha1": "747df84c3b3b6e8bf8f24fa1e8244fd45d781d6f",
        "id": 682484
    },
    {
        "content": "import math\n\n\ndef _kt_estim_update(new_bit, counts):\n    \"\"\"Computes log(P(Next_bit=new_bit|counts))\n    for the the Krichevski-Trofimov estimator.\n    \"\"\"\n    return math.log((counts[new_bit] + 0.5) / float((sum(counts) + 1)))",
        "sha1": "1d60c70a32ab4195a9acb787aa9f75cc7be72585",
        "id": 91051
    },
    {
        "content": "import itertools\n\n\ndef groupby(iterable, keyfunc):\n    \"\"\"\n    Group iterable by keyfunc\n    :param iterable: iterable obj\n    :param keyfunc: group by keyfunc\n    :return: dict {<keyfunc result>: [item1, item2]}\n    \"\"\"\n    data = sorted(iterable, key=keyfunc)\n    return {\n        k: list(g)\n        for k, g in itertools.groupby(data, keyfunc)\n    }",
        "sha1": "8d695e0a03256fc5042eb75cad613e62c6dea89d",
        "id": 503170
    },
    {
        "content": "def scalar_product(a,b):\n    \"\"\"\n    `scalar_product(a,b)`\n\n    Returns the scalar product of the arrays `a` and `b`.\n    Assumption: `len(a) == len(b)`\n    \"\"\"\n    assert len(a) == len(a), f\"len(a) == len(b)\"\n    # return np.dot(a,b)\n    return sum(a*b)",
        "sha1": "d61b79c24a0df3c95406e5f483b9d8d6027350ac",
        "id": 431482
    },
    {
        "content": "import math\nimport random\n\n\ndef pi(n=100):\n    \"\"\"\n    Approximate pi using stochastic method with n iterations.\n\n    Parameters\n    ----------\n    n : int, Optional, default: 100\n        Number or iterations in stochastic method; must be >= 0\n\n    Returns\n    -------\n    sum : float\n        Resulting approximation of pi\n    \"\"\"\n\n    count = 0\n    for i in range(n):\n        if math.sqrt(random.random()**2 + random.random()**2) < 1:\n            count += 1\n    ratio = count / n  # ratio of in circle to outside circle\n    pi = ratio * 4\n    return (pi)",
        "sha1": "374bfaa3a73b7e6e57acff5feed95c96eef76dd4",
        "id": 412641
    },
    {
        "content": "def intersection(rect1, rect2):\n    \"\"\"\n    Calculates square of intersection of two rectangles\n    rect: list with coords of top-right and left-boom corners [x1,y1,x2,y2]\n    return: square of intersection\n    \"\"\"\n    x_overlap = max(0, min(rect1[2], rect2[2]) - max(rect1[0], rect2[0]));\n    y_overlap = max(0, min(rect1[3], rect2[3]) - max(rect1[1], rect2[1]));\n    overlapArea = x_overlap * y_overlap;\n    return overlapArea",
        "sha1": "6f71bd2dce8c2fb3a238633fe48d5b135022f81c",
        "id": 314992
    },
    {
        "content": "def _library_identifier(*, architectures, environment, platform):\n    \"\"\"Return a unique identifier for an embedded framework to disambiguate it from others.\n\n    Args:\n        architectures: The architectures of the target that was built. For example, `x86_64` or\n            `arm64`.\n        environment: The environment of the target that was built, which corresponds to the\n            toolchain's target triple values as reported by `apple_support.link_multi_arch_binary`.\n            for environment. Typically `device` or `simulator`.\n        platform: The platform of the target that was built, which corresponds to the toolchain's\n            target triple values as reported by `apple_support.link_multi_arch_binary` for platform.\n            For example, `ios`, `macos`, `tvos` or `watchos`.\n\n    Returns:\n        A string that can be used to determine the subfolder this embedded framework will be found\n        in the final XCFramework bundle. This mirrors the formatting for subfolders as given by the\n        xcodebuild -create-xcframework tool.\n    \"\"\"\n    library_identifier = \"{}-{}\".format(platform, \"_\".join(architectures))\n    if environment != \"device\":\n        library_identifier += \"-{}\".format(environment)\n    return library_identifier",
        "sha1": "a62d114789af64d48d39fa606a4bf719774701a1",
        "id": 110276
    },
    {
        "content": "def truncate(value, length):\n    \"\"\"Truncate the value (a string) to the given length.\"\"\"\n    if value is None:\n        return None\n    return value[:length]",
        "sha1": "e3e4b5f37a1fae9970fb6dc0e1ae261112e16f7b",
        "id": 551794
    },
    {
        "content": "import re\n\n\ndef parse_indications(indications, split_keys):\n    \"\"\"\n    Split a string of indications into a list of terms to be queried vs HPO\n    \"\"\"\n\n    split_ind = re.split('[;/]', indications)\n\n    for split_key in split_keys:\n        subsplit = [t.split(split_key) for t in split_ind]\n        split_ind = [item for sublist in subsplit for item in sublist]\n\n    split_ind = [s for s in split_ind if s is not '']\n\n    return split_ind",
        "sha1": "b0adc4c1df3e1e820c22fd5b173d4f669a981bff",
        "id": 213280
    },
    {
        "content": "def extract_data(filename):\n    \"\"\"\n    Extracts the sentences from m2 file.\n    Each sentence is formed into a tuple format (sentence, error_spans)\n    Where error_spans is a list with the error spans in the sentence\n    The error span is a tuple with the form (start_index, end_index, error_type, error_correction)\n    Args:\n        filename: the name of the m2 input file\n\n    Returns:\n        list of sentence tuples of format (sentence, error_spans)\n    \"\"\"\n\n    # open file\n    fce_file = open(filename)\n\n    # read the lines\n    iter_lines = fce_file.readlines()\n\n    # array of tuples - (sentence, [error_spans])\n    data = []\n\n    # packet the error spans with their corresponding sentence\n    for line in iter_lines:\n        # appending the sentence to the data\n        if line[0] == 'S':\n            data.append((line[1:-1], []))\n        elif line[0] == 'A':\n            tokens = line.split(' ')\n            start = int(tokens[1])\n            error_details = tokens[2].split('|||')\n            end = int(error_details[0])\n            error_type = error_details[1]\n            error_replace = error_details[2]\n            data[-1][1].append((start, end, error_type, error_replace))\n    \n    # close file\n    fce_file.close()\n    \n    return data",
        "sha1": "3a38cb769feecbff62b8aacdfb8284d29c36a47e",
        "id": 248326
    },
    {
        "content": "def linear(a, b, x, min_x, max_x):\n    \"\"\"\n    b             ___________\n                /|\n               / |\n    a  _______/  |\n              |  |\n           min_x max_x\n    \"\"\"\n    return a + min(max((x - min_x) / (max_x - min_x), 0), 1) * (b - a)",
        "sha1": "d6bbac95a0a53663395cd4d25beeadbaa5f9f497",
        "id": 30118
    },
    {
        "content": "def is_prime(n):\n    \"\"\"\n    Return True if n is prime, false otherwise.\n\n    >>> is_prime(1)\n    False\n    >>> is_prime(2)\n    True\n    >>> is_prime(19)\n    True\n    \"\"\"\n    if n == 1:\n        return False\n    i = n - 1\n\n    while i > 1:\n        if n % i == 0 or n == 1:\n            return False\n        i -= 1\n    return True",
        "sha1": "baf0d24cae6858b1d7c42b750a4849ba866eb4c6",
        "id": 72471
    },
    {
        "content": "def partition(sample, list_of_samples):\n    \"\"\"\n    Group indices from the parameter sample \n    that belong to the same list.\n\n    Returns a list of lists with partitioned indices, and \n    a list of lists with the corresponding sample list they were found in.\n\n    E.g.:\n    sample : [1,3]\n    list_of_samples : [[1,2,3,4],[5,6]]\n    returns: [[1,3]], [[1,2,3,4]]\n\n    Indices not part of any of the lists should be in singleton list and \n    have corresponding empty list:\n\n    E.g.:\n    sample : [7]\n    list_of_samples : [[1,2,3,4],[5,6]]\n    returns: [[7]], [[]]\n\n    \"\"\"\n\n    partitions, sample_lists  = [], []\n\n    for s in sample:\n        for one_sample_list in list_of_samples:\n            if s in one_sample_list:\n                try:\n                    index = sample_lists.index(one_sample_list)\n                    partitions[index].append(s)\n                except ValueError:\n                    partitions.append([s])\n                    sample_lists.append(one_sample_list)                    \n                \n                break\n        else:# s does not belong to any list of samples\n            partitions.append([s])\n            sample_lists.append([])\n\n    return partitions, sample_lists",
        "sha1": "b4c5033a163b6543dca84400d8c274455db0cd12",
        "id": 195278
    },
    {
        "content": "def add_dicts(*args, **kwargs):\n    \"\"\"\n    Utility to \"add\" together zero or more dicts passed in as positional\n    arguments with kwargs.  The positional argument dicts, if present, are not\n    mutated.\n    \"\"\"\n    result = {}\n    for d in args:\n        result.update(d)\n    result.update(kwargs)\n    return result",
        "sha1": "8a2ee0486798d7a292610a2b197445cc6d902235",
        "id": 654384
    },
    {
        "content": "def sort_genomic_ranges(rngs):\n  \"\"\"sort multiple ranges\"\"\"\n  return sorted(rngs, key=lambda x: (x.chr, x.start, x.end))",
        "sha1": "5cacfafc713c816997cb607c84069fe133badc5d",
        "id": 87703
    },
    {
        "content": "def _get(kwargs, name, default=None):\n    \"\"\"\n    Returns kwargs['name'] if it exists and is not None.\n    Otherwise returns default.\n\n    This function exists so users can pass some_arg=None to get its default\n    value, instead of literally passing None.\n    \"\"\"\n    val = kwargs.get(name)\n    if val is None:\n        val = default\n    return val",
        "sha1": "788462e54861455d763a85d2d25f687c3bbf56c3",
        "id": 338414
    },
    {
        "content": "def is_upvoted(submission):\n    \"\"\"\n    If a comment is upvoted, we assume the question is welcomed, and that\n    there's no need for a template answer.\n    \"\"\"\n    min_score = 3\n    min_comment_count = 1\n    return (\n        submission.score > min_score and\n        len(submission.comments) > min_comment_count\n    )",
        "sha1": "90fe43e6cd681a15daa97dba039e7fa94ac617ca",
        "id": 24572
    },
    {
        "content": "def get_start_middle_end_point(line):\n    \"\"\"\n    Ger three point on line\n    Transform it to DB.Curve\n\n    :param line: Line\n    :type line: DB.Line\n    :return: Start, middle and end point of line\n    :rtype: [DB.XYZ, DB.XYZ, DB.XYZ]\n    \"\"\"\n    curve = line.GeometryCurve\n    start = curve.GetEndPoint(0)\n    end = curve.GetEndPoint(1)\n    middle = (start + end)/2\n    return start, middle, end",
        "sha1": "b6b4f578cd8ce44663a3c7ebb0c6b7dff3ab18d8",
        "id": 668754
    },
    {
        "content": "def applies_to_product(file_name, product):\n    \"\"\"\n    A OVAL or fix is filtered by product iff product is Falsy, file_name is\n    \"shared\", or file_name is product. Note that this does not filter by\n    contents of the fix or check, only by the name of the file.\n    \"\"\"\n\n    if not product:\n        return True\n\n    return file_name == \"shared\" or file_name == product or product.startswith(file_name)",
        "sha1": "3b816fec99a6353b69dc61629aaa91adbc949488",
        "id": 669759
    },
    {
        "content": "def _get_usable_value(prop):\n    \"\"\"Return the current or the modified value of a property.\"\"\"\n    value = prop.value if prop.new_value is None else prop.new_value\n    return value, prop.is_dirty",
        "sha1": "57a00b6613cda8a09d80eb68c2872c94739d02e9",
        "id": 148860
    },
    {
        "content": "def get_feature_type(\n        feature_text: str) -> str:\n    \"\"\"\n    Args:\n        feature_text: endswith '\\n'\n            For example:\n            '     CDS             complement(12..2189)\n            '                     /gene=\"rIIA\"\n            '                     /locus_tag=\"T4p001\"\n            '                     /db_xref=\"GeneID:1258593\"\n\n    Returns:\n        The example would be 'CDS'\n    \"\"\"\n    line1 = feature_text.splitlines()[0]\n    return line1.strip().split()[0]",
        "sha1": "6218e625a339f028968060d45c13542dd2569ee2",
        "id": 600739
    },
    {
        "content": "def read_gene_table(f):\n    \"\"\"Read coordinates of genes on genomes.\n\n    Parameters\n    ----------\n    f : file handle\n        Gene table file.\n\n    Returns\n    -------\n    dict of list of tuple of (int, bool, bool, str)\n        Flattened list of gene coordinates per nucleotide.\n            Coordinate (nt).\n            Whether start (True) or end (False).\n            Whether gene (True) or read (False).\n            Identifier of gene.\n\n    See Also\n    --------\n    map_read_gene\n\n    Notes\n    -----\n    This data structure is central to this algorithm. Starting and ending\n    coordinates of each gene are separated and flattened into a sorted list.\n    which enables only one round of list traversal for the entire set of genes\n    plus reads.\n    \"\"\"\n    res = {}\n    nucl = None\n    for line in f:\n        line = line.rstrip('\\r\\n')\n\n        # \">\" or \"#\" indicates nucleotide name\n        if line.startswith(('>', '#')):\n            nucl = line[1:].strip()\n\n            # double \">\" or \"#\" indicates genome name\n            if not nucl.startswith(('>', '#')):\n                res[nucl] = []\n        else:\n            x = line.split('\\t')\n            idx = x[0]\n\n            # start and end are based on genome, not gene itself\n            start, end = sorted([int(x[1]), int(x[2])])\n            res[nucl].extend((\n                (start, True, True, idx),\n                (end,  False, True, idx)))\n    return res",
        "sha1": "8f80e634a20d5544a793048c6c523c2c6b9e6630",
        "id": 606967
    },
    {
        "content": "def calc_default_resistance_coeffs(test_mass, regression_curves):\n    \"\"\"\n    Approximating typical P_resistance based on vehicle test-mass.\n\n    The regression-curves are in the model `resistance_coeffs_regression_curves`.\n    Use it for rough results if you are missing the real vehicle data.\n    \"\"\"\n    a = regression_curves\n    f0 = a[0][0] * test_mass + a[0][1]\n    f1 = a[1][0] * test_mass + a[1][1]\n    f2 = a[2][0] * test_mass + a[2][1]\n\n    return (f0, f1, f2)",
        "sha1": "49e3a554162baa763d8c450c7b1a8e7a68e7ff52",
        "id": 378361
    },
    {
        "content": "import math\n\n\ndef rotateSlope(m, theta):\n    \"\"\" Rotates the given slope for a certain angle. \"\"\"\n    return (math.sin(theta) + m * math.cos(theta)) / (math.cos(theta) - m * math.sin(theta))",
        "sha1": "b0a29b6cd481db80a162d0dd65734c588d1a92b4",
        "id": 624295
    },
    {
        "content": "import math\n\n\ndef get_bpd(log_p, dimentions=28*28):\n    \"\"\"\n    bpd = (nll_val / num_pixels) / numpy.log(2).\n\n    log_p: log probability\n    dimentions: dimentions (resolution) of image\n    \"\"\"\n    return ((-log_p / dimentions) / math.log(2)).mean().item()",
        "sha1": "a1ba8c8e688988ef0b02ec555e5b31ffc5408d2a",
        "id": 23452
    },
    {
        "content": "from typing import Sequence\n\n\ndef seqeq(seq1: Sequence, seq2: Sequence):\n    \"\"\"\n    Compare whether two sequences are equal\n    \"\"\"\n    for a, b in zip(seq1, seq2):\n        if a != b:\n            return False\n    return True",
        "sha1": "1ff673cbb219e7406c9e49ebbc8f52df0c2a7fa0",
        "id": 460718
    },
    {
        "content": "def get_words_from_sentences(sentences):\n    \"\"\" Returns a list of words from a list of sentences.\n\n    Args:\n        sentences:  A list of sentences\n    Returns:\n        A list of words as they appear in the input.\n    \"\"\"\n    words = []\n    for sentence in sentences:\n        sentence_words = sentence.split()\n        words.extend(sentence_words)\n    return words",
        "sha1": "68b37db2a0bf0ef12d2f318b629ffd84479f6e45",
        "id": 324056
    },
    {
        "content": "def get_latests(qs, n):\n    \"\"\"\"\n    :param qs: queryset\n    :param n: positive integer\n    :returns queryset with the n latest entry of qs\n    \"\"\"\n    if len(qs) <= n:\n        return qs\n    ids = []\n    qs_copy = qs.all()\n    for i in range(n):\n        ids.append(qs_copy.latest().pk)\n        qs_copy = qs_copy.exclude(pk=ids[i])\n    return qs.filter(pk__in=ids)",
        "sha1": "9398b07a15321c8b9ad0546505ac17032cb5c422",
        "id": 533688
    },
    {
        "content": "from io import BytesIO\n\n\ndef bytes_to_bytesio(bytestream):\n    \"\"\"Convert a bytestring to a BytesIO ready to be decoded.\"\"\"\n\n    fp = BytesIO()\n    fp.write(bytestream)\n    fp.seek(0)\n    return fp",
        "sha1": "d59e4f5ccc581898da20bf5d3f6e70f8e8712aa6",
        "id": 707700
    },
    {
        "content": "def arrival_notification(\n    email_from: str,\n    reply_to: str,\n    subject: str,\n    filename: str,\n    mime_type: str\n) -> dict:\n    \"\"\"\n    Create an arrival notification message to be published to our outbound\n    queue.\n\n    Args:\n        email_from (str): Email we received the message from.\n        reply_to (str): Email we should reply to.\n        subject (str): Subject line from email.\n        filename (str): Name of file we have extracted.\n        mime_type (str): MIME-type for this file.\n\n    Returns:\n        (dict): Item to publish to outbound queue.\n    \"\"\"\n    return {\n        'publisher': \"mailgetter\",\n        'email_from': email_from,\n        'reply_to': reply_to,\n        'subject': subject,\n        'filename': filename,\n        'mime_type': mime_type\n    }",
        "sha1": "ddbdefb7487aaf98d362d93e5dda786d8b3d1b35",
        "id": 433011
    },
    {
        "content": "import re\n\n\ndef remove_superscript_numbers_in_passage(text):\n    \"\"\"\n    A helper function that removes all superscript numbers with optional trailing space from a given string.\n    Mainly used to hide passage numbers in a given block of text.\n\n    :param text: String to process\n    :type text: str\n    :return: String with the superscript numbers that have a trailing space removed\n    :rtype: str\n\n    >>> remove_superscript_numbers_in_passage('\u2070 \u00b9 \u00b2 \u00b3 \u2074 \u2075 \u2076 \u2077 \u2078 \u2079 ')\n    ''\n    >>> remove_superscript_numbers_in_passage('E=mc\u00b2')\n    'E=mc'\n    \"\"\"\n    return re.sub(r'[\u2070\u00b9\u00b2\u00b3\u2074\u2075\u2076\u2077\u2078\u2079]+\\s?', '', text)",
        "sha1": "767630743a8a55e1bcb139dc303ffbd778734690",
        "id": 696139
    },
    {
        "content": "def export_task_results(db, task_data=None, denormalize=False):\n  \"\"\" Export the bbox task results. Saves a list of task results to `output_path`.\n  Args:\n    task_data: Use this to specify which task results to export.\n    denormalize: Should the annotations be stored in image coordinates?\n  \"\"\"\n  if task_data != None:\n    assert 'tasks' in task_data,  \"Failed to find `tasks` in task_data object.\"\n    task_ids = list(set([task['id'] for task in task_data['tasks']]))\n    task_results = list(db.bbox_task_result.find({'task_id' : {\"$in\" : task_ids}}, projection={'_id' : False}))\n  else:\n    task_results = list(db.bbox_task_result.find(projection={'_id' : False}))\n\n  if denormalize:\n    for task_result in task_results:\n      for image_result in task_result['results']:\n        image = image_result['image']\n        width = image['width']\n        height = image['height']\n        for anno in image_result['annotations']:\n          x, y, w, h = anno['bbox']\n          anno['bbox'] = [x * width, y * height, w * width, h * height]\n\n  return task_results",
        "sha1": "b56dedf564bb372568fd98d945af0aa113d0dbe2",
        "id": 349572
    },
    {
        "content": "def Pathlen_ULdigraph_Range2(N,L):\n    \"\"\"\n    Longest possible pathlength of a strongly connected digraph (ultra-long limit).\n    Valid in the range when L >= (N-1) + 1/2 N(N-1), or density >= 1/2 + 1/N.\n\n    Calculates the longest possible pathlength a connected directed graph of\n    arbitrary number of nodes N and number of directed arcs L could possibly\n    have, whenever L > 1/2 N(N-1) + (N-1).\n\n    Reference and citation\n    ^^^^^^^^^^^^^^^^^^^^^^\n    G. Zamora-L\u00f3pez & R. Brasselet \"Sizing complex networks\" Commun Phys 2:144 (2019)\n\n    Parameters\n    ----------\n    N : integer\n        Number of nodes in the digraph.\n    L : integer\n        Number of directed arcs in the digraph.\n\n    Returns\n    -------\n    avpathlen : float\n        The average pathlength of a strongly connected ultra-short digraph.\n\n    See Also\n    --------\n    Pathlen_ULdigraph :\n    Pathlen_ULdigraph_Range1_MBS :\n    Pathlen_ULdigraph_Range1_Approx :\n    Pathlen_ULdigraph_Intermediate :\n    \"\"\"\n    # 0) SECURITY CHECKS\n    if N < 2: raise ValueError( \"Network needs at least two nodes, N > 1\" )\n    Lstart = int( 0.5*N*(N-1) + (N-1) );    Ltot = N*(N-1)\n    if L < Lstart:  raise ValueError( \"L out of range, min(L) = 1/2*N*(N-1) + (N-1)\" )\n    if L > Ltot:    raise ValueError( \"L out of range, max(L) = N*(N-1).\" )\n\n    # 1) CALCULATE THE CORRESPONDING PATHLENGTH\n    Lr = L - Lstart\n    term1 = float(N + 4.0)/6\n    term2 = float(Lr) / N\n\n    # Calculate term #3, which includes a series without a closed formula.\n    partsum = 0\n    finished = False\n    counter = 0\n    for i in range(1,N):\n        if finished: break\n        for j in range(i):\n            if counter == Lr:\n                finished = True\n                break\n            partsum += i\n            counter += 1\n    term3 = float(partsum) / Ltot\n\n    # Sum all the contributiona\n    avpathlen = term1 - term2 + term3\n    return avpathlen",
        "sha1": "73e3493812707e874d5e1d717f92ce838af916f3",
        "id": 597325
    },
    {
        "content": "def _parse_gcs_path(path: str):\n    \"\"\"Parses the provided GCS path into bucket name and blob name\n    Args:\n        path (str): Path to the GCS object\n    \n    Returns:\n        bucket_name, blob_name: Strings denoting the bucket name and blob name\n    \"\"\"\n\n    header, rest = path.strip().split('//')\n    if header != 'gs:':\n        raise ValueError('Invalid GCS object path: %s', header)\n\n    bucket_name, blob_name = rest.split('/', 1)\n    return bucket_name, blob_name",
        "sha1": "ab77a7bbe41b7e6da4e1bd75e44fe006706a7a93",
        "id": 625647
    },
    {
        "content": "def _get_axes_ndim(axes):\n    \"\"\"\n    Quick function to determine if an Axes object is 3D (can accept x, y, z data)\n    or 2d (can only accept x, y data)\n    \"\"\"\n    if hasattr(axes, \"get_zlim\"):\n        n = 3\n    else:\n        n = 2\n    return n",
        "sha1": "c1335893628a0f89b7a3c3727a6f1a5d5522caae",
        "id": 524435
    },
    {
        "content": "def analytical_value_h_phi(distr, par, c):\n    \"\"\" Analytical value of the Phi entropy for the given distribution.\n    \n    Parameters\n    ----------    \n    distr : str\n            Name of the distribution.\n    par : dictionary\n          Parameters of the distribution. If distr = 'uniform': par.a,\n          par.b in U[a,b].\n    c : float, >=1\n        Parameter of the Phi-entropy: phi = lambda x: x**c\n      \n    Returns\n    -------\n    h : float\n        Analytical value of the Phi entropy.\n           \n    \"\"\"\n    \n    if distr == 'uniform': \n        a, b = par['a'], par['b']\n        h = 1 / (b-a)**c\n    else:\n        raise Exception('Distribution=?')\n    \n    return h",
        "sha1": "0be52c4a254f08181f739415e6f04d117ca60910",
        "id": 306244
    },
    {
        "content": "from typing import Sequence\n\n\ndef all_of_type(seq: Sequence, element_type) -> bool:\n    \"\"\"Return true if all elements in sequence are of the same type.\"\"\"\n    for item in seq:\n        if type(item) != element_type:\n            return False\n    return True",
        "sha1": "feb2f8dadf35795d85742b6f70eba07f365aaa31",
        "id": 82542
    },
    {
        "content": "def get_top_n_kmers(kmer_count, num):\n    \"\"\"Get a list of top_n most frequent kmers.\"\"\"\n    return [item[0] for item in sorted(kmer_count.items(), key=lambda x: x[1], reverse=True)[:num]]",
        "sha1": "c42c5722a5d0e578d451336896afaeb1f84a03d8",
        "id": 654483
    },
    {
        "content": "def default_key(x):\n    \"\"\" Default comparison function \"\"\"\n    return x",
        "sha1": "ab3efdc498f34d46892680e5d830b6cfec92ae0d",
        "id": 385721
    },
    {
        "content": "import random\n\n\ndef paperer_player_pick(li: list):\n    \"\"\"\n    Picks 'Rock', 'Paper' or 'Scissors' for the Paperer CPU player\n    Has a higher chance of picking 'Paper' than 'Rock' or 'Scissors'\n    :param li: list of the valid actions ('Rock', 'Paper', 'Scissors')\n    :type li: list\n    :return: the selected action for the Paperer CPU player\n    :rtype: str\n    \"\"\"\n    return random.choices(population=li, weights=[25, 50, 25], k=1)[0]",
        "sha1": "95440ebca5d49762c38b6d3ce44c343731642e5a",
        "id": 431548
    },
    {
        "content": "def score_illegals(illegals):\n    \"\"\" Take a list of illegal closing characters and return a score per the question's criteria\n    \"\"\"\n    sum = illegals.count(')') * 3\n    sum += illegals.count(']') * 57\n    sum += illegals.count('}') * 1197\n    sum += illegals.count('>') * 25137\n    return sum",
        "sha1": "9e3e6e2556cf252d51ce7199a660514c3442fc1c",
        "id": 239994
    },
    {
        "content": "def default_migration(random, population, args):\n    \"\"\"Do nothing.\n    \n    This function just returns the existing population with no changes.\n    \n    \"\"\"\n    return population",
        "sha1": "a781c07c16de62fccf217df7c25d83d18d23be0e",
        "id": 357244
    },
    {
        "content": "def is_square(X):\n    \"\"\" Check if matrices are square.\n\n    Parameters\n    ----------\n    X : ndarray, shape (..., n, n)\n        The set of square matrices, at least 2D ndarray.\n\n    Returns\n    -------\n    ret : boolean\n        True if matrices are square.\n    \"\"\"\n    return X.ndim >= 2 and X.shape[-2] == X.shape[-1]",
        "sha1": "559d4e801d6b59329c86d7d9e3aef4499de8b7e8",
        "id": 591884
    },
    {
        "content": "def read_file(path: str) -> list:\n    \"\"\"\n    Function reads file 'locations.list' and returns it as a list of lists.\n    >>> read_file(\"locations.list\")[:3]\n    [['\"#1 Single\" (2006)', '', '', '', '', 'Los Angeles, California, USA'],\\\n    ['\"#1 Single\" (2006)', '', '', '', '', 'New York City, New York, USA'],\\\n    ['\"#15SecondScare\" (2015) {It\\'s Me Jessica (#1.5)}', 'Coventry, West Midlands, England, UK']]\n    \"\"\"\n    with open(path, encoding=\"utf8\", errors='ignore') as file:\n        \n        data = [(line.strip()).split('\\t') for line in file]\n\n    return data",
        "sha1": "b22febaf5b474f2e3a7411cb21a5b14ea2662174",
        "id": 154773
    },
    {
        "content": "def binary_encoding_gs1_ai_key(key: str):\n    \"\"\"\n    Encode a GS1 application identifier key into a binary number.\n    :param key: GS1 application identifier key.\n    :return: Encoded binary string.\n    \"\"\"\n    binary_ai_key = ''.join([\"{0:b}\".format(int(char_, 16)).zfill(4)\n                             for char_ in key])\n    return binary_ai_key",
        "sha1": "e601b672a18c7ea44730fc9fe06b068e13a109f9",
        "id": 182869
    },
    {
        "content": "def second_key_from_tuple(tuple):\n    \"\"\"\n    Return second value of a tuple, used for sorting array of dimension [n, 2] on the second value\n    \"\"\"\n\n    return tuple[1]",
        "sha1": "981864bb44417a8f2b2b5f9f7b553a7965809a95",
        "id": 452037
    },
    {
        "content": "from datetime import datetime\n\n\ndef date_time_format(value: datetime, fmt: str = '%Y-%m-%d %H:%M:%S') -> str:\n    \"\"\"\n    Add datetime format filter.\n\n    :param value: date\n    :type value: datetime\n    :param fmt: format for the returned string, defaults to '%Y-%m-%d %H:%M:%S'\n    :type fmt: str, optional\n    :return: string representing date-time in given format\n    :rtype: str\n    \"\"\"\n    return value.strftime(fmt)",
        "sha1": "73490c77964cfe02cdd0eae620ece5df78282cd2",
        "id": 422669
    },
    {
        "content": "def dot_product(u1, u2):\n    \"\"\"returns the dot product of vectors u1 and u2\"\"\"\n    return u1.x*u2.x+u1.y*u2.y+u1.z*u2.z",
        "sha1": "fe21ba2c62cb866b517641a003bbaf092e24894f",
        "id": 271607
    },
    {
        "content": "def mag2D(v2D):\n    \"\"\"Returns the magnitude of the vector\"\"\"\n    return (v2D[0]**2 + v2D[1]**2)**0.5",
        "sha1": "608c9b12a8a1d55a814b1acd9c9a1ad63caa8068",
        "id": 377290
    },
    {
        "content": "def coord_to_code(x, y):\n    \"\"\" Utility function to encode pixel coordinates so we can unravel our distribution in a 1D array \"\"\"\n    return 1000 * x + y",
        "sha1": "e88c34842789f293941a86891ba516417f4cbae3",
        "id": 475871
    },
    {
        "content": "from typing import List\n\n\ndef align_lines(lines: list, column_separator: str = \"|\") -> List[str]:\n    \"\"\"\n    Pads lines so that all rows in single column match. Columns separated by '|' in every line.\n    :param lines: list of lines\n    :param column_separator: column separator. default is '|'\n    :return: list of lines\n    \"\"\"\n    rows = []\n    col_len: List[int] = []\n    for line in lines:\n        line = str(line)\n        cols = []\n        for col_index, col in enumerate(line.split(column_separator)):\n            col = str(col).strip()\n            cols.append(col)\n            if col_index >= len(col_len):\n                col_len.append(0)\n            col_len[col_index] = max(col_len[col_index], len(col))\n        rows.append(cols)\n\n    lines_out: List[str] = []\n    for row in rows:\n        cols_out = []\n        for col_index, col in enumerate(row):\n            if col_index == 0:\n                col = col.ljust(col_len[col_index])\n            else:\n                col = col.rjust(col_len[col_index])\n            cols_out.append(col)\n        lines_out.append(\" \".join(cols_out))\n    return lines_out",
        "sha1": "2ec537752bfce3d261d7202e9b9266a9ae0dd87f",
        "id": 16189
    },
    {
        "content": "import json\n\n\ndef parser(chunks):\n    \"\"\"\n    Parse a data chunk into a dictionary; catch failures and return suitable\n    defaults\n    \"\"\"\n\n    dictionaries = []\n    for chunk in chunks:\n        try:\n            dictionaries.append(json.loads(chunk))\n        except ValueError:\n            dictionaries.append({\n                'unparsed': chunk\n            })\n\n    return dictionaries",
        "sha1": "385b73026c079b635b6e33b35bfd8f5ebb453f64",
        "id": 32154
    },
    {
        "content": "from typing import Sequence\n\n\ndef evaluate_poly(poly: Sequence[float], x: float) -> float:\n    \"\"\"Evaluate a polynomial f(x) at specified point x and return the value.\n\n    Arguments:\n    poly -- the coeffiecients of a polynomial as an iterable in order of\n            ascending degree\n    x -- the point at which to evaluate the polynomial\n\n    >>> evaluate_poly((0.0, 0.0, 5.0, 9.3, 7.0), 10.0)\n    79800.0\n    \"\"\"\n    return sum(c * (x ** i) for i, c in enumerate(poly))",
        "sha1": "deaff8810fccb4b75d4c18316ec19e70e15ed8c3",
        "id": 655262
    },
    {
        "content": "def is_hyponym(syn1, syn2):\n    \"\"\" Checks if syn1 is a child of syn2 \"\"\"\n    while syn1 != syn2:\n        hypernyms = syn1.hypernyms()\n        if len(hypernyms) == 0:\n            return False\n        syn1 = hypernyms[0]\n    return True",
        "sha1": "0102da3dadb5aa1ee9c3b0c0f4f14082c51d28ac",
        "id": 685579
    },
    {
        "content": "import warnings\n\n\ndef rainbow_to_vector(r, timeformat='h'):\n    \"\"\" Convert Rainbow object to np.arrays\n    Parameters\n    ----------\n        r : Rainbow object\n            chromatic Rainbow object to convert into array format\n        timeformat : str\n            (optional, default='hours')\n            The time format to use (seconds, minutes, hours, days etc.)\n    Returns\n    ----------\n        rflux : np.array\n            flux (MJy/sr)        [n_wavelengths x n_integrations]\n        rfluxe : np.array\n            flux error (MJy/sr)  [n_wavelengths x n_integrations]\n        rtime : np.array\n            time (BJD_TDB, houra) [n_integrations]\n        rwavel : np.array\n            wavelength (microns) [n_wavelengths]\n    \"\"\"\n    secondformat = ['second', 'seconds', 'sec', 's']\n    minuteformat = ['minute', 'minutes', 'min', 'm']\n    hourformat = ['hour', 'hours', 'h']\n    dayformat = ['day', 'days', 'd']\n    yearformat = ['year', 'years', 'y']\n\n    rflux = r.fluxlike['flux']  # flux (MJy/sr)        : [n_wavelengths x n_integrations]\n    rfluxe = r.fluxlike['uncertainty']  # flux error (MJy/sr)  : [n_wavelengths x n_integrations]\n    rtime = r.timelike['time']  # time (BJD_TDB, hours) : [n_integrations]\n    rwavel = r.wavelike['wavelength']  # wavelength (microns) : [n_wavelengths]\n\n    # change the time array into the requested format (e.g. seconds, minutes, days etc.)\n    if timeformat in secondformat:\n        rtime = rtime * 3600\n    elif timeformat in minuteformat:\n        rtime = rtime * 60\n    elif timeformat in hourformat:\n        # hours is the default time setting\n        pass\n    elif timeformat in dayformat:\n        rtime = rtime / 24.\n    elif timeformat in yearformat:\n        rtime = rtime / (24 * 365.)\n    else:\n        warnings.warn(\"Unrecognised Time Format!\")\n        return\n\n    return rflux, rfluxe, rtime, rwavel",
        "sha1": "6b65c11c103b7ad6dc992d4f4bcb34a50d52c34c",
        "id": 247921
    },
    {
        "content": "import logging\n\n\ndef create_project_network(neutron_client, project_id, net_name='private',\n                           shared=False, network_type='gre', domain=None):\n    \"\"\"Create the project network.\n\n    :param neutron_client: Authenticated neutronclient\n    :type neutron_client: neutronclient.Client object\n    :param project_id: Project ID\n    :type project_id: string\n    :param net_name: Network name\n    :type net_name: string\n    :param shared: The network should be shared between projects\n    :type shared: boolean\n    :param net_type: Network type: GRE, VXLAN, local, VLAN\n    :type net_type: string\n    :param domain_name: Name of the domain\n    :type domain_name: string or None\n    :returns: Network object\n    :rtype: dict\n    \"\"\"\n    networks = neutron_client.list_networks(name=net_name)\n    if len(networks['networks']) == 0:\n        logging.info('Creating network: %s',\n                     net_name)\n        network_msg = {\n            'network': {\n                'name': net_name,\n                'shared': shared,\n                'tenant_id': project_id,\n            }\n        }\n        if network_type == 'vxlan':\n            network_msg['network']['provider:segmentation_id'] = 1233\n            network_msg['network']['provider:network_type'] = network_type\n        network = neutron_client.create_network(network_msg)['network']\n    else:\n        logging.warning('Network %s already exists.', net_name)\n        network = networks['networks'][0]\n    return network",
        "sha1": "fe3e2ba5241c6b5c0e497d225ea0a0a738c0b786",
        "id": 340448
    },
    {
        "content": "def all_is_same(vec):\n\t\"\"\"\n\tTest that all elements in a vector are the same\n\t\"\"\"\n\treturn all(el == vec[0] for el in vec)",
        "sha1": "7a6fba83c0c5c528992c17d82197a20ff6d0f2da",
        "id": 168563
    },
    {
        "content": "def _get_log_group_name_for_cluster(cluster_name):\n    \"\"\"Return the name of the log group to be created for the given cluster if CloudWatch logging is enabled.\"\"\"\n    return \"/aws/parallelcluster/{0}\".format(cluster_name)",
        "sha1": "51aeadc3138cdc93d112a159c25f438c0af78742",
        "id": 414426
    },
    {
        "content": "def xml_find_non_text_child(_node):\n    \"\"\"Finds the first child that is not of the Text node type.\"\"\"\n    nodelist = _node.childNodes\n    for node in nodelist:\n        if node.nodeType != node.TEXT_NODE:\n            return node\n\n    return None",
        "sha1": "12cb68c6a554163df70de111f5c647f2e0ecd24d",
        "id": 372950
    },
    {
        "content": "def restrict(d, languages=['en', 'de']):\n    \"\"\"Restrict a dictionary with the labels or aliases to the specified \n    languages only\"\"\"\n    # lang = ['en', 'de', 'zh', 'hi', 'es', 'fr', 'ar', 'bn', 'ru', 'pt', 'id']\n    return dict((k, v) for (k, v) in d.items() if k in languages)",
        "sha1": "04338004f062d1eaa9e26eb73c5b959a83cf2dda",
        "id": 90629
    },
    {
        "content": "def shipping_charge(method, basket, postcode):\n    \"\"\"\n    Template tag for calculating the shipping charge for a given shipping\n    method and basket, and injecting it into the template context.\n    \"\"\"\n    return method.calculate(basket, postcode)",
        "sha1": "ced6bb9b0029a81540ae3816e5002e376a4b5a3b",
        "id": 683417
    },
    {
        "content": "from pathlib import Path\nfrom typing import Sequence\n\n\ndef _structure_exists(base_dir: Path, paths: Sequence[str]) -> bool:\n    \"\"\"\n    Helper function for match_structure to check if\n    all subpaths exist at some base directory\n\n    For example:\n\n    dir1\n    \u251c\u2500\u2500 index.json\n    \u2514\u2500\u2500 messages\n        \u2514\u2500\u2500 messages.csv\n\n    _structure_exists(Path(\"dir1\"), [\"index.json\", \"messages/messages.csv\"])\n    \"\"\"\n    for p in paths:\n        target: Path = base_dir / p\n        if not target.exists():\n            return False\n    return True",
        "sha1": "00af3c4578c98335e0ff63528e2fe0db7df6ac4c",
        "id": 129863
    },
    {
        "content": "import hashlib\n\n\ndef md5(content):\n    \"\"\"Finds the md5 hash of the content.\"\"\"\n    return hashlib.md5(content).hexdigest()",
        "sha1": "b9309ceab854b1bb458210f39004557d55024b22",
        "id": 485162
    },
    {
        "content": "def import_sensors_from_device(client, device):\n    \"\"\"\n    Imports sensor data of specified devices from the Rayleigh Connect API\n\n    :param client: An instance of the RayleighClient class\n    :param device: A list of Node objects describing the Rayleigh devices\n    :return: A JSON object containing the sensors for each device, retrieved from the Rayleigh Connect API\n    \"\"\"\n    return client.retrieve_sensors(device)",
        "sha1": "275f818cb56aa244e79aa5b9b3ad077472026e0e",
        "id": 36578
    },
    {
        "content": "def get_account_age_in_days(numpy_time_difference):\n    \"\"\"\n    Args\n        numpy_time_difference (numpy timedelta): a numpy timedelta object\n        that is the difference between the user's account creation date\n        and the date of their most recent tweet\n    Return\n        account_age (int)\n    \"\"\"\n    return int(numpy_time_difference/1000000000/60/60/24)+1",
        "sha1": "c0c539994b9d051c01f181338c04ad0df4d4a442",
        "id": 224185
    },
    {
        "content": "def scale(pix, pixMax, floatMin, floatMax):\n    \"\"\" scale takes in\n        pix, the CURRENT pixel column (or row)\n        pixMax, the total # of pixel columns\n        floatMin, the min floating-point value\n        floatMax, the max floating-point value\n        scale returns the floating-point value that\n        corresponds to pix\n    \"\"\"\n    return (pix / pixMax) * (floatMax - floatMin) + floatMin",
        "sha1": "455d0233cbeeafd53c30baa4584dbdac8502ef94",
        "id": 707822
    },
    {
        "content": "import random\n\n\ndef chance(gen_function, chance=100):\n    \"\"\"\n    Wrap a gen function so that it has an X percent chance (`chance`)\n    of returning a value; returns None otherwise.\n    \"\"\"\n    def gen(record):\n        val = gen_function(record)\n        if chance < 100:\n            val = random.choice([None] * (100 - chance) + [val] * chance)\n        return val\n    return gen",
        "sha1": "b13824282b37b5de2832523a9caa3b7eddcb92d2",
        "id": 531937
    },
    {
        "content": "from datetime import datetime\n\n\ndef in_date_set(date_set, check_date):\n    \"\"\"Checks whether check_date is within any of the bounds in date_set\"\"\"\n    check_month = datetime(year=check_date.year, month=check_date.month, day=1).date()\n    for pair in date_set:\n        p0 = datetime.strptime(pair[0], '%Y-%m-%d').date()\n        p1 = datetime.strptime(pair[1], '%Y-%m-%d').date()\n        if p0 <= check_month <= p1:\n            return True\n    return False",
        "sha1": "9923980d221c9bd7c51c9315dc33010db869bbc0",
        "id": 601272
    },
    {
        "content": "def get_mask(doc_idxs, vocab, dividers = ['.', '!', '?'], add_final_posn=True, max_length=None):\n    \"\"\" Return the indices from a integer-encoded document\n    representing the non-contiguous instances of divider characters\n\n    Args:\n        doc_idxs (list): document to mask, encoded as int according to the mapping in vocab\n        vocab (dict): map of token (str) to id (int)\n        dividers (list of str): which characters to divide on?\n        add_final_posn (bool): Add an index for the last posn in doc_idxs, even if not a divider\n\n    Returns:\n        list of list indices where dividers occur\n    \"\"\"\n\n    doc_length = len(doc_idxs)\n    last_tkn_was_mask = False\n    sentence_mask = []\n    divider_idx = set(vocab[divider] for divider in dividers)\n    for idx, tkn in enumerate(doc_idxs):\n        if tkn in divider_idx and not last_tkn_was_mask:\n            last_tkn_was_mask = True\n            sentence_mask.append(idx)\n        else:\n            last_tkn_was_mask = False\n    #check to ensure there are no mask values greater than the maximum value\n    if max_length and doc_length-1>max_length - 1:\n        max_mask = max_length - 1\n    else:\n        max_mask = doc_length-1\n    sentence_mask = [a for a in sentence_mask if a<max_mask]\n    if add_final_posn:\n        # make sure to add in the last index if it is not already there\n        if len(sentence_mask)==0 or sentence_mask[-1] != max_mask:\n            sentence_mask.append(max_mask)\n    return sentence_mask",
        "sha1": "8883fccc65c34d4703b17f32e74eb2ae22842906",
        "id": 200073
    },
    {
        "content": "from datetime import datetime\nimport time\n\n\ndef get_end_date_of_schedule(schedule):\n    \"\"\"Return the end date of the provided schedule in ISO 8601 format\"\"\"\n    currenttime = datetime.today()\n    endtime = datetime(\n        currenttime.year, currenttime.month, currenttime.day, schedule['end-hour'], schedule['end-minute'])\n    # manually create ISO8601 string because of tz issues with Python2\n    ts = time.time()\n    utc_offset = ((datetime.fromtimestamp(\n        ts) - datetime.utcfromtimestamp(ts)).total_seconds()) / 3600\n    offset = str(int(abs(utc_offset * 100))).zfill(4)\n    sign = \"+\" if utc_offset >= 0 else \"-\"\n\n    return endtime.strftime(\"%Y-%m-%dT%H:%M:%S{sign}{offset}\".format(sign=sign, offset=offset))",
        "sha1": "bf17c699455a4c7c57abd7751888ba12cd539170",
        "id": 690910
    },
    {
        "content": "def van_der_corput(n_sample, base=2):\n    \"\"\"Van der Corput sequence.\n\n    :param int n_sample: number of element of the sequence.\n    :param int base: base of the sequence.\n    :return: sequence of Van der Corput.\n    :rtype: list (n_samples,)\n    \"\"\"\n    sequence = []\n    for i in range(n_sample):\n        n_th_number, denom = 0., 1.\n        while i > 0:\n            i, remainder = divmod(i, base)\n            denom *= base\n            n_th_number += remainder / denom\n        sequence.append(n_th_number)\n\n    return sequence",
        "sha1": "2daad36f1f33359ca5ab305a29ee2d814fe71e61",
        "id": 313753
    },
    {
        "content": "def get_pixel(image, i, j):\n    \"\"\"\n    Return the value of the pixel (i,j) in image, with mirror boundary conditions\n    Note that (i,j) = (y,x) in pixel coordinate (assuming image conventions)\n    Input:\n        image (numpy.ndarray): the image array\n                               size:(width, height, num_channel) OR (width, height)\n        i: the row-coordinate of the pixel\n        j: the col-ccordinate of the pixel\n    Output:\n        the value(s) of the pixel; size:(num_channel,) OR simple scalar\n    \"\"\"\n    width = image.shape[0]\n    height = image.shape[1]\n    if i < 0:\n        i = -i\n    if i >= height:\n        i = 2*height - 2 - (i % (2*height-2))\n    if j < 0:\n        j = -j\n    if j >= width:\n        j = 2*width - 2 - (j % (2*width-2))\n    if len(image.shape) == 3:\n        return image[int(i),int(j),:]\n    else:\n        return image[int(i),int(j)]",
        "sha1": "628f86f62995e6d1a01b3435d7d03490b84f7bb1",
        "id": 432946
    },
    {
        "content": "def divisor(field, data):\n    \"\"\"\n    Construct a divisor from the data.\n\n    INPUT:\n\n    - ``field`` -- function field\n\n    - ``data`` -- dictionary of place and multiplicity pairs\n\n    EXAMPLES::\n\n        sage: K.<x> = FunctionField(GF(2)); R.<t> = K[]\n        sage: F.<y> = K.extension(t^3 - x^2*(x^2 + x + 1)^2)\n        sage: from sage.rings.function_field.divisor import divisor\n        sage: p, q, r = F.places()\n        sage: divisor(F, {p: 1, q: 2, r: 3})\n        Place (1/x, 1/x^2*y + 1)\n         + 2*Place (x, (1/(x^3 + x^2 + x))*y^2)\n         + 3*Place (x + 1, y + 1)\n    \"\"\"\n    divisor_group = field.divisor_group()\n    return divisor_group.element_class(divisor_group, data)",
        "sha1": "f87e3bdb40e88f448f5d286b39decf77044c56ec",
        "id": 79818
    },
    {
        "content": "def get_healer_cd_text_from_matches(healer_cd_matches) -> str:\n    \"\"\"Returns a concatenated string of all cds give the list of matches\n\n    Args:\n        healer_cd_matches(List[Match]): The list of match objects we can use\n        to get the cd string from.\n    \"\"\"\n\n    cd_text = \"\"\n    for match in healer_cd_matches:\n        cd_text += match[0]\n\n    return cd_text",
        "sha1": "788e0fd71fbfb59c2b1bcba2f34138e98e86b418",
        "id": 509011
    },
    {
        "content": "def get_default_bbox(kind):\n    \"\"\"\n    Get default rough estimate of face bounding box for `get_identity_descriptor()`.\n\n    Args:\n        kind:\n            `str`\n            Crop type that your model's pose encoder consumes.\n            One of: 'ffhq', 'x2face', 'latentpose'.\n\n    Returns:\n        bbox:\n            `tuple` of `int`, length == 4\n            The bounding box in pixels. Defines how much pixels to clip from top, left,\n            bottom and right in a 256 x 256 image.\n    \"\"\"\n    if kind == 'ffhq':\n        return (0, 30, 60, 30)\n    elif kind == 'x2face':\n        return (37, (37+45)//2, 45, (37+45)//2)\n    elif kind == 'latentpose':\n        return (42, (42+64)//2, 64, (42+64)//2)\n    else:\n        raise ValueError(f\"Wrong crop type: {kind}\")",
        "sha1": "8798d0bbbf122064665ec9eec4a014470a5c8297",
        "id": 418245
    },
    {
        "content": "def get_form_field_chart_url(url, field):\n    \"\"\"Append 'field_name' to a given url\"\"\"\n\n    return u'%s?field_name=%s' % (url, field)",
        "sha1": "239147fdac8d2f953aba3152b1e709b0057207d5",
        "id": 216978
    },
    {
        "content": "def get_line(pt1, pt2):\n    \"\"\"get line slope and bias from two points\n    y = slope * x + bias\n    \"\"\"\n    slope, bias = None, None\n    if pt1[0] != pt2[0]:\n        slope = (pt1[1] - pt2[1]) / (pt1[0] - pt2[0])\n        bias = pt1[1] - slope * pt1[0]\n    return slope, bias",
        "sha1": "f5b6482c1a6ad8adfcb4e83394b162af25d955f8",
        "id": 121528
    },
    {
        "content": "def GetCoalitionTypeString(type):\n    \"\"\" Convert a coalition type field into a string\n    Currently supported types (from <mach/coalition.h>):\n        COALITION_TYPE_RESOURCE\n        COALITION_TYPE_JETSAM\n    \"\"\"\n    if type == 0: # COALITION_TYPE_RESOURCE\n        return 'RESOURCE'\n    if type == 1:\n        return 'JETSAM'\n    return '<unknown>'",
        "sha1": "1d99529977aa6a7f80f3bfa5ea2f821f32456563",
        "id": 433997
    },
    {
        "content": "def casadi2List(x):\n    \"\"\"\n    Converts casadi vector to list.\n\n    Parameters\n    ----------\n    x : casadi.SX.sym\n        Input casadi symbolic vector.\n\n    Returns\n    -------\n    list\n        List that contains element of vector x.\n    \"\"\"\n    n = x.shape[0]\n    z = []\n    for k in range(n):\n        z.append(float(x[k]))\n    return z",
        "sha1": "374ef0a5ba67ec2fc49df101d28f79633ae985ae",
        "id": 354717
    },
    {
        "content": "import torch\n\n\ndef _find_quantized_op_num(model, white_list, op_count=0):\n    \"\"\"This is a helper function for `_fallback_quantizable_ops_recursively`\n\n    Args:\n        model (object): input model\n        white_list (list): list of quantizable op types in pytorch\n        op_count (int, optional): count the quantizable op quantity in this module\n\n    Returns:\n        the quantizable op quantity in this module\n    \"\"\"\n    quantize_op_num = op_count\n    for name_tmp, child_tmp in model.named_children():\n        if type(child_tmp) in white_list \\\n            and not (isinstance(child_tmp, torch.quantization.QuantStub)\n                     or isinstance(child_tmp, torch.quantization.DeQuantStub)):\n            quantize_op_num += 1\n        else:\n            quantize_op_num = _find_quantized_op_num(\n                child_tmp, white_list, quantize_op_num)\n    return quantize_op_num",
        "sha1": "c51b06e476ff4804d5bdfca5a187717536a0418f",
        "id": 4602
    },
    {
        "content": "def find_char(char, word):\n    \"\"\"\n  find the position of a character in a word, exactly the same thing as str.index but with error handling\n  \"\"\"\n    assert len(char) == 1 and type(word) is str\n    i = 0\n    while i < len(word):\n        if word[i] == char: break\n        else: i += 1\n    if i == len(word):\n        return None\n    return i",
        "sha1": "23ba373c978dfb129f505875544050936db3b152",
        "id": 69250
    },
    {
        "content": "def nframes(dur, hop_size=3072, win_len=4096) -> float:\n    \"\"\"\n    Compute the numbero of frames given a total duration, the hop size and\n    window length. Output unitiy of measure will be the same as the inputs\n    unity of measure (e.g. samples or seconds).\n\n    N.B. This returns a float!\n    \"\"\"\n    return (dur - win_len) / hop_size + 1",
        "sha1": "c59058259c9e2c98b45789848002fc55bb7b4971",
        "id": 100204
    },
    {
        "content": "def _source_key(source):\n    \"\"\"Creates a key to use to sort a list of sources.\n\n    Arguments:\n        source {metadata_pb2.Source} -- the Source for which to formulate a sort key\n\n    Returns:\n        tuple -- A key to use to sort a list of sources.\n    \"\"\"\n    if source.HasField(\"git\"):\n        return (\"git\", source.git.name, source.git.remote, source.git.sha)\n    if source.HasField(\"generator\"):\n        return (\n            \"generator\",\n            source.generator.name,\n            source.generator.version,\n            source.generator.docker_image,\n        )\n    if source.HasField(\"template\"):\n        return (\n            \"template\",\n            source.template.name,\n            source.template.origin,\n            source.template.version,\n        )",
        "sha1": "1b2809120d6dd66fe344db7bfe31e802739e213b",
        "id": 476056
    },
    {
        "content": "def _var(x, ddof=0):\n    \"\"\" Calculate variance for an array\n\n    Uses Welford's algorithm[1] for online variance calculation.\n\n    Parameters\n    ----------\n    x : array-like\n        The data\n    ddof : int\n        Degrees of freedom\n\n    References\n    ----------\n    .. [1] https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Online_algorithm\n    \"\"\"\n    n = 0.0\n    mean_ = M2 = 0.0\n\n    for x_ in x:\n        n += 1\n        delta = x_ - mean_\n        mean_ += delta / n\n        delta2 = x_ - mean_\n        M2 += delta * delta2\n    return M2 / max(n - ddof, 0)",
        "sha1": "e9d2180d4f894a8ab92f15f170e794ef772c210f",
        "id": 99155
    },
    {
        "content": "def pause_group(torrent_client, params):\n    \"\"\"\n    Pause several torrents\n\n    params['info_hashes']: list of str - the list of info-hashes in lowercase\n    :return: 'OK'\n    \"\"\"\n    for info_hash in params['info_hashes']:\n        torrent_client.pause_torrent(info_hash)\n    return 'OK'",
        "sha1": "6041dcbddb7ec527f7c0f9fc97c82f3f5815e3af",
        "id": 59432
    },
    {
        "content": "def getattritem(o,a):\n    \"\"\"\n    Get either attribute or item `a` from a given object `o`. Supports multiple evaluations, for example\n    `getattritem(o,'one.two')` would get `o.one.two`, `o['one']['two']`, etc.\n    :param o: Object\n    :param a: Attribute or Item index. Can contain `.`, in which case the final value is obtained.\n    :return: Value\n    \"\"\"\n    flds = a.split('.')\n    for x in flds:\n        if x in dir(o):\n            o = getattr(o,x)\n        else:\n            o = o[x]\n    return o",
        "sha1": "7b928b2405691dcb5fac26b7a3d7ebfcfa642f6d",
        "id": 13807
    },
    {
        "content": "def getMostStrictType(typeA: str, typeB: str) -> str:\n\t\"\"\"Return the most 'strict' type of license from the available types.\n\n\tArgs:\n\t\ttypeA (str): type of the first license\n\t\ttypeB (str): type of the second license\n\n\tReturns:\n\t\tstr: the most 'strict' type\n\t\"\"\"\n\tstrict = [\"Public Domain\", \"Permissive\", \"Weak Copyleft\", \"Copyleft\", \"Viral\"]\n\tif len(typeA) == 0:\n\t\treturn typeB\n\tif len(typeB) == 0:\n\t\treturn typeA\n\treturn strict[max(strict.index(typeA), strict.index(typeB))]",
        "sha1": "e8eb0ced58e3f79c827078d5abe708eb86820da6",
        "id": 406030
    },
    {
        "content": "def sqlitedb_file(tmpdir):\n    \"\"\"\n    Returns the path at which the initialized, golden SQLite database file will be placed.\n    \"\"\"\n    test_db_file = tmpdir.mkdir(\"quaydb\").join(\"test.db\")\n    return str(test_db_file)",
        "sha1": "41f51f7ef56e4843ff679c664a0019a8fe7dde10",
        "id": 256464
    },
    {
        "content": "from typing import Dict\n\n\ndef _results_data_row(row: Dict[str, str]):\n    \"\"\"\n    Transform the keys of each CSV row. If the CSV headers change,\n    this will make a single place to fix it.\n    \"\"\"\n    return {\n        \"k\": row[\"Key\"],\n        \"v\": row[\"Content\"],\n    }",
        "sha1": "97dc7f9842d7e6a564cbe66fd93e3dbaeca08517",
        "id": 698635
    },
    {
        "content": "import re\n\n\ndef pct_arg_to_int(arg):\n    \"\"\"Parse a numeric string that represents a percentage in units of 0.1%.  1000 == 100%\"\"\"\n    pat = re.compile(\"[0-9]{4,4}$\")\n    m = pat.match(arg)\n    if m is None:\n        raise ValueError(f\"Invalid percent string: {arg!r}\")\n    pct = int(m.group(0))\n    if pct < 0 or pct > 1000:\n        raise ValueError(f\"Invalid percent string: {arg!r}\")\n    return pct",
        "sha1": "f398377e23b0bbe82b0eb8da05960ef70020f72d",
        "id": 656899
    },
    {
        "content": "def intercept(exc_type, func, *args, **kw):\n    \"\"\"Intercept and return a raised exception.\"\"\"\n    try:\n        func(*args, **kw)\n    except exc_type as e:\n        return e\n    else:\n        assert False, \"Expected exception to be raised, but nothing happened! :-s\"",
        "sha1": "e76d72712e4d543e2206008640a5837789c357d0",
        "id": 314251
    },
    {
        "content": "import random\n\n\ndef get_words_of_encouragement(ratio):\n    \"\"\"Return an English sentence summarising how good the ratio is\"\"\"\n\n    if 0 <= ratio <= 0.2:\n        return random.choice(\n            [\n                \"there is nowhere to go but up\",\n                \"that is not that good sorry\",\n                \"clean your room\",\n                \"there is always next month\",\n                \"lol\",\n                \"to be honest katsu curry is really nice so fair play\",\n                \"could not be worse\",\n                \"nah fam\",\n                \"peak\",\n                \"problematic\",\n            ]\n        )\n    elif 0.2 < ratio <= 0.4:\n        return random.choice(\n            [\n                \"it is an ok ratio i think but i think you can do even better next time but well done though\",\n                \"cool\",\n                \"not bad\",\n                \"if you don't like having money then this is a pretty good ratio\",\n                \"with more dedication i'm sure you can do better next time but well done\",\n                \"hm\",\n                \"ok\",\n                \"i am feeling lukewarm about this score\",\n                \"could be worse\",\n            ]\n        )\n    elif 0.4 < ratio <= 0.6:\n        return random.choice(\n            [\n                \"it is a good ratio well done\",\n                \"not bad at all\",\n                \"it is a nice one\",\n                \"this is very respectable congratulations\",\n                \"you are good\",\n                \"yes\",\n                \"merely hench\",\n            ]\n        )\n    elif 0.6 < ratio <= 0.8:\n        return random.choice(\n            [\n                \"that really is great\",\n                \"gawn yersel\",\n                \"that is a fantastic ratio, enjoy your money\",\n                \"this is how you do it everyone\",\n                \"WOAH!!! good one\",\n                \"a picture of excellence\",\n                \"perfect 5/7\",\n            ]\n        )\n    elif 0.8 < ratio <= 1:\n        return random.choice(\n            [\n                \"a tremendous display of dedication to the art of preparation\",\n                \"a true hero\",\n                \"wow that is an inspiring ratio\",\n                \"well well well !!! look at what we have here, that is really incredible. well done\",\n                \"i never thought i would see this in my lifetime wow\",\n                \"now you are just showing off\",\n            ]\n        )\n    else:\n        raise ValueError(f\"Unhandled ratio {ratio}\")",
        "sha1": "480c957106ade8978441d15b7aa5fb0da2fec273",
        "id": 331406
    },
    {
        "content": "def parse_tool_output(text):\n    \"\"\"Given the tab-delimited output from an invocation of mp3gain\n    or aacgain, parse the text and return a list of dictionaries\n    containing information about each analyzed file.\n    \"\"\"\n    out = []\n    for line in text.split('\\n'):\n        parts = line.split('\\t')\n        if len(parts) != 6 or parts[0] == 'File':\n            continue\n        out.append({\n            'file': parts[0],\n            'mp3gain': int(parts[1]),\n            'gain': float(parts[2]),\n            'peak': float(parts[3]),\n            'maxgain': int(parts[4]),\n            'mingain': int(parts[5]),\n        })\n    return out",
        "sha1": "e154d586541ec2267f0388abe58ab8bec995dc23",
        "id": 597775
    },
    {
        "content": "def raw_resolution(resolution, splitter=False):\n    \"\"\"\n    Round a (width, height) tuple up to the nearest multiple of 32 horizontally\n    and 16 vertically (as this is what the Pi's camera module does for\n    unencoded output).\n    \"\"\"\n    width, height = resolution\n    if splitter:\n        fwidth = (width + 15) & ~15\n    else:\n        fwidth = (width + 31) & ~31\n    fheight = (height + 15) & ~15\n    return fwidth, fheight",
        "sha1": "73616dbcb9cda8979f0450553677206e426f8c9f",
        "id": 142154
    },
    {
        "content": "def must_be_known(in_limit, out_limit):\n    \"\"\"\n    Logical combinatino of limits enforcing a known state\n\n    The logic determines that we know that the device is fully inserted or\n    removed, alerting the MPS if the device is stuck in an unknown state or\n    broken\n\n    Parameters\n    ----------\n    in_limit : ``bool``\n        Whether the in limit is active\n\n    out_limit: ``bool``\n        Whether the out limit is active\n\n    Returns\n    -------\n    is_known\n        Whether the logical combination of the limit switch ensure that the\n        device position is known\n    \"\"\"\n    return in_limit != out_limit",
        "sha1": "241b66b359643d069aa4066965879bb6ac76f2ae",
        "id": 703564
    },
    {
        "content": "def extractAlgoAndDigest(checksum):\n  \"\"\"Given a checksum string formatted as ``<algo>:<digest>`` returns\n  the tuple ``(algo, digest)``.\n\n  ``<algo>`` is expected to be `SHA256`, `SHA512`, or `MD5`.\n  ``<digest>`` is expected to be the full length hexdecimal digest.\n\n  :raises ValueError: if checksum is incorrectly formatted.\n  \"\"\"\n  if checksum is None:\n    return None, None\n  if len(checksum.split(':')) != 2:\n    raise ValueError(\"invalid checksum '%s'. Expected format is '<algo>:<digest>'.\" % checksum)\n  (algo, digest) = checksum.split(':')\n  expected_algos = ['SHA256', 'SHA512', 'MD5']\n  if algo not in expected_algos:\n    raise ValueError(\"invalid algo '%s'. Algo must be one of %s\" % (algo, \", \".join(expected_algos)))\n  expected_digest_length = {'SHA256': 64, 'SHA512': 128, 'MD5': 32}\n  if len(digest) != expected_digest_length[algo]:\n    raise ValueError(\"invalid digest length %d. Expected digest length for %s is %d\" % (len(digest), algo, expected_digest_length[algo]))\n  return algo, digest",
        "sha1": "edb248dc1c6c1fd6071488345727445d612db3d5",
        "id": 540309
    },
    {
        "content": "def ensure_prefix(string, prefix):\n    \"\"\"Ensure, that a string is prefixed by another string.\"\"\"\n\n    if string[:len(prefix)] == prefix:\n        return string\n    return prefix + string",
        "sha1": "5a300e8754ce1d295f0bb58a15b01b98f1380703",
        "id": 546888
    },
    {
        "content": "def do_folder_create(client, args):\n    \"\"\"Create directory\"\"\"\n    for folder_uri in args.uris:\n        client.create_folder(folder_uri, recursive=True)\n    return True",
        "sha1": "f0b33ca408fa42fc44380d6b7653f796cda9ad0e",
        "id": 303487
    },
    {
        "content": "def increment_version(current):\n    \"\"\"\n    Returns current version incremented by 1 minor version number\n    \"\"\"\n    minor = current.split('.')[-1]\n    major = '.'.join(current.split('.')[:-1])\n    inc_minor = int(minor) + 1\n    return major + '.' + str(inc_minor)",
        "sha1": "a6af43a25cc5f2eaa606adbabdd300aa5f3829ea",
        "id": 201820
    },
    {
        "content": "import math\n\n\ndef area_triangle_sss(side1, side2, side3):\n    \"\"\"Returns the area of a triangle, given the lengths of its three sides.\"\"\"\n    \n    # Use Heron's formula\n    s = (side1 + side2 + side3)/2.0\n    return math.sqrt(s * (s-side1) * (s-side2) * (s-side3))",
        "sha1": "b4f0ffd1646cbea000f446297cf3465d7ea9cd95",
        "id": 71993
    },
    {
        "content": "def _parameter_present(query_string, key, value=None):\n    \"\"\"\n    Check whether the given key/value pair is present in the query string.\n    If the value is blank, it simply checks for the presence of the key\n    \"\"\"\n    already_there = False\n    if query_string:\n        for p in query_string.split('&'):\n            k, v = p.split('=')\n            if str(k) == str(key):\n                if value:\n                    if str(v) == str(value):\n                        already_there = True\n                else:\n                    already_there = True\n    return already_there",
        "sha1": "81f92197ba3caefbe9957102c8d29bfe48a255c8",
        "id": 58872
    },
    {
        "content": "def smooth_series(radius, col):\n    \"\"\"\n        Generates \"smoothed\" copy of input data by applying a rolling mean of the requested radius.\n\n        Args:\n            radius (int): number of values to include in rolling mean\n                        (e.g. radius = 1 means average values i, i-1 and i+1)\n            col (pd.Series): column data to be smoothed\n    \"\"\"\n\n    # Return original column if radius is less than 1\n    if radius <= 0:\n        return col\n\n    window = (radius * 2) + 1\n    return col.rolling(window, min_periods=1, center=True).mean()",
        "sha1": "a1ad9eb1af09c10547092c3a54f816596094c5dd",
        "id": 104750
    },
    {
        "content": "import re\n\n\ndef get_latest_hub_per_task(hub_module_paths):\n  \"\"\"Get latest hub module for each task.\n\n  The hub module path should match format \".*/hub/[0-9]*/module/.*\".\n  Example usage:\n  get_latest_hub_per_task(expand_glob([\"/cns/el-d/home/dune/representation/\"\n                                       \"xzhai/1899361/*/export/hub/*/module/\"]))\n  returns 4 latest hub module from 4 tasks respectivley.\n\n  Args:\n    hub_module_paths: a list of hub module paths.\n\n  Returns:\n    A list of latest hub modules for each task.\n\n  \"\"\"\n  task_to_path = {}\n  for path in hub_module_paths:\n    task_name, module_name = path.split(\"/hub/\")\n    timestamp = int(re.findall(r\"([0-9]*)/module\", module_name)[0])\n    current_path = task_to_path.get(task_name, \"0/module\")\n    current_timestamp = int(re.findall(r\"([0-9]*)/module\", current_path)[0])\n    if current_timestamp < timestamp:\n      task_to_path[task_name] = path\n  return sorted(task_to_path.values())",
        "sha1": "17d5f763ca3837dd6cf1fe1dff5b265a64dfa976",
        "id": 671915
    },
    {
        "content": "def survey_media(instance, filename):\n    \"\"\"Return an upload path for survey media.\"\"\"\n    if not instance.survey.id:\n        instance.survey.save()\n    return 'survey/{0}/{1}'.format(instance.survey.id, filename)",
        "sha1": "5a7aadf99634a19ef6d3dbba59a1a3935548defe",
        "id": 683856
    },
    {
        "content": "from typing import List\nfrom pathlib import Path\nfrom typing import Optional\n\n\ndef construct_matlab_script(\n    filepaths: List[Path],\n    fail_warnings: bool,\n    enable_cyc: bool,\n    enable_mod_cyc: bool,\n    ignore_ok_pragmas: bool,\n    use_factory_default: bool,\n    checkcode_config_file: Optional[Path] = None,\n) -> str:\n    \"\"\"Return the inline MATLAB script to run on the MATLAB instance.\n\n    Parameters\n    ----------\n\n    use_factory_default\n    filepaths: list of Path\n                            List of all filepaths to validate through MATLAB's checkcode function\n    fail_warnings: bool\n                            Whether to treat warnings as errors\n    enable_mod_cyc: bool\n                            Enable display of modified cyclomaticity complexity calculations for each file.\n    enable_cyc: bool\n                            Enable display of McCabe cyclomaticity camplexity calculations for each file.\n    ignore_ok_pragmas: bool\n                            Ignore %#ok checkcode suppression pragmas\n    use_factory_default: bool\n                            Ignore any checkcode config files and use factory defaults\n    checkcode_config_file: Path, optional\n                            An absolute path to a checkcode config file\n    Returns\n    -------\n    str\n        The MATLAB script to run on the MATLAB instance\n    \"\"\"\n    file_list = [f\"'{str(f)}'\" for f in filepaths]\n\n    level_option = \"'-m0'\" if fail_warnings else \"'-m2'\"\n    command: List = [level_option, \"'-id'\", \"'-struct'\"]\n    if enable_cyc:\n        command.append(\"'-cyc'\")\n\n    if enable_mod_cyc:\n        command.append(\"'-modcyc'\")\n\n    if ignore_ok_pragmas:\n        command.append(\"'-notok'\")\n\n    if use_factory_default:\n        command.append(\"'-config=factory'\")\n    elif checkcode_config_file:\n        command.append(f\"'-config={str(checkcode_config_file)}'\")\n\n    command = command + file_list\n    command_string: str = \", \".join(command)\n    return f\"clc;disp(jsonencode(checkcode({command_string})));quit;\"",
        "sha1": "363a664205ea3cadc7772f83840e2225d15d0418",
        "id": 32435
    },
    {
        "content": "def to_list(ls):\n    \"\"\"\n    Converts ``ls`` to list if it is a tuple, or wraps ``ls`` into a list if\n    it is not a list already\n    \"\"\"\n    if isinstance(ls, (list, tuple)):\n        return list(ls)\n    else:\n        return [ls]",
        "sha1": "e518c59d792b214e6b822258ce7bd01260627b85",
        "id": 175824
    },
    {
        "content": "def __equivalent_lists(list1, list2):\n    \"\"\"Return True if list1 and list2 contain the same items.\"\"\"\n    if len(list1) != len(list2):\n        return False\n\n    for item in list1:\n        if item not in list2:\n            return False\n\n    for item in list2:\n        if item not in list1:\n            return False\n\n    return True",
        "sha1": "f9ef5b643ed0ba4b6bddd3a6394af9a22ed118ee",
        "id": 674396
    },
    {
        "content": "def cover(X, itemset): # could be called \"rows_covered_by_itemset\"\n    \"\"\"\n    Returns the rows in X which satisfy the itemset\n    An itemset is satisfied when all elements of the itemset are evaluated to 1 (True)\n\n    Input:\n        X: pandas DataFrame (all one-hot encoded)\n        itemset: an iterable of column names representing an itemset\n\n    Returns:\n        X (subset): pandas DataFrame whose rows satisfy all itemset elements (have value 1)\n    \"\"\"\n    return X[(X[itemset]==1).all(axis=1)]",
        "sha1": "ebb70df60d9bb440807002d53cb9ef209cb679b0",
        "id": 119856
    },
    {
        "content": "def force_factor(rij2):\n    \"\"\"Lennard-Jones force magnitudefd exerted by atom j on atom i.\n\n    :param float|np.array rij2: squared interatomic distance from atom i to atom j\n    :return: fij\n    \"\"\"\n    rm2 = 1.0/rij2\n    rm6 = (rm2*rm2*rm2)\n    f = (1.0 - 2.0*rm6 )*rm6*rm2*6.0\n    return f",
        "sha1": "403dd12294925e33e1eaa8e1decba3cb249a487b",
        "id": 421446
    },
    {
        "content": "def document_to_dict(doc):\n    \"\"\"Convert a Firestore document to dictionary\"\"\"\n    if not doc.exists:\n        return None\n    doc_dict = doc.to_dict()\n    doc_dict['id'] = doc.id\n    return doc_dict",
        "sha1": "e1dfd1a5f3164b284fff658927cd9bc86dac393f",
        "id": 430797
    },
    {
        "content": "def _acct(v):\n    \"\"\"Convert to accounting (float) for excel, but default to original value.\"\"\"\n    try:\n        if v is None or v == '':\n            return ''\n        return float(v)\n    except ValueError:\n        return v",
        "sha1": "5d7985a3b5392a30a496a0a480d1109681024197",
        "id": 521296
    },
    {
        "content": "import time\n\n\ndef convertToUnixTime(rtstamp):\n    \"\"\"Converts given timestamp, seconds since Epoch(1/1/1970), to Ripple Timestamp, seconds since Ripple Epoch (1/1/2000)\n\n    Args:\n        tstamp (timestamp, optional): The timestamp (seconds from Epoch). Defaults to time.time().\n\n    Returns:\n        timestamp: Ripple Timestamp, seconds since Ripple Epoch (1/1/2000)\n    \"\"\"\n    ripple_epoch = time.mktime(time.strptime(\"20000101000000\", \"%Y%m%d%H%M%S\"))\n    return rtstamp + ripple_epoch",
        "sha1": "e810153d02b7450fe28811ef2a0e3f32137d76d2",
        "id": 421445
    },
    {
        "content": "def apply_at_node(modifier, primitive_set, terminal_set, tree, node_id):\n    \"\"\"\n    Recurse through the tree until the node is found\n    Then apply an operation to the node\n\n    Args:\n        modifier: callable function that modifies a Node\n        primitive_set: dictionary where (key, value) is (output_type, [{\"name\", \"input_types\", \"group\"}, ...])\n        terminal_set:  dictionary where (key, value) is (output_type, [{\"name\", \"generator\", \"static\"}, ...])\n        tree: Node containing full tree\n        node_id: string id directing which nodes to traverse\n\n    Returns:\n        Node containing full tree\n    \"\"\"\n    # We have reached the correct node if the string is empty\n    if node_id == \"\":\n        # Apply the modification\n        tree = modifier(primitive_set, terminal_set, tree)\n\n        # Return the modified tree node\n        return tree\n\n    # This gets casted to an integer so it can be used as an index\n    node_index = int(node_id[0])\n    # The next node is the correct node if the length of the node_id is 1\n    if len(node_id) == 1:\n        next_id = \"\"\n    else:\n        next_id = node_id[1:]\n\n    # Set the correct child to be the result of the recursion\n    tree.args[node_index] = apply_at_node(modifier, primitive_set, terminal_set, tree.args[node_index], next_id)\n\n    # Update the tree_ids and input_ids of the tree\n    tree.update_tree_ids()\n    tree.update_input_ids()\n\n    return tree",
        "sha1": "7d5e3f456bb2294dc74ec4e7ba0cc272fe205f3f",
        "id": 287672
    },
    {
        "content": "def calcResult(dataf,assL,row):\n    \"\"\"\n    calcResult takes a given row in a module marks file and calculates the overall module mark earned.\n    Inputs: \n    dataf: file reference\n    assL: list of component weights\n    row: index of line being evaluated\n    \n    Output: \n    module mark earned\n    \"\"\"\n    result=0.\n    for i, val in enumerate(assL):\n        #print i, val, dataf.iloc[row,i+4]\n        result=result+float(dataf.iloc[row,i+4])*val/100.\n    \n    return result",
        "sha1": "9a0d553ffccf52bee8f63829d0398ab1348991a8",
        "id": 559081
    },
    {
        "content": "from typing import List\n\n\ndef find_closest_pair_1d(points: List[float]) -> List[float]:\n    \"\"\"\n    Finds the closest pair in the given 1D points.\n    :param points: list[float]\n    :return: list[float]\n    \"\"\"\n    # Check whether the input point list is null or empty\n    if not points:\n        return []\n\n    if len(points) == 1:\n        return []\n\n    # Sort the points   [O(nlog n)]\n    points.sort()\n    # The closest pair must be adjacent in the ordering.\n\n    # Iterate over the adjacent pairs, and find the closest pair   [O(n)]\n    p1, p2 = points[0], points[1]\n    min_distance = points[1] - points[0]\n    for i in range(1, len(points) - 1):\n        if points[i + 1] - points[i] < min_distance:\n            p1, p2 = points[i], points[i + 1]\n            min_distance = points[i + 1] - points[i]\n    return [p1, p2]",
        "sha1": "5e0c03e7a80aa6a97754cebf82bd09585ca73f24",
        "id": 452993
    },
    {
        "content": "def add(x, y):\n    \"\"\"Return the addition of values x & y\"\"\"\n    return (x  + y)",
        "sha1": "9cd2540c29e801c49ad03765c27cb31d3f37d0d5",
        "id": 79793
    },
    {
        "content": "def _find_interval(knots, spline_degree, x_val, last_left, num_bases):\n    \"\"\"\n    Finds the knot interval containing the x-value.\n\n    Parameters\n    ----------\n    knots : numpy.ndarray, shape (K,)\n        The array of knots for the spline. Should be padded on each end with\n        `spline_degree` extra knots.\n    spline_degree : int\n        The spline degree.\n    x_val : float\n        The x-value to find the interval for.\n    last_left : int\n        The previous output of this function. For the first call, use any value\n        less than `spline_degree` to start.\n    num_bases : int\n        The total number of basis functions. Equals ``len(knots) - spline_degree - 1``,\n        but is precomputed rather than having to recompute each function call.\n\n    Returns\n    -------\n    int\n        The index in `knots` such that ``knots[index] <= x_val < knots[index + 1]``.\n\n    \"\"\"\n    left = last_left if spline_degree < last_left < num_bases else spline_degree\n\n    # x_val less than expected so shift knot interval left\n    while x_val < knots[left] and left != spline_degree:\n        left -= 1\n\n    left += 1\n    while x_val >= knots[left] and left != num_bases:\n        left += 1\n\n    return left - 1",
        "sha1": "8f1786cfede16af25b6096c74cec6edadc1f0f59",
        "id": 324673
    },
    {
        "content": "def sse_derivative(y, output):\n    \"\"\"\n    Derivative of quadratic cost\n    \"\"\"\n    return (output - y)",
        "sha1": "8e62558c9b476bc267334b542809f7dbe803c525",
        "id": 407187
    },
    {
        "content": "import requests\n\n\ndef served_by_nginx(url):\n    \"\"\"Return True if url returns 200 and is served by Nginx.\"\"\"\n    r = requests.get(url, allow_redirects=False)\n    status = (r.status_code == 200)\n    nginx = ('x-served' in r.headers and r.headers['x-served'] == 'Nginx')\n    return all([status, nginx])",
        "sha1": "2070f6cfd503af511099e0615980b35bd1ed3dae",
        "id": 252212
    },
    {
        "content": "import smtplib\n\n\ndef sendEmail(\n    sendAddr: str,\n    password: str,\n    recvAddr: str,\n    body: str,\n    server: str,\n    port: int,\n    sub: str = \"No Subject\",\n) -> bool:\n    \"\"\"Sends an email using given arguments\"\"\"\n    with smtplib.SMTP(server, port) as smtp:\n        smtp.ehlo()\n        smtp.starttls()\n        smtp.ehlo()\n\n        smtp.login(sendAddr, password)\n\n        subMsg: str = \"Subject: {}\".format(sub)\n        bodyMsg: str = \"\\n\\n {}\".format(body)\n\n        finalMsg: str = subMsg + bodyMsg\n\n        smtp.sendmail(sendAddr, recvAddr, finalMsg)\n        return True",
        "sha1": "40c3e5402b79658c4906d1f31c07961d495e75fa",
        "id": 111050
    },
    {
        "content": "from pathlib import Path\nfrom typing import Any\nimport hashlib\n\n\ndef recursive_sha256(path: Path, hashsum: Any = None) -> str:\n    \"\"\"\n    Calculates sha256 hash of the file contents recursively.\n\n    Args:\n        path (Path): Parent path of contents\n        hashsum (Optional[hashlib._HASH]): Current checksum of files if any\n\n    Returns:\n        str: Accumulated digest hex number string with lowercase letters like\n            \"03e93aae89012a2b06b77d5684f34bc2e27cd64e42108175338f20bec11c770a\"\n\n    Raises:\n        ValueError: When `path` does not exist in the system\n    \"\"\"\n    if not path.exists():\n        raise ValueError(\"Path does not exist\")\n\n    hashsum = hashlib.sha256() if not hashsum else hashsum\n\n    if path.is_dir():\n        for item in path.iterdir():\n            recursive_sha256(item, hashsum)\n    else:\n        hashsum.update(path.read_bytes())\n\n    return str(hashsum.hexdigest())",
        "sha1": "d8388793f50419536bba864bd24f54fc2f361e2c",
        "id": 72169
    },
    {
        "content": "def visible(filename):\n    \"\"\"Exclude hidden files\"\"\"\n    return not filename.startswith('.')",
        "sha1": "987c209ba189247241dca3b6b70888283f5eaf9e",
        "id": 584017
    },
    {
        "content": "def build_cells(width, height):\n    \"\"\"Create and return a 'width' x 'height' grid of two-tuples\n\n    Parameters\n    ----------\n    width : int\n        Width of the cells\n    height : int\n        Height of the cells\n\n    >>> cells = build_cells(2, 2)\n    >>> len(cells)\n    4\n\n    \"\"\"\n    cells = []\n    for y in range(height):\n        for x in range(width):\n            cells.append((x, y))\n    return cells",
        "sha1": "d146d5f9ffed33bfd719048146fdc53c8c00610b",
        "id": 279369
    },
    {
        "content": "def convert_symbol(text, l1, l2, quote='\"'):\n  \"\"\"convert symbol l1 to l2 if inside quote\"\"\"\n  text2 = ''\n  inside = False\n  for c in text:\n    if c == quote:\n      inside = not inside\n    elif c == l1:\n      if inside:\n        text2 += l2\n      else:\n        text2 += l1\n    else:\n       text2 += c\n  return text2",
        "sha1": "a77b27fc5a2eaacb8f65db8e1771f89cb24e3ca4",
        "id": 153793
    },
    {
        "content": "def from_hex_msb(text):\n    \"\"\"Decode a hex encoded fingerprint string where the bits and bytes are in MSB order\n\n    >>> from_hex_msb('10f2')\n    (None, '\\\\xf2\\\\x10')\n    >>>\n\n    Raises a ValueError if the hex string is not a multiple of 2 bytes long\n    or if it contains a non-hex character.\n    \"\"\"\n    return (None, text.decode(\"hex\")[::-1])",
        "sha1": "426e52b3359d628caa54ed98dbb00f8cf2e0e1fe",
        "id": 151292
    },
    {
        "content": "def _rhex(byte):\n    \"\"\"converts bytecode to hex, while reading from right to left\"\"\"\n    rhex = byte[::-1].hex()\n    return (rhex)",
        "sha1": "62cbab235ad1638d1c4a9665ae5e6761e4648b68",
        "id": 247017
    },
    {
        "content": "def greedy_cow_transport(cows,limit=10):\n    \"\"\"\n    Uses a greedy heuristic to determine an allocation of cows that attempts to\n    minimize the number of spaceship trips needed to transport all the cows. The\n    returned allocation of cows may or may not be optimal.\n    The greedy heuristic should follow the following method:\n\n    1. As long as the current trip can fit another cow, add the largest cow that will fit\n        to the trip\n    2. Once the trip is full, begin a new trip to transport the remaining cows\n\n    Does not mutate the given dictionary of cows.\n\n    Parameters:\n    cows - a dictionary of name (string), weight (int) pairs\n    limit - weight limit of the spaceship (an int)\n    \n    Returns:\n    A list of lists, with each inner list containing the names of cows\n    transported on a particular trip and the overall list containing all the\n    trips\n    \"\"\"\n    def all_in(status):\n        for i in range(len(status)):\n            if (status[i] == 0):\n                return False\n        return True\n\n    def summed(list, cows):\n        sum = 0\n        for i in cows:\n            if i in list:\n                sum = sum + cows[i]\n        return sum\n    \n    all_trips = []\n    cows_sort = sorted(cows.items(), key = lambda c:(c[1], c[0]), reverse = True)\n    print(cows_sort)\n    status = [0]*len(cows)\n\n    while(not(all_in(status))):\n        this_trip = []\n        j = 0\n        for i,k in cows_sort:\n            if(status[j] == 0 and summed(this_trip, cows)<limit):\n                if(cows[i] > limit):\n                    status[j] = 2\n                elif (cows[i] > limit - summed(this_trip, cows)):\n                    status[j] = 0\n                else:\n                    status[j] = 1\n                    this_trip.append(i)\n            j = j + 1\n        if not(len(this_trip) == 0):\n            all_trips.append(this_trip)\n            this_trip = []\n    print(all_trips)\n    return all_trips",
        "sha1": "91b6e020bf1660290a540fbce260b9e8be32546d",
        "id": 161797
    },
    {
        "content": "def hash_bytesiter(bytesiter, hasher, ashexstr=False):\n    \"\"\"Calculates the hash of an interation of bytes\n\n    :param bytesiter: an iterator of bytes\n    :param hasher: a hasher function like `hashlib.md5()` or `hashlib.sha256()`\n    :param ashexstr: whether to output the hexdigest or the normal packed bytes digest \n    :returns: either a bytes object or a str if `ashexstr`\n    :rtype: bytes or str\n    \"\"\"\n    for block in bytesiter:\n        hasher.update(block)\n    return hasher.hexdigest() if ashexstr else hasher.digest()",
        "sha1": "51ecba99415eee555844aa7666e4b3935c9bdc57",
        "id": 295224
    },
    {
        "content": "def do_nothing_decorator(*_args, **_kws):\n    \"\"\"\n    Callable decorator that does nothing.\n\n    Arguments are catched, but ignored.\n    This is very useful to provide proxy for decorators that may not be\n    defined.\n\n    Args:\n        *_args: Positional arguments.\n        **_kws: Keyword arguments.\n\n    Returns:\n        result (callable): The unmodified callable.\n    \"\"\"\n\n    def wrapper(f):\n        return f\n\n    if len(_args) > 0 and not callable(_args[0]) or len(_kws) > 0:\n        return wrapper\n    elif len(_args) == 0:\n        return wrapper\n    else:\n        return _args[0]",
        "sha1": "8b8dff61d9248c767025e65fb5771b73e9c661c1",
        "id": 667144
    },
    {
        "content": "def qrcode_size_table() -> dict[int, int]:\n    \"\"\"Dictionary that contains the size of the QR-code depending\n    on the version.\n\n    Returns:\n        dict[int, int]: A dictionary that contains data in the\n        form (version: size)\n    \"\"\"\n    table = {\n        1: 21, 2: 25, 3: 29, 4: 33, 5: 37, 6: 41, 7: 45, 8: 49, 9: 53, 10: 57,\n        11: 61, 12: 65, 13: 69, 14: 73, 15: 77, 16: 81, 17: 85, 18: 89, 19: 93,\n        20: 97, 21: 101, 22: 105, 23: 109, 24: 113, 25: 117, 26: 121, 27: 125,\n        28: 129, 29: 133, 30: 137, 31: 141, 32: 145, 33: 149, 34: 153, 35: 157,\n        36: 161, 37: 165, 38: 169, 39: 173, 40: 177\n    }\n\n    return table",
        "sha1": "f4de761348a2331f047786b09f58b45d039efb7f",
        "id": 577659
    },
    {
        "content": "def is_acceptable_multiplier(m):\n    \"\"\"A 61-bit integer is acceptable if it isn't 0 mod 2**61 - 1.\n    \"\"\"\n    return 1 < m < (2 ** 61 - 1)",
        "sha1": "d099dd53296138b94ca5c1d54df39b9cf7ad2b5d",
        "id": 7653
    },
    {
        "content": "from typing import Dict\n\n\ndef to_label_selector(tags: Dict[str, str]) -> str:\n    \"\"\"Convert tags to label selector to embed in query to K8s API server.\"\"\"\n    label_selector = \"\"\n    for k, v in tags.items():\n        if label_selector != \"\":\n            label_selector += \",\"\n        label_selector += \"{}={}\".format(k, v)\n    return label_selector",
        "sha1": "faa9bf49f879a455893ee10220262fa9c240b203",
        "id": 259219
    },
    {
        "content": "def first(seq):\n    \"\"\" Returns the first element in the sequence `seq`. \"\"\"\n    return seq[0]",
        "sha1": "b65b824da4b006fc916fd457ee4288613078985b",
        "id": 611165
    },
    {
        "content": "def canonicalize_paths(paths):\n    \"\"\"Canonicalize a list of paths.\"\"\"\n    paths = [p.canonicalize() for p in paths]\n    paths.sort()\n    return paths",
        "sha1": "be12c8df75ac4e9a82edfd4c271508e0b9b9db8c",
        "id": 346515
    },
    {
        "content": "def hash_coord(coord):\n    \"\"\" For a dictionary with an x and y key, return a string of the int values of those keys.  Lets us use a coordinate for a dictionary key. \"\"\"\n    return f\"{coord['x']}:{coord['y']}\"",
        "sha1": "757570ff59be3b851091bc03f9096853b338f73c",
        "id": 387941
    },
    {
        "content": "def phase1(p1, child, idx):\n    \"\"\"Phase a genotype given only one parents genotype.\n\n    Arguments:\n        p1 (string): genotype of parent  expressed in form 0/1, 0/0 etc\n        child (string): genotype of child\n        idx (integer): 0 or 1. If 0 then the allele inherited from the parent\n                       appears to the left of | symbol in the phased genotype,\n                       and to the right of the | symbol if idx is 1\n    Returns:\n        The phased genotype of the child if it is unambiguously resolvable.\n        Otherwise the genotype is returned as is, i.e. unphased\n\n    >>> phase1('0/1', '0/1', 0)\n    '0/1'\n    >>> phase1('0/2', '1/2', 0)\n    '2|1'\n    >>> phase1('0/3', '1/3', 1)\n    '1|3'\n    >>> phase1('./.', '0/1', 1)\n    '0/1'\n    >>> phase1('./.', './.', 0)\n    './.'\n    \"\"\"\n    #ca, _, cb = child\n    ca, cb = child.split('/')\n    if ca == '.':\n        sep = '/'\n    elif ca == cb and ca in p1:\n        sep = '|'\n    elif ca in p1 and cb not in p1:\n        sep = '|'\n    elif cb in p1 and ca not in p1:\n        cb, ca = ca, cb\n        sep = '|'\n    else:\n        sep = '/'\n    if sep == '|' and idx == 1:\n        ca, cb = cb, ca\n    return ca + sep + cb",
        "sha1": "ffac44df9cd942b4232629b75375cf59d89efa42",
        "id": 504540
    },
    {
        "content": "def PySequence_GetSlice(space, w_obj, start, end):\n    \"\"\"Return the slice of sequence object o between i1 and i2, or NULL on\n    failure. This is the equivalent of the Python expression o[i1:i2].\"\"\"\n    return space.getslice(w_obj, space.newint(start), space.newint(end))",
        "sha1": "639e890930dee2a20cbe1b9f5be6bd3b89a82c8a",
        "id": 617778
    },
    {
        "content": "def gen_title(title, special_issue=None):\n    \"\"\"\n    Generates article title in APA format.\n\n    :param title:           original title\n    :type title:            str\n    :param special_issue:   special issue number of article, default is None\n    :type special_issue:    str or None\n    :return:                title plus special issue (if any) printed in APA format\n    :rtype:                 str\n    \"\"\"\n\n    ret = ''\n    if title:\n        ret += ' %s' % title.capitalize()\n        if special_issue:\n            ret += ' [%s]' % special_issue\n        if not ret.endswith('.'):\n            ret += '.'\n    return ret",
        "sha1": "6f44e74ba7ec5bf7e96c48fec88d3c10bdbe2a46",
        "id": 371039
    },
    {
        "content": "def does_object_pass_filter(obj, filter):\n    \"\"\"\n    Returns True iff the given obj satisfies the given filter dictionary.\n    Note that this only supports simple equality constraints (although it\n    can traverse a relation to a single related object specified with double\n    underscore notation). It does not support more complex filter conditions\n    such as __gt.\n    Example:\n        does_object_pass_filter(obj, {\"key\": \"value\", \"parent__key\": \"value2\"})\n    \"\"\"\n    for field, expected_value in filter.items():\n        assert field != \"\"\n        components = field.split(\"__\")\n        actual_value = getattr(obj, components[0])\n        for component in components[1:]:\n            actual_value = getattr(actual_value, component)\n        if actual_value != expected_value:\n            return False\n    return True",
        "sha1": "b1d941f8b6a607265b9ef900d444260281b02be4",
        "id": 549124
    },
    {
        "content": "def find_best_argname(iarg: int, pname: str, cname: str):\n    \"\"\"Find the best name for the ith argument of a function.\n\n    Args:\n        iarg: Index of the argument, use for default.\n        pname: Name of the argument in the python signature.\n        cname: Name of the argument in the C++ signature.\n\n    Returns:\n        The best name for the corresponding argument.\n    \"\"\"\n    if not cname and not pname:\n        return \"arg{}\".format(iarg + 1)\n\n    return pname",
        "sha1": "f4de3cf78348e860a9350aacfc5bb0bbb1a027ca",
        "id": 577112
    },
    {
        "content": "def adjust_country_name(country, purpose):\n    \"\"\"\n    Use a dictionary to adapt the basic country name to the name used in specific spreadsheets.\n\n    Args:\n        country: String for the original country name\n        purpose: The \"purpose\", which is the string to index the second tier of the country_name_adaptations dictionary\n    Returns:\n        adjusted_country_name: Adjusted string\n    \"\"\"\n\n    country_name_adaptations =\\\n        {'Kyrgyzstan': {'demographic': 'Kyrgyz Republic'},\n         'Moldova': {'tb': 'Republic of Moldova'},\n         'Philippines': {'tb': 'Philippines (the)'}}\n    if country in country_name_adaptations:\n        if purpose in country_name_adaptations[country]:\n            return country_name_adaptations[country][purpose]\n    return country",
        "sha1": "f544188c7933455aa4b24356979f0d78f5300fb8",
        "id": 452336
    },
    {
        "content": "def check_protocol(url):\n    \"\"\"Check URL for a protocol.\"\"\"\n    if url and (url.startswith('http://') or url.startswith('https://')):\n        return True\n    return False",
        "sha1": "b4b169e9717cabcdd543ad57e57bde0b941f4f26",
        "id": 523228
    },
    {
        "content": "def release_branch_name(config):\n    \"\"\"\n    build expected release branch name from current config\n\n    \"\"\"\n    branch_name = \"{0}{1}\".format(\n        config.gitflow_release_prefix(),\n        config.package_version()\n    )\n    return branch_name",
        "sha1": "0d97c515aca8412882c8b260405a63d20b4b0f63",
        "id": 706636
    },
    {
        "content": "def unchanged_shape(op):\n  \"\"\"Shape function for ops that output an tensor like their first input.\"\"\"\n  return [op.inputs[0].get_shape()]",
        "sha1": "e917c31c9c59f275877478177463d6877590dba2",
        "id": 548917
    },
    {
        "content": "from typing import Sequence\n\n\ndef filter_overrides(overrides: Sequence[str]) -> Sequence[str]:\n    \"\"\"\n    :param overrides: overrides list\n    :return: returning a new overrides list with all the keys starting with hydra. filtered.\n    \"\"\"\n    return [x for x in overrides if not x.startswith(\"hydra.\")]",
        "sha1": "37750b6a9699e258c7b0f35b24342e56e8a1c34e",
        "id": 658232
    },
    {
        "content": "def update(pos, dims, orig_dims):\n    \"\"\" Finds neg number in dims and restores it to the value in\n    original-dims and subtracts one from the number after it\n    e.g (0 -1 0) (0 1 0)-> (0 1 -1) \"\"\"\n    ret_dim = []\n    for i, dim in enumerate(dims):\n        if pos == i:\n            ret_dim.insert(i, orig_dims[i])\n        else:\n            ret_dim.insert(i, dim)\n    ret_dim[pos+1] = dims[pos+1] - 1\n    return tuple(ret_dim)",
        "sha1": "1c6c1618890d6c73fcaee28053cec5b946309a44",
        "id": 622861
    },
    {
        "content": "def rename_and_subset_cols(df, dict_rename, list_cols, include=True):\n    \"\"\"\n    Method to rename and subset certain columns from a DataFrame.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        DataFrame with several columns\n    dict_rename : dict(str:str)\n        Dictionary with a dictionary where the keys are the original columnnames and the values are the new column names\n    list_cols : list(str)\n        List of columns to keep/drop\n    include : bool\n        Boolean value to indicate if the columns from list_cols should be kept or dropped. Default 'true' to keep.\n\n    Returns\n    -------\n    pd.DataFrame\n    \"\"\"\n\n    df = df.rename(columns=dict_rename)\n    if include:\n        df = df[list_cols]\n    else:\n        df = df.drop(list_cols, axis=1)\n\n    return df",
        "sha1": "ae9a56cffa9120fa86b8410b3f7db5757a9ea73f",
        "id": 96067
    },
    {
        "content": "from pathlib import Path\nimport json\n\n\ndef abilities() -> dict:\n    \"\"\"\n    Fixture to read abilities.json and provide the dict\n    :return: The abilities dict\n    \"\"\"\n    # Read in the abilities data\n    path_to_file = (\n        Path(__file__).resolve().parents[1]\n        / \"worlds_worst_serverless\"\n        / \"worlds_worst_combat\"\n        / \"abilities.json\"\n    )\n    with path_to_file.open() as json_file:\n        abilities = json.load(json_file)\n\n    return abilities",
        "sha1": "db2b50c4e2bda8d9b04d4562d153aec0c6fe8766",
        "id": 493759
    },
    {
        "content": "from typing import Union\nfrom typing import List\n\n\ndef scope_to_list(scope: Union[str, List]) -> List:\n    \"\"\"Convert a token scope to a list if necessary.\n\n    A scope in a theme should always be a string or list,\n    but just in case return an empty list if not\n\n    :param scope: The scope\n    :returns: Scope as list\n    \"\"\"\n    if isinstance(scope, list):\n        return scope\n    if isinstance(scope, str):\n        return [scope]\n    return []",
        "sha1": "41da6f9d70da060074e2bf58ad3782563b897949",
        "id": 103597
    },
    {
        "content": "import yaml\n\n\ndef parse_as_yaml(data):\n    \"\"\"Attempt to parse data as yaml.\"\"\"\n    return yaml.load(data, Loader=yaml.FullLoader)",
        "sha1": "c9552cff2836907eaf7ca78ebd0dfcd6852592d1",
        "id": 295003
    },
    {
        "content": "import hashlib\nimport json\n\n\ndef get_unique_id(d, n=5):\n    \"\"\"Generate a unique hash string from a dictionary.\"\"\"\n\n    unique_id = hashlib.sha1(\n        json.dumps(d, sort_keys=True).encode()\n    ).hexdigest()\n\n    return unique_id[:n]",
        "sha1": "695402c0ecfdef3b73025a8b483343116e5a7649",
        "id": 465084
    },
    {
        "content": "def set_crs(df, crs):\n    \"\"\"\n    Sets the coordinate reference system `crs` based on the information in the dataframe `df`.\n    :param df: DataFrame\n    :param crs: Coordinate Reference System\n    :return:\n    \"\"\"\n    if type(df.crs).__name__ in ['CRS']:\n        if str(df.crs).upper() != crs:\n            df = df.to_crs(crs)\n    else:\n        df.crs = crs\n    return df",
        "sha1": "d83916c4d07688065dc781d4462c663621dbf1dd",
        "id": 513947
    },
    {
        "content": "import re\n\n\ndef parse_deployment(x):\n    \"\"\"Extract information from deployment command.\"\"\"\n    # address is 64, tx_hash is 64 chars long\n    address, tx_hash = re.findall(\"0x[\\\\da-f]{1,64}\", str(x))\n    return address, tx_hash",
        "sha1": "005c3dfc85094dccf96a65349c2abcd58e1b32ad",
        "id": 441466
    },
    {
        "content": "def get_paragraph_number(identifier):\n    \"\"\"\n    Extracts the number of a paragraph from the identifier, and the parent directory of the\n    paragraph.\n\n    Parameters\n    ----------\n    directory : Path\n        A parent directory of a paragraph.\n    identifier : str\n        An identifier of a paragraph.\n\n    Returns\n    -------\n    int\n        The number of the paragraph.\n    \"\"\"\n    paragraph_number = int(identifier.split('_')[-1]) - 1\n    assert paragraph_number >= 0\n    return paragraph_number",
        "sha1": "73009df5f9287fa8a31984516a91367fdf388e54",
        "id": 328884
    },
    {
        "content": "def shift_and_trim(array, dist):\n    \"\"\"Shift and trim unneeded item.\n\n    :params array: list like iterable object\n    :params dist: int\n\n    Example::\n\n        >>> array = [0, 1, 2]\n\n        >>> shift_and_trim(array, 0)\n        [0, 1, 2]\n        >>> shift_and_trim(array, 1)\n        [0, 1]\n\n        >>> shift_and_trim(array, -1)\n        [1, 2]\n\n        >>> shift_and_trim(array, 3)\n        []\n\n        >>> shift_and_trim(array, -3)\n        []\n    \"\"\"\n    length = len(array)\n    if length == 0:\n        return []\n\n    if (dist >= length) or (dist <= -length):\n        return []\n    elif dist < 0:\n        return array[-dist:]\n    elif dist > 0:\n        return array[:-dist]\n    else:\n        return list(array)",
        "sha1": "44ab59931e63b747e464a4837fea492fc0a41990",
        "id": 431173
    },
    {
        "content": "def postprocess(data):\n    \"\"\"\n    Multiply column x by 2.\n    \"\"\"\n    \n    col = data.cols['x']\n    data.data[:, col] = 2 * data.data[:, col]\n    return data",
        "sha1": "09a6edfe1e143f69720420fa847878f4322dcc98",
        "id": 373195
    },
    {
        "content": "def getDescribe(series, percentiles = [.25, .5, .75]):\n    \"\"\"Get describe of series\n\n    Args:\n        series (Series): data series\n        percentiles: the percentiles to include in the output\n\n    Returns:\n        Series: the describe of data include mean, std, min, max and percentiles\n    \"\"\"\n    d = series.describe(percentiles)\n    return d.drop('count')",
        "sha1": "ce95f06081617a591ca94de9d2025983efcaea68",
        "id": 57318
    },
    {
        "content": "def title(text, level=0):\n    \"\"\"Given a title, format it as a title/subtitle/etc.\"\"\"\n    return '\\n' + text + '\\n' + '=-~_#%^' [level] * len(text) + '\\n\\n'",
        "sha1": "739d11a9a085feb0bdf6748b9854d436dc1b0295",
        "id": 631648
    },
    {
        "content": "def course_in(courses):\n    \"\"\"Constraint the course to a given list of courses.\n\n    List of courses is represented as \"{abbreviation} {course_number}\"\n    e.g. course_in([\"COMPSCI 61A\", \"BIO 1A\"])\n    \"\"\"\n    def satisfies(course):\n        key = \"%s %s\" % (course.abbreviation, course.course_number)\n        return key in courses\n    return satisfies",
        "sha1": "da390461505f7ff8924c0e9c009549fefcf275d9",
        "id": 235589
    },
    {
        "content": "def clean_records (json_recs, args=None):\n  \"\"\"\n  Remove unwanted fields from each record (dictionary) in the given list and\n  return the list of cleaned records.\n  Arguments:\n    json_recs: a list of records, each one representing metrics for a single image.\n    args: an optional dictionary of arguments which may contain a list of\n          fields to be removed.\n  \"\"\"\n  if (args):\n    fields_to_remove = args.get('fields_to_remove')\n    if (fields_to_remove):\n      for rec in json_recs:\n        for field in fields_to_remove:\n          if (field in rec):\n            rec.pop(field)\n  return json_recs",
        "sha1": "54d55b99650f1a17ac571c23cfbc481bb5439073",
        "id": 269502
    },
    {
        "content": "def format_input_source(input_source_name, input_source_number):\n    \"\"\"Format input source for display in UI.\"\"\"\n    return \"{} {}\".format(input_source_name, input_source_number)",
        "sha1": "d40fd07f86d41bb0a9e25d85b645aff6812c34d7",
        "id": 534705
    },
    {
        "content": "import torch\n\n\ndef get_legal_action_mask_torch(n_actions, legal_actions_list, device, dtype=torch.uint8):\n    \"\"\"\n    Args:\n        legal_actions_list (list):  List of legal actions as integers, where 0 is always FOLD, 1 is CHECK/CALL.\n                                    2 is BET/RAISE for continuous PokerEnvs, and for DiscretePokerEnv subclasses,\n                                    numbers greater than 1 are all the raise sizes.\n\n        device (torch.device):      device the mask shall be put on\n\n        dtype:                      dtype the mask shall have\n\n    Returns:\n        torch.Tensor:               a many-hot representation of the list of legal actions.\n    \"\"\"\n    idxs = torch.LongTensor(legal_actions_list, device=device)\n    mask = torch.zeros((n_actions), device=device, dtype=dtype)\n    mask[idxs] = 1\n    return mask",
        "sha1": "acd0d5497b16a1c72fd5f04e43301f2d0766d2bf",
        "id": 614086
    },
    {
        "content": "from pathlib import Path\n\n\ndef create_non_existent_folder(folder_path):\n    \"\"\"Create a folder if it does not exist yet.\n\n    Parameters\n    ----------\n    folder_path : Path\n        Path of folder to verify/create.\n\n    Returns\n    -------\n    Path\n        Path of verified/created folder.\n    \"\"\"\n    # Normalize input path and attempt to create folder. If it already\n    # exists, do nothing.\n    folder_path = Path(folder_path)\n    try:\n        folder_path.mkdir()\n        print(str(folder_path), 'folder created')\n        return folder_path\n    except FileExistsError:\n        return folder_path",
        "sha1": "15b0531c8fb7d61d149617f5ff6e0468a650389c",
        "id": 438500
    },
    {
        "content": "def check_execution(executor, nb, raise_fast):\n    \"\"\"Check that all code cells with source have been executed without error.\"\"\"\n    error = None\n    for cell in nb.get(\"cells\", []):\n\n        # Only check code cells\n        if cell[\"cell_type\"] != \"code\":\n            continue\n\n        if cell[\"source\"] and cell[\"execution_count\"] is None:\n            error = \"Notebook has unexecuted code cell(s).\"\n            if raise_fast:\n                raise RuntimeError(error)\n            break\n        else:\n            for output in cell[\"outputs\"]:\n                if output[\"output_type\"] == \"error\":\n                    if output[\"ename\"] in executor.allow_error_names:\n                        continue\n                    error = \"\\n\".join(output[\"traceback\"])\n                    if raise_fast:\n                        raise RuntimeError(\"\\n\" + error)\n                    break\n\n    return error",
        "sha1": "0b50c44d4c454c655f8077c9c07385fc8bad2b7b",
        "id": 437023
    },
    {
        "content": "import math\n\n\ndef haversine(phi1, lambda1, phi2, lambda2, radius):\n\t\"\"\" Calculates distance between two points on sphere of given radius.\n\tPoints are specified by their angles:\n\t- phi: vertical position (latitude);\n\t- lambda: horizontal position (longitude).\n\t\"\"\"\n\tphi1, lambda1, phi2, lambda2 = map(math.radians, (phi1, lambda1, phi2, lambda2))\n\td_phi = phi2 - phi1\n\td_lambda = lambda2 - lambda1\n\ta = math.sin(d_phi/2.) * math.sin(d_phi/2.) + math.cos(phi1) * math.cos(phi2) * math.sin(d_lambda/2.) * math.sin(d_lambda/2.)\n\tc = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n\td = radius * c\n\treturn d",
        "sha1": "331d7721613b62a26928c9e5c6c8bd8b105f0f78",
        "id": 559083
    },
    {
        "content": "def backhaul_quantity(i, new_backhaul):\n    \"\"\"\n    Indicator for whether a new backhaul needs to be built or not.\n\n    Parameters\n    ----------\n    i : int\n        Site build number.\n    new_backhaul : int\n        Number of new backhaul links needing to be built.\n\n    Returns\n    -------\n    backhaul_quant : bool\n        Indicator for whether to build a new backhaul or not.\n\n    \"\"\"\n    if i <= new_backhaul:\n        return 1\n    else:\n        return 0",
        "sha1": "5fb3f775f0e76305f1f8c80aaa8c6edf76d26be0",
        "id": 577278
    },
    {
        "content": "def dmp_transformation_system(Y, V, alpha_y, beta_y, goal_y, goal_yd, goal_ydd,\n                              execution_time):\n    \"\"\"Compute acceleration generated by transformation system of DMP.\"\"\"\n    return (alpha_y * (beta_y * (goal_y - Y) + execution_time * (goal_yd - V))\n            ) / execution_time ** 2 + goal_ydd",
        "sha1": "18bd6ecc6bede852e976b9808a131c8e86adeb82",
        "id": 237022
    },
    {
        "content": "def get_setup(job=None):\n    \"\"\"\n    Return the resource specific setup.\n\n    :param job: optional job object.\n    :return: setup commands (list).\n    \"\"\"\n\n    setup_commands = ['source /ccs/proj/csc108/athena_grid_env/setup.sh',\n                      'source $MODULESHOME/init/bash',\n                      'tmp_dirname=/tmp/scratch',\n                      'tmp_dirname+=\"/tmp\"',\n                      'export TEMP=$tmp_dirname',\n                      'export TMPDIR=$TEMP',\n                      'export TMP=$TEMP',\n                      'export LD_LIBRARY_PATH=/ccs/proj/csc108/AtlasReleases/ldpatch:$LD_LIBRARY_PATH',\n                      'export ATHENA_PROC_NUMBER=16',\n                      'export G4ATLAS_SKIPFILEPEEK=1',\n                      'export PANDA_RESOURCE=\\\"ORNL_Titan_MCORE\\\"',\n                      'export ROOT_TTREECACHE_SIZE=1',\n                      'export RUCIO_APPID=\\\"simul\\\"',\n                      'export RUCIO_ACCOUNT=\\\"pilot\\\"',\n                      'export CORAL_DBLOOKUP_PATH=/ccs/proj/csc108/AtlasReleases/21.0.15/nfs_db_files',\n                      'export CORAL_AUTH_PATH=$SW_INSTALL_AREA/DBRelease/current/XMLConfig',\n                      'export DATAPATH=$SW_INSTALL_AREA/DBRelease/current:$DATAPATH',\n                      'unset FRONTIER_SERVER',\n                      ' ']\n\n    return setup_commands",
        "sha1": "fea5757a9882c816e5b1f10112c0a23e12aea7fb",
        "id": 352120
    },
    {
        "content": "import re\n\n\ndef edit_dist_func(value):\n    \"\"\"Edit function expression to be latex and plot readable.\n\n    Args:\n        value (str): the distribution function expressed as a string.\n            ex: \"(-0.8696 + 2.087*math.exp((x)*0.0031))*3.1236056887012746e-06\"\n\n    Returns:\n        a tuple containing\n\n        - latex (str): a latex-compatible version of the distrib. function\n          ex: \"(-0.8696 + 2.087*e^{x*0.0031})*3.1236056887012746e-06\"\n        - value (str): a plottable version of the distrib. function\n          ex: \"(-0.8696 + 2.087exp(x*0.0031))*3.1236056887012746e-06\"\n    \"\"\"\n    if \"math\" in value:\n        value = value.replace(\"math.\", \"\")\n    if \"(x)\" in value:\n        value = value.replace(\"(x)\", \"x\")\n    latex = re.sub(r\"exp*\\(([0-9x*-.]*)\\)\", \"e^{\\\\1}\", value)\n    return latex, value",
        "sha1": "2f784d3c8119e1dea620e869c0b9f63d6b3fe75d",
        "id": 589162
    },
    {
        "content": "def find_sols(f, arr, sign_check_func = lambda x, y: x*y < 0, verbose = False):\n    \"\"\"\n    Given an array of section points, checks if function *f* has a solution\n    in any of the intervals formed by said points.\n    \"\"\"\n    sols = []\n    for i in range(len(arr)-1):\n        if sign_check_func(f(arr[i]), f(arr[i+1])):\n            sols.append([arr[i], arr[i+1]])\n\n    if verbose:\n        print(\"solution(s) in: \", *sols, sep = '\\n')\n    return sols",
        "sha1": "36ff973789c645d65645ea4a902c2a80f9548b5a",
        "id": 648142
    },
    {
        "content": "import random\n\n\ndef create_secure_string(length: int) -> str:\n    \"\"\"Create secure random string for username, client key, and tokens.\"\"\"\n    character_array = \"ABCDEFabcdef0123456789\"\n    return \"\".join(random.SystemRandom().choice(character_array) for _ in range(length))",
        "sha1": "f3c2f22c6f2311613f787d9bc3aeebff274c61c5",
        "id": 272226
    },
    {
        "content": "import math\n\n\ndef regular_poly_side_length_to_apothem(n_sides, side_length):\n    \"\"\"Compute apothem for regular polygon with given side length.\"\"\"\n    return side_length / (2 * math.tan(math.pi / n_sides))",
        "sha1": "38969b7d33086b9b9fd39f8b49b25001da595c97",
        "id": 525077
    },
    {
        "content": "def ensure_trailing_slash(expression):\n    \"\"\"\n    Add a trailing slash to rsync source/destination locations.\n\n    :param expression: The rsync source/destination expression (a string).\n    :returns: The same expression with exactly one trailing slash.\n    \"\"\"\n    if expression:\n        # Strip any existing trailing slashes.\n        expression = expression.rstrip('/')\n        # Add exactly one trailing slash.\n        expression += '/'\n    return expression",
        "sha1": "3a3d7d34f0c1265dca246fcb109febc55f21a6ac",
        "id": 245390
    },
    {
        "content": "def getattr_recursive(variable, attribute):\n    \"\"\"\n    Get attributes recursively.\n    \"\"\"\n    if '.' in attribute:\n        top, remaining = attribute.split('.', 1)\n        return getattr_recursive(getattr(variable, top), remaining)\n    else:\n        return getattr(variable, attribute)",
        "sha1": "e91401be53c287f392a123ec3a88a19c0f4b8095",
        "id": 70238
    },
    {
        "content": "def state_fixture(state_1):\n    \"\"\"Get a mocked state.\"\"\"\n    return state_1",
        "sha1": "fc1b7d6e920b26d20073db8ab30b8ffe6924ce98",
        "id": 274178
    },
    {
        "content": "def load_haiku(filename):\n    \"\"\"Open and return training corpus of haiku as a set.\"\"\"\n    with open(filename) as in_file:\n        haiku = set(in_file.read().replace('-', ' ').split())\n        return haiku",
        "sha1": "b7604218820faaa51161586801e585551cb396d1",
        "id": 619344
    },
    {
        "content": "def compute_kolmogorov_lengthscale_simple(epsilon, nu):\n    \"\"\"\n    Return Kolmogorov length scale for a given set of dissipation rate and viscosity\n    Parameters\n    ----------\n    epsilon: float, dissipation rate\n    nu: float, viscosity\n\n    Returns\n    -------\n    float, Kolmogorov length scale\n    \"\"\"\n    return (nu ** 3 / epsilon) ** (0.25)",
        "sha1": "4b38aa4d902f2ead5443031a46d70b586a7f8050",
        "id": 515023
    },
    {
        "content": "def float2byte(arr):\n    \"\"\"\n    Convert float [0 ... 1]-valued array to uint byte array.\n    \"\"\"\n    return (arr*255.).astype('uint8')",
        "sha1": "802a783e6c53c88f5f77b9dd645e83587a30bd5d",
        "id": 286092
    },
    {
        "content": "import uuid\n\n\ndef create_api_release(\n        client, resource_group_name, service_name, api_id, api_revision, release_id=None, if_match=None, notes=None):\n    \"\"\"Creates a new Release for the API.\"\"\"\n\n    if release_id is None:\n        release_id = uuid.uuid4().hex\n\n    api_id1 = \"/apis/\" + api_id + \";rev=\" + api_revision\n\n    return client.api_release.create_or_update(\n        resource_group_name, service_name, api_id, release_id, \"*\" if if_match is None else if_match, api_id1, notes)",
        "sha1": "a9b2490ca857712b2b8e4be36e76cc7dba9fbc99",
        "id": 677554
    },
    {
        "content": "import re\n\n\ndef MakeHeaderToken(filename):\n  \"\"\"Generates a header guard token.\n\n  Args:\n    filename: the name of the header file\n\n  Returns:\n    the generated header guard token.\n  \"\"\"\n  return re.sub('[^A-Z0-9_]', '_', filename.upper()) + '__'",
        "sha1": "d29f756e30c3214aac174302175c52ca28cad6cb",
        "id": 13624
    },
    {
        "content": "import requests\n\n\ndef query_phenomizer(usr, pwd,  *hpo_terms):\n    \"\"\"\n    Query the phenomizer web tool\n    \n    Arguments:\n        usr (str): A username for phenomizer\n        pwd (str): A password for phenomizer\n        hpo_terms (list): A list with hpo terms\n    \n    Returns:\n        raw_answer : The raw result from phenomizer\n    \"\"\"\n    base_string = 'http://compbio.charite.de/phenomizer/phenomizer/PhenomizerServiceURI'\n    questions = {'mobilequery':'true', 'terms':','.join(hpo_terms), 'username':usr, 'password':pwd}\n    try:\n        r = requests.get(base_string, params=questions, timeout=10)\n    except requests.exceptions.Timeout:\n        raise RuntimeError(\"The request timed out.\")\n        \n    if not r.status_code == requests.codes.ok:\n        raise RuntimeError(\"Phenomizer returned a bad status code: %s\" % r.status_code)\n    \n    r.encoding = 'utf-8'\n    \n    return r",
        "sha1": "00b2b7fea2fcf76d3d0a7dfcf2c3e69f1e1ffbeb",
        "id": 183790
    },
    {
        "content": "def reorder(x, indexList=[], indexDict={}):\n    \"\"\"\n    Reorder a list based upon a list of positional indices and/or a dictionary of fromIndex:toIndex.\n\n        >>> l = ['zero', 'one', 'two', 'three', 'four', 'five', 'six']\n        >>> reorder( l, [1, 4] ) # based on positional indices: 0-->1, 1-->4\n        ['one', 'four', 'zero', 'two', 'three', 'five', 'six']\n        >>> reorder( l, [1, None, 4] ) # None can be used as a place-holder\n        ['one', 'zero', 'four', 'two', 'three', 'five', 'six']\n        >>> reorder( l, [1, 4], {5:6} )  # remapping via dictionary: move the value at index 5 to index 6\n        ['one', 'four', 'zero', 'two', 'three', 'six', 'five']\n    \"\"\"\n\n    x = list(x)\n    num = len(x)\n    popCount = 0\n    indexValDict = {}\n\n    for i, index in enumerate(indexList):\n        if index is not None:\n            val = x.pop(index - popCount)\n            assert index not in indexDict, indexDict\n            indexValDict[i] = val\n            popCount += 1\n    for k, v in indexDict.items():\n        indexValDict[v] = x.pop(k - popCount)\n        popCount += 1\n\n    newlist = []\n    for i in range(num):\n        try:\n            val = indexValDict[i]\n        except KeyError:\n            val = x.pop(0)\n        newlist.append(val)\n    return newlist",
        "sha1": "9649c5392657fe6ab329fbb1b829ffc3bae19543",
        "id": 679481
    },
    {
        "content": "import torch\n\n\ndef aggregate_results(scale,\n                      aggregated_heatmaps,\n                      tags_list,\n                      heatmaps,\n                      tags,\n                      test_scale_factor,\n                      project2image,\n                      flip_test,\n                      align_corners=False):\n    \"\"\"Aggregate multi-scale outputs.\n\n    Note:\n        batch size: N\n        keypoints num : K\n        heatmap width: W\n        heatmap height: H\n\n    Args:\n        scale (int): current scale\n        aggregated_heatmaps (torch.Tensor | None): Aggregated heatmaps.\n        tags_list (list(torch.Tensor)): Tags list of previous scale.\n        heatmaps (List(torch.Tensor[NxKxWxH])): A batch of heatmaps.\n        tags (List(torch.Tensor[NxKxWxH])): A batch of tag maps.\n        test_scale_factor (List(int)): Multi-scale factor for testing.\n        project2image (bool): Option to resize to base scale.\n        flip_test (bool): Option to use flip test.\n        align_corners (bool): Align corners when performing interpolation.\n\n    Return:\n        tuple: a tuple containing aggregated results.\n\n        - aggregated_heatmaps (torch.Tensor): Heatmaps with multi scale.\n        - tags_list (list(torch.Tensor)): Tag list of multi scale.\n    \"\"\"\n    if scale == 1 or len(test_scale_factor) == 1:\n        if aggregated_heatmaps is not None and not project2image:\n            tags = [\n                torch.nn.functional.interpolate(\n                    tms,\n                    size=(aggregated_heatmaps.size(2),\n                          aggregated_heatmaps.size(3)),\n                    mode='bilinear',\n                    align_corners=align_corners) for tms in tags\n            ]\n        for tms in tags:\n            tags_list.append(torch.unsqueeze(tms, dim=4))\n\n    heatmaps_avg = (heatmaps[0] +\n                    heatmaps[1]) / 2.0 if flip_test else heatmaps[0]\n\n    if aggregated_heatmaps is None:\n        aggregated_heatmaps = heatmaps_avg\n    elif project2image:\n        aggregated_heatmaps += heatmaps_avg\n    else:\n        aggregated_heatmaps += torch.nn.functional.interpolate(\n            heatmaps_avg,\n            size=(aggregated_heatmaps.size(2), aggregated_heatmaps.size(3)),\n            mode='bilinear',\n            align_corners=align_corners)\n\n    return aggregated_heatmaps, tags_list",
        "sha1": "6a4af5c94014a4d3969009aeb0ae8084d8b62e48",
        "id": 393301
    },
    {
        "content": "def read_file(path: str) -> str:  # pragma: nocover\n    \"\"\"\n    This function simply reads contents of the file. It's moved out to a function\n    purely to simplify testing process.\n\n    Args:\n        path: File to read.\n\n    Returns:\n        content(str): Content of given file.\n    \"\"\"\n    return open(path).read()",
        "sha1": "743db60ee698baa67166c21dc514e93ccb0de912",
        "id": 674419
    },
    {
        "content": "def rgb2lab(R_G_B):\n    \"\"\"Convert RGB colorspace to Lab\n    \n    Adapted from http://www.easyrgb.com/index.php?X=MATH.\n    \"\"\"\n    \n    R, G, B = R_G_B\n    \n    # Convert RGB to XYZ\n    \n    var_R = ( R / 255.0 )        # R from 0 to 255\n    var_G = ( G / 255.0 )        # G from 0 to 255\n    var_B = ( B / 255.0 )        # B from 0 to 255\n\n    if ( var_R > 0.04045 ): var_R = ( ( var_R + 0.055 ) / 1.055 ) ** 2.4\n    else:                   var_R = var_R / 12.92\n    if ( var_G > 0.04045 ): var_G = ( ( var_G + 0.055 ) / 1.055 ) ** 2.4\n    else:                   var_G = var_G / 12.92\n    if ( var_B > 0.04045 ): var_B = ( ( var_B + 0.055 ) / 1.055 ) ** 2.4\n    else:                   var_B = var_B / 12.92\n\n    var_R = var_R * 100.0\n    var_G = var_G * 100.0\n    var_B = var_B * 100.0\n\n    # Observer. = 2\u00b0, Illuminant = D65\n    X = var_R * 0.4124 + var_G * 0.3576 + var_B * 0.1805\n    Y = var_R * 0.2126 + var_G * 0.7152 + var_B * 0.0722\n    Z = var_R * 0.0193 + var_G * 0.1192 + var_B * 0.9505\n    \n    # Convert XYZ to L*a*b*\n    \n    var_X = X / 95.047         # ref_X =  95.047   Observer= 2\u00b0, Illuminant= D65\n    var_Y = Y / 100.000        # ref_Y = 100.000\n    var_Z = Z / 108.883        # ref_Z = 108.883\n\n    if ( var_X > 0.008856 ): var_X = var_X ** ( 1.0/3.0 )\n    else:                    var_X = ( 7.787 * var_X ) + ( 16.0 / 116.0 )\n    if ( var_Y > 0.008856 ): var_Y = var_Y ** ( 1.0/3.0 )\n    else:                    var_Y = ( 7.787 * var_Y ) + ( 16.0 / 116.0 )\n    if ( var_Z > 0.008856 ): var_Z = var_Z ** ( 1.0/3.0 )\n    else:                    var_Z = ( 7.787 * var_Z ) + ( 16.0 / 116.0 )\n\n    CIE_L = ( 116.0 * var_Y ) - 16.0\n    CIE_a = 500.0 * ( var_X - var_Y )\n    CIE_b = 200.0 * ( var_Y - var_Z )\n    return (CIE_L, CIE_a, CIE_b)",
        "sha1": "aa1c2befa547e358c46ab7533709ad41ccd0f1a2",
        "id": 621472
    },
    {
        "content": "import inspect\n\n\ndef fqn(namespace) -> str:\n    \"\"\" Return fully qualified name of ``thing``.\n\n    :param namespace: must be a module, class, or instance.\n    :rtype: str\n\n    \"\"\"\n    if inspect.ismodule(namespace):\n        return namespace.__name__\n    elif inspect.isclass(namespace):\n        return namespace.__module__ + '.' + namespace.__qualname__\n    try:\n        # namespace should be an instance.\n        klass = namespace.__class__\n        return '%s.%s(%s)' % (klass.__module__, klass.__qualname__, id(namespace))\n    except AttributeError:\n        raise TypeError(\"Argument must be a module, class, or instance.\") from None",
        "sha1": "d3435f0919c5af893650046ad60462d3d728e80f",
        "id": 367776
    },
    {
        "content": "def normalize_scheme(scheme):\n    \"\"\"Normalize the scheme component.\"\"\"\n    return scheme.lower()",
        "sha1": "97775225c898950cb2d61b8d626b02574979794e",
        "id": 352227
    },
    {
        "content": "def serialize_softlearning_class_and_config(cls_name, cls_config):\n    \"\"\"Returns the serialization of the class with the given config.\"\"\"\n    return {'class_name': cls_name, 'config': cls_config}",
        "sha1": "881f369e1691d28ed84fc67d804091419c5f8665",
        "id": 281946
    },
    {
        "content": "def group_by_company(devices):\n    \"\"\"Group a list of devices by company.\"\"\"\n    grouped = {}\n    for dev in devices:\n        try:\n            grouped[dev['company']['name']].append(dev)\n        except KeyError:\n            grouped[dev['company']['name']] = [dev]\n    return grouped",
        "sha1": "9b5b1c56a95132a8777e3206116819e528483a43",
        "id": 29344
    },
    {
        "content": "def is_cg_developed(properties):\n    \"\"\"Check if a colorgroup is fully developed.\"\"\"\n    # check validity of colorgroup\n    assert len(set([c.color for c in properties])) == 1\n    return all([c.has_hotel for c in properties])",
        "sha1": "9ef92a7695b9deb73d2fdd78dc13df44b70599a1",
        "id": 690622
    },
    {
        "content": "import torch\n\n\ndef tracklet_pair_collect(batch):\n    \"\"\"Collect the tracklet_pair data for one batch.\n    \"\"\"\n    imgs_1 = []\n    imgs_2 = []\n    loc_mat = []\n    tracklet_mask_1 = []\n    tracklet_mask_2 = []\n    real_window_len = []\n    connectivity = []\n\n    for sample in batch:\n        imgs_1.append(sample[0])\n        imgs_2.append(sample[1])\n        loc_mat.append(sample[2])\n        tracklet_mask_1.append(sample[3])\n        tracklet_mask_2.append(sample[4])\n        real_window_len.append(sample[5])\n        connectivity.append(sample[6])\n        \n    return imgs_1, imgs_2, torch.stack(loc_mat, 0), torch.stack(tracklet_mask_1, 0), torch.stack(tracklet_mask_2, 0), real_window_len, torch.stack(connectivity, 0)",
        "sha1": "cbb41e89c8a22378b34f61431872f05786928a01",
        "id": 318381
    },
    {
        "content": "def get_harmonic_bonds(system):\n    \"\"\"Get a list of all harmonic bonds in the system\"\"\"\n    bonds = []\n    for f in system.getForces():\n        if \"HarmonicBond\" in str(f.__class__):\n            for i in range(f.getNumBonds()):\n                bonds.append(f.getBondParameters(i))\n    return bonds",
        "sha1": "a989037a4e9fa98ef3919305538a255ddb41c0c9",
        "id": 265090
    },
    {
        "content": "def average_runs(df, group=[]):\n    \"\"\"Average all dataframe columns across runs\n\n    Attributes:\n        group (list): Additional list of columns to group by before taking the average\n\n    \"\"\"\n    return df.groupby([\"step\"] + group).mean().reset_index()",
        "sha1": "76434e437cc47544631ba610cefbf255473830fc",
        "id": 212827
    },
    {
        "content": "def convert_bbox_coord(coord, im_height_or_width=512):\n    \"\"\" Simple function designed to be used in a pd.apply() statement. Converts TF's \n      preferred format for bounding box coordinates (float percent to PASCAL VOC's \n      preferred format (pixel coordinates, top left origin). \n      \n      NOTE: this currently assumes images are of equal height or width OR the user\n        has explicitly provided the proper im_height_or_width.\"\"\"\n    return int(coord * im_height_or_width)",
        "sha1": "4b4124b7c7935461108fb0bca8abfa35887ab286",
        "id": 581003
    },
    {
        "content": "def coalesce(*values):\n    \"\"\"Returns the first not-None arguement or None\"\"\"\n    return next((v for v in values if v is not None), None)",
        "sha1": "245177f43962b4c03c2347725a2e87f8eb5dc08a",
        "id": 1997
    },
    {
        "content": "import multiprocessing\n\n\ndef get_optimal_thread_count(default=5):\n    \"\"\"Try to guess optimal thread count for current system.\"\"\"\n    try:\n        return multiprocessing.cpu_count() * 5\n    except NotImplementedError:\n        return default",
        "sha1": "69ca391889c256f13ae13b3e3845ba86f37287ba",
        "id": 398822
    },
    {
        "content": "def filter_noise_lab(X, labels):\n    \"\"\"\n    Filter noise points\n\n    Parameters\n    ----------\n    X : array-like, shape (n_samples, n_features)\n        List of n_features-dimensional data points. Each row corresponds\n        to a single data point.\n    labels : array-like, shape (n_samples,)\n        Predicted labels for each sample.  (-1 - for noise)\n\n    Returns\n    -------\n    filterLabel : array-like, shape (n_samples,)\n        Filtered predicted labels for each sample.\n    filterXYZ : array-like, shape (n_samples, n_features)\n        List of n_features-dimensional data points. Each row corresponds\n        to a single data point. Data points which label = -1 was removed.\n    \"\"\"\n    filterLabel = labels[labels != -1]\n    filterXYZ = X[labels != -1]\n    return filterLabel, filterXYZ",
        "sha1": "07715b2c361b8bf4d787f0138922e9a1aaa62475",
        "id": 156321
    },
    {
        "content": "import unicodedata\n\n\ndef convert_single_latins(input_string: str) -> str:\n    \"\"\"Convert other characters that are based on single Latin characters.\n\n    \u00d8 LATIN SMALL LETTER O WITH STROKE\n    \u0142 LATIN SMALL LETTER L WITH STROKE\n\n    https://stackoverflow.com/questions/8935111/translating-letters-not-in-7bit-ascii-to-ascii-like-%C5%84-to-n-and-%C4%85-to-a\n\n    What happens to non-Latin-based characters in unknown at this time.\n    \"\"\"\n\n    def base_single_char(c: str) -> str:\n        try:\n            name = unicodedata.name(c)\n            name = name[: name.index(\" WITH\")]\n            return unicodedata.lookup(name)\n        except (ValueError, KeyError):\n            return c\n\n    return \"\".join([base_single_char(c) for c in input_string])",
        "sha1": "961857f71423c059b0cdc11c155c9ca10f77b54e",
        "id": 587029
    },
    {
        "content": "def addPaddingToBase64String(base64String: str) -> str:\n    \"\"\"\n    Return a padded version of a base64String that is not long enough to be decoded by base64.b64decode()\n\n    :param base64String: The base64String that is not long enough to be decoded by base64.b64decode()\n    :type base64String: str\n    :return: base64String padded with \"=\" so it can be decoded by base64.b64decode()\n    :rtype: str\n    \"\"\"\n    return base64String + '=' * (-len(base64String) % 4)",
        "sha1": "e8f4c90369b1ff19eb79c5662ecc1c3b3515fb27",
        "id": 560137
    },
    {
        "content": "def inc(value):\n    \"\"\"takes a value and increment by 1\n\n    Arguments:\n        value {int} -- the number to increment\n\n    Returns:\n        int -- the increased number\n    \"\"\"\n\n    return value+1",
        "sha1": "c07272373fd0e7ff5150a397836146c9d4801507",
        "id": 431624
    },
    {
        "content": "def has_upper_letters(password):\n    \"\"\"Return True if password has at least one upper letter.\"\"\"\n    return any(char.isupper() for char in password)",
        "sha1": "9c73928795054556cc89d8263595ae795d09e897",
        "id": 469945
    },
    {
        "content": "import copy\n\n\ndef weak_execute(t, pn, m):\n    \"\"\"\n    Execute a transition even if it is not fully enabled\n\n    Parameters\n    ----------\n    :param t: transition to execute\n    :param pn: Petri net\n    :param m: marking to use\n\n    Returns\n    -------\n    :return: newly reached marking if :param t: is enabled, None otherwise\n    \"\"\"\n\n    m_out = copy.copy(m)\n    for a in t.in_arcs:\n        m_out[a.source] -= a.weight\n        if m_out[a.source] <= 0:\n            del m_out[a.source]\n    for a in t.out_arcs:\n        m_out[a.target] += a.weight\n    return m_out",
        "sha1": "3178d09c139a3cb009712a81bf4de720caac6cdd",
        "id": 100419
    },
    {
        "content": "def getModClass(name):\n    \"\"\"converts 'xgds_planner.forms.PlanMetaForm' to ['xgds_planner.forms', 'PlanMetaForm']\"\"\"\n    try:\n        dot = name.rindex('.')\n    except ValueError:\n        return name, ''\n    return name[:dot], name[dot + 1:]",
        "sha1": "3abf63af034ee02b4af15b99f09c9e0433c4fa15",
        "id": 402042
    },
    {
        "content": "def list_pairs(validation_dates):\n    \"\"\"\n    For a set of ndates dates, return a list of indexes and IDs for all possible unique pairs.\n    For example, for ndates=3 -> [(0, 1), (0,2), (1,2)]\n    \"\"\"\n    ndates = len(validation_dates)\n    indexes = []\n    pair_ids = []\n\n    for k1 in range(ndates):\n        for k2 in range(k1 + 1, ndates):\n            indexes.append((k1, k2))\n            date1 = validation_dates[k1]\n            date2 = validation_dates[k2]\n            pair_ids.append(f\"{date1[:4]}_{date2[:4]}\")  # year1_year2)\n\n    return indexes, pair_ids",
        "sha1": "ab0826c43f5c85f40c3be7b63218bacc5225a503",
        "id": 675781
    },
    {
        "content": "def pretty_print_error(err_json):\n    \"\"\"Pretty print Flask-Potion error messages for the user.\"\"\"\n    # Special case validation errors\n    if len(err_json) == 1 and \"validationOf\" in err_json[0]:\n        required_fields = \", \".join(err_json[0][\"validationOf\"][\"required\"])\n        return \"Validation error. Requires properties: {}.\".format(required_fields)\n\n    # General error handling\n    msg = \"; \".join(err.get(\"message\", \"\") for err in err_json)\n\n    # Fallback\n    if not msg:\n        msg = \"Bad request.\"\n    return msg",
        "sha1": "f7786a1b19e3b4e3329f36e5b4185f477eda6161",
        "id": 317745
    },
    {
        "content": "from typing import List\nimport shlex\n\n\ndef split_line_elements(text: str) -> List[str]:\n    \"\"\"Separates by space and tabulator.\n\n    Args:\n        text (str): Text to break into separate fields.\n\n    Returns:\n        List[str]: List of formatted values.\n    \"\"\"\n    # parts = re.split(\" |\\t\", text.strip())\n    parts = shlex.split(text.strip())\n    values = list(filter(lambda part: part != \"\", parts))\n    return values",
        "sha1": "b4ac18f7b60faf29f042c4f7ff02fb5d85d8749e",
        "id": 53986
    },
    {
        "content": "def bin_to_int(x: str) -> int:\n    \"\"\" Convert from a binary string to a integer.\n\n    Parameters\n    ----------\n    x: str\n        Binary string to convert.\n\n    Returns\n    -------\n    int\n        Corresponding integer.\n    \"\"\"\n    return int(x, 2)",
        "sha1": "2c39753fd7a9c134d17979d3ff5cd6c56eaf9baa",
        "id": 480641
    },
    {
        "content": "def AD(kw, mu):\n    \"\"\"function to compute aggregate deviation of multigraph pattern as given in SIMP paper\n\n    Args:\n        kw (int): number of edges in a multigraph pattern\n        mu (float): expected number of edges in a pattern as per the prior distribution\n\n    Returns:\n        float: the aggregate deviation\n    \"\"\"\n    return kw - mu",
        "sha1": "b737f895b31f51e49dc3d4ea80227683ba65fe0a",
        "id": 432121
    },
    {
        "content": "def patch(program: list[int], noun: int, verb: int) -> list[int]:\n    \"\"\"Restore the gravity assist program\n\n    :param program: Intcode program\n    :param noun: updated noun value\n    :param verb: updated verb value\n    :return: patched Intcode program\n    \"\"\"\n    program[1] = noun\n    program[2] = verb\n    return program",
        "sha1": "0822a15abb354f235e7a14ff05c255a2bc300af4",
        "id": 624542
    },
    {
        "content": "import itertools\n\n\ndef _intersperse(elem, xs):\n    \"\"\"Return a list like: [elem, xs[0], elem, xs[1], elem, xs[2], ...]\"\"\"\n    return list(itertools.chain(*zip(itertools.repeat(elem), xs)))",
        "sha1": "1a428a87df80b27fa152720b12b08b382f9e7e14",
        "id": 147311
    },
    {
        "content": "def recursiveIndex(nestedList, query):\n    \"\"\"\n    Find index of element (first occurrence) in an arbitrarily nested list.\n\n    Args:\n        nestedList(list): list object to search in\n        query: target element to find\n\n    Returns:\n        list: Position indices\n    \"\"\"\n    for index, element in enumerate(nestedList):\n        if isinstance(element, (list, tuple)):\n            path = recursiveIndex(element, query)\n            if path:\n                return [index] + path\n        if element == query:\n            return [index]\n    return []",
        "sha1": "6386feee441e6c687f1b0b68e8e319ca79653041",
        "id": 109796
    },
    {
        "content": "def enforce_is_callable(var, error_msg):\n    \"\"\"\n    Raises an exception with provided error_msg if the variable\n    is not a callable.\n    \"\"\"\n    if not callable(var):\n        raise TypeError(error_msg)\n    return var",
        "sha1": "a34ab8611c0c20b401c9ee31284aa0daa5097d8e",
        "id": 200023
    },
    {
        "content": "def mod10_to_mod2(dec, length=0):\n \n    \"\"\"Converts a decimal number to a binary number, with optional padding\n    to produce a binary vector of a given length.\n\n    Parameters\n    ----------\n    dec : int\n        Decimal number (base 10).\n    length : int, optional\n        The length of the binary string. If the specified `length' is greater\n        than the bit-length of the binary number the output is left-padded\n        with zeros. The default `length' is set to zero.\n\n    Returns\n    -------\n    list\n        A binary integer list encoding the binary represenation of the\n        inputted decimal number \n    \n    Examples\n    --------\n    >>> mod10_to_mod2(2,length=5)\n    [0, 0, 0, 1, 0]\n    \"\"\"\n\n    # Convert dec to a binary string, with <length> leading zeros\n    bin_str = format(dec, '0{}b'.format(length))\n\n    # Split that string into array, converting chars to ints\n    return [int(b) for b in bin_str]",
        "sha1": "26ebd4126f8a650b130bf0481b83d5d84e378d6c",
        "id": 526076
    },
    {
        "content": "import torch\n\n\ndef zero_pad(tensor, target_len, dim):\n    \"\"\"\n    Add zeros to a tensor alonng the specified dimension to reach the target length in that dimension.\n    \"\"\"\n    tensor_size = list(tensor.size())\n    assert dim <= len(tensor_size), \"The specified dimension is larger than the tensor rank\"\n    pad_size = tensor_size\n    pad_size[dim] = target_len - tensor_size[dim]\n    pad = torch.zeros(pad_size)\n    pad_tensor = torch.cat([tensor, pad], dim=dim)\n    return pad_tensor",
        "sha1": "d8dd17e08335f9770b2c6dc9d26f9509d0ad2255",
        "id": 345413
    },
    {
        "content": "def fill_color(color: str):\n    \"\"\"\n    Returns an SVG fill color attribute using the given color.\n\n    :param color: `string` fill color\n    :return: fill=\"<color>\"\n    \"\"\"\n    return f'fill=\"{color}\"'",
        "sha1": "8a035e9763535188b85a81741b4c1d301b7c22e3",
        "id": 281171
    },
    {
        "content": "def del_from_list(target, index_positions):\n    \"\"\"\n    Deletes the elements in a list given by a index_positions list\n\n    :param target: a target list to have items removed\n    :param index_positions: a list of index positions to be removed from\n                            the target list\n\n    :type target: list\n    :type index_positions: list\n\n    :returns: a list with the elements removed defined by the index_positions\n              list\n    \"\"\"\n    if target == []:\n        raise ValueError(\"target list must not be empty\")\n    if len(index_positions) > len(target):\n        raise ValueError(\"target list contains less elements then \"\n                         \"to be removed\")\n    if not all(x >= 0 for x in index_positions):\n        raise ValueError(\"index_positions need to be positive\")\n    for e in index_positions:\n        if e >= len(target):\n            raise ValueError(\"index_positions > len target list\")\n    for offset, index in enumerate(index_positions):\n        index -= offset\n        del target[index]\n    return target",
        "sha1": "c5837d16f608cf7e8a404223cd0c7d3b3e02042f",
        "id": 593537
    },
    {
        "content": "import torch\n\n\ndef qexp_t(q):\n    \"\"\"\n    Applies exponential map to log quaternion\n    :param q: N x 3\n    :return: N x 4\n    \"\"\"\n    n = torch.norm(q, p=2, dim=1, keepdim=True)\n    n = torch.clamp(n, min=1e-8)\n    q = q * torch.sin(n)\n    q = q / n\n    q = torch.cat((torch.cos(n), q), dim=1)\n    return q",
        "sha1": "6db3edab4c2bbaf4176625f35381d456ee1aeaa5",
        "id": 644690
    },
    {
        "content": "def pytest_report_collectionfinish(config, items):\n    \"\"\"Log how many and, if verbose, which items are tested in this shard.\"\"\"\n    msg = \"Running {num} items in this shard\".format(num=len(items))\n    if config.option.verbose > 0:\n        msg += \": \" + \", \".join([item.nodeid for item in items])\n    return msg",
        "sha1": "e81d3663f60505543a1b554d36e2dee26b7107f9",
        "id": 696880
    },
    {
        "content": "from typing import Iterable\n\n\ndef round_values(data, places=3):\n    \"\"\"\n    Round values in nested structures\n\n    Args:\n        data: Value / structure with values\n        places (int): Decimal places\n\n    Returns:\n        Rounded value / Structure with rounded values\n    \"\"\"\n    if isinstance(data, dict):\n        return {k: round_values(v, places) for k, v in data.items()}\n    if isinstance(data, Iterable):\n        return [round_values(x, places) for x in data]\n    return round(data, places)",
        "sha1": "68bdcabff375b81567c0c710b55f21572e9ccda1",
        "id": 39483
    },
    {
        "content": "def _look_before (index_sentence,context) :\n    \"\"\"Generate the look before context starting with the sentence index and looking no less than the first sentence\"\"\"\n\n    context_pairs=[]\n    for i in range(1,context+1) :\n        s_index=index_sentence-i\n        if  s_index>=0 :\n            context_pairs.append(( s_index,index_sentence))\n    return context_pairs",
        "sha1": "ce48596b37be997fa9c7176b9c0f18b2c6a78e4b",
        "id": 302910
    },
    {
        "content": "from typing import Union\nfrom typing import Callable\n\n\ndef maybe_recursive_call(\n    object_or_dict,\n    method: Union[str, Callable],\n    recursive_args=None,\n    recursive_kwargs=None,\n    **kwargs,\n):\n    \"\"\"Calls the ``method`` recursively for the ``object_or_dict``.\n\n    Args:\n        object_or_dict: some object or a dictionary of objects\n        method: method name to call\n        recursive_args: list of arguments to pass to the ``method``\n        recursive_kwargs: list of key-arguments to pass to the ``method``\n        **kwargs: Arbitrary keyword arguments\n\n    Returns:\n        result of `method` call\n    \"\"\"\n    if isinstance(object_or_dict, dict):\n        result = type(object_or_dict)()\n        for k, v in object_or_dict.items():\n            r_args = None if recursive_args is None else recursive_args[k]\n            r_kwargs = (\n                None if recursive_kwargs is None else recursive_kwargs[k]\n            )\n            result[k] = maybe_recursive_call(\n                v,\n                method,\n                recursive_args=r_args,\n                recursive_kwargs=r_kwargs,\n                **kwargs,\n            )\n        return result\n\n    r_args = recursive_args or []\n    if not isinstance(r_args, (list, tuple)):\n        r_args = [r_args]\n    r_kwargs = recursive_kwargs or {}\n    if isinstance(method, str):\n        return getattr(object_or_dict, method)(*r_args, **r_kwargs, **kwargs)\n    else:\n        return method(object_or_dict, *r_args, **r_kwargs, **kwargs)",
        "sha1": "59fc871ad8713ebf0ff7e1757b5acb1af99e96b9",
        "id": 545763
    },
    {
        "content": "import uuid\n\n\ndef new_token(*args, **kwargs):\n    \"\"\"Generator for new random tokens\n\n    For now, just UUIDs.\n    \"\"\"\n    return uuid.uuid4().hex",
        "sha1": "2633a38c064f46f2c36b2c4d2cca613bd956879f",
        "id": 429862
    },
    {
        "content": "def feet_to_meters(feet: float) -> float:\n    \"\"\" Convert feet to meters.\n\n    :param feet: a measurement in feet.\n    :returns: measurement in meters\n    \"\"\"\n    return feet / 0.3048",
        "sha1": "0ea73d0d66100405b5355844feedde5525000e60",
        "id": 230778
    },
    {
        "content": "def set_generator(random, args):\n    \"\"\"\n    Generates a list containing non-repeated elements of a discrete or\n    continuous representation.\n\n    Parameters\n    ----------\n\n    random : Random\n    args : dict\n        representation: set containing the possible values\n        max_candidate_size: int, default: 9\n        variable_candidate_size: bool, default: True\n\n\n    Returns\n    -------\n    list\n        A list containing a sample of the elements. If variable_candidate_size is\n        True the list size is up to max_candidate_size, otherwise the candidate\n        size equals candidate_size\n    \"\"\"\n    representation = args.get('representation')\n    indices = list(range(len(representation)))\n    max_size = args.get('max_size', 9)\n    variable_size = args.get('variable_size', True)\n    if variable_size and max_size > 1:\n        size = random.randint(1, max_size)\n    else:\n        size = max_size\n    candidate = random.sample(indices, size)\n    return sorted(candidate)",
        "sha1": "25f41cef5291564b5673db7ded92af965e67c828",
        "id": 379483
    },
    {
        "content": "def team_member_context(report):\n    \"\"\"\n    Gets context for adding a team members listing to a report.\n    \"\"\"\n    return {\n        \"team_members\": report.get_team_members(),\n    }",
        "sha1": "2a1adc73bff0001e01ba1d4fa67f002a9b302a1c",
        "id": 360347
    },
    {
        "content": "def succeeded(job):\n    \"\"\"Check if the CR has either a Ready or Succeeded condition\"\"\"\n    if \"status\" not in job:\n        return False\n\n    if \"conditions\" not in job[\"status\"]:\n        return False\n\n    for condition in job[\"status\"][\"conditions\"]:\n        if \"Succeeded\" in condition[\"type\"]:\n            return condition[\"status\"] == \"True\"\n\n        if \"Ready\" in condition[\"type\"]:\n            return condition[\"status\"] == \"True\"\n\n    return False",
        "sha1": "dff2dea56cfdcba75f089541bf8207709b4ac4e5",
        "id": 540913
    },
    {
        "content": "def create_item_dict(df, id_col, name_col):\n    \"\"\"\n    Function to create an item dictionary based on their item_id and item name\n    Required Input -\n        - df = Pandas dataframe with Item information\n        - id_col = Column name containing unique identifier for an item\n        - name_col = Column name containing name of the item\n    Expected Output -\n        item_dict = Dictionary type output containing item_id as key and item_name as value\n    \"\"\"\n    item_dict = {}\n    for i in range(df.shape[0]):\n        item_dict[(df.loc[i, id_col])] = df.loc[i, name_col]\n    return item_dict",
        "sha1": "462288b67492e37cb2cad714e4002ca61a20bc1d",
        "id": 200302
    },
    {
        "content": "def matrix_transpose(matrix):\n    \"\"\"\n    Function to transpose a Matrix\n    Returns the transpose of a 2D matrix\n    \"\"\"\n    return [[matrix[j][i] for j in range(len(matrix))]\n            for i in range(len(matrix[0]))]",
        "sha1": "bbe1d600f622140d16e012d9f3c243f6673ac584",
        "id": 121544
    },
    {
        "content": "import unicodedata\n\n\ndef read_ss_table(table_fd):\n    \"\"\"\n    Read a two-column translation table file for substitutions.\n\n    Given an open file object, read and return the contents of a two-column\n    translation table. If uppercase or lowercase variants of the source\n    term are available, add records for these variants automatically. For a\n    record with more than two columns, a RuntimeError exception is raised.\n\n    \"\"\"\n    tab = []\n    table_str = unicodedata.normalize('NFC', table_fd.read())\n    for line in table_str.split('\\n'):\n        rec = line.split('|')\n        if len(rec) == 2:\n            tab.append(rec)\n            term1, term2 = rec\n            term1_lower = term1.lower()\n            term1_upper = term1.upper()\n            if term1 != term1_lower:\n                tab.append([term1_lower, term2.lower()])\n            if term1 != term1_upper:\n                tab.append([term1_upper, term2.upper()])\n        elif line.strip() != '':\n            raise RuntimeError('Table error: ' + line.strip())\n    return tab",
        "sha1": "27e827f202890a906a647db4369c4582016433e2",
        "id": 225490
    },
    {
        "content": "import re\n\n\ndef categorize_tags(title):\n    \"\"\"Parses tags out of the post title\n    Valid submission tags are things between [], {}, (), and ||\n\n    Valid tag values are:\n\n    * a single number (shorthand for part #)\n    * Pt/Pt./Part + number (integral or textual)\n    * Vol/Vol./Volume  + number (integral or textual)\n    * Update\n    * Final\n    * Finale\n    \"\"\"\n\n    tag_cats = {'valid_tags': [], 'invalid_tags': []}\n\n    # this regex might be a little too heavy-handed but it does support the valid tag formats\n    allowed_tag_values = re.compile(\"^(?:(?:vol(?:\\.|ume)?|p(?:ar)?t|pt\\.)?\\s?(?:[1-9][0-9]?|one|two|three|five|ten|eleven|twelve|fifteen|(?:(?:four|six|seven|eight|nine)(?:teen)?))|finale?|update(?:[ ]#?[0-9]*)?)$\")\n    matches = [m.group() for m in re.finditer(\"\\[([^]]*)\\]|\\((.*?)\\)|\\{(.*?)\\}|\\|(.*?)\\|\", title)]\n    # for each match check if it's in the accepted list of tags\n\n    for m in matches:\n        # remove the braces/brackets/parens\n        text = m.lower()[1:-1].strip()\n        if not allowed_tag_values.match(text):\n            tag_cats['invalid_tags'].append(text)\n        else:\n            tag_cats['valid_tags'].append(text)\n\n    return tag_cats",
        "sha1": "fabe91cca4923fe687583a8d723923c8abb080f0",
        "id": 329238
    },
    {
        "content": "def shapes(tensors):\n    \"\"\"Get the static shapes of tensors in a list.\n\n    Arguments:\n      tensors: an iterable of `tf.Tensor`.\n\n    Returns:\n      a `list` of `tf.TensorShape`, one for each tensor in `tensors`,\n        representing their static shape (via `tf.Tensor.get_shape()`).\n    \"\"\"\n    return [t.get_shape() for t in tensors]",
        "sha1": "d5958286782a3ce73621831e5b69d205feeb74d8",
        "id": 675998
    },
    {
        "content": "def convert_to_float(text, default=0.0):\n    \"\"\"\n    Converts a string to a float. If the string can't be converted to a string, return the default.\n\n    :param text: The string to be converted to a float (e.g. \"0.4\")\n    :type text: str\n    :param default: The default value\n    :type default: float\n    :return: The resulting float\n    :rtype: float\n    \"\"\"\n    try:\n        return float(text)\n    except ValueError:\n        return default",
        "sha1": "4793017c9cb6ca087812d74d0dd1e344c95f5083",
        "id": 485597
    },
    {
        "content": "def get_alignment_identities(A, B):\n    \"\"\"\n    This function returns the number of identities between a pair\n    of aligned sequences {A} and {B}. If {A} and {B} have different\n    lengths, returns None.\n\n    @input:\n    A {string} aligned sequence A (with residues and gaps)\n    B {string} aligned sequence B (with residues and gaps)\n    @return: {int} or None\n\n    \"\"\"\n    if len(A) == len(B):\n        return len([i for i in range(len(A)) if A[i] == B[i]])\n\n    return None",
        "sha1": "2f47a79108b5567b233332fe4bdb2aa050372275",
        "id": 537565
    },
    {
        "content": "def calc_max_match(match_percent, match_salary, income):\n    \"\"\"\n    Calculates the max 401k match the user can contribute.\n\n    \"\"\"\n    match_needed = match_percent * match_salary * income\n    return match_needed",
        "sha1": "c781ccd2deddb654db2aae988c563dbce2102736",
        "id": 311788
    },
    {
        "content": "def _get_reward_for_key_hash(key_hash: str, rec: list) -> int:\n    \"\"\"Get reward value for key hash in ledger state snapshot record.\"\"\"\n    for r in rec:\n        if r[0][\"key hash\"] != key_hash:\n            continue\n        rew_amount = 0\n        for sr in r[1]:\n            rew_amount += sr[\"rewardAmount\"]\n        return rew_amount\n    return 0",
        "sha1": "29a83abd79537ea6d103c9f028e96852d4ab834d",
        "id": 378560
    },
    {
        "content": "def tag_name(name):\n    \"\"\"Removes expanded namespace from tag name.\"\"\"\n    result = name[name.find('}') + 1:]\n    if result == 'encoded':\n        if name.find('/content/') > -1:\n            result = 'content'\n        elif name.find('/excerpt/') > -1:\n            result = 'excerpt'\n    return result",
        "sha1": "5b45593d06107258983d7ccf6a3cd8a94acc2e24",
        "id": 462208
    },
    {
        "content": "import ctypes\n\n\ndef get_name() -> str:\n    \"\"\"Get the name of the current thread as a string.\"\"\"\n\n    name = (ctypes.c_char * 16)()  # pytype: disable=not-callable\n    ffi.prctl(ffi.PR_GET_NAME, name, 0, 0, 0)  # type: ignore\n    return name.value.decode()",
        "sha1": "c3f54e169e8f99c23f118a93cdb814093ed6e30e",
        "id": 253075
    },
    {
        "content": "import math\n\n\ndef visibility(nov, nol, a):\n  \"\"\"Compute visibility using height-correlated GGX.\n\n  Heitz 2014, \"Understanding the Masking-Shadowing Function in Microfacet-Based\n  BRDFs\"\n\n  Args:\n    nov: Normal dot view direction.\n    nol: Normal dot light direction.\n    a: Linear roughness.\n\n  Returns:\n    The geometric visibility (V) term.\n  \"\"\"\n  a2 = a * a\n  ggxl = nov * math.sqrt((nol - nol * a2) * nol + a2)\n  ggxv = nol * math.sqrt((nov - nov * a2) * nov + a2)\n  return 0.5 / (ggxv + ggxl)",
        "sha1": "04fb867d55622951fee64c7edb22d042f890ae74",
        "id": 676061
    },
    {
        "content": "from datetime import datetime\nimport uuid\n\n\ndef _get_file_name() -> str:\n    \"\"\"\n    Create a random file name.\n\n    Returns:\n        str:\n            Randomised file name.\n    \"\"\"\n\n    return f'{datetime.now().strftime(\"%d-%m-%Y_%I-%M-%S_%p\")}_{str(uuid.uuid4())[:8]}.avro'",
        "sha1": "3fb18df2c9b6326037b4aa3d9576ef7f8b87209b",
        "id": 579606
    },
    {
        "content": "def index_of(val, listOfVals):\n  \"\"\" \n  Useful method for lists\n  (returns -1 instead of ValueError when value not in list)\n  \"\"\"\n  try:\n    index = listOfVals.index(val)\n  except ValueError:\n    index = -1\n\n  return index",
        "sha1": "74720cf76dc250c53b573175ef51588b4a2e4aae",
        "id": 591810
    },
    {
        "content": "def is_attorney(descr):\n    \"\"\" If a string description is a attorney description \"\"\"\n    return any(map(lambda s: s in descr.upper(), ('ATTORNEY','ATTNY')))",
        "sha1": "aa45fcada75e74abb9dee686d2b662f527f0f883",
        "id": 599176
    },
    {
        "content": "def trim_leading_org(repo, org):\n    \"\"\"Strips leading organization from repository names.\n    Example: autopkg/recipes --> recipes\n    \"\"\"\n    if repo.startswith(org + \"/\"):\n        return repo.replace(org + \"/\", \"\")\n    return repo",
        "sha1": "a8735908bf7be0238a2d869fb4d53fa2b2062f79",
        "id": 436682
    },
    {
        "content": "def credential_exist(cls,site_name):\n    \"\"\"\n    method that checks if a credential account exists from the credential list\n    Args:\n    site_name: Site_name to search if it exists\n    Returns:\n    Boolean: True or false depending if the credential exists\n    \"\"\"\n    for credential in cls.credentials_list:\n        if credential.site_name == site_name:\n            return True\n    return False",
        "sha1": "4348bec6f9e4ed0fccb68395c73de393c6036f9f",
        "id": 667594
    },
    {
        "content": "import re\n\n\ndef split_kwargs(kwargs_dict, prefix='shadow_'):\n    \"\"\"\n    Splits dictionary into two new dicts by checking the keys for a given prefix. Those key-value pairs\n     with the prefix will be added to a new dictionary with prefix removed from the keys.\n    :param kwargs_dict: original dict to be checked and split. As dict\n    :param prefix: prefix to search keys for. As string\n    :return: copy of original dict with preficed keys removed, new dict based on prefixed keys\n    \"\"\"\n    input_dict = kwargs_dict.copy()\n    output_dict = {}\n    shadow_dict = {}\n\n    for k, v in input_dict.items():\n        if bool(re.match(prefix + '.*', k)):  # find keys with prefix\n            shadow_k = re.sub(prefix, '', k)  # remove prefix\n            shadow_dict[shadow_k] = v\n        else:\n            output_dict[k] = v\n\n    return output_dict, shadow_dict",
        "sha1": "e5f07d03e5ffa44e69ed05e11e97bb58549d3246",
        "id": 107809
    },
    {
        "content": "def boxproj_(z,p,l,u):\n    \"\"\" This subroutine projects the vector z onto the box [l,u]\n\n    z,l,u are vectors of length p\n    \"\"\"\n\n    z = z.flatten()\n    for i in range(0,p):\n        z[i] = min(max(l[i],z[i]),u[i])\n    return z",
        "sha1": "b62542d05ee63e36562ff3df715e7811abcdb2be",
        "id": 140894
    },
    {
        "content": "def oa_syntax(file):\n    \"\"\"\n    Return the full name of an Override Audit syntax based on the short name.\n    \"\"\"\n    return \"Packages/OverrideAudit/syntax/%s.sublime-syntax\" % file",
        "sha1": "14f8c84a6584a406f328270dd29cec429bfb7299",
        "id": 556367
    },
    {
        "content": "def get_name(spec):\n    \"\"\"Retrieves the name from the provided Kubernetes object\n\n    Args:\n        spec - Spec for a properly constructed kubernetes object\n\n    Returns:\n        The name of the Kubernetes object as a string\n    \"\"\"\n    return spec[\"metadata\"][\"name\"]",
        "sha1": "5140b8230bd8448072a92b73c1c9ce213e750e33",
        "id": 482474
    },
    {
        "content": "def plural(n, singular, plural=None, show_n=True):\n    \"\"\"Pluralize <singular> word by adding an s if n != 1.\n\n    Arguments:\n        n (int): number of things there are\n        singular (str): singular form of word\n        plural (str, optional): optional plural form, for when it's not just\n            singular + 's'\n        show_n (bool): whether to include n in the result string (default True)\n\n    Returns:\n        (str): \"1 thing\" if n == 1 or \"n things\" if n != 1\n    \"\"\"\n    number = '%s ' % n if show_n else ''\n    if n == 1:\n        return \"%s%s\" % (number, singular)\n    elif plural is not None:\n        return \"%s%s\" % (number, plural)\n    else:\n        return \"%s%ss\" % (number, singular)",
        "sha1": "cee3adda4a198fc38bdb94d815dd2cf295dee83b",
        "id": 511827
    },
    {
        "content": "def first_char_upper(str):\n    \"\"\"\n    This function converts the first char of a string to upper case\n    \"\"\"\n\n    return str[0].upper() + str[1:]",
        "sha1": "af56f00bda57c1539d437567cc90675c4e93a8e8",
        "id": 208667
    },
    {
        "content": "def transform(response: dict) -> float:\n    \"\"\"Process the response and extract windspeed information\n    Args:\n        response (dict): Response JSON from extract task\n    Returns:\n        float: current wind speed\n    \"\"\"\n    return response.get(\"wind\", {}).get(\"speed\", 0.0)",
        "sha1": "49a31e728d36bff20563ec62c904e80f4d478caa",
        "id": 307709
    },
    {
        "content": "from typing import List\n\n\ndef brute_force_profit(arr: List[int]) -> int:\n    \"\"\"\n    Brute force O(n^2) approach to find the largest jump in a list.\n    Note that there must be at least two entries, and a possible profit.\n    :param arr: the array of stock values\n    :return: the maximum profit that could be made per stock\n\n    >>> brute_force_profit([])\n    0\n    >>> brute_force_profit([0])\n    0\n    >>> brute_force_profit([1, 4 ,2, 6, 3])\n    5\n    >>> brute_force_profit([9, 11, 8, 5, 7, 10])\n    5\n    \"\"\"\n    if len(arr) < 2:\n        return 0\n\n    val = max([arr[y] - arr[x] for x in range(len(arr)) for y in range(x+1, len(arr)) if x <= y])\n    return max(0, val)",
        "sha1": "0c009301f233e28ef99344438e6480cba968c1ff",
        "id": 220528
    },
    {
        "content": "import pwd, grp\n\n\ndef get_uid_gid(user, group=None):\n    \"\"\"\n    Retrieve the ``uid`` and ``gid`` corresponding to ``user`` and ``group``.\n\n    User and group are given as names like 'nobody' not integer.\n\n    :Source: `http://mail.mems-exchange.org/durusmail/quixote-users/4940/1/ <http://mail.mems-exchange.org/durusmail/quixote-users/4940/1/>`_\n\n    :param user: user name\n    :param group: group name\n    :returns: (uid, gid)\n    \"\"\"\n    uid, default_grp = pwd.getpwnam(user)[2:4]\n    if group is None:\n        gid = default_grp\n    else:\n        try:\n            gid = grp.getgrnam(group)[2]\n        except KeyError:\n            gid = default_grp\n    return (uid, gid)",
        "sha1": "b1dbd1f51de42205d8e7e72eba2c4f3f7de881b7",
        "id": 369144
    },
    {
        "content": "def clamp(x, lower=float('-inf'), upper=float('inf')):\n    \"\"\"Limit a value to a given range.\n\n    Args:\n        x (int or float): Number to be clamped.\n        lower (int or float): Minimum value for x.\n        upper (int or float): Maximum value for x.\n\n    The returned value is guaranteed to be between *lower* and\n    *upper*. Integers, floats, and other comparable types can be\n    mixed.\n\n    >>> clamp(1.0, 0, 5)\n    1.0\n    >>> clamp(-1.0, 0, 5)\n    0\n    >>> clamp(101.0, 0, 5)\n    5\n    >>> clamp(123, upper=5)\n    5\n\n    Similar to `numpy's clip`_ function.\n\n    .. _numpy's clip: http://docs.scipy.org/doc/numpy/reference/generated/numpy.clip.html\n\n    \"\"\"\n    if upper < lower:\n        raise ValueError('expected upper bound (%r) >= lower bound (%r)'\n                         % (upper, lower))\n    return min(max(x, lower), upper)",
        "sha1": "73f80eef0283530db36148e32b85d15a815ce130",
        "id": 375716
    },
    {
        "content": "def comment_out(text, comment=\"#\"):\n    \"\"\"\n    Comment out some text, using the specified comment character(s) at the\n    start of each line.\n    \"\"\"\n    text = text.strip()\n\n    result = \"\"\n    for line in text.split(\"\\n\"):\n        if line: result += comment+\" \"+line+\"\\n\"\n        else: result += comment+\"\\n\"\n\n    return result.strip()",
        "sha1": "11a6a1d99fb6ed47ae380b0215115d168d5cf491",
        "id": 109154
    },
    {
        "content": "def directory_fmt(directory):\n  \"\"\"In ensure that directories end with '/'.\n\n  Frequently we need to ensure that directory paths end with a forward slash.\n  Pythons dirname and split functions in the path library treat this\n  inconsistently creating this requirement. This function is simple but was\n  written to centralize documentation of an often used (and often explained)\n  requirement in this codebase.\n\n  >>> os.path.dirname('gs://bucket/folder/file.txt')\n  'gs://bucket/folder'\n  >>> directory_fmt(os.path.dirname('gs://bucket/folder/file.txt'))\n  'gs://bucket/folder/'\n  >>> os.path.dirname('/newfile')\n  '/'\n  >>> directory_fmt(os.path.dirname('/newfile'))\n  '/'\n\n  Specifically we need this since copy commands must know whether the\n  destination is a directory to function properly. See the following shell\n  interaction for an example of the inconsistency. Notice that text files are\n  copied as expected but the bam is copied over the directory name.\n\n  Multiple files copy, works as intended in all cases:\n      $ touch a.txt b.txt\n      $ gsutil cp ./*.txt gs://mybucket/text_dest\n      $ gsutil ls gs://mybucket/text_dest/\n            0  2017-07-19T21:44:36Z  gs://mybucket/text_dest/a.txt\n            0  2017-07-19T21:44:36Z  gs://mybucket/text_dest/b.txt\n      TOTAL: 2 objects, 0 bytes (0 B)\n\n  Single file copy fails to copy into a directory:\n      $ touch 1.bam\n      $ gsutil cp ./*.bam gs://mybucket/bad_dest\n      $ gsutil ls gs://mybucket/bad_dest\n               0  2017-07-19T21:46:16Z  gs://mybucket/bad_dest\n      TOTAL: 1 objects, 0 bytes (0 B)\n\n  Adding a trailing forward slash fixes this:\n      $ touch my.sam\n      $ gsutil cp ./*.sam gs://mybucket/good_folder\n      $ gsutil ls gs://mybucket/good_folder\n               0  2017-07-19T21:46:16Z  gs://mybucket/good_folder/my.sam\n      TOTAL: 1 objects, 0 bytes (0 B)\n\n  Args:\n    directory (str): a uri without an blob or file basename.\n\n  Returns:\n    the directory with a trailing slash.\n  \"\"\"\n  return directory.rstrip('/') + '/'",
        "sha1": "9e3bb8c6dfab5a1f3eed2aabe821a1aeced6123f",
        "id": 86217
    },
    {
        "content": "def to_latlon(coords):\n    \"\"\"Flip coordinates from (lon, lat) to (lat, lon)\"\"\"\n    return (coords[1], coords[0])",
        "sha1": "e4698479ea81ff5ed731dc480b44e59dc697e728",
        "id": 558836
    },
    {
        "content": "def make_paper_toc_linear(paper_soup):\n    \"\"\"\n        Extracts H1 and H2 elements from the page and writes \n        a list of titles with their relative ids.\n    \"\"\"\n    toc = []\n    for title in paper_soup.find_all(['h1', 'h2']):\n        title['id'] = title['data-usually-unique-id']\n        toc.append(\n            { \n                'string': title.span.string,\n                'anchor': title['id'],\n                'name': title.name,\n            }\n        )\n    return toc",
        "sha1": "96b1f4098c3e46ebecde931bff302a76cfa3ee70",
        "id": 462529
    },
    {
        "content": "def parse_new_activity_created(json):\n    \"\"\"\n    Parse a module created statement.\n\n    Receives a statement of module created.\n    Extract from that statement the course id, course name and name and type of the new activity.\n\n    :param json: A module created statement.\n    :type json: dict(str, dict(str, str))\n    :return: A dictionary containing the course id and name, and module name and type.\n    :rtype: dict(str, int)\n    \"\"\"\n    statement = json['statement']\n    temp_course_id = statement['context']['contextActivities']['grouping'][1]['id']\n    temp_object_id = statement['object']['id']\n    activity_type = \"assignment\"\n    if \"quiz\" in temp_object_id:\n        activity_type = \"quiz\"\n\n    statement_new_activity = {\n        'statement_id': statement['id'],\n        'courseId': int(temp_course_id.split(\"id=\")[1]),\n        'course_name': statement['context']['contextActivities']['grouping'][1]['definition']['name']['en'],\n        'activity_name': statement['object']['definition']['name']['en'],\n        'activity_type': activity_type\n    }\n    return statement_new_activity",
        "sha1": "60fc3d20d9048ba7a4809974c50d6c11477bd0d0",
        "id": 298982
    },
    {
        "content": "from typing import List\n\n\ndef requirements() -> List[str]:\n    \"\"\"\n    Reads the requirements file of the project to use its content to determine\n    the dependencies of the package.\n\n    :return: The dependencies of Metrics.\n    \"\"\"\n    return [\n        \"Pillow\",\n        \"cycler\",\n        \"kiwisolver\",\n        \"matplotlib\",\n        \"nose\",\n        \"numpy\",\n        \"pip\",\n        \"plotly\",\n        \"pyparsing\",\n        \"python-dateutil\",\n        \"retrying\",\n        \"setuptools\",\n        \"six\",\n        \"Cython\"\n    ]",
        "sha1": "655d778e9802cf254a2fdcdd1d94cc62391514a2",
        "id": 288912
    },
    {
        "content": "import struct\n\n\ndef unstr64(i):\n    \"\"\"Convert an int64 to a string.\"\"\"\n    b = struct.pack(\"@q\", i)\n    return b.decode(\"ascii\").strip(\"\\0\")",
        "sha1": "30a0806f0141c4bc742a16001b1057e9c66d3ac3",
        "id": 66991
    },
    {
        "content": "import math\n\n\ndef absolute_humidity(t, rh):\n    \"\"\"\n    Gives you the mass of water vapor in volume of dry air. Units in g/m^3 See [SensIntroHum]_\n\n    Different pressure seem to affect absolute humidity slightly. For a more accurate calculation that uses pressure,\n    see [PlanetAbsHum]_.\n\n    :param t: Temperature in Celsius.\n    :type t: float\n    :param rh: Relative Humidity 0-100\n    :return: Absolute humidity g/m^3\n    :rtype: float\n    :Example:\n\n        >>> import hygrometry\n        >>> hygrometry.absolute_humidity(25, 60)\n        13.780667458722558\n\n    \"\"\"\n    t_n = 243.12  # C\n    m = 17.62\n    a = 6.112  # hPa\n    \n    dv = 216.7*(rh/100.0*a*math.exp(m*t/(t_n+t))/(273.15+t))\n    return dv",
        "sha1": "b0fff00073e44a115468ade9f30f714cff7af5f0",
        "id": 571585
    },
    {
        "content": "def get_orders(value):\n    \"\"\"\n    Return a list of orders for context tiers.\n\n    Parameters\n    ----------\n    value : int or string\n        The maximum context length or a string in the set \"bigram\" (for\n        context 1, and 2), \"trigram\" (for context 1, 2, and 3), or\n        \"fourgram\" (for contexts 1, 2, 3, and 4).\n    \"\"\"\n\n    # Dictionary used for mapping string descriptions of window size to\n    # actual Python ranges; by mapping to `range()` here in advance\n    # (and consuming such range into a list), computations is a bit\n    # faster and, in particular, it is clearer. Note that we always start\n    # from 1, so no zero-length is included in the lists (the zero distance\n    # is the actual alignment site itself).\n    order_map = {\n        \"bigram\": list(range(1, 2)),\n        \"trigram\": list(range(1, 3)),\n        \"fourgram\": list(range(1, 4)),\n    }\n\n    # get mapping\n    if isinstance(value, int):\n        orders = list(range(1, value + 1))\n    elif isinstance(value, str):\n        orders = order_map[value]\n    else:\n        orders = []\n\n    return orders",
        "sha1": "7f473e50a9fbb0ec0927016994367150aee7aa70",
        "id": 34005
    },
    {
        "content": "def level_message(sequence, level):\n    \"\"\"Return simon level message which contains the simon sequence\"\"\"\n    message = \"\\033[1;95mSequence {}:\\n\\n\\033[4m{}\".format(level, \n        \" \".join(map(str, sequence)))\n    return message",
        "sha1": "d9f883cdf99621d46cb0ec40b0140ccfa58fb62e",
        "id": 693384
    },
    {
        "content": "def is_oppo_pass(event_list, team):\n\t\"\"\"Returns whether an opponent passed the ball (likely a bad pass)\"\"\"\n\toppo_pass = False\n\tfor e in event_list[:1]:\n\t\tif e.type_id == 1 and e.team != team and e.outcome == 0:\n\t\t\toppo_pass = True\n\t\t\t\n\treturn oppo_pass",
        "sha1": "0e7d4e97a60f9e907f91caee2a7b1eeae6e52e96",
        "id": 261876
    },
    {
        "content": "def parseJobName(name):\n    \"\"\"\n    Parses job name of parametric tests\n    \"\"\"\n\n    s = name.split(\"_\")\n\n    output = dict()\n\n    # Find the index of the first integer (value of parameter)\n    idxFirstInt = 0\n    while True:\n        try:\n            int(s[idxFirstInt])\n            break\n        except ValueError:\n            idxFirstInt += 1\n\n    # Everything before the first integer is the basename\n    output[\"baseName\"] = \"_\".join(s[0:idxFirstInt - 1])\n\n    # Assume everything after the basename is parameters and values\n    for n in range(idxFirstInt - 1, len(s), 2):\n        output[s[n]] = s[n + 1]\n\n    return output",
        "sha1": "d602baa7a3bf3e34c842bb1bcf2252bd35ad1693",
        "id": 268104
    },
    {
        "content": "def add(a, b):\n    \"\"\"\n    Add two cube coords together.\n    :param a: A cube coord x, z, y.\n    :param b: A cube coord x, z, y.\n    :return: THe added cube coordcoords.\n    \"\"\"\n    ax, az, ay = a\n    bx, bz, by = b\n    return ax + bx, az + bz, ay + by",
        "sha1": "940f7011b11f6c3901f804eae0374cd88a424cdb",
        "id": 559995
    },
    {
        "content": "def matrix_augment(A,b):\n    \"\"\" return an augmented matrix\n        for a linear system Ax=b\n        where A is an NxN matrix and x,b are\n        both column vectors of length N\n        an augmented matrix is an Nx(N+1) matrix\n        where each value in b is appended to the\n        end of the corresponding row in A.\n\n        for simplicity, b is a row-vector\n        instead of a column vector.\n    \"\"\"\n    B = [ row + [v,] for row,v in zip(A,b) ]\n    return B;",
        "sha1": "b0c6ff26699452714a9c7b51ef49224426f4e085",
        "id": 131734
    },
    {
        "content": "def is_runtime_parameter(argument):\n    \"\"\"Return True if the directive argument defines a runtime parameter, and False otherwise.\"\"\"\n    return argument.startswith('$')",
        "sha1": "60dce287302ccbfb0cbc32e31c6485951d1e861c",
        "id": 255717
    },
    {
        "content": "import inspect\n\n\ndef is_from_module(obj, module):\n    \"\"\"Return whether `obj` is from module `module`.\"\"\"\n    mod = inspect.getmodule(inspect.unwrap(obj))\n    return mod is None or mod.__name__ == module.__name__",
        "sha1": "1ec842e353b0c81c32fd2d56c07b34ca24b139e5",
        "id": 594764
    },
    {
        "content": "def read_header(f):\n    \"\"\"Get valid time, run, WNO # and # of levels from a L2E profile header.\"\"\"\n    line1 = f.readline()\n    line2 = f.readline()\n    line3 = f.readline()\n    if (not line1.startswith(\"H\") or not line2.startswith(\"M\")\n            or not line3.startswith(\"*\")):\n        raise ValueError(\"File is not at a header position.\")\n    #      valid        run           wmo no      levels\n    return line1[3:11], line3[24:32], line3[3:8], line3[34:37]",
        "sha1": "aba69317e37252c889bf72dd19e5f1127c6bec4a",
        "id": 466162
    },
    {
        "content": "def gotoHostGnx(c, target):\n    \"\"\"Change host node selection to target gnx.\n\n    This will not change the node displayed by the\n    invoking window.\n\n    ARGUMENTS\n    c -- the Leo commander of the outline hosting our window.\n    target -- the gnx to be selected in the host, as a string.\n\n    RETURNS\n    True if target was found, else False\n    \"\"\"\n    if c.p.gnx == target:\n        return True\n    for p in c.all_unique_positions():\n        if p.v.gnx == target:\n            c.selectPosition(p)\n            return True\n    return False",
        "sha1": "22eeb26137b2f082abb32ab9fd1daedf44c08033",
        "id": 36675
    },
    {
        "content": "def sigma_cr_plate(\n        thickness,\n        width,\n        psi=None\n):\n    # Docstring\n    \"\"\"\n    Critical stress of a plate.\n\n    Calculates the critical stress for a simply supported plate.\n\n    Parameters\n    ----------\n    thickness : float\n        [mm] Plate thickness\n    width : float\n        [mm] Plate width\n    psi : float, optional\n        [_] Ratio of the min over max stress for a linear distribution,\n        (sigma_min / sigma_max)\n        Default = 1, which implies a uniform distribution\n\n    Returns\n    -------\n    float\n        [MPa] Plate critical stress\n\n    Notes\n    -----\n    To be extended to include cantilever plate (outstand members)\n\n    References\n    ----------\n    .. [1] Eurocode 3: Design of steel structures - Part 1-5: Plated structural elements. Brussels: CEN, 2005.\n\n    \"\"\"\n    # Convert inputs to floats\n    thickness, width = float(thickness), float(width)\n\n    # Default value for psi\n    if psi is None:\n        psi = 1.\n    else:\n        psi = float(psi)\n\n    # Calculate kapa_sigma\n    k_sigma = 8.2 / (1.05 + psi)\n\n    # Elastic critical stress acc. to EN3-1-5 Annex A\n    sigma_e = 190000 * (thickness / width) ** 2\n    sigma_cr = sigma_e * k_sigma\n\n    # Return value\n    return sigma_cr",
        "sha1": "a78d3160725da4039524a05718a48ce83158ab0a",
        "id": 450473
    },
    {
        "content": "from datetime import datetime\n\n\ndef check_expiration(expiry_date: bytes) -> bool:\n    \"\"\"Check if the MRZ expiry date is older than today's date.\"\"\"\n    date = expiry_date.decode(\"utf-8\")\n    date_obj = datetime.strptime(date, \"%y%m%d\")\n    if date_obj.date() < datetime.now().date():\n        return False\n    return True",
        "sha1": "fb0b6af3fd144fa31965074277aff3c9a6b665eb",
        "id": 321862
    },
    {
        "content": "def add(arg1, arg2):\n    \"\"\"Summary line <should be one one line>.\n\n    Extended description of function.  Can span multiple lines and\n    provides a general overview of the function.\n\n    .. warning::\n       Use the ``.. warning::`` directive within the doc-string for\n       any warnings you would like to explicitly state.  For example,\n       if this method will be deprecated in the next release.\n\n    Parameters\n    ----------\n    arg1 : int\n        Description of arg1.\n    arg2 : str\n        Description of arg2.  Note how there is no space between the\n        previous parameter definition.  Also note that lines are being\n        wrapped at 72 characters.\n\n    Returns\n    -------\n    int\n        Description of return value.  This description ends with a\n        sentence, and we also don't name the return value.\n\n    Examples\n    --------\n    >>> from ansys.product.library import add\n    >>> add(1, 2)\n    3\n\n    See Also\n    --------\n    :class:`ExampleClass <ansys.product.library.ExampleClass>`\n\n    \"\"\"\n    return arg1 + arg2",
        "sha1": "83e4ac1fcdbcc0537b2543b42e7bf18e5b8cf33b",
        "id": 185473
    },
    {
        "content": "import random\n\n\ndef normalDistrib(a, b, gauss=random.gauss):\n    \"\"\"\n    NOTE: assumes a < b\n\n    Returns random number between a and b, using gaussian distribution, with\n    mean=avg(a, b), and a standard deviation that fits ~99.7% of the curve\n    between a and b.\n\n    For ease of use, outlying results are re-computed until result is in [a, b]\n    This should fit the remaining .3% of the curve that lies outside [a, b]\n    uniformly onto the curve inside [a, b]\n\n    ------------------------------------------------------------------------\n    http://www-stat.stanford.edu/~naras/jsm/NormalDensity/NormalDensity.html\n\n    The 68-95-99.7% Rule\n    ====================\n    All normal density curves satisfy the following property which is often\n      referred to as the Empirical Rule:\n    68% of the observations fall within 1 standard deviation of the mean.\n    95% of the observations fall within 2 standard deviations of the mean.\n    99.7% of the observations fall within 3 standard deviations of the mean.\n\n    Thus, for a normal distribution, almost all values lie within 3 standard\n      deviations of the mean.\n    ------------------------------------------------------------------------\n\n    In calculating our standard deviation, we divide (b-a) by 6, since the\n    99.7% figure includes 3 standard deviations _on_either_side_ of the mean.\n    \"\"\"\n    while True:\n        r = gauss((a+b)*.5, (b-a)/6.)\n        if (r >= a) and (r <= b):\n            return r",
        "sha1": "cbb6d6e10998a46c7686956820987fb5e125a598",
        "id": 36489
    },
    {
        "content": "from typing import Sequence\n\n\ndef bit_array_to_int(bit_array: Sequence[int]) -> int:\n    \"\"\"\n    Converts a bit array into an integer where the right-most bit is least significant.\n\n    :param bit_array: an array of bits with right-most bit considered least significant.\n    :return: the integer corresponding to the bitstring.\n    \"\"\"\n    output = 0\n    for bit in bit_array:\n        output = (output << 1) | bit\n    return output",
        "sha1": "d902a8e3db7b65ad348ef86cbfac371361aedd58",
        "id": 699279
    },
    {
        "content": "def _add_extension_assets(client, customer_id, language_code):\n    \"\"\"Creates new assets for the hotel callout.\n\n    Args:\n        client: an initialized GoogleAdsClient instance.\n        customer_id: a client customer ID.\n        language_code: the language of the hotel callout feed item text.\n\n    Returns:\n        a list of asset resource names.\n    \"\"\"\n    operations = []\n    # Create a hotel callout asset operation for each of the below texts.\n    for text in [\"Activities\", \"Facilities\"]:\n        operation = client.get_type(\"AssetOperation\")\n        asset = operation.create\n        asset.hotel_callout_asset.text = text\n        asset.hotel_callout_asset.language_code = language_code\n        operations.append(operation)\n\n    asset_service = client.get_service(\"AssetService\")\n    # Issues the create request to create the assets.\n    response = asset_service.mutate_assets(\n        customer_id=customer_id, operations=operations\n    )\n    resource_names = [result.resource_name for result in response.results]\n\n    # Prints information about the result.\n    for resource_name in resource_names:\n        print(\n            \"Created hotel callout asset with resource name \"\n            f\"'{resource_name}'.\"\n        )\n\n    return resource_names",
        "sha1": "bc213f9be915845b9ebb977a8b10ec941a640089",
        "id": 30402
    },
    {
        "content": "def _get_tarfile_size( tarfile ):\n  \"\"\"\n  Determine the size of a file inside the tarball.\n  If the object has a size attribute, use that. Otherwise seek to the end\n  and report that.\n  \"\"\"\n  try:\n    return tarfile.size\n  except AttributeError:\n    pass\n\n  size = tarfile.seek( 0, 2 )\n  tarfile.seek( 0, 0 )\n  return size",
        "sha1": "7d7d2e072167dc1170c4cb7f2a65af286d4daafa",
        "id": 272134
    },
    {
        "content": "def encipher_rsa(pt, puk):\n    \"\"\"\n    In RSA, a message `m` is encrypted by computing\n    `m^e` (mod `n`), where ``puk`` is the public key `(n,e)`.\n\n    Examples\n    ========\n\n    >>> from sympy.crypto.crypto import encipher_rsa, rsa_public_key\n    >>> p, q, e = 3, 5, 7\n    >>> puk = rsa_public_key(p, q, e)\n    >>> pt = 12\n    >>> encipher_rsa(pt, puk)\n    3\n\n    \"\"\"\n    n, e = puk\n    return pow(pt, e, n)",
        "sha1": "01ba60b355cdb97eb1e7528f89ea0bbec53cd0dd",
        "id": 77427
    },
    {
        "content": "def consider_trip_number(trip_strategy, total_trips, trip_num):\n    \"\"\"Determines if the vehicle should charge given trip strategy and current trip\n    \n    :param int trip_strategy: a toggle that determines if should charge on any trip or only after last trip (1-anytrip number, 2-last trip)\n    :param int total_trips: total trips that the vehicle makes\n    :param int trip_num: the trip number of the current trip\n    :return: (*bool*) -- boolean that represents if the vehicle should charge\n    \"\"\"\n    if trip_strategy == 1:\n        return True\n    \n    elif trip_strategy == 2:\n        return total_trips == trip_num",
        "sha1": "b2fe3a43b95fead4ebf65981cfba39857d1bee3c",
        "id": 699157
    },
    {
        "content": "from typing import List\nimport requests\n\n\ndef get_translations(expr_id: List[int], trg: str, trans_qual_min: int=50, timeout: int=30, batch_size: int=200) -> dict:\n    \"\"\"\n    Request translation(s) for expression id in target language.\n\n    :param expr_id int: unique identifier for a word\n    :param trg str: ISO 639 code for the target language\n    :param trans_qual_min int: minimum threshold of translation quality score\n    :return expr2trans dict: {expr_id: {trans_quality: [trans_1, ..., trans_n]}\n    \"\"\"\n    # -000: pull most common language variety\n    trg_lang = trg + \"-000\"\n    data = (f'{{\"include\": \"trans_quality\", '\n            f'\"trans_expr\": {expr_id}, '\n            f'\"uid\": \"{trg_lang}\", '\n            f'\"trans_quality_min\": {trans_qual_min}}}')\n    response = requests.post('http://api.panlex.org/v2/expr', data=data, timeout=timeout)\n    results = response.json()['result']\n    # expr2trans >> {expr: {trans_quality: [txt_1, ..., txt_n]}\n    expr2trans = {}\n    for result in results:\n        trans_expr = result['trans_expr']\n        trans_quality = result['trans_quality']\n        txt = result['txt']\n        if not trans_expr in expr2trans:\n            expr2trans[trans_expr] = {}\n        if not trans_quality in expr2trans[trans_expr]:\n            expr2trans[trans_expr][trans_quality] = []\n        expr2trans[trans_expr][trans_quality].append(txt)\n    return expr2trans",
        "sha1": "d6cf5fbbb9093f9a3452c8c782bcb0257fe92e32",
        "id": 557392
    },
    {
        "content": "def unique_fname(full_path: str) -> str:\n    \"\"\"Get unique file name for given full path to MELD data file. The return format is '[dialog]_[utterance]'.\n\n    :param full_path: full path to MELD .mp4 data file\n    :return: unique id of data file (only unique within dataset directory)\n    \"\"\"\n    fname = full_path.split('/')[-1].split('.')[0]\n    return fname.replace('dia', '').replace('utt', '').replace('final_videos_test', '')",
        "sha1": "29bffc8a2028ac126709fe17d9e2e1d2914bf769",
        "id": 26658
    },
    {
        "content": "def pax_to_human_time(num):\n    \"\"\"Converts a pax time to a human-readable representation\"\"\"\n    for x in ['ns', 'us', 'ms', 's', 'ks', 'Ms', 'G', 'T']:\n        if num < 1000.0:\n            return \"%3.3f %s\" % (num, x)\n        num /= 1000.0\n    return \"%3.1f %s\" % (num, 's')",
        "sha1": "c0cf037a6a170e9fda15cc68361c38b8e1e7761c",
        "id": 677209
    },
    {
        "content": "def count_trees(map: list, start: tuple, vector: tuple) -> int:\n    \"\"\"\n    Count the number of '#' trees encountered by following \n    the movement of vector until reach the last line.\n    \"\"\"\n    x, y = start\n    count = 0\n    width = len(map[0])\n    while y < len(map)-1:\n        x += vector[0]\n        y += vector[1]\n        if x >= width:\n            x -= width\n        if map[y][x] == '#':\n            count += 1\n    return count",
        "sha1": "31b609b117fa541a2d6960dc02ee827a209987ee",
        "id": 497237
    },
    {
        "content": "import csv\n\n\ndef read_headers(file_name):\n    \"\"\"\n    Generate headers list from file\n    \"\"\"\n    with open(file_name) as f:\n        reader = csv.reader(f, delimiter=',', quotechar='\"')\n        # Skip the header\n        headers = next(reader)\n    return headers",
        "sha1": "852c5ced337dd00c38a6fc191f58450b41018e52",
        "id": 15377
    },
    {
        "content": "def is_wikidata_entity(entity: str) -> bool:\n    \"\"\"Assert that a string is a wikidata entity\"\"\"\n    wrong_entity_format = entity and (entity[0] != \"Q\" or any([j not in \"0123456789\" for j in entity[1:]]))\n    return not wrong_entity_format",
        "sha1": "2e5f879c5ad45f69d8d0102967a71d6f3543b8d0",
        "id": 619662
    },
    {
        "content": "from typing import Any\n\n\ndef format_with_optional_units(value: Any, units: str) -> str:\n    \"\"\"\n    Ex:\n    > format_with_optional_units(25.0, \"cm\") # returns \"25.0 cm\"\n    > format_with_optional_units(25.0, None) # returns \"25.0\"\n    \"\"\"\n    if units is None:\n        return f\"{value}\"\n\n    if units == \"%\":\n        return f\"{value}{units}\"\n    else:\n        return f\"{value} {units}\"",
        "sha1": "3e763bbfdaa35dfd5c4ab294a2a1e3d4ae8d6a4e",
        "id": 641177
    },
    {
        "content": "def unique_sample_names(sample_names):\n    \"\"\"\n    unique_sample_names takes a given list of sample names and returns a list\n    of all the unique names in the list\n    \"\"\"\n    ret = []\n    for name in sample_names:\n        if name not in ret:\n            ret.append(name)\n    return ret",
        "sha1": "0f71cc0e495bb8832832293d79c50143f733d306",
        "id": 283802
    },
    {
        "content": "def padovan_pythonic(number: int) -> int:\n    \"\"\"\n    Examples:\n        >>> assert padovan_pythonic(2) == 1\n        >>> assert padovan_pythonic(5) == 3\n    \"\"\"\n    p0 = p1 = p2 = 1\n    for _ in range(number):\n        p0, p1, p2 = p1, p2, p0 + p1\n    return p0",
        "sha1": "e84764be857324fbcc67425f1e9e996765408b52",
        "id": 574346
    },
    {
        "content": "def should_perform_aggr_query(hesabi_body):\n    \"\"\"\n    this function specifies if we should go for agg field from sources or not\n    :param hesabi_body:\n    :return:\n    \"\"\"\n    if \"agg_field\" in hesabi_body:\n        return True\n    return False",
        "sha1": "a7f4c326ff517ab670316435a7fd8669a920cb5e",
        "id": 81935
    },
    {
        "content": "import re\n\n\ndef _is_auspex(path):\n    \"\"\"\n    Test if a given file path is an Auspex data file\n    \"\"\"\n    path = path.rstrip('/')\n    filename = path.split('/')[-1]\n    if re.findall(r\".+.auspex\", filename):\n        return True\n    else:\n        return False",
        "sha1": "a644f14d9c8e7647546efa7f0f63694edd65e951",
        "id": 637308
    },
    {
        "content": "def filter_generate_files(\n    s3_prefix,\n    target_prefix,\n    files,\n    keywords=[\"T1w\"],\n):\n    \"\"\"Filter files based on keywords, generate target path\n\n    Parameters\n    ----------\n\n    s3_prefix, target_prefix: str, str\n        Path to BIDS dataset on S3 and your dowload desination\n\n    files: list of str\n        List of S3 file paths\n\n    keywords: list of str\n        List of keywords you want to keep\n\n    Return\n    ------\n\n    List of zipped tuple of source and target path\n\n    \"\"\"\n    s3_keep = []\n    local = []\n    for f in files:\n        for keyword in keywords:\n            if keyword in f and (f not in s3_keep):\n                print(f)\n                target_path = f.replace(s3_prefix, target_prefix)\n                s3_keep.append(f)\n                local.append(target_path)\n    return list(zip(s3_keep, local))",
        "sha1": "5300080653a48ae073a115ba943aa7209656ddbb",
        "id": 567863
    },
    {
        "content": "def _get_version_from_arguments(arguments):\n    \"\"\"Checks the arguments passed to `nox -s release`.\n\n    Only 1 argument that looks like a version? Return the argument.\n    Otherwise, raise a ValueError describing what's wrong.\n    \"\"\"\n    if len(arguments) != 1:\n        raise ValueError(\"Expected exactly 1 argument\")\n\n    version = arguments[0]\n    parts = version.split(\".\")\n\n    if len(parts) != 2:\n        # Not of the form: YY.N\n        raise ValueError(\"not of the form: YY.N\")\n\n    if not all(part.isdigit() for part in parts):\n        # Not all segments are integers.\n        raise ValueError(\"non-integer segments\")\n\n    # All is good.\n    return version",
        "sha1": "82b20de70d4f2da07bfe98b72721b5ec970a1218",
        "id": 327637
    },
    {
        "content": "def conv_params(sz_in: int, sz_out: int):\n    \"\"\"Solves for filter_size, padding and stride per the following equation,\n       sz_out = (sz_in - filter_size + 2*padding) / stride + 1\n       Attempts to find a solution by iterating over various filter_size, stride and padding\n       in that order. If no solution is found, raises an error\n    \"\"\"\n    filter_size, stride, padding = [3,2,4,5], [2,1,3], [1,2,3]\n    for f in filter_size:\n        for s in stride:\n            for p in padding:\n                if ((sz_in - f + 2*p) / s + 1) == sz_out:\n                    return (f, s, p)\n    raise Exception(\"Unable to find valid parameters for {0} to {1} convolution\".format(sz_in, sz_out))",
        "sha1": "86c1a2437231d2fb6515a6581719d3568cdee813",
        "id": 30799
    },
    {
        "content": "def _kappamstar(kappa, m, xi):\n    \"\"\"\n    Computes maximized cumulant of order m\n\n    Parameters\n    -----\n    kappa : list\n        The first two cumulants of the data\n    xi : int\n        The :math:`\\\\xi` for which is computed the p value of :math:`H_0`\n    m : float\n        The order of the cumulant\n\n    Returns\n    -----\n    k_out : list\n        The maximized cumulant of order m\n    \"\"\"\n\n    if xi == 1:\n        kappa_out = kappa[1]\n    else:\n        kappa_out = \\\n            (kappa[1] * (xi ** (m - 1) - 1) -\n                kappa[0] * (xi ** (m - 1) - xi)) / (xi - 1)\n    return kappa_out",
        "sha1": "756d42c3b45f02fd0850cf91ab33bae0e3c21994",
        "id": 660722
    },
    {
        "content": "def remove_chars(s, chars):\n    \"\"\"Remove any character in chars from string s\n    s: string\n    chars: string of characters\n    \"\"\"\n    for c in chars:\n        if c in s:\n            s = s.replace(c,'')\n    return s",
        "sha1": "f26df5a8661d8ff74bd215c1b06d97d7415fd30c",
        "id": 326932
    },
    {
        "content": "def _resolve_resource_dependencies(resource_defs):\n    \"\"\"Generates a dictionary that maps resource key to resource keys it requires for initialization\n    \"\"\"\n    resource_dependencies = {\n        key: resource_def.required_resource_keys for key, resource_def in resource_defs.items()\n    }\n    return resource_dependencies",
        "sha1": "76d7c020e105b0843174373e7a63d379fcabed76",
        "id": 152942
    },
    {
        "content": "def _sign(number):\n    \"\"\"\n    Get sign of the given number\n\n    :param number:\n    :type number: int\n\n    :return: -1, 0, 1\n    :rtype: int\n    \"\"\"\n    if number > 0:\n        return 1\n\n    if number < 0:\n        return -1\n\n    return 0",
        "sha1": "410a10d914b4c75b767159ea005ae3fda93b02e8",
        "id": 376201
    },
    {
        "content": "import copy\n\n\ndef codemirror_settings_update(configs, parameters, on=None, names=None):\n    \"\"\"\n    Return a new dictionnary of configs updated with given parameters.\n\n    You may use ``on`` and ``names`` arguments to select config or filter out\n    some configs from returned dict.\n\n    Arguments:\n        configs (dict): Dictionnary of configurations to update.\n        parameters (dict): Dictionnary of parameters to apply on selected\n            configurations.\n\n    Keyword Arguments:\n        on (list): List of configuration names to select for update. If empty,\n            all given configurations will be updated.\n        names (list): List of configuration names to keep. If not empty, only\n            those configurations will be in returned dict. Else every\n            configs from original dict will be present.\n\n    Returns:\n        dict: Dict of configurations with updated parameters.\n    \"\"\"\n    # Deep copy of given config\n    output = copy.deepcopy(configs)\n\n    # Optionnaly filtering config from given names\n    if names:\n        output = {k: output[k] for k in names}\n\n    # Select every config if selectors is empty\n    if not on:\n        on = output.keys()\n\n    for k in on:\n        output[k].update(parameters)\n\n    return output",
        "sha1": "29e0b3ca123120f59f4547852546dd9d49cebaae",
        "id": 440104
    },
    {
        "content": "def complex_lorentzian_k(x, gamma=1., amplitude=1.j):\n    \"\"\"\n    Complex Lorentzian kernel function.\n    \"\"\"\n    return (gamma/2.)/(gamma/2.+1j*x)*amplitude",
        "sha1": "e83e496c0b7d3c8881d37fa57ec77e8ce6321a62",
        "id": 201957
    },
    {
        "content": "def callback(_):\n    \"\"\"Stub callback.\"\"\"\n    return {\"Water\": True}",
        "sha1": "302f321588e6fb2273887219ae3c750cb7a52e83",
        "id": 106896
    },
    {
        "content": "def slices(start, xs):\n    \"\"\"Returns slices of a given sequence separated by the specified indices.\n\n    If we wanted to get the slices necessary to split range(20) in\n    sub-sequences of 5 items each we'd do:\n\n    >>> seq = range(20) \n    >>> indices = [5, 10, 15]\n    >>> for piece in slices(0, indices):\n    ...     print seq[piece]\n    [0, 1, 2, 3, 4]\n    [5, 6, 7, 8, 9]\n    [10, 11, 12, 13, 14]\n    [15, 16, 17, 18, 19]\n\n    @param start: Index of the first element of the sequence we want to\n    partition.\n    @type start: C{int}.\n\n    @param xs: Sequence of indexes where 'cuts' must be made.\n    @type xs: C{list}\n\n    @return: A sequence of C{slice} objects suitable for splitting a list as\n    specified.\n    @rtype: C{list} of C{slice}\n    \"\"\"\n    if xs == []:\n        # The last slice includes all the remaining items in the sequence.\n        return [slice(start, None)]\n    return [slice(start, xs[0])] + slices(xs[0], xs[1:])",
        "sha1": "85a3e7ea7b081edce9e9943a9f5a310c5cb38c43",
        "id": 209709
    },
    {
        "content": "import typing\n\n\ndef _split_bits(i: int) -> typing.Tuple[bool, bool, bool, bool, bool, bool, bool, bool]:\n\t\"\"\"Split a byte (an int) into its 8 bits (a tuple of 8 bools).\"\"\"\n\t\n\tassert i in range(256)\n\treturn (\n\t\tbool(i & (1 << 7)),\n\t\tbool(i & (1 << 6)),\n\t\tbool(i & (1 << 5)),\n\t\tbool(i & (1 << 4)),\n\t\tbool(i & (1 << 3)),\n\t\tbool(i & (1 << 2)),\n\t\tbool(i & (1 << 1)),\n\t\tbool(i & (1 << 0)),\n\t)",
        "sha1": "cdb7b08ab9969d56de284c214ae4ecfbb03b63ac",
        "id": 350194
    },
    {
        "content": "def populate_array_id(arr, spots):\n    \"\"\"\n    receives an empty array\n    populates the array with values corresponding to spot ID:\n        ID like : \"spot-1-2\", \"spot-2-4\", \"spot-3-3-\", etc...\n\n    :param arr: np.ndarray\n        numpy array generated from \"create_array\"\n    :param spots: dict\n        dict from \"create_xml_dict\"\n    :return: np.ndarray\n        populated array\n    \"\"\"\n    for spot in spots:\n        r = int(spot['@row'])\n        c = int(spot['@col'])\n        ID = spot['@id']\n\n        arr[r, c] = ID\n\n    return arr",
        "sha1": "63840bf061a3653e1b1ec5c0014453ea0f408a9c",
        "id": 584573
    },
    {
        "content": "import re\n\n\ndef _infer_dtype(val):\n    \"\"\"\n    Using regex, infer a limited number of dtypes for string `val`\n    (only dtypes expected to be found in a Google Trends CSV export).\n    \"\"\"\n    if re.match(r'\\d{4}-\\d{2}(?:-\\d{2})?', val):\n        return 'date'\n    elif re.match(r'[+-]?\\d+$', val):\n        return 'int'\n    elif re.match(r'[+-]?\\d+%$', val):\n        return 'pct'\n    elif re.match(r'[a-zA-Z ]+', val):\n        return 'text'\n    else:\n        msg = \"val={0} dtype not recognized\".format(val)\n        raise ValueError(msg)",
        "sha1": "546b43c6ae6bf55c6ca0705e64f5cff91bc5c22a",
        "id": 432289
    },
    {
        "content": "def is_tiein_survey(sheet):\n    \"\"\"Is this worksheet a \"tie-in survey\"?\"\"\"\n    return sheet.name.replace('-','').lower() == 'tiein'",
        "sha1": "87168ead3a576c681ed27af1ab2906aad7e3b1aa",
        "id": 115495
    },
    {
        "content": "import struct\n\n\ndef s15f16l(s):\n    \"\"\"Convert sequence of ICC s15Fixed16 to list of float.\"\"\"\n\n    # Note: As long as float has at least 32 bits of mantissa, all\n    # values are preserved.\n    n = len(s) // 4\n    t = struct.unpack('>%dl' % n, s)\n    return map((2**-16).__mul__, t)",
        "sha1": "c5f5b707736d95684e8e03ae88e6e986e217200d",
        "id": 402227
    },
    {
        "content": "def get_align_from_hjust(hjust: float) -> str:\n    \"\"\"\n    Maps 0 -> 'left', 0.5 -> 'center', 1 -> 'right'\n    ... and in-between things get nudged to the nearest of those three.\n    \"\"\"\n    if hjust <= 0.25:\n        return 'left'\n    elif hjust >= 0.75:\n        return 'right'\n    return 'center'",
        "sha1": "a16454ac384757ab66ea835fee2ebc4893b2a29b",
        "id": 410029
    },
    {
        "content": "def sumdiffsquared(x, y):\n    \"\"\"\nTakes pairwise differences of the values in lists x and y, squares\nthese differences, and returns the sum of these squares.\n\nUsage:   lsumdiffsquared(x,y)\nReturns: sum[(x[i]-y[i])**2]\n\"\"\"\n    sds = 0\n    for i in range(len(x)):\n        sds = sds + (x[i] - y[i]) ** 2\n    return sds",
        "sha1": "02caa05775bc543dd0015986cf8af9c7e531bfd8",
        "id": 520169
    },
    {
        "content": "import string\nimport random\n\n\ndef id_generator(size=15, chars=string.ascii_uppercase + string.digits):\n    \"\"\" Generates a random identifier for the given size and using the\n    specified characters.\n    If no size is specified, it uses 15 as default.\n    If no characters are specified, it uses ascii char upper case and\n    digits.\n    :arg size: the size of the identifier to return.\n    :arg chars: the list of characters that can be used in the\n        idenfitier.\n    \"\"\"\n    return ''.join(random.choice(chars) for x in range(size))",
        "sha1": "8ddd201a1f39b8311510570e0940c580c0cf89e0",
        "id": 165490
    },
    {
        "content": "import torch\n\n\ndef _moments(a, b, n):\n    \"\"\"\n    Computes nth moment of Kumaraswamy using using torch.lgamma\n    \"\"\"\n    arg1 = 1 + n / a\n    log_value = torch.lgamma(arg1) + torch.lgamma(b) - torch.lgamma(arg1 + b)\n    return b * torch.exp(log_value)",
        "sha1": "00e38aedd21e7e2ed89bde68b5f6464d4be5a707",
        "id": 673315
    },
    {
        "content": "def get_col(df, col_name):\n    \"\"\" Collect any column including the index.\n    \"\"\"\n    if col_name in {df.index.name, \"index\"}:\n        return df.index\n    else:\n        return df[col_name]",
        "sha1": "d383a5f0bab80f325376a842582baa243e11744e",
        "id": 321968
    },
    {
        "content": "def get_valid_columns(dataframes, contribution_type, row_sheet_name):\n    \"\"\"\n    Function is used to determine the columns to be investigated in\n    across all of the site report dataframes. The columns should all\n    be the same in the 'source' sheet (otherwise an error is thrown).\n\n    The columns of interest are those that can help elucidate an\n    HPO site's relative contribution (total rows, well defined rows,\n    etc.)\n\n    :param\n    dataframes (lst): list of pandas dataframes loaded from the Excel\n        files generated from the analysis reports\n\n    contribution_type (str): string representing the types of columns to\n        look at for the dataframe. either can represent the 'total' row\n        metrics or the 'error' metrics for a particular column.\n\n    row_sheet_name (str): sheet name within the analytics files that\n        show the total number of rows and the number of well defined\n        rows\n\n    :return:\n    valid_cols (lst): list of the columns that are consistent across all\n        of the sheets and relevant to the HPO weighting report needed\n    \"\"\"\n    valid_cols = []\n\n    # find the columns you want to investigate\n    for df in dataframes:\n        for column in df:\n            if contribution_type == 'total' and len(column) > 9 and \\\n                    column[-9:] == 'total_row':\n                valid_cols.append(column)\n            elif contribution_type == 'valid' and len(column) > 16 and \\\n                    column[-16:] == 'well_defined_row':\n                valid_cols.append(column)\n\n    valid_cols = list(dict.fromkeys(valid_cols))\n    valid_cols.sort()\n\n    return valid_cols",
        "sha1": "36b27265597c3b5640b55cf23f75eb2436bcc2b1",
        "id": 530099
    },
    {
        "content": "def get_items_markdown(items, indent=\"\"):\n    \"\"\"Return a list of NEWS entries as a Markdown list\"\"\"\n    return \"\".join([indent + \"* %s\\n\" % item for item in items])",
        "sha1": "bac0abdda2f0716084a96c00685fac9ed682031b",
        "id": 143158
    },
    {
        "content": "def _flatten_dict(input_dict: dict) -> dict:\n    \"\"\"\n    Flattens a nested dictionary, keeping only keys with non-dict values. Note\n    that keys will be the underscore-delimited concatenation of the nested path\n    of keys from the original dict.\n    \"\"\"\n    res = {}\n    for k, v in input_dict.items():\n        if type(v) is not dict:\n            res[k] = v\n        else:\n            nest = _flatten_dict(v)\n            for key, value in nest.items():\n                new_key = \"{}_{}\".format(k, key)\n                res[new_key] = value\n    return res",
        "sha1": "9273b1b7b08d54b3b7e69a2710285abbac8d22e4",
        "id": 334372
    },
    {
        "content": "from pathlib import Path\n\n\ndef get_dictionaries(base: Path) -> list[tuple[str, list[Path]]]:\n    \"\"\"Make a list of available dictionaries and the interesting files.\n\n    :param base: The Apple Dictionaries root directory.\n    :return: A list of tuples of name, files for each dictionary.\n    \"\"\"\n    all_dicts = sorted(\n        (\n            (\n                dic.stem,\n                [\n                    f\n                    for f in (dic / \"Contents\" / \"Resources\").iterdir()\n                    if f.suffix != \".lproj\"\n                ],\n            )\n            for dic_path in Path(base).iterdir()\n            if dic_path.is_dir()\n            for dic in (dic_path / \"AssetData\").iterdir()\n        ),\n        key=lambda x: x[0],\n    )\n    return [(name, files) for name, files in all_dicts if files]",
        "sha1": "98068ecb847aa6975cc8dc7eed59b8948316f191",
        "id": 699366
    },
    {
        "content": "def negate(sys):\n    \"\"\"\n    Return the negative of a system.\n\n    Parameters\n    ----------\n    sys : StateSpace, TransferFunction or FRD\n\n    Returns\n    -------\n    out : StateSpace or TransferFunction\n\n    Notes\n    -----\n    This function is a wrapper for the __neg__ function in the StateSpace and\n    TransferFunction classes.  The output type is the same as the input type.\n\n    Examples\n    --------\n    >>> sys2 = negate(sys1) # Same as sys2 = -sys1.\n\n    \"\"\"\n    return -sys",
        "sha1": "1bd19fae9f69ad993e85bf589f30ba107028b415",
        "id": 376477
    },
    {
        "content": "def SnapshotDisplayDict(snapshot, error_msg=''):\n  \"\"\"Creates a serializable from a Cloud Pub/Sub Snapshot operation for display.\n\n  Args:\n    snapshot: (Cloud Pub/Sub Snapshot) Snapshot to be serialized.\n    error_msg: (string) An error message to be added to the serialized\n               result, if any.\n  Returns:\n    A serialized object representing a Cloud Pub/Sub Snapshot operation (create,\n    delete).\n  \"\"\"\n  return {\n      'snapshotId': snapshot.name,\n      'topic': snapshot.topic,\n      'expireTime': snapshot.expireTime,\n      'success': not error_msg,\n      'reason': error_msg or '',\n  }",
        "sha1": "59141a367e39167cda63af6147393cffda2fefb2",
        "id": 532939
    },
    {
        "content": "def unsigned_to_varint(num: int) -> bytes:\n    \"\"\"\n    convert an unsigned int to varint.\n    \"\"\"\n    if num < 0 or num > 0xffffffffffffffff:\n        raise OverflowError(f\"can't convert {num} to varint\")\n    if num <= 0xfc:\n        return num.to_bytes(1, 'little')\n    elif num <= 0xffff:\n        return b'\\xfd' + num.to_bytes(2, 'little')\n    elif num <= 0xffffffff:\n        return b'\\xfe' + num.to_bytes(4, 'little')\n    else:\n        return b'\\xff' + num.to_bytes(8, 'little')",
        "sha1": "62c7df5a2c98e8e9f0dd5a3901c76fad44df2059",
        "id": 150451
    },
    {
        "content": "def compute_MIF(ic):\n    \"\"\"\n    Args:\n        ic (IC): indepentent component.\n\n    Returns:\n        float: Myogenic identification feature.\n    \"\"\"\n    freqs, psd = ic.psd(verbose=False)\n    mean_psd = psd.mean(axis=0)\n    return mean_psd[freqs > 20].sum() / mean_psd.sum()",
        "sha1": "423a96a9dfcc3c05b2fb05f97e7912a534b41c95",
        "id": 46711
    },
    {
        "content": "def _gen_table_cols(col_ids):\n    \"\"\"Generate Dash table columns in the expected format.\n\n    :param col_ids: list of columns; must be in format <table-alias.name>,\n                    like \"s.serial_number\", as in the SQL select statement\n                    -- except for derived column values which must literally\n                    use \"alias.\" plus the name.\n    :return: List of dictionaries, where each contains an 'id' and a 'name' key\n             for a Dash DataTable.\n    \"\"\"\n    col_list = []\n    for col in col_ids:\n        split_col = col.partition('.')\n        if split_col[0] == 'alias':\n            col_list.append({'id' : 'alias_{}'.format(split_col[2]),\n                             'name' : split_col[2]})\n        else:\n            col_list.append({'id' : split_col[2], 'name' : split_col[2]})\n\n    return col_list",
        "sha1": "64bba3c801408e05eba9d77792c05a566351a008",
        "id": 405952
    },
    {
        "content": "import math\n\n\ndef start_workers(cluster, config, config_file, parallel):\n    \"\"\"Initiate worker nodes on cluster, returning jobs IDs for management.\n    \"\"\"\n    # we can manually specify workers or dynamically get as many as needed\n    num_workers = config[\"distributed\"].get(\"num_workers\", None)\n    if num_workers in [None, \"all\"]:\n        cores_per_host = config[\"distributed\"].get(\"cores_per_host\", 1)\n        if cores_per_host == 0:\n            raise ValueError(\"Set num_workers or cores_per_host in YAML config\")\n        assert parallel[\"cores\"] is not None, \\\n               \"Supply workers needed if not configured in YAML\"\n        num_workers = int(math.ceil(float(parallel[\"cores\"]) / cores_per_host))\n    program_cl = [config[\"analysis\"][\"worker_program\"], config_file]\n    if parallel.get(\"task_module\", None):\n        program_cl.append(\"--tasks={0}\".format(parallel[\"task_module\"]))\n    if parallel.get(\"queues\", None):\n        program_cl.append(\"--queues={0}\".format(parallel[\"queues\"]))\n    args = config[\"distributed\"][\"platform_args\"].split()\n    return [cluster.submit_job(args, program_cl) for _ in range(num_workers)]",
        "sha1": "3de473144dbb88bf17f5de512528d69c5c79a8d5",
        "id": 380879
    },
    {
        "content": "def insertion_sort(array):\n    \"\"\"\n    Sort an input array with insertion sort algorithm\n\n    The insertion sort algorithm compares an element with the preceeding\n    ordered element to determine whether the two should be swapped.  This will\n    continue until the preceeding element is no longer greater than the\n    current element.\n\n    Best case scenario: O(n)\n    - Best case, if an array is already sorted, this algorithm will inspect\n    every element of the list once to verify it is sorted, no swaps required.\n\n    Worst case scenario: O(n^2)\n    - Worst case, if an array is reversely sorted, each element must be\n    compared and swapped with every element preceeding it until it reaches\n    the beginning.\n    \"\"\"\n    for elem in range(len(array)):\n        curr = elem\n        while curr > 0 and array[curr - 1] > array[curr]:\n            # swap values\n            array[curr - 1], array[curr] = array[curr], array[curr - 1]\n            curr -= 1\n    return array",
        "sha1": "18657cb64dfb88c7ab6e41bb79caa9ffd23da07d",
        "id": 367561
    },
    {
        "content": "def plot_data(d, field):\n    \"\"\"\n    Return the x and y series to be used for plotting\n\n    Args:\n        d (OrderedDict)\n        field (str)\n\n    Returns:\n        Tuple[list, list]:\n            [0] The x-series\n            [1] The y-series\n\n    \"\"\"\n    return ([year for year in d.keys() if d[year][field] is not None],\n            [i[field] for k, i in d.items() if i[field] is not None])",
        "sha1": "a5c7de18f1da44da0641362a5ace24f7bc35c1b5",
        "id": 692282
    },
    {
        "content": "def get_doc_str(fun):\n    \"\"\"Get the doc string for a function and return a default if none is found.\"\"\"\n    if fun.__doc__:\n        return '\\n'.join([line.strip() for line in fun.__doc__.split('\\n')])\n    else:\n        return 'No documentation provided for %s()' % fun.__name__",
        "sha1": "ab7653a8975fc632c6682e74b9bf67e21b7ffdd1",
        "id": 80182
    },
    {
        "content": "import uuid\n\n\ndef gen_id() -> str:\n    \"\"\"Generate new Skybell IDs.\"\"\"\n    return str(uuid.uuid4())",
        "sha1": "77cdcf546c95071f618119b5f493d7a46426c5bd",
        "id": 681350
    },
    {
        "content": "def camel_case_convert(item: str) -> str:\n    \"\"\"Converts strings that look_like_this to\n    strings that lookLikeThis\"\"\"\n\n    item = (\n        item.title().replace(\" \", \"\").replace(\"_\", \"\").replace(\"-\", \"\")\n    )  # removing all '_', '-', and spaces\n    item = item[0].lower() + item[1:] if item else \"\"\n    return item",
        "sha1": "5231831e6f1c66ea8f9dd5f74da0922f8ce434af",
        "id": 373550
    },
    {
        "content": "def initializeRadii(animal_obj,boundary_vertices):\n  \"\"\" Initializes the radii for the circle packing of the triangulation associated to an animal.\n    Different radii are assigned to interior and boundary vertices.\n\n    :Parameters:\n      animal_obj : animal object, initialized with regular coordinates and triangulation set/updated\n      boundary_vertices : list of ints whose values are between 0 and one less than the number of regular vertices stored in the animal object\n\n    :Returns:\n      list of floats, specifying the initial radii to begin the circle packing algorithm for the triangulation associated to an animal \n  \"\"\"\n\n  #store relevant parameters\n  num_verts = animal_obj.getNumVerts()\n\n  #initialize return list\n  radii = []\n\n  #assign radius 0.001 to each boundary vertex and radius 0.5 to each interior vertex\n  for vertex in range(num_verts):\n    if vertex in boundary_vertices:\n      radii.append(0.001)\n    else:\n      radii.append(0.5)\n  \n  return radii",
        "sha1": "aafb11766e2fbb83d68588e89a92c759d6efa09a",
        "id": 208303
    },
    {
        "content": "def key_for_name(name):\n    \"\"\"Return the key name used to store the given queue name in Redis.\"\"\"\n    return 'hotqueue:%s' % name",
        "sha1": "d7e3f6b723da967a9b2f7c5e72b0289073b80fc3",
        "id": 159406
    },
    {
        "content": "from typing import Tuple\nimport torch\n\n\ndef mock_hcl_mm_values() -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"A set of energies, gradients and hessians analytically computed using\n    for the ``mock_hcl_conformers`` and ``mock_hcl_system``.\n    \"\"\"\n\n    return (\n        torch.tensor([[0.0], [1.0]]),\n        torch.tensor(\n            [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0]], [[-2.0, 0.0, 0.0], [2.0, 0.0, 0.0]]]\n        ),\n        torch.tensor(\n            [\n                [\n                    [+2.0, 0.0, 0.0, -2.0, 0.0, 0.0],\n                    [+0.0, 0.0, 0.0, +0.0, 0.0, 0.0],\n                    [+0.0, 0.0, 0.0, +0.0, 0.0, 0.0],\n                    [-2.0, 0.0, 0.0, +2.0, 0.0, 0.0],\n                    [+0.0, 0.0, 0.0, +0.0, 0.0, 0.0],\n                    [+0.0, 0.0, 0.0, +0.0, 0.0, 0.0],\n                ],\n                [\n                    [+2.0, +0.0, +0.0, -2.0, +0.0, +0.0],\n                    [+0.0, +1.0, +0.0, +0.0, -1.0, +0.0],\n                    [+0.0, +0.0, +1.0, +0.0, +0.0, -1.0],\n                    [-2.0, +0.0, +0.0, +2.0, +0.0, +0.0],\n                    [+0.0, -1.0, +0.0, +0.0, +1.0, +0.0],\n                    [+0.0, +0.0, -1.0, +0.0, +0.0, +1.0],\n                ],\n            ]\n        ),\n    )",
        "sha1": "b4e6c19911e9a2271e21dd513d29964620f09a2a",
        "id": 522563
    },
    {
        "content": "def unzip(p, n):\n    \"\"\"Split a list-like object, 'p', into 'n' sub-lists by taking\n    the next unused element of the main list and adding it to the\n    next sub-list. A list of tuples (the sub-lists) is returned.\n    Each of the sub-lists is of the same length; if p%n != 0, the\n    shorter sub-lists are padded with 'None'.\n        Example:\n        >>> unzip(['a','b','c','d','e'], 3)\n        [('a', 'd'), ('b', 'e'), ('c', None)]\n    \"\"\"\n    (mlen, lft) = divmod(len(p),n)          # find length of longest sub-list\n    if lft != 0: mlen += 1\n\n    lst = [[None]*mlen for i in range(n)]   # initialize list of lists \n        \n    for i in range(len(p)):\n        (j, k) = divmod(i, n)\n        lst[k][j] = p[i]\n\n    return map(tuple, lst)",
        "sha1": "b526d2f71303c739b68035e5ad6b5644f47c547a",
        "id": 83118
    },
    {
        "content": "def bind_rt_nodes(srcnodelist, newnodes_list):\n    \"\"\"Return srcnodelist with all RuntimeNodes replaced by nodes\n    coming from newnodes_list.\n    \"\"\"\n    memo = {}\n    newnodes_iter = iter(newnodes_list)\n    result = [node.bind_rt_nodes(memo, newnodes_iter) for node in srcnodelist]\n    rest = list(newnodes_iter)\n    assert rest == [], \"too many nodes in newnodes_list\"\n    return result",
        "sha1": "a9141767b177a8dbbfcc291d3b4415a197872818",
        "id": 112678
    },
    {
        "content": "def create_lookup(results):\n    \"\"\"Create a lookup table by key name.\"\"\"\n    return {key[\"key_name\"][\"web\"]: key for key in results}",
        "sha1": "809c247b85a9b807c47848490572316f29e70107",
        "id": 630511
    },
    {
        "content": "def get_local_address(address):\n\t\"\"\"\n\tReturn the local address of a rom address.\n\t\"\"\"\n\tbank = address / 0x4000\n\taddress &= 0x3fff\n\tif bank:\n\t\treturn address + 0x4000\n\treturn address",
        "sha1": "380e07e67a4bbbf7896c44374c50330dc10825dc",
        "id": 240966
    },
    {
        "content": "import requests\nfrom bs4 import BeautifulSoup\n\n\ndef visit_homepage(url):\n    \"\"\"Requests a given url and returns a BeautifulSoup object.\n\n    Parameters\n    ----------\n    url : str\n\n    Returns\n    -------\n    BeautifulSoup object\n    \"\"\"\n    response = requests.get(url, timeout=10)\n    soup = BeautifulSoup(response.content, 'html.parser')\n    return soup",
        "sha1": "4b89b6facbea2431112354601f607659beee464d",
        "id": 250289
    },
    {
        "content": "def calc_deltas(numbers) -> list:\n    \"\"\"Calculate the delta between each point and the next.\"\"\"\n    deltas = list()\n    for n in range(len(numbers) - 1):\n        d = numbers[n + 1] - numbers[n]\n        deltas.append(d)\n    return deltas",
        "sha1": "b3ef22ab1a1623d854fcb2263f46ad0417d9cb40",
        "id": 67672
    },
    {
        "content": "def pluralize(word, count):\n    \"\"\"\n    Given a word and a count, return the pluralized version of the word.\n    >>> pluralize('cat', 1)\n    'cat'\n    >>> pluralize('cat', 2)\n    'cats'\n    \"\"\"\n    if count == 1:\n        return word\n    else:\n        return word + \"s\"",
        "sha1": "7ce845083d4ba1465931221861c5f04d71dd5ff5",
        "id": 369875
    },
    {
        "content": "def _relabelAxes(plot, text):\n    \"\"\"Relabel {xlabel} and {ylabel} from this text using the corresponding\n    plot axis label. If the axis label is empty, label it with \"X\" and \"Y\".\n\n    :rtype: str\n    \"\"\"\n    xLabel = plot.getXAxis().getLabel()\n    if not xLabel:\n        xLabel = \"X\"\n    yLabel = plot.getYAxis().getLabel()\n    if not yLabel:\n        yLabel = \"Y\"\n    return text.format(xlabel=xLabel, ylabel=yLabel)",
        "sha1": "357a2234e3615bac08f6e23f0321a60476e80dc4",
        "id": 501218
    },
    {
        "content": "def normalize_dogc_title(title):\n    \"\"\" Normalize a DOGC title in order to capitalize it \"\"\"\n    normalized_title = title.replace(\"CORRECCI\u00d3 D'ERRADES\", \"Correcci\u00f3 d'errades\").replace(\"DECRET\", \"Decret\")\\\n                       .replace(\"EDICTE\", \"Edicte\").replace(\"LLEI\", \"Llei\").replace(\"ORDRE\", \"Ordre\")\\\n                       .replace(\"RESOLUCI\u00d3\", \"Resoluci\u00f3\")\n    normalized_title = f'{normalized_title}\\n'\n\n    return normalized_title",
        "sha1": "0a6497f6699f1a16231cac70715a94d2097d2c9a",
        "id": 262406
    },
    {
        "content": "import importlib\n\n\ndef get_version_name(problem, version):\n    \"\"\"Read the version file and return its name.\n\n    This function does NOT handle exceptions - the version and problem\n    must be implemented when you call this function.\n\n    Parameters:\n        problem     The problem associated with the version\n        version     The version whose name to return\n\n    Return:\n        The name of the specified version.\n    \"\"\"\n    import_path = \"solutions.{p}.{v}\".format(p=problem, v=version)\n    imported = importlib.import_module(import_path)\n    return getattr(imported, \"VERSION_NAME\")",
        "sha1": "d8a8c362ac5d1e521d1d5ab2def8eddec53ea92e",
        "id": 461143
    },
    {
        "content": "def c2str(rgb):\n    \"\"\" Represent a color tuple as a string\n\n    Args:\n        rgb (:obj:`tuple` of int): Color representation as (R, G, B), where\n            each color is an integer between 0-255.\n\n    Returns:\n        string: Comma separated string with values for each channel\n\n    \"\"\"\n    return '%d,%d,%d' % rgb",
        "sha1": "fdbfc1cb3baef7f520f0735f92a58bc889309654",
        "id": 321415
    },
    {
        "content": "def has_self_cycles(graph):\n    \"\"\"\n    Determine if nodes in a graph contain self-cycles.\n\n    Args:\n        graph (nx.Graph): graph to test.\n    Returns:\n        (boolean): whether any node in the graph has an edge connecting to\n            itself.\n    \"\"\"\n    edges = graph.edges()\n    for node in graph.nodes():\n        if (node, node) in edges:\n            return True\n    return False",
        "sha1": "525846cd4cee987ce68c0972ad7867f9ad72c001",
        "id": 398210
    },
    {
        "content": "def createc_fbz(stm):\n    \"\"\"   \n    Function returning Createc channel feedback z value\n\n    Parameters\n    ----------\n    stm : createc.CreatecWin32\n        Createc instance\n    Returns\n    -------\n    value : str\n\n    \"\"\"\n    # from createc.Createc_pyCOM import CreatecWin32\n    # stm = CreatecWin32()\n    return stm.client.getdacvalfb()",
        "sha1": "affda33fd1050fdf865544cfc66e3899788fccc2",
        "id": 27922
    },
    {
        "content": "def pages_to_show(paginator, page, page_labels={}):\n    \"\"\"Generate a dictionary of pages to show around the current page. Show\n    3 numbers on either side of the specified page, or more if close to end or\n    beginning of available pages.\n\n    :param paginator: paginator object, populated with objects\n    :param page: number of the current page\n    :param page_labels: optional dictionary of page labels, keyed on page number\n    :rtype: dictionary\n    \"\"\"\n    show_pages = {}\n    if page != 1:\n        before = 3      # default number of pages to show before the current page\n        if page >= (paginator.num_pages - 3):   # current page is within 3 of end\n            # increase number to show before current page based on distance to end\n            before += (3 - (paginator.num_pages - page))\n        for i in range(before, 0, -1):    # add pages from before away up to current page\n            if (page - i) >= 1:\n                # if there is a page label available, use that as dictionary value\n                show_pages[page - i] = page_labels[page - i ] if (page - 1) in page_labels else None\n    # show up to 3 to 7 numbers after the current number, depending on how many we already have\n    for i in range(7 - len(show_pages)):\n        if (page + i) <= paginator.num_pages:\n            # if there is a page label available, use that as dictionary value\n            show_pages[page + i] = page_labels[page + i] if (page + i) in page_labels else None\n\n    return show_pages",
        "sha1": "e61b4c93e17b1c1860163c67d167224496f3c8aa",
        "id": 156352
    },
    {
        "content": "def at_midnight(dt):\n    \"\"\"\n    Given a datetime, return a new datetime at midnight on the\n    same day.\n    \"\"\"\n    return dt.replace(hour=0, minute=0, second=0, microsecond=0)",
        "sha1": "2d62c001ca2d158598e3b82a6e4073677790539a",
        "id": 413348
    },
    {
        "content": "from functools import reduce\n\n\ndef urljoin(*args: str):\n    \"\"\"Gets arguments to join as url\n\n    Args:\n        *args: args to join\n\n    Returns:\n        Joined url\n\n    Examples:\n        >>> urljoin('https://www.example.com', 'suffix/', '/suffix2', 'suffix', 'file.json')\n        'https://www.example.com/suffix/suffix2/suffix/file.json'\n    \"\"\"\n    return reduce(lambda a, b: str(a).rstrip('/') + '/' + str(b).lstrip('/'), args).rstrip(\"/\")",
        "sha1": "34a579b44882e2a4206ff053316ebecb08df030d",
        "id": 591588
    },
    {
        "content": "def get_url(ticker, interval):\n    \"\"\"Sets the URL\n    Args:\n        ticker (str): Ticker Symbol\n        interval (str): Time interval\n    Returns:\n        str: The final URL for the API\n    \"\"\"\n\n    # Get the final API url\n    alpha_api = 'O1C7ECLZQODUYN6D'\n    url = 'https://www.alphavantage.co/query?function=TIME_SERIES_INTRADAY'\n    outputsize = 'full'\n    return url + '&symbol=' + ticker + '&outputsize=' + outputsize + \\\n        '&interval=' + interval + '&apikey=' + alpha_api",
        "sha1": "861044ee2922cf1420639e5ada1e73583d5b1539",
        "id": 114314
    },
    {
        "content": "def flip(image, **kwargs):\n    \"\"\"\n    Flips an image. Expects \"horizontal\" or \"vertical\" as argument.\n    \"\"\"\n    image.flip(**kwargs)\n    return image",
        "sha1": "4894d59acb5f60af480b844e9ae6929d55ba5a8a",
        "id": 154754
    },
    {
        "content": "import torch\n\n\ndef entropy(x):\n    \"\"\"Calculate entropy of a pre-softmax logit Tensor\"\"\"\n    exp_x = torch.exp(x)\n    A = torch.sum(exp_x, dim=1)  # sum of exp(x_i)\n    B = torch.sum(x * exp_x, dim=1)  # sum of x_i * exp(x_i)\n    return torch.log(A) - B / A",
        "sha1": "edf2c98dc3784b7840a794e1f7d5c01a43e8a35a",
        "id": 413988
    },
    {
        "content": "from pathlib import Path\nfrom typing import Tuple\nfrom typing import Set\n\n\ndef packages_find(workspace_path: Path, tracing: str = \"\") -> Tuple[Path, ...]:\n    \"\"\"Find the various packages within a ROS2 workspace.\"\"\"\n    # next_tracing: str = tracing + \" \" if tracing else \"\"\n    if tracing:\n        print(f\"{tracing}=>packages_find({workspace_path})\")\n    src_path: Path = workspace_path / \"src\"\n    assert src_path.is_dir(), f\"{src_path} is not a directory\"\n\n    all_packages: Set[Path] = set()\n    package_path: Path\n    for package_path in src_path.glob(\"*/package.xml\"):\n        all_packages.add(package_path.parent)\n    for package_path in src_path.glob(\"*/CMakeLists.txt\"):\n        all_packages.add(package_path.parent)\n    final_packages: Tuple[Path, ...] = tuple(sorted(all_packages))\n\n    if tracing:\n        print(f\"{tracing}<=packages_find({workspace_path})=>{final_packages}\")\n    return tuple(final_packages)",
        "sha1": "6f59433ab0cf60e843a197b50c95a54853cc3061",
        "id": 69340
    }
]